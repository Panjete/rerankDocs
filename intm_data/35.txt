a scientometric overview of cord-19 giovanni colavizza rodrigo costas vincent traag a nees jan van eck thed van leeuwen ludo waltman   the covid-19 pandemic is attracting the attention of the global scientific community medical research on the virus and on the management of the crisis from an epidemiological and healthcare point of view has full priority furthermore many research communities funding agencies and third-parties are taking action to support the fight against the pandemic with their own expertise and resources multidisciplinary open collaboration will prove instrumental to fight the current and future pandemics 6 27  several initiatives have been taken to share covid-19-related scientific research as openly as possible from public private and non-profit organizations it has been readily recognized that health crises are also information crises 32 10 25 17  for example the world health organization maintains a list of relevant research updated daily 33  as well as a portal to provide information to the public 3  similar to the the european commission 4  publishers are opening access to relevant publications and some open-access publishers have lifted publishing fees for the same reason another initiative is the release of the covid-19 open research dataset cord-19 2  cord-19 is a growing weekly-updated dataset of covid-19 publications capturing new as well as past research on covid-19 and the coronavirus family of viruses for use by the global research community 1 the reason to release this dataset is to mobilize researchers to apply recent advances in natural language processing to generate new insights in support of the fight against this infectious disease the initiative has the backing of the us white house 1 and is a partnership of several institutions including the chan zuckerberg initiative georgetown universitys center for security and emerging technology microsoft research the national library of medicine of the national institutes of health and unpaywall cord-19 is released together with a set of challenges hosted by kaggle mainly focused on automatically extracting structured and actionable information from such a large set of publications the release of this dataset is a positive call for action directed towards the natural language processing machine learning and related research communities this call has been taken up for example the acl conference has announced an emergency nlp covid-19 workshop which mentions the cord-19 dataset on its call for papers 2 and a trec-covid challenge has been announced on cord-19 3  in order to contribute to an informed use of the cord-19 dataset we here present a scientometric analysis of its contents in particular we raise some questions on the subject matter delineation of cord-19 field or subject delineation is a complex task which usually benefits from an interplay of information retrieval or search-based approaches and bibliometric mapping 16 34 18  we compare the coverage of cord-19 to the web of science and find that cord-19 on the one hand contains more than just research on covid-19 and coronaviruses while on the other hand it might be missing some content we then provide an overview of the research it contains enriching the cord-19 dataset with data from dimensions 15 and using both citation analysis and text analysis our results show that the cord-19 corpus broadly focuses on biomedical research on viruses and related health issues and the articles it contains are quite heterogeneous cord-19 contains a core of research directly on covid-19 and coronaviruses but in addition it contains many articles on related yet distinct streams of virus research such as on influenza molecular biology and public health secondly we discover three periods in the accumulation of literature in this corpus a pre-sars 2003 period a post-sars period and the current pandemic 2020 we also present a brief analysis of altmetric 24 21 data related to the papers in the corpus we find that recent research 2020 is disproportionately represented in social media and news by altmetric indicators especially on twitter we conclude by suggesting a critical stance when using cord-19 and by proposing some directions for future work to facilitate the analysis and use of the cord-19 corpus we release our code see si in combination with valid access to dimensions altmetric the web of science and twitter our results can be replicated finally we underline that we are not domain experts we invite experts to improve the interpretation of our results and rectify it wherever necessary the cord-19 publication dataset contains over 47 000 articles of which 36 000 are equipped with full text april 4 2020 the dataset collects publications from the following sources  pubmeds pmc open access corpus via the query covid-19 or coronavirus or corona virus or 2019-ncov or sars-cov or mers-cov or severe acute respiratory syndrome or middle east respiratory syndrome  covid-19 research articles from the who 5   biorxiv and medrxiv pre-prints using the same query as in pmc the relatively low number of publications prior to the sars 2003 outbreak is followed by a steady increase in the number of publications up to 2020 when the growth accelerated even more substantially figure 2  the top 20 journals by number of publications are given in figure 1  highlighting how the corpus is composed of publications from varied sources the journal of virology and plos one stand out in particular the main contributors to the dataset are pmc and elsevier figure 12b  which have made a large part of their relevant literature openly available the availability of full texts is relatively high even if not yet complete and is proportionally stable over time figure 12a  this is because even though most publishers and journals have opened up their publications some have not yet done so in particular the journal of virology top by number of papers in  the journal of clinical microbiology and the journal of biological chemistry provide almost no article in full text to this date using dimensions data we study the number of citations to papers in the corpus  figure 13  we can observe how the literature in this corpus gained more attention from the early 2000 and how some publications from 2020 are already accumulating citations at an extreme rate the most cited articles include epidemiology and causes of preterm birth by goldenberg et al 4 except for about 10 000 articles without any classification we find that as expected the medical and health sciences cover most ground followed by biological sciences figure 14  the second level for classification shows the presence of several sub-areas of these two top level fields medical microbiology being the largest more details and plots are provided in the accompanying repository the search terms used to find cord-19 publications were used to perform a search within the cwts in-house version of the web of science wos 7  5 we used the following wos citation indices science citation index expanded social sciences citation index arts  humanities citation index and conference proceedings citation index other wos citation indices were not used because we do not have access to them we performed the search in the titles abstracts and author keywords of the publications in wos abstracts and keywords are available from 1991 onward cord-19 and wos coverage in publications can then be compared using their doi or pubmed identifier pmid in figure 3 we show the trends in research output on covid-19-related diseases the green line represents the cord-19 dataset while this dataset goes back to 1951 for reasons of clarity we only show trends from 1980 to 2019 n  42 496 excluding an additional 4 095 publications from 2020 and 403 from before 1980 the red line represents the set of publications collected from wos using the procedure we just described n  6 958 from now on we refer to this search procedure as strict finally the blue line shows all publications from cord-19 also available in wos n  33 970 including those in the red line some differences between the coverage of cord-19 and wos are in themselves not surprising cord-19 relies on pubmed and pre-print repositories and has a mono-disciplinary focus including more recent research while wos is a multidisciplinary service however as we further detail in figure 4  within the cord-19 publications found in wos we can distinguish between the set of 6 958 papers that carry cord-19 search terms in their titles abstracts and keywords in red in figures 3 and 4  and a set of papers which do not n  33 970  6 958  27 012 in purple in figure 4  not strict if we assume that authors use the titles abstracts and keywords of their publications to indicate the main focus of their research by explicitly using terms such as corona virus sars-cov mers-cov or similar many cord-19 publications do not clearly focus on covid-19 and coronaviruses presumably many such publications are more indirectly related to core covid-19 and coronavirus research we observe an increase of publications on covid-19-related research in the cord-19 dataset which is mirrored in wos  figure 3  this trend starts in figure 5  we consider publications that are linked to wos from the cord-19 dataset the red and purple lines identify cord-19 publications from wos with a strict red and not strict purple search procedure respectively the other two lines represent the set of all publications that are collected from wos by using cord-19 search terms in the titles abstracts and keywords of publications the dark green line represents papers that are in wos and not in the cord-19 dataset but carry the relevant terms in their titles abstracts and keywords n  4 830 of these publications which are in wos and should have been in cord-19 according to search criteria 3 863 have at least a doi or a pmid all wos publications matching the cord-19 search criteria in their titles abstracts and keywords are represented by the pink line n  6 958  4 830  11 788 from this analysis we can draw two preliminary conclusions firstly only part of the the cord-19 dataset concerns publications explicitly related to coronavirus research while other papers are more indirectly related secondly we identified a set of covid-19 and coronavirus publications in wos n  4 830 that are likely not included in cord-19 these considerations highlight the need to further qualify the delineation of covid-19 and coronavirus research that is embedded in cord-19 we conducted the following analyses on the cord-19 dataset a term map and a topic modelling analysis using cord-19 titles and abstracts a citation network analysis using dimensions citation data an altmetrics analysis using altmetric data the purpose of the former two analyses is to cluster cord-19 publications and further clarify the contents of the dataset for our analyses we focus on 46 994 articles out of the 47 351 available in cord-19 filtering out 53 articles without any known identifier doi pmid or pmcid and a further 304 due to duplicates to get an accessible high-level overview of the cord-19 dataset we used vosviewer 28 to create a so-called term map of the publications in this dataset the titles and abstracts were provided as input to vosviewer by concatenating them into a single string using text mining algorithms of vosviewer we included 1 923 most relevant terms of the titles and abstracts a term is defined as a sequence of nouns and adjectives ending with a noun only terms occurring in at least 55 publications were considered plural terms were converted to singular for each pair of terms vosviewer counted the number of publications in which the terms both occur in the title or abstract in this way a co-occurrence network was obtained indicating for each pair of terms the number of publications in which they occur together this co-occurrence network was visualized in a term map in figure 6  the map shows the 1 923 terms included in the network the size of a term reflects the number of publications in which the term occurs the proximity of two terms in the map approximately indicates the relatedness of the terms based on their number of co-occurrences in general the closer two terms are located to each other the stronger they are related this means that groups of terms located closely together in the map usually can be interpreted as topics the horizontal and vertical axes have no special meaning in figure 6  labels are shown only for a selection of the terms in order to avoid overlapping labels for many of the less prominent terms no label is shown the term map can also be explored interactively online httpsbitly3aijpv3 including all the overlays used in this article and more the labels of the less prominent terms can then be made visible by zooming in on specific areas in the map colors are used to present additional information in the term map in figure 6  for instance the color of a term reflects the average publication year of the publications in which the term occurs in this way temporal trends can be made visible the term map shows a clear divide between biologically focused topics in the left area in the map and clinically focused topics and health research in the right area for similar findings see 29  as shown by the colors of the terms the topics in the right area in the map received a lot of attention in more recent years while the topics in the left area received attention mostly in earlier years in the next sections we use this term map to illustrate some of  we conducted a topic modelling analysis making use of the titles and abstracts available from the cord-19 metadata by concatenating them into a single string similarly to what we did for the term map of the 46 994 articles 8 349 have no abstract we then applied a pre-processing pipeline using scispacys en core sci md model 20 to convert each document into a bag-of-words representation which includes the following steps entity detection and inclusion in the bag-of-words for entities strictly longer than one token lemmatisation removal of isolated punctuation and stopwords inclusion of frequent bigrams we started by training a latent dirichlet allocation lda model 9  using gensims implementation 22 and 15 topics from a topic coherence analysis 19  we found 15 to 25 to be a good value for the number of topics we decided to remain on the lower end to facilitate the interpretation of results we further filtered out tokens which appear fewer than 10 times or in more than half of the documents lastly we verified our results using a correlated topic model 8  6 which reduces to a structural topic model without covariates 23  this model explicitly captures topic correlations and confirms the topic structure we found with lda as well as the topics temporal unfolding more details are provided in the accompanying repository we report results of the lda model in what follows the top 20 words per topic are given in the si in figure 15  we show the yearly topic intensity from 1980 to 2020 included three periods in the accumulation of literature seem to emerge pre-sars research mainly focused on immunology topic 1 and molecular biology work on viruses topics 2 13 rotaviruses topic 5 and the genetics of viruses topic 8 coronavirus-specific research is mainly covered in three topics one related to sars topic 6 one related to mers topic 9  and one related to the clinical care of coronavirus patients and the more recent covid-19 topic 10 see figure 16  lastly another prominent topic started around 2003 and is related to the management of epidemic outbreaks topic 0 presently in 2020 new research is dominated by topics 0 and 10 hence epidemics outbreak management and covid-19 especially in view of dedicated patients clinical care from this first analysis we observe how coronavirus research seems to be produced in bursts following outbreaks instead of following a more steady progress most of the recent and ongoing research at the moment appears to focus on the management of the current crisis in order to discuss topics in cord-19 at a higher level of granularity we grouped the identified topics into macrotopics all related to research on viruses and their effects as follows  coronavirus topics 6 9 10  public health and epidemics topic 0  molecular biology topics 2 8 13  influenza topics 4 11 14  immunology topic 1  rotavirus topic 5 7  antivirals topic 12  clinical trials topic 3  testing and diagnosing topic 7 the cord-19 corpus is dominated by literature on coronaviruses public health and epidemics molecular biology and to a lesser degree influenza and immunology five macrotopics above we plot the relative yearly mean and absolute yearly sum macrotopic intensities in figure 7  we see from these plots that the periodization of the cord-19 dataset discussed above is largely confirmed the 2003 sars outbreak generated a shift associated with a rise in publications on coronaviruses and on the management of epidemics molecular biology work on viruses remains dominant over time lastly the ongoing covid-19 pandemic is generating a high proportion of publications primarily on the topics of coronaviruses and the management of epidemics as is to be expected  the previous two analyses were based on text we now turn to a view based on citations to characterized the cord-19 dataset we constructed a citation network based on references of all papers included in the cord-19 dataset as provided by dimensions we not only included references to papers part of cord-19 but also all other references in this directed citation network these external references provide additional information regarding the knowledge structure of the cord-19 dataset for example two papers may not be immediately connected via papers in the cord-19 dataset but they may have common external references we only use the giant weakly connected component which amounts to 903 607 nodes and 1 985 767 edges of which 38 718 nodes belong to the cord-19 dataset this is the citation network that we work with in the remainder of this section we cluster the citation network using the leiden algorithm 8 26  each citation link is weighted as 1k i  where k i is the number of references of publication i the inclusion of the external references in the context of clustering is also known as extended direct citation 31  in this approach the publications contained in cord-19 are weighted with a so-called node weight of 1 while the external publications are weighted with a weight of 0 see 31 for more de-tails we cluster the citation network in a hierarchical fashion the lowest cluster level which is most detailed is clustered using a resolution of 2  10 5  we then aggregate the citation network according to this clustering and then cluster the resulting clustered citation network at a resolution of 1  10 5  we refer to the former clustering solution as the lowest level and to the latter as the highest level the external publications help to cluster cord-19 publications from cord-19 more accurately in what follows we only focus on 38 718 publications from cord-19 the clustering at the lowest level has 9 clusters that have more than 1 000 cord-19 publications each as shown in figure 17b  in total these 9 clusters cover more than 84 of the cord-19 publications there are 14 clusters that have fewer than 1 000 publications but more than 100 covering an additional 11 of the cord-19 publications the remaining 5 of publications are scattered across 779 small clusters of just a few publications of which 651 clusters consist of only a single publication the largest cluster consisting of 7 300 publications seems to cover publications dealing with various coronaviruses and contains a high proportion of molecular biology analyses as well as publications dealing with epidemic outbreaks figure 8a  the second largest cluster consisting of 6 176 publications has a clear biochemical focus dealing with various proteins transcriptions and pathways figure 8b  the third largest cluster consisting of 4 629 publications covers clinical studies revolving around patient care and respiratory conditions figure 8c  the fourth largest cluster consisting of 3 764 publications deals with both clinical and epidemiological studies figure 8d  this interpretation is largely substantiated by comparing the clustering results to the topic model these relatively detailed clusters are hierarchically clustered at a higher level the largest cluster at the highest level consists mainly of a combination of the first fourth and fifth largest clusters at the lowest level and in total it contains 14 325 publications this cluster focuses mostly on clinical and epidemiological studies and outbreak management figure 9a  the second largest cluster at the highest level consists mainly of the second sixth and ninth largest clusters at the lowest level and in total it contains 10 913 publications with a clear molecular biology focus figure 9b  these two clusters cover 65 of all publications we conclude this section by using topic intensity to qualify the results of the citation network clustering in particular clusters can be characterized by the average topic intensity of the publications they contain we start from the highest clustering level and focus on the top two clusters by size we then use macrotopics average intensity to characterize them  figure 18  and confirm that the first cluster contains publications focused on all coronaviruses including biomedical and public health research the second cluster instead has a clear molecular biology and immunology focus we then consider the four largest clusters in the lowest clustering level figure 19  while the second cluster is again focused on molecular biology and immunology the first and fourth specialize the first cluster contains publications on coronaviruses and their molecular biology while the fourth contains publications on coronavirus outbreaks and their public healthepidemiology impact lastly the third cluster focuses on influenza this citation based analysis corroborates our earlier findings based on text analyses both point to the existence of distinct research areas within cord-19 coronaviruses molecular biology research on viruses public health and epidemics other viruses such as influenza and other related topics immunology diagnosing trials and testing these areas of research are interrelated yet also contain specialized information and accumulated at different rate over time the 46 996 cord-19 publications have also been explored using altmetric data with the aim of describing their reception on social media paying special attention to the dissemination of the publications across various social media sources as can be seen in table 1  a total of 28 146 publications in the cord-19 dataset 60 have received some mention in altmetric this is a rather high coverage of publications compared to previous studies 14  that reported an overall coverage of 215 of 2012 publications on twitter and about 31 for publications in the biomedical and health sciences this high coverage is even higher when the focus is on the most recent publications ie those published in the early months of 2020 of which over 80 have received some social media mentioning covered by altmetric table 2 presents a more detailed description of the type of social media events around cord-19 publications we selected some of the most relevant sources covered by altmetric namely twitter blogs recommendations in f1000prime news media mentions citations in policy documents and citations in wikipedia entries clearly the most important source is twitter which accounts for over 95 of all the social media interactions analysed the second most important source are news mentions 36 and blog citations 06 the observation that news media mentions outperform blogs contrasts with previous studies 14  this may signal the particular relevance of publications in the cord-19 dataset for mainstream news media another additional characteristic is the recency of the publications being mentioned particularly on twitter and in news media since about 75 of all mentions of publications relate to publications from 2020 while 57 of all mentions in mainstream news media also relate to 2020 publications a trend analysis of both publications and tweets  figure 10  confirms this recency in the uptake of cord-19 publications the vast majority of tweets referring to cord-19 publications concentrate around those published in 2020 which in practical terms indicates that the social media activity is mostly focused on the ongoing covid-19 crisis figure 11 presents term maps showing the altmetrics reception of cord-19 publications we cover the most immediate social media sources or fast sources 12  which provide the earliest signals of the reception of publications twitter blogs and news all present a similar pattern with a strong orientation towards the most recent covid-19 publications well-captured by the fourth largest cluster of the lowest-level citation network clustering see figures 8d  and 19d  this preliminary altmetrics analysis shows a strong present-day attention for research covered by the cord-19 dataset the majority of the tweets blogs and news mentions are focused on research produced during the current covid-19 pandemic this demonstrates the important role of social media especially twitter in discussing covid-19 research during a global pandemic like covid-19 research is subject to high degrees of uncertainty 11 30  rapidly increasing levels of social media activity around topics on which there is little academic consensus may increase the risk of scientific advise being misun- derstood or misused by substantial segments of society social media analysis can provide tools for identifying and characterizing areas with high levels of social media activity that may be in dissonance with the academic discourse in future work we plan to find and characterize these areas of discrepancy in order to inform scientists science communicators journalists and the public at large we analysed the cord-19 dataset of publications on covid-19 and coronavirus research 2  comparing the cord-19 delineation with a wos-based delineation suggested that on the one hand cord-19 is broader than just covid-19 and coronavirus research while on the other hand it may also miss some literature that is about covid-19 and coronaviruses we carried out a deeper analyses of cord-19 in various ways we created a map of relevant terms extracted from the titles and abstracts of covid-19 publications this map confirmed the broad content of the dataset a topic modelling analysis showed that cord-19 publications are related more broadly to medical research on viruses of which covid-19 and coronaviruses are part dominant topics in cord-19 include research on public health and epidemics molecular biology coronaviruses influenza and other families of viruses immunology and antivirals methodology testing diagnosing trials furthermore the topic intensity over time is far from uniform showing in particular that coronavirus research has followed known outbreaks sars mers and that until 2020 this research represented only a small portion of cord-19 we performed a citation network clustering analysis using data from dimensions citation network clusters highlight the relative cohesiveness of cord-19 in line with the textual analyses the clusters confirm the broad coverage of the dataset overall there seem to be two prominent citation clusters one that covers research on specific coronaviruses with a public health and epidemiological focus and another one with a molecular biology focus molecular biology research on viruses is in general a very prominent component of lastly we considered altmetric data in order to gauge how much attention cord-19 research attracted over time the current covid-19 outbreak dominates attention from social media in particular from twitter highlighting the public interest for scientific results during this pandemic far from constituting a critique of cord-19 our work acknowledges that research on viruses and coronaviruses specifically does not exist in a vacuum delimiting research on a certain subject matter requires difficult choices that inevitably involve a certain degree of arbitrariness we praise the breadth and relative coherence of cord-19 this dataset rightly merits attention and is useful to allow many researchers to engage with the topic nevertheless we also suggest that critical awareness is required when using cord-19 as our results demonstrate that its contents cover a broad set of topics different subsets of cord-19 should be used for specific purposes for example for making historical analyses on funding of specific research topics or for automatically extracting structured information we exemplified some approaches to segment cord-19 in various ways in addition we also hinted at possible missing content in cord-19 some missing content is by design as cord-19 relies on pubmed biorxiv and medrxiv and does not consider for example notable pre-print servers such as arxiv chemrxiv jmir preprints ssrn electronic journal research square and psyarxiv some missing content is less clearly motivated as the use of the same search criteria in wos allowed us to surface published research not currently included in cord-19 this is an issue that deserves deeper scrutiny clearly there are many areas for future covid-19 work by the scientometric community we conclude by suggesting three areas in particular firstly there seems to be a need for a comprehensive mapping of covid-19-related research a multidisciplinary map of covid-19-related research considering diverse disciplinary perspectives and information needs will be useful to surface relevant research also outside the biomedical domain secondly cord-19 provides a virtuous example of open data sharing the scientometric community can contribute by creating and maintaining additional datasets on covid-19 research thirdly as shown by our results there is a lot of social media attention for covid-19 research indeed the role of information and especially reliable scientific information has been central to the unfolding of the current pandemic 33  consequently a relevant area for future work is to better understand the mechanics of online scientific information diffusion using altmetrics and other data sources this line of work has the potential to provide valuable information to experts and governments during the current and future pandemics most of our analysis can be replicated using code and following the instructions given in the accompanying repository httpsgithubcomcwtsleidencwts covid please note this repository is ongoing work things might not be perfect we welcome contributions and suggestions ideally by opening an issue or doing a pull request analyses based on altmetric dimensions twitter and web of science data require access to these services top 20 words per topic for the lda model we filter out words shorter than 3 characters compare with figure 15 for the topic intensity over time  topic 0 health disease public datum model outbreak public health infectious epidemic system research control infectious disease study country review health number provide method  topic 1 cell mouse infection response immune expression disease induce role viral gene increase type level study result cytokine activation lung 1 9 8 0 1 9 8 1 1 9 8 2 1 9 8 3 1 9 8 4 1 9 8 5 1 9 8 6 1 9 8 7 1 9 8 8 1 9 8 9 1 9 9 0 1 9 9 1 1 9 9 2 1 9 9 3 1 9 9 4 1 9 9 5 1 9 9 6 1 9 9 7 1 9 9 8 1 9   advertisers jump on coronavirus bandwagon politics news and business yelena mejova kyriaki kalimeri  in the age of social media disasters and epidemics usher not only a devastation and affliction in the physical world but also prompt a deluge of information opinions prognoses and advice to billions of internet users the coronavirus epidemic of 2019-2020 or covid-19 is no exception with the world health organization warning of a possible infodemic of fake news in this study we examine the alternative narratives around the coronavirus outbreak through advertisements promoted on facebook the largest social media platform in the us using the new facebook ads library we discover advertisers from public health and non-profit sectors alongside those from news media politics and business incorporating coronavirus into their messaging and agenda we find the virus used in political attacks donation solicitations business promotion stock market advice and animal rights campaigning among these we find several instances of possible misinformation ranging from bioweapons conspiracy theories to unverifiable claims by politicians as we make the dataset available to the community we hope the advertising domain will become an important part of quality control for public health communication and public discourse in general  the coronavirus disease covid-19 started in december 2019 in wuhan the capital of hubei china at the time of the data collection for this paper on february 20 2020 over 72 thousand cases have been recorded in china including over 1870 deaths and around 700 people mostly travellers were diagnosed in the rest of the world 48  although the number of cases detected outside china remains much smaller than inside the worlds media and public attention remain focused on the ongoing developments prompted by a whirlwind of mainstream and social media coverage the world health organization who has warned of a possible infodemicincorrect or malicious information being spread quickly and to a wide audience 1  while major social media platforms have 1 httpswwwbbccomnewstechnology-51497800 permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for third-party components of this work must be honored for all other uses contact the ownerauthors  2020 copyright held by the ownerauthors doi tbd pledged to use their networks third-party fact-checkers to ensure quality of information available to the public 2  adapted to the agendas of various actors the narrative around the epidemic is largely fragmented such fragmentation competes and sometimes contradicts the fact-based educational messaging of the public health organizations despite the best efforts of fact-checking organizations the coronavirus outbreak has been taken up by social and political movements as an opportunity to communicate their messages outside china xenophobic attacks have increased against businesses and individuals of asian origin prompting public statements from political leaders condemning racism 3  further unscrupulous parties are taking to the social media platforms to promote dubious claims of remedies and cures 4  while politicians are propagating conspiracy theories about the origins of the epidemic 5  adapted to the agendas of various actors the narrative around the epidemic is largely fragmented such fragmentation competes and sometimes contradicts the fact-based educational messaging of the public health organizations in this work we examine a novel dataset of advertisements posted to facebook at the time of the epidemics beginning in order to answer the following questions  how is facebook advertisement used as a platform for conversation around coronavirus  what alternative narratives are present around coronavirus outside public health messaging is this messaging emotionally provoking  finally do the alternative narratives contain possible misinformation the resource we use is the facebook ads library which was launched by facebook in march 2019 6 as a way for the public to learn more about ads related to politics or issues that have run on facebook or instagram accessible through facebook ads library api 7  it provides the title and text of the advertisement the id of the facebook page and funding entity alongside these basic descriptors it shows a demographic distribution of users reached by the ad in terms of age groups and genders as well as a range of funds spent on the campaign the data is available for the us eu countries and a handful of others including brazil israel and ukraine we find a variety of advertisers invoking the epidemic from public health and non-profit organizations updating their audience on the latest news and soliciting donations to political and business entities coopting the threat of epidemic to their messaging and profit coinciding with the democratic primary season in the us the virus is often mentioned in association with political figures -both in support and in opposition despite facebooks near-ubiquitous reach across the us these ads often target very specific demographics and locales especially favoring california new york and texas furthermore we find a range of possible erroneous information within these ads ranging from conspiracy theories about bioweapons to milder claims of political mismanagement and misunderstanding however as the situation develops more will be known about the veracity of some of the content we discovered thus we make the full dataset along with the manual labels available to the research community in accordance with the terms of service of facebook findings of this study have wide implications for public health messaging as we discover there is a strong competition for the audience and the framing around the epidemic on the side of politics and news potentially supplanting or contradicting the messages public health organizations may promote employing the epidemic as a tool for political attacks may distract the audience from more useful information and encourage anxiety we hope this case study spurs more research into a holistic analysis of public perception of health crises and encourages collaboration between social media platforms and public health organizations advertising is the lifeblood of the majority of internet giants such as google and facebook -two companies that accounted the for nearly 20 of global advertising spending in 2016 8  marketing research shows that consumers have a positive attitude toward social media advertising 5  though moderated by the relevance of the ad and existing perception of the company 2  as well as informativeness and creativity 29  among the most popular social media platforms advertising on facebook has been shown to evoke engagement and social sharing 46  having a potential reach of 250 billion monthly active users mau as of december 2019 9  it is not surprising that facebook and its other property instagram has become a destination for businesses non-profits and politicians recently the 7 httpswwwfacebookcomadslibraryapi 8 httpswwwtheguardiancommedia2017may02 google-and-facebook-bring-in-one-fifth-of-global-ad-revenue 9 httpszephoriacomtop-15-valuable-facebook-statistics company has come under criticism for allowing political advertising of questionable quality on its platform 10  in response the platform has published an ad library accessible to the researchers and watchdogs to monitor advertising related to social issues after initial reports of bugs in the api provided by facebook to access the information in bulk 39  the library has become a valuable resource for watchdogs of political communication 17 31  however it has not yet been used in a systematic study of the relationship between such public messaging and public health attitudes beyond advertising in particular social media in general has been a popular venue for public health campaigning spanning efforts in smoking cessation 15  organ donor registration 8  and sexual health promotion 7  although it is not clear that increased online engagement results in desired health behaviors research in commercial sphere shows a relationship between engagement with facebook and sales 6 26  to encourage engagement public health campaigns strive for an engaging experience as the campaigns with a clear call to action have an opportunity to quantify the impact of the message such as signing up for future contacts or encourage the community to propagate the message in best scenario having it go viral ie very popular 18  to encourage wider sharing campaigns may personalize the messages to individuals or demographic groups 35 or use highly engaged seed users who promote the content in their immediate social network 44 22  however unlike in traditional advertising sponsorships partnerships and use of persons of authority may hurt engagement whereas partnering with celebrities and sportspeople results in increased likes and shares 24  this trend can be attributed to a variety of reasons perceived credibility associated with success social acceptance and confirmation bias all provide positive reinforcement for people to follow health advice of celebrities possibly propagating harmful behaviors and beliefs 23  public perception of health issues has long been entangled with the opinions expressed by celebrities and politicians historically celebrity health narratives resulted in a coconstruction of meaning allowing the audience to participate in the experience of a health event and reflect on its significance in their own lives 4  however many perceptions may happen subconsciously for example a recent study showed that children who saw influencers with unhealthy snacks had significantly increased overall food intake with no accompanying positive response to seeing them with healthy snacks 10  the popularization of social media and the expansion of authority and celebrity have brought informal signals about health and medicine to a vast number of internet users -to their possible detriment early on information available on internet has been shown to be problematic with a wide variability in the quality of health-related information 42 16  the openness and scale of social media makes it especially susceptible to harmful information such as youtube videos promoting tobacco to consumers 28  twitter posts sowing doubt about the safety of vaccination 32  and flickr communities supporting its members in maintaining anorexic behaviors 49  especially during epidemics social media allows a rapid spread of rumors and misinformation to track such content a hybrid approach has been proposed wherein expert knowledge is combined with citizen science and machine learning such pipelines have been proposed for the 2014 ebola 19 and 2016 zika 14 20 11 outbreaks resources such as healthmap 11 and aidr 12 attempt to streamline the processing of news social media and medical reports to build an up-to-date model of an ongoing crisis 25 40  however little has been done in understanding advertisement plays in public health communication during an epidemic although social media websites are attempting to apply quality control to the ads on their platforms such as facebook down-ranking ads mentioning vaccine hoaxes 13  until now it has been difficult to examine the quality of information posted via such paid channels in this study we examine the case of the 2019-2020 coronavirus epidemic via a newly available window into advertising on the facebook platform we begin by querying facebook ads library api on february 20 2020 with the keywords coronavirus and covid-19 collecting all available ads ongoing or finished from all available countries the fields returned by the api include the numerical ad identifier span of the time in which the ad is shown a range indicating money spent on the ad a range of impressions the ad received id and name of the facebook page posting the ad and the name of the funding entity of the campaign furthermore the viewership is broken down into gender 3 and age 7 buckets as well as country regions such as states in the usa note that the purpose of this library is to expose ads facebook deems relevant to social issues elections or politics which likely do not encompass all advertising from the public health domain thus we focus on the mentions of the epidemic in alternative domains in total we collect 923 ads from 34 countries however only a few countries had a substantial number of ads figure 1 shows the number of ads active per day for the countries which have have at least 10 ads in total we find that the majority of captured advertising came from the united states having 359 ads in our dataset followed by italy at 228 and india at 64 the first advertisement we find is in the united states on january 13 a news report on the emergence of a new strain of coronavirus in china in this study we focus on the advertisements originating from the united states as this makes possible an examination of a cohesive picture of public health attitudes as well as political context in which the ads were placed however we make the full dataset available to the research community under the terms of service of facebook 14   we begin by examining the 78 distinct facebook pages which have posted advertisements in the span of our dataset we manually examine the link to their pages as provided by the api and note their category as stated on the page number of likes and self-description if one is provided by the page authors we then use open coding in order to classify the pages into public health 6 non-profit 16 news 26 political 26 business 12 and personal 5 with the rest of categories accounting for 9 of the advertisers coding result was examined by all authors and uncertain cases were decided jointly the most difficult distinction proved to be between news and political as most such pages posted politically-relevant news thus only pages representing a political figure or party were classified as political also note the small contribution of public health to the dataset it is possible the library selection criteria facebook applies to the ads simply does not match that of most public health campaigns and a more varied resource is necessary to fully capture them thus we focus on the non-public health campaigns figure 2 shows the pages in a co-occurrence network such that each node is a facebook page posting an advertisement and an edge is the jaccard similarity between the text of all of their ads in our dataset to clean the text we remove urls special characters and stopwords and lemmatize the remaining words for visual clarity we threshold jaccard similarity at 005 the nodes are colored by the category of the page we use gephi force-directed layout forceatlas 2 9 which positions nodes which are strongly connected closer together we notice a cluster of non-profit organizations green nodes in lower right which include unicef melinda gates foundation and several animal rights organizations note that highly specialized animal rights organization united poultry concerns is disconnected from this group as they use highly specific language in reference to the treatment of birds the rest of the network shows a mixing of news blue and politics green which is expected given most news pages are highly political areas of specializations are seen in the clusters of business pages yellow in bottom left and candidates for political office red in upper left as well as two educational institutions green and grey in upper right overall the pages figure 2  network of pages linked using jaccard similarity of ad text at threshold 005 node colors red -politics bluenews light green -non-profit dark green -public health yellow -business pink -personal force-directed gephi layout force atlas 2 form a well-connected giant connected component with few pages excluded indicating that the language used in the ads is largely similar next we turn to the sizes of the advertising campaigns figure  3 shows the number of ads active in a day by for each page category we find the most active to be non-profit pages followed by news and political ads however the number of impressions can vary greatly between different campaigns the volume of non-profit pages jumps especially around february 5 when the first coronavirus case is reported in us 15  in figure 4 we show the total expenditures left total impressions attained center and dollars per impression center by top pages in our dataset as both expenditures and impressions come as ranges such as 200-299 usd for 30000-34999 impressions we take the average of min and max of each range and sum up these averages for all ads posted by the page pages spending the most on their campaigns are american medical association association of physicians national nurses united labor union melinda gates philanthropic foundation and team mike 2020 presidential election campaign for michael bloomberg these expenditures are also associated with some of the highest audience reaches as can be seen from the plot on the right the advertisement with the highest expenditure is from national nurses united urging viewers to take a coronavirus preparedness survey asking whether their employers must be prepared for coronavirus 16  yet another advertisement with high viewership and cost is by american medical association linking to a story of a us clinic that cared for patients infected with the coronavirus 17  we observe that most visible ads were not providing basic information about coronavirus combining the two measures in figure 4 right we show the dollar per impression spent we observe that the highest such ratio reaches 700 usd per impression this may be due to the actual numbers being closer to the lower end of the range than the average which is what we employ along with expenditure figures for each ad the library provides a breakdown of the audience reached in terms of demographics age and gender and geography at the state level in attempt to reduce the effect of one advertiser posting many ads on the category statistics we first aggregate the demographic distributions of ads for each page and aggregate this distribution per category the resulting demographic and the geographic targeting distributions for public health non-profit news politics and business are shown in figure 5  both public health and non-profit pages reach more women then men and the latter -more women of age 65 and over public health campaigns on the other hand reach younger men the gender is more balanced for business political and news however all seem to reach older women more than men it is unclear whether these distributions are attributable to the conscious targeting on the part of advertisers or due to the peculiarities of their existing audiences we discuss the extent to which targeting can be tracked in discussion section examining the geographical targeting visible in maps of figure  5  we find state-specific focus which often falls on california texas new york oregon florida and others note that although some political pages concern political candidates from particular states most other pages are geography neutral this focus on a few states indicates purposeful geo-targeting of audiences throughout the advertising campaigns for all categories of pages thus we would discourage the use of this ad library to make country-wide assertions and pay special attention to the targeted populations note that although the ads in this dataset are supposed to be from the us we detected several cases of audiences reached from other regions including scotland england new south wales and puerto rico further we examine the language used in the ads from these categories to do this we consider the difference between the probability distribution ie language model of words occurring in ads of particular category and overall probability of their occurrence in the entire dataset table 1 lists 20 most distinguishing words for each of the major categories based on this probability difference starting from most distinguishing ones the score itself is not included for brevity and clarity of presentation we observe words relevant to each of the category such as the emphasis on science and response in public health domain animals and wildlife in non-profit stock market in business support and campaigns in politics and emphasis on particular cases and facts in news despite the most funded public health-related campaign being a personal account of a doctor dealing with the disease as discussed in the earlier section the rest are informational articles about the currently known facts about the virus and its spread the two main emphases of non-profit ads are donation drives and animal welfare with some of the most seen ads coming from melinda gates private charitable foundation and care2 online community encouraging green lifestyle most business ads come from financial advisors and insurers who focus on the performance of the markets and whether ones insurance plan would cover coronavirus a notable exception is a page dubbed mask the virus advertising masks that would prevent the spread of coronavirus 18  the webpage provides no description of the business and the facebook page was created in jan 30 2020 -we discuss it at greater length in the next section general preparedness is also promoted by a page 4patriots established on jul 20 2014 which encourages its audience to stock up here 19  the political ads come from both sides of the us political divide such as those criticizing or supporting the us president donald trump examples of such posts are in figure  6  as at the same time the us is participating in primaries wherein democratic party is choosing a contestant for us presidential election thus ads both pro and anti several contenders also appear mentioning bernie sanders joe biden pete buttigieg and michael bloomberg with the latter having the ad with highest impressions at over 800000 yet other 18 httpswwwfacebookcomadslibraryid531005304180280 19 httpswwwfacebookcomadslibraryid196433268175469 similar critique can be found in the ads of the news pages however mostly the articles provide general overview of the situation and notify public of new cases as mentioned before some are politically charged and have some of the themes mentioned in political sphere other articles provide commentary on social impact of the situation such as anti-asian racism and raising awareness of possible misinformation surrounding the topic in the following section we delve deeper into potential misinformation found in our dataset lastly we compute emotional connotation of the words used in the ads of these pages shown in figure 7  we employed the depechemood lexicon 3  which provides fine-grained emotion analysis for seven basic emotions fear amusement anger annoyance happiness inspiration and sadness as for the narrative analysis we lemmatised the message of every advertisement and calculated the average emotion per message according to the lexicon in this way we avoid introducing biases due to the varied lengths of the messages we observe the highest levels of emotion afraid as would be expected considering the subject matter news and non-profit ads are especially high on this emotion compared to the others interestingly second most prevalent emotion is inspired with public health and non-profit showing the highest levels possibly due to evocative messages during donation drives as expected happy is the least detected emotion during annotation of the ads we examine the text and material to which the ad links in terms of potentially erroneous material guided by ongoing news coverage of misinformation on mainstream news sources fact checking sources such as snopes and corresponding article on wikipedia 21  as at the time of writing no comprehensive list has been provided by the public health authorities 22  out of 359 ads we find 16 ads which have mentioned possible erroneous information or if we consider only ads with distinct text 8 out of 152 unique ads 53 coming mostly from categories of politics business and news also 5 ads were debunking erroneous claims below we expand on some of this content perhaps the most egregious post is shown if figure 8 upper left wherein the advertiser mentions bioweapons martial law fema camps hot tea and lemon killing corona virus and no need for extra vaccines as the page was created on feb 6 2020 it is possible that the account is a troll created specifically for posting content about this epidemic a less obvious example is shown in figure 8 upper right which links to an article stating ambiguously that there have also been suggestions that the virus may have escaped from a peo-ples liberation army biowarfare unit a claim that has been circulating on daily mail 23 and washington times 24  and which the research lab is denying 25  yet less contentious but more politicized potentially erroneous information comes from the claim by us president donald trump that as the weather starts to warm and the virus hopefully becomes weaker and then gone see figure 8 lower left although the claim has some grounding in the behavior of known viruses it is still unclear how the new strain will behave 26  interestingly the claim was mentioned both by news pages simply reporting the claim and by politically-inclined pages such as team mike 2020 disparaging the statement note that beyond 21 httpsenwikipediaorgwikimisinformationrelatedto the2019e2809320coronavirusoutbreak 22 we welcome a re-examination of ads in this dataset at a later time when more is known about various coronavirus claims and will make the data available for the research community 23 httpswwwdailymailcoukhealtharticle-7922379 chinas-lab-studying-sars-ebola-wuhan-outbreaks-center html 24 httpswwwwashingtontimescomnews2020jan26 coronavirus-link-china-biowarfare-program-possible 25 httpswwwscmpcomnewschinasocietyarticle3050872 chinese-research-lab-denies-rumours-links-first-coronavirus 26 httpswwwfactcheckorg202002 will-the-new-coronavirus-go-away-in-april questionable information relating to coronavirus specifically we detected instances of unrelated political claims and accusations captured in the political ads though it is outside the scope of this paper to verify political statements on the business side we found a page created on jan 30 2020 to be advertising face masks figure 8 lower right  neither the facebook page or the website the ad links to provide any medical certification of the sellers of the masks or disclose from where the business is originating 27  the webpage claims these are anti-virus masks and lists several versions with optional n95 and n99 activated carbon filters which are able to filter at least 95 of particles in the air when mask has a tight fit due to limited information provided it is difficult to determine whether these would achieve the fit necessary for protection or whether they comply with the standards set by cdc for respirators 28  this is also the page having most reach outside the us finally we find other topics susceptible to rumors being connected to the ongoing epidemic such as by page maverick doctor linking guillainbarrl syndrome gbs to vaccination even though there is a greater risk of getting gbs after getting a flu 29  as can be seen from the above examples there is a range of possible misinformation on the facebooks advertising platform we refer to the research community to determine whether some of these claims are intentional dis-information and as the crisis develops we will better understand the veracity of some of the claims meanwhile we observe the myriad contexts in which the topic of coronavirus is used outside public health messaging clear fear-mongering political attacks and conducting business although in our dataset we find that these ads are not promoted widely by their posters with all but one having expenditure in the range of 0-99 usd the other being in 100-199 usd interestingly the ads having possible misinformation in news and political categories have a distinct demographic targeting as seen in figure 9  while news reaches age groups more evenly except for spike for females in 55-64 age range political ads seem to target the older generation with highest viewership in over 65 age range finally we check whether the emotional content in these advertisements is different from the rest of the dataset and we find no statistical significance which may indicate that misinformation indeed sounds like the rest of the content or that we simply do not have enough data to make this assessment at this point at the time of data collection there were fewer than 15 cases in the united states not counting those repatriated from other parts of the world however we find over 300 advertisements in a span of a month speaking about the epidemic and potentially reaching millions of people many ad campaigns started even before february 5 when the first case was discovered in wisconsin despite few cases us was the most active country in the dataset however note that many of worst affected countries in asia are not represented in the facebook ads library so a direct comparison with them cannot be made findings in this work have several implications for public health communication much of information provided by the news-related ads in our dataset have echoed the latest information provided by major public health organizations such as us center for disease control 30  these include information on the latest cases travel advisories and steps people can take to protect themselves some even provide links to surveys possibly using the advertising platform as a recruitment tool for estimating awareness although it is possible the surveys are simply meant to attract attention thus facebook advertising may be a useful way to propagate the message through alternative sources and collect data on readership 28 however we find most advertising campaigns having extremely narrow targeting especially geographically although our sample of public health campaigns is limited by facebooks selection of ads to make available through the library those that we find cover a small geographic locale the situation is similar to ads from other categories in fact the targeting of public health and non-profit categories does not favor older men who have the highest mortality from covid-19 31  personalized messaging has been studied in public health literature such as for improving hivaids medication adherence 13 and smoking cessation 34 12  but the application of such messaging must be coordinated among the agencies to achieve a consistent message and a thorough coverage of the population moreover the reach and personalization of facebook for public health messaging can be used to reach underserved populations 33 27  supplementing traditional means such as physical billboards and rural outreach programs 36  another concern stems from the competition alternative narratives present to the messaging by public health organizations the adage if it bleeds it leads 43 encourages news and media providers to focus on the negative and sensationalize potentially vague and uncertain knowledge about an evolving epidemic we find that fear was the most detected emotion in ads of non-profit and news pages historically epidemics such as aids in 1980s and 1990s 21  sars in the early 2000s 41  and ebola in 2014 37 tended to become associated with specific minorities and countries with people putting the blame of the disease on the other in the case of covid-19 the source of the virus prompted sinophobia 32  though we only find calls against racism in our dataset beside the competition for the narrative the news political business and other entities are competing for the views and clicks of the audience with the public health organizations who may not have as many resources to bid up the price for instance we find numerous mentions of coronavirus in association with us politicians some of which are top spenders in our dataset unlike in for example china during sars epidemic that prompted a renews sense of patriotism through narrative of self-sacrifice 30  in the us covid-19 seems to be adopted as a weapon in the ongoing partisan struggle especially in a us presidential election year a similar weaponization of the epidemic narrative occurred in africa during the ebola epidemic resulting in several deaths 47  although it is difficult to achieve a comprehensive political message during a crisis the extent of panic amongst the public as well as the stability of world markets depends not only on public health institutions but on the leaders who have far-reaching platforms finally among the advertisements in our dataset we find about 5 to contain possible erroneous information ranging from accusations of using bioweapons to questioning correctness of comparisons to other viruses the reasons for such content may be diverse however we do not find the posters of these ads to be investing much money although some achieve up to 15000 and more impressions it is also concerning that based on our sentiment analysis these messages may feel similar to the other content about coronavirus unfortunately despite facebooks collaboration with a myriad of fact-checking institutions it is possible that small campaigns like this are not popular enough to warrant examination the design of fully and partially automated tools to detect early appearance of fake news and misinformation is becoming a hot research topic 1 20 38 45  but it is imperative to use them within a larger public health communication strategy guided by subject matter experts and enforced by both governments and social media platforms the above conclusions must be taken with several limitations in mind first and foremost we are aware of the fluidity of the ongoing situation in the light of new findings and developments some of the annotations provided in this work may change thus we will make the annotated dataset available to the research community 33  secondly facebook ads library was not created for monitoring public health messaging -this is why this study focuses on the alternative narratives it would be extremely helpful if the social media platforms were to collaborate more closely with public health researchers so that a full picture of the discourse may be examined an increased worldwide coverage would also allow the tracking of pandemics across different parts of the world recall currently the library covers eu countries us and a handful of others however even if we were to attain information on all advertising happening on facebook and instagram this will capture by far not all internet users and only a fraction of people in the us internet as well as traditional media present a plethora of alternative communication channels and more studies must be conducted to capture the full scope of communication related to public health and its possible impacts privacy as potentially any facebook page may have an advertising campaign it is possible that not only large companies and agencies but also individuals will be captured in the library and in our own dataset we find several pages we labeled as personal note that although we will make the data available in accordance with the facebooks terms of service we have chosen not to anonymize the facebook pages other than those labeled as personal since it is important to understand the sources of ad content also all information in this dataset is freely available via ads library website we even provide links to some of the ads however we call upon the research community to establish privacy standards for the collection reporting and sharing of advertising data and we hope this study will spur the conversation in the research field  scientometric trends for coronaviruses and other emerging viral infections dima kagan jacob moran-gilad michael fire  covid-19 is the most rapidly expanding coronavirus outbreak in the past two decades to provide a swift response to a novel outbreak prior knowledge from similar outbreaks is essential here we study the volume of research conducted on previous coronavirus outbreaks specifically sars and mers relative to other infectious diseases by analyzing over 35 million papers from the last 20 years our results demonstrate that previous coronavirus outbreaks have been understudied compared to other viruses we also show that the research volume of emerging infectious diseases is very high after an outbreak and drops drastically upon the containment of the disease this can yield inadequate research and limited investment in gaining a full understanding of novel coronavirus management and prevention independent of the outcome of the current covid-19 outbreak we believe that measures should be taken to encourage sustained research in the field  infectious diseases remain a major cause of morbidity and mortality worldwide in developed countries and particularly in the developing world 1  according to the world health organization out of the top-10 causes of death globally three are infectious diseases 1  in light of the continuous emergence of infections the burden of infectious diseases is expected to become even greater in the near future 2 3 currently the world is struggling with a novel strain of coronavirus that emerged in china during late 2019 and by the time of this writing has infected more than 156000 people and killed more than 5800 4 5  covid-19 is the latest and third serious human coronavirus outbreak in the past 20 years additionally of course there are several more typical circulating seasonal human coronaviruses causing respiratory infections it is still too early to predict the epidemic course of covid-19 but it is already a pandemic which currently appears more difficult to contain than its close relative sars-cov 6 7  much can be learned from past infectious disease outbreaks to improve preparedness and response to future public health threats three key questions arise in light of the covid-19 outbreak to what extent were the previous human coronaviruses studied is research on emerging viruses being sustained aiming to understand and prevent future epidemics are there lessons from academic publications on previous emerging viruses that could be applied to the current covid-19 epidemic in this study we answer these vital questions by utilizing state-of-the-art data science tools to perform a large-scale analysis of 35 million papers of which 1908211 concern the field of virology we explore nearly two decades of infectious disease research published from 2002 up to today we particularly focus on public health crises such as sars influenza including seasonal pandemic h1n1 and avian influenza mers and ebola virus disease and compare them to hivaids and viral hepatitis b and c three bloodborne viruses that are associated with a significant global health burden for more than two decades our results highlight that the earlier human coronaviruses sars and mers are understudied compared to bloodborne viruses about 047 of virology studies and 7  10 5  of all studies from the past 20 years involved human coronaviruses while hivaids accounts for 81 of all virology studies we observed that unlike the research in the domain of hivaids and avian influenza that has been published at a high and steady pace over the last 20 years sars was studied at an overwhelming rate after the 2002-2003 outbreak and then sharply dropped after 2005  figure 4  additionally we noticed that the sars research community had a smaller percentage of relatively prolific researchers than other diseases moreover researchers with multiple papers related to sars published on average 38 papers while hepatitis c researchers published on average 52 papers during the same period when it comes to global collaboration and research efforts most of the research stemmed from china and the us figure 1  with only about 17 of sars papers first authors being located in europe overall researchers from 57 and 67 countries have studied mers and sars respectively however the vast majority of sars papers 73 were written by researchers in only 6 countries  figure 9  a crucial aspect of being prepared for future epidemics is sustained ongo- ing research of emerging infectious diseases even at times of peace when such viruses do not pose an active threat our results demonstrate that research on previous coronaviruses such as sars and mers was conducted by a relatively small number of researchers centered in a small number of countries suggesting that such research could be better encouraged we propose that regardless of the fate of covid-19 in the near future sustained research efforts should be encouraged to be better prepared for the next outbreak 2 related work health in particular see section 22 there is great promise in utilizing big data to study epidemiology 8  one approach is to gather data using different surveillance systems for example one such system is promed promed was launched 25 years ago as an email service to identify unusual worldwide health events related to emerging and reemerging infectious diseases 9  it is used daily around the globe by public health policy makers physicians veterinarians and other healthcare workers researchers private companies journalists and the general public reports are produced and commentary is provided by a global team of subject-matter experts in a variety of fields promed has over 80000 subscribers and over 60000 cumulative event reports from almost every country in the world additionally there are many different systems used by different countries and health organizations worldwide in 2006 cowen et al 10 evaluated the promed dataset from the years 1996 to 2004 they discovered that there are diseases that received more extensive coverage than others 86 disease subjects had thread lengths of at least 10 reports and 24 had 20 or more they note that the pattern of occurrence is hard to explain even by an expert in epidemiology also with the level of granularity of promed data it is very challenging to predict the frequency that diseases are going to accrue in 2008 jones et al 2 analyzed the global temporal and spatial patterns of emerging infectious diseases eids they analyzed 305 eids between 1940 and 2004 and demonstrated that the threat of eids to global health is increasing the same year freifeld et al 11 developed healthmap an interactive surveillance system that integrates disease outbreak reports from various sources using their system they created a classifier that was able to classify diseases with an accuracy of 84 data about infectious diseases can also come from web-and social-based sources for instance in 2009 ginsberg et al 12 used google search queries to monitor the spread of the influenza epidemics they used the fact that many people search online before going to doctors and they found that during a pandemic the volume of searches differs from normal they then created a mathematical model to forecast the spread of flu this research was later converted into a tool called google flu trends and at its peak google flu trends was deployed in 29 countries worldwide however not everything worked well for google flu trends in 2009 it underestimated the flu volume and in 2013 it predicted more than double the number of cases than the true volume 13  as a result of such discrepancies google shut down the google flu trends website in 2015 and transferred its data to academic researchers 14  also in 2009 carneiro and mylonakis 15 used large amounts of data to predict flu outbreaks a week earlier than prevention surveillance systems similar to ginsberg et al 12  they used google trends as a data source but instead of only monitoring seasonal flu they monitored avian influenza and west nile virus since there was a constant growth in the number of google searches they normalized the data to get meaningful results they found a correlation between web searches about influenza and the cdc data in 2010 lampos and cristianini 16 extended the idea of carneiro and mylonakis 15 to use temporal data to monitor outbreaks instead of using google trends they used twitter as their data source they collected 160000 tweets from the uk and as ground truth they used hpa weekly reports about the h1n1 epidemic using textual markers to measure flu on twitter they demonstrated that twitter can be used to study disease outbreaks similar to google trends the same year seifter et al 17 found google trends to be a good approximation for lyme disease outbreaks in 2011 althouse et al 18 also used google trends but unlike seifter et al 17 and carneiro and mylonakis 15  they demonstrated that this kind of method was effective for dengue also the same year salath and khandelwal 19 analyzed twitter and demonstrated that it is possible to use social networks to study not only the spread of infectious disease but also vaccinations they found a correlation between the sentiment in tweets toward an influenza vaccine and the vaccination rate in 2013 yuan et al 20 showed that in addition to twitter and google baidu can be used to monitor influenza spread in 2015 santillana et al 21 took the influenza surveillance one step further by fusing multiple data sources they used five datasets twitter google trends near real-time hospital visit records flunearyou and google flu trends they used all these data sources with a machine-learning algorithm to predict influenza outbreaks in 2017 mcgough et al 22 dealt with the problem of significant delays in the publication of official government reports about zika cases to solve this problem they used the combined data of google trends twitter and the healthmap surveillance system to predict estimates of zika cases in latin america there is substantial controversy surrounding the use of web-based data to predict the volume of outbreaks the limitations of google flu trends mentioned above raised the question of reliability of social data for assessing disease spread lazer 23 noted that these types of methods are problematic since companies like google facebook and twitter are constantly changing their products studies based on such data sources may be valid today but not be valid tomorrow and may even be unreproducible in 2005 vergidis et al 24 used pubmed and jcr to study trends in microbiology publications they discovered that microbiology research in the us had the highest average impact factor but in terms of research production western europe was first in 2008 uthman 25 analyzed trends in paper publications about hiv in nigeria he found growth from 1 to 33 of the number of publications about hiv in nigeria and that papers with international collaborations were published in journals with a higher impact factor in 2009 ramos et al 26 used web of science to study publications about infectious diseases in european countries they found that more papers in total were published about infectious diseases in europe than in the us in 2012 takahashi-omoe and omoe 27 surveyed publications of 100 journals about infectious diseases they discovered that the us and the uk had the highest number of publications and relative to the countrys socioeconomic status the netherlands india and china had relatively high productivity in 2014 similar to wislar et al 28  kennedy et al 29 studied ghost authorship in nursing journals instead of biomedical journals they found that there were 276 and 42 of ghost and honorary authorships respectively in 2015 wiethoelter et al 30 explored worldwide infectious disease trends at the wildlife-livestock interface they found that 7 out of the top 10 most popular diseases were zoonoses in 2017 dong et al 31 studied the evolution of scientific publications by analyzing 89 million papers from the microsoft academic dataset similar to the increase found by aboukhalil 32  they also found a drastic increase in the number of authors per paper in 2019 fire and guestrin 33 studied the over-optimization in academic publications they found that the number of publications has ceased to be a good metric for academic success as a result of longer author lists shorter papers and surging publication numbers citation-based metrics such as citation number and h-index are likewise affected by the flood of papers self-citations and lengthy reference lists in this study we fused four data sources to extract insights about research on emerging viruses in the rest of this subsection we describe these data sources 1 mag -microsoft academic graph is a dataset containing scientific publication records citation relationships between those publications as well as authors institutions journals conferences and fields of study 34  the mag dataset we used was from 22 march 2019 and contains data on over 210 million papers 35  this dataset was used as the main dataset of the study similar to fire and guestrin 33  we only used papers that had at least 5 references in order to filter non peer-reviewed publications such as news columns which are published in journals pubmed -pubmed is a dataset based on the pubmed search engine of academic publications on the topics of medicine nursing dentistry veterinary medicine health care systems and preclinical sciences 36 one of the major advantages of using the pubmed dataset is that it contains only medical-related publications the data on each pubmed paper contains information about its venue authors and affiliations but it does not contain citation data in this study we used the 2018 annual baseline pubmed dataset containing 29138919 records 1 we mainly utilized the pubmed dataset to analyze journal publications see section 321 3 sjr -sjr is a dataset containing the information and ranking of over 34100 journals from 1999 to 2018 37  including their sjr indicator 2 the best quartile of the journal 3 and more we utilized the sjr dataset to compare the rankings of different journals to assess the level of their prestige wikidata -wikidata is a dataset holding a vast knowledge about the world containing data on over 78252808 items 40  wikidata stores metadata about items and each item has an identifier and can be associated with other items we utilized the wikidata dataset to extract geographic information for academic institutions in order to match a paper with its authors geographic locations to study the research of emerging viruses over time we analyzed the datasets described in section 31 in pursuing this goal we used the code framework recently published by fire and guestrin 33  which enables the easy extraction of the structured data of papers from the mag dataset the mag and pubmed datasets were filtered according to a predefined list of keywords the keyword search was performed in the following way given a set of diseases d and a set of papers p  from each paper title p t  where p  p  we created a set of word-grams word-grams are defined as n-grams of words ie all the combinations of a set of words in a phrase without disrupting the order of the words for example the word-grams of the string information on swine flu word-gramsinformation on swine flu will return the following set information on swine flu information on on swine swine flu information on swine on swine flu information on swine flu next for each p we calculated word-gramp t   d which was considered as the diseases with which the paper was associated in the current study we focused on the past emerging coronaviruses sars and mers additionally we also analyzed ebola virus disease influenza seasonal avian influenza swine flu hivaids hepatitis b and hepatitis c as comparators that represent other important emerging infectious diseases from the past two decades for these nine diseases we collected all their aliases which were added to the set of diseases d and were used as keywords to filter the datasets to reduce the false-positive rate we analyzed only papers that according to the mag dataset were in the categories of medicine or biology and had at least five references additionally to explore the trend in the core categories of infectious disease research we performed the same analysis on the 2 the sjr indicator is a measure used to assess the prestige of a journal the measure takes into account the number of citations and the prestige of the source of the citing paper 38 3 the journal impact factor quartile is the quotient of a journals rank in category x and the total number of journals in the category y so that x  y  percentile rank z 39 virology category in the rest of this section we describe the specific calculations and analyses we performed to explore the volume of studies on emerging viruses we examined the publication of papers about infectious diseases first we defined several notions that we used to define publication and citation rates let d be a set of disease names and p a set of papers namely for a paper p  p  p disease is defined as the disease that matches the papers keywords p year as the papers publication year and p citations as the set of papers citing p using these notions we defined the following features  number of citations -the total number of citations for a specific infectious disease  number of papers -the total number of published papers for a specific infectious disease n p r y d  i  p i y ear  y and i disease  d i  p i y ear  y using these metrics we inspected how the coronavirus publication and citations rates differed from other examined eids we analyzed how trends of citations and publications have changed over time to investigate the relationship between journals and their publication of papers about emerging viruses we combined the semantic scholar and pubmed datasets with the sjr dataset using issn and selected all the journals from sjr categories related to infectious diseases immunology epidemiology infectious diseases virology and microbiology first we inspected whether coronavirus papers are published in the top journals we selected the top-10 journals by sjr and calculated the number of papers published for each disease over time next we inspected how published papers about coronavirus are regarded relative to other eids in terms of ranking to this end we defined a new metric jscore t  jscore t is defined as the average sjr score of all published papers on a specific topic t we used jscore t to observe how the prominence of each disease in the publication world has changed over time lastly we explored publications by looking at the quartile ranking of the journal over time to study how scientific authorship has changed in the field of infectious diseases we explored what characterizes the authors of papers on different diseases we inspected the number of new authors over time to check how attractive emerging viruses are to new researchers additionally we analyzed the number of experienced authors where author experience is defined as the time that has passed from his or her first publication we also analyzed the number of authors who wrote multiple papers about each disease to inspect the state of international collaborations in emerging virus research we mapped academic institutions to geolocation however it is not a trivial task to match institution names institution names are sometimes written differently for example aalborg university hospital and aalborg university are affiliated however there are cases where two similar names refer to different institutions for example the university of washington and washington university are entirely different institutions to deal with this problem we used the affiliation table in the mag dataset to determine the country and city of each author we applied a five-step process 1 for each institution we looked for the institutions page on wikidata from each wikidata page we extracted all geography-related fields 5 to first merge all the wikidata location fields we used the coordinate location with reverse geocoding to determine the city and country of the institution 3 for all the institutions that did not have a coordinate location field we extracted the location data from the other available fields we crossed the data against city and country lists to determine whether the data in the field described a city or a country 4 to acquire country data for an institution that had only city data on wikidata we used city-to-country mapping lists 5 to get city and country data for institutions that did not have the relevant fields on wikidata we extracted geographic coordinates from wikipediacom even though wikidata and wikipediacom are both parts of the wikimedia foundation they are independent projects which have different data similar to wikidata coordinates we used reverse geocoding to determine the city and country of the institution using the extracted geodata we explored how international collaborations change over time in coronavirus research finally we explored which countries have the highest number of papers about coronavirus and which countries have the highest number of international collaborations over time in the following subsections we present all the results of the experiments which were described in section 3  in recent years there has been a surge in publications about infectious diseases yielding almost 2 million new papers related to medicine and biology each year see figure 2a  in contrast to the overall growth in the number of infectious disease papers there has been a relative decline in the number of papers about the coronaviruses sars and mers see figure 2b  generally new eids such as mers sars and ebola has low number of papers while hiv influenza hepatitis b and c are extremely popular research topic  figure 3  also we found that in the past 16 years only 07 of infectious disease studies were about sars and mers it can be observed that hiv alone is responsible for 20 of all studies and hepatitis b and c together are holding 82 of all infectious disease studies in terms of normalized paper rate see figure 4  after the first sars outbreak there was a peak in publishing sars-related papers with npr twice as high as ebolas however the trend dropped very quickly and a similar phenomenon can be observed for the swine flu pandemic the mers outbreak achieved a much lower npr than sars specifically more than 16 times lower when comparing the peaks in sars and mers trends in terms of normalized citation rate  figure 5  we observed the same phenomenon as we did with npr from analyzing the trends in journal publications we discovered the numbers of papers published by journal quartile are very similar to normalized paper rate and normalized citation rate see figure 6  we observed that for most of the diseases the trends are quite similar a growth in the study rate is coupled by a growth in the number of published papers in q1 journals we discovered that for sars mers the swine flu and ebola q1 publication trends were almost parallel to their npr trends see figures 4 and 6  also we noticed that hiv avian influenza influenza and hepatitis b and c have steady publication numbers in q1 journals looking at papers in highly ranked journals  figure  7  we observed that the diseases which are being continuously published in top-10 ranked journals are mainly persisting diseases such as hiv and influenza additionally we inspected how the average journal ranking of publications by disease has changed over time figure 8  we found that only mers had a decline of jscore we also noticed that current papers about sars had the highest jscore by studying the authorship trends in the research of emerging viruses we discovered that there is a difference in the average experience of authors among diseases sars researchers had the lowest experience in years and hepatitis c had the most experienced researchers see table 1  additionally from analyzing authors who published multiple papers on a specific disease we found that  by inspecting global collaboration and research efforts we found that the geolocation of researchers correlated with publication trends for instance most sars mers and avian influenza research was done by investigators based in the us and china figure 9  while the us was dominant in the research of all inspected diseases china showed an increased output in only these three diseases also mers and sars were studied in the least number of countries and hiv was studied in the highest number of countries  figure 9  moreover sars and mers were the diseases least studied in europe with only 17 and 19 of sars and mers studies respectively as opposed to ebola studies 29 of which were conducted in europe in this study we analyzed trends in the research of emerging viruses over the past two decades with emphasis on emerging coronavirus sars and mers we compared the research of these two coronavirus epidemics to seven other emerging viral infectious diseases as comparators to this end we used multiple bibliometric datasets fusing them to get additional insights using this data we explored the research of epidemiology from the perspectives of papers journals authors and international collaborations by analyzing the results presented in section 4 the following can be noted first the surge in infectious disease publications figure 2  supports the results of fire and guestrin 33 that found there has been a general escalation of scientific publications we found that the growth in the number of infectious disease publications is very similar to other fields hence goodharts law 6 did not skip the world of virology research however alongside the general growth in the number of papers we observed that there was a decline in the relative number of papers on the specific infectious diseases we inspected the most evident drastic drop in the publication rate happened after an epidemic ended it appears that for a short while many researchers study an outbreak but later their efforts are reduced this is strengthened by considering the average number of multiple papers per author for each disease see table 2  second when looking at journal publications we noted very similar patterns occurred for citations and publications this result emphasizes that fewer publications and hence fewer citations translate into fewer papers in q1 journals  figure 6  also we observed the same patterns as fire and guestrin 33  with most of the papers being published in q1 journals and the minority published in q2-q4 journals this trend started to change when zooming in and analyz- ing publications in top-10 ranked journals figure 7  while we can see some correlation to outbreaks in ebola swine flu and sars it is harder to interpret the curve of hiv since there were no focused epidemics in the past 20 years but a global burden and we did not observe similar patterns in publications and citations observing the jscore section 322 results figure 8  most diseases showed a steady increase but two diseases behaved rather anomalously mers had a decline since 2013 which is reasonable to expect after the initial outbreak but we did not see the same trends in the other diseases and there is a general trend of increasing average sjr 33  the second anomaly is that sars had an increase in jscore alongside a decrease in citations and publication numbers inspecting the data we discovered that in 2017 there were three published papers in lancet infectious diseases and in 2015 two papers in journal of experimental medicine about sars and both journals have a very high sjr these publications increased the jscore drastically third we observed that on average authors write a fewer number of multiple papers on diseases that are characterized by large epidemics such as the swine flu and sars on the other side of the scale are hepatitis c and hiv which are persistent viral diseases with high global burdens these diseases involve more prolific authors regarding ebola and mers it is too early to predict if they will behave similarly to sars since they are relatively new and require further follow up fourth looking at international collaboration we observed the us to be very dominant in all the disease studies  figure 9  looking at china we found it to be mainly dominant in diseases that were epidemiologically relevant to public health in china such as sars avian influenza and hepatitis b when looking at ebola which has not been a threat to china for the last two decades we observed a relatively low investment in its research in china many of the trends we observed are related to the pattern of the diseases we observed two main types of infectious diseases with distinct trends the first type were emerging viral infections like sars and ebola their academic outputs tend to peak after an epidemic and then subside the second type were viral infections with high burdens such hepatitis b and hiv for which there is a more or less constant trend these trends were most evident in publication and citation numbers as well as journal metrics the collaboration and author distributions were more affected by where the outbreak occurred or where there was a high burden this study may have several limitations to analyze the data we relied on titles to associate papers with diseases while a title is very important in classifying the topic of a paper some papers may discuss a disease without mentioning its name in the title additionally there may be false positives for instance an acronym might not be recognized as an infectious disease term an additional limitation is our focus on a limited number of distinct diseases there are other emerging infections not evaluated herein which could have followed other trends to deal with some of these limitations we only analyzed papers that were categorized as medicine and biology papers as a means to reduce false positives furthermore we show that the same trends appeared even when we filtered all the papers by the category of virology see figures 9 and 10  finally we compared papers that were tagged with a mesh term on pubmed to the papers we retrieved using our keyword search of the title we found that we matched mesh terms with 73 recall the covid-19 outbreak has emphasized the insufficient knowledge available on emerging coronaviruses here we explored how previous coronavirus outbreaks and other emerging viral epidemics have been studied over the last two decades from inspecting the research outputs in this field from several different angles we demonstrate that the interest of the research community in an emerging infection is temporarily associated with the dynamics of the incident and that a drastic drop of interest is evident after the initial epidemic subsides this translates into limited collaborations and a non-sustained investment in the research of the coronavirus such a short-lived investment may also involve reduced funding not evaluated herein and may slow down important developments such as new drugs vaccines or preventive strategies there has been an unprecedented explosion of publications on covid-19 since january 2020 and also a significant allocation of research funding we believe the lessons learned from scientometrics of previous epidemics argue that regardless of the outcome of covid-19 efforts to sustain research in this field should be made this study is reproducible research the data that support the findings of this study are available on request from the corresponding author   a first instagram dataset on covid-19 koosha zarei reza farahbakhsh nol crespi gareth tyson  the novel coronavirus pandemic outbreak is drastically shaping and reshaping many aspects of our life with a huge impact on our social life in this era of lockdown policies in most of the major cities around the world we see a huge increase in people and professionals engagement in social media social media is playing an important role in news propagation as well as keeping people in contact at the same time this source is both a blessing and a curse as the coronavirus infodemic has become a major concern and is already a topic that needs special attention and further research in this paper we provide a multilingual coronavirus covid-19 instagram dataset that we have been continuously collected since march 30 2020 we are making our dataset available to the research community at httpsgithubcomkooshazareicovid-19-instapostids we believe that this contribution will help the community to better understand the dynamics behind this phenomenon in instagram as one of the major social media this dataset could also help study the propagation of misinformation related to this outbreak  the novel coronavirus covid- 19 was declared a pandemic by the world health organisation who on 11 march 2020 1 since then the world has experienced almost 3 million cases to mitigate its spread many government have therefore imposed unprecedented social distancing measures that have led to millions become housebound this has resulted in a flurry of research activity surrounding both understanding and countering the outbreak 1  as part of this social media has become a vital tool in disseminating public health information and maintaining connectivity amongst people several recent studies have relied on twitter data to better understand this 2 - 5  these have primarily focused on health related misinformation but there have also been studies into online hate 6  despite this there has been only limited exploration of other social modalities such as image content we argue this represents a limitation particularly considering the importance of image-based content in the dissemination of news and misinformation 7  8  this paper introduces a covid- 19 instagram dataset which we make available for the research community we have gathered data between january 5 and march 30 2020  iii the dataset covers 185k comments and 329k likes from 53k posts these posts have been distributed by 25k publishers the data 1 httpstinyurlcomwhopandemicannouncement predominantly covers english language posts and we provide a number of important features covering both the content and the publisher  iv we hope that this dataset can help support a number of use cases hence we conclude the paper by highlighting a number of potential uses related to covid-19 social media analysis  v details of how to access the data is presented in vi most related to this work is the set of covid-19 social media datasets recently released to date this predominantly covered textual data eg twitter to assist in this kazemi et al 9 provide a toolbox for processing textual data related to covid-19 in terms of data the first efforts in this direction was from authors in 2 which provide a large twitter dataset related to coronavirus by crawling major hashtags and trusted accounts another similar study 3  provides an arabic twitter dataset with a similar data collection methodology lopez et al 4 provide another twitter dataset including the geolocated tweets there are some further efforts on providing similar datasets from twitter 10- 12  sharma et al 5 also made a public dashboard 2 available summarising data across more than 5 million real-time tweets these twitter datasets are being used for various use cases for example saire and navarro 13 use the data to show the epidemiological impact of covid-19 on press publications singh et al 14 are also monitoring the flow of misinformation flow across 27m tweets and correlating it with infection rates to find that misinformation and myths are discussed but at lower volume than other conversations to the best of our knowledge the only paper that has covered instagram is by cinelli et al 15  who analyse twitter instagram youtube reddit and gab data about covid-19 we complement this by making a public instagram dataset available to the community we redirect readers to 1 for a comprehensive survey of ongoing data science research related to covid-19 we have collected public posts from instagram by crawling all posts associated with a set of covid-19 hashtags presented in table i methodology to be able to collect instagram public content in the shape of post we use the official instagram apis 16  in particular to get posts that are tagged with specific hashtags the instagram hashtag engine is used 17  this api returns public photos and videos that have been tagged with particular hashtags mongodb is used as the core database and data is stored as json records the crawler is responsible for gathering both posts and reactions a reaction can be active comment or passive like as it is infeasible to collect all reactions in this dataset we define a limitat of 500 comments and 500 likes per post our crawler is running on several virtual machines in parallel 247 note that we do not manually filter any posts and therefore we gather all posts containing the hashtags regardless of the specific topics discussed within the complete architecture of our crawler is described in this paper 18  release v10 april 20 2020 the first version of this data collection process started on january 5 2020 and continued until march 30 2020 the data gathering is still running as the lockdown has not been finished in many countries around the world at the time of writing this paper during this time 185k comments and 329k likes from 53k public posts have been collected these posts are distributed by 25k publishers ethics in line with instagram policies as well as user privacy we only gather publicly available data that is obtainable from instagram to provide context for potential users of our dataset we next brifely summarise the dataset and describe the characteristics of the content hashtags recall that we gather the data by querying certain hashtags figure 2 presents the top hashtags tagged within the posts figure 1 also presents a wordcloud of the hashtags in our dataset this naturally includes hashtags outside of our seed set used for crawling there are intuitive examples such as corona covid19 covid 19 stayathome quarantine love covid virus and instagram the are therefore the most repeated hashtags that appear with coronavirus note that this means will might miss posts that mention these concepts in other languages post language in order to identify the language of the post we use spacy library 19 and we apply it on the text of the caption the language distribution is displayed in table ii  the dataset is dominated by english language content making up almost 60 of posts this is driven by our choice of an english-language hashtag seed set used for data collection that said we have broad coverage of other widely spoken languages too eg spanish 99 notice that there is no official metric to determine the post language therefore we highlight that this analysis could mis-classify certain posts such as those solely hashtags or emojis  we hope that the dataset can support diverse research activities below we list a subset of potential topics we believe the dataset could support 1 fake news misinformation and rumors spreading several researcher have started to inspect covid-19 misinformation as an example an infodemic observatory have analyzed more than 100m public messages to understand the digital response in online social media to covid-19 outbreakhttpscovid19obsfbkeu in another study sharma et al 5 made a public dashboard available summarising data from real-time tweets in in httpsusc-meladygithubiocovid-19-tweet-analysis with a focus to misinformation spread analysis we believe that our instagram data could be used to evaluate the flow of misinformation eg memes on instagram 2 bot population and bot generated content it is well known that bot content plays a prominent role in social media data 20  these have the capacity of amplify misinformation or even act against public health policies eg encourgaging a breakdown in social distancing we posit that the data could be used to explore the role of bots in this dissemination 3 behavioral change analysis during the pandemic the social distancing measures are created an unprecedented change to millions of peoples lives understanding the behavioral consequences of this is vital for understanding things like adherence to social distancing policies and mental health consequences 4 information sharing related covid-19 information flow is vital during periods of emergency we posit that the dataset can be used to understand the flow of information as well as peoples reactions to such information vi dataset access the presented dataset is accessible in this address on github platform httpsgithubcomkooshazareicovid-19-instapostids this is the first version of the dataset and we are still collecting data hence we hope to make further versions available in the coming weeks and months we publish this dataset in agreement with instagrams terms  conditions 21  and as it is not possible to release the post content and reactions we just distribute the post ids these are known as the shortcodes researchers can then simply retrieve post content through ids by the help of some open-source projects such as instaloader 22 that have been developed for such purposes for any further question please contact koosha zarei at kooshazareitelecom-sudpariseu   thanasis vergoulis ilias kanellos serafeim chatzopoulos danae karidi pla theodore dalamagas  since the beginning of the 2019-20 coronavirus pandemic a large number of relevant articles has been published or become available in preprint servers these articles along with earlier related literature compose a valuable knowledge base affecting contemporary research studies or even government actions to limit the spread of the disease and treatment decisions taken by physicians however the number of such articles is increasing at an intense rate making the exploration of the relevant literature and the identification of useful knowledge in it challenging in this work we describe bip4covid19 an open dataset compiled to facilitate the coronavirus-related literature exploration by providing various indicators of scientific impact for the relevant articles finally we provide a publicly accessible web interface on top of our data allowing the exploration of the publications based on the computed indicators june 5 2020 17 101 the bip4covid19 dataset is comprised of three files in tab separated tsv 102 format the files contain identical information however in each of them the records 103 are ordered based on a different impact measure popularity influence social media 104 attention the data attributes included in each file are summarised in table 1 105 6 the dataset is updated on a regular basis june 5 2020 47 129 comparing each records complete content and eliminate them finally manual 130 inspection is performed to produce the correct metadata for a limited number of 131 duplicates that remain eg duplicate records containing the title of the same 132 publication in two different languages  end of may more than 4 000 000 cases had been recorded in more than 200 countries 7 counting more than 320 000 fatalities 8 at the time of writing an extensive amount of coronavirus related articles have been 9 published since the virus outbreak indicatively our collected data contain about 10 14 954 articles published in 2020 taking additionally into account previous literature 11 on coronaviruses and related diseases it is evident that there is a vast literature on the 12 subject however it is critical for researchers or other interested parties eg 13 government officers physicians to be able to identify high-impact articles a variety of 14 impact measures have been proposed in the fields of bibliometrics and 15 scientometrics 7 8  some of them rely on the analysis of the underlying citation 16 network other approaches utilise measures commonly known as altmetrics which 17 analyse data from social media andor usage analytics in online platforms eg in 18 publishers websites  bip4covid19 is a regularly updated dataset data production and update is based on 32 the semi-automatic workflow presented in figure 1  in the following subsections the 33 major processes involved are elaborated the list of covid-19-related articles is created based on two main data sources the 36 cord-19 1 open research dataset 3  provided by the allen institute for ai and the 37 litcovid 2 collection 2 provided by the nlmncbi bionlp research group cord-19 offers a full-text corpus of more than 63 000 articles on coronavirus and 39 covid-19 collected based on articles that contain a set of covid-19 related keywords 40 from pmc arxiv biorxiv and medrxiv and the further addition of a set of 41 publications on the novel coronavirus maintained by the who litcovid is a curated 42 dataset which currently contains more than 13 000 papers on the novel coronavirus semi-automatically processed to remove duplicate records the resulting dataset 48 contains one entry for each distinct article each entry contains the pmid the doi the 49 pmcid and the publication year of the corresponding article this information is the 50 minimum required for the calculation of the selected impact metrics a prerequisite for calculating the citation-based impact measures of the collected 53 articles is the compilation of their citation network ie the network which has articles 54 as nodes and citations between them as directed edges the citations of the articles 55 required to construct this network are gathered using ncbis elink tool the tool 56 returns for a given article the identifiers pmidspmcids of all articles that cite or are 57 cited by it two citation-based impact measures are calculated on the constructed 58 network the pagerank 5 and the ram scores 6  these two measures were selected 59 based on the results of a recent experimental study 7  which found them to perform 60 best in capturing the overall and the current impact of an article ie its influence  the corresponding tweet objects were collected using the twitter api 77 the result was a collection of 76 046 064 tweet objects 66 1 gb in zipped format the difference between the number of ids and hydrated objects is due to the fact that 79 7 952 595 tweets have been deleted in the meantime 9 and are therefore impossible 80 to retrieve to find those tweets which are related to the articles in our database we rely on the 82 urls of the articles in doiorg pubmed and pmc these urls are easily produced figure 3  unique identifier of the article in pubmed as collected from the source data files articles missing this identifier are indicated with the value na the digital object identifier of the article as collected from pubmed articles missing a doi are indicated with the value na unique identifier of the article in pubmed central pmc as collected from the source data files articles missing an identifier in pmc have the value na the value of the corresponding citation-based measure ram 6  for the respective article the value of the corresponding citation-based measure page-rank 5  for the respective article social media attention the calculated tweet count for the article corresponding to the record table 1  data attributes inside the tsv files a web interface has been developed on top of the bip4covid19 data 7 its aim is to 107 facilitate the exploration of covid-19-related literature the option to order articles 108 according to different impact measures is provided this is expected to be useful since 109 users can better prioritise their reading based on their needs for example a user that 110 wants to delve into the background knowledge about a particular covid-19-related 111 sub-topic could select to order the articles based on their influence on the other hand 112 another user that needs to get an overview of the latest trends in the same topic could 113 select to order the articles based on their popularity the information shown to users per publication includes its title venue year and 115 the source dataset where it was found moreover each result is accompanied by color 116 coded icons that denote the publications importance based on each calculated impact 117 measure in this way the users can easily get a quick insight about the different impact 118 aspects of each article the tooltips of these icons provide the exact scores for each 119 measure each publication title functions as a link to the corresponding articles entry 120 in its publishers website or to pubmed finally a page containing interesting statistics 121 is provided this page contains various charts that visualise for example the number of 122 articles per year or the number of articles that have substantial impact based on each 123 of the provided impact measures per year to ensure the proper integration and cleaning of the cord-19 and litcovid datasets 126 we rely on ncbis etool suite in particular we collect pmids and pmcids from both 127 datasets and use them as queries to gather each articles metadata after cleaning the 128 article title eg removing special characters we automatically identify duplicates by further to guarantee the correctness of the compiled citation graph we apply the 134 following procedures after gathering all citing -cited records using ncbis etools 135 those that include identifiers not found in the source data are removed since many 136 citing -cited pairs may have been found both with pmids and pmcids the resulting 137 data may still contain duplicate records these records are removed after mapping all 138 pmidspmcids to custom identifiers with pmid-pmcid pairs that refer to the same article 139 being mapped to the same identifier the final resulting citation graph is based on 140 these mapped identifiers as an extra cleaning step any links in the graph that denote 141 citations to articles published at a later time than the citing article are removed 8 to ensure that we retrieve a set of tweets about each article that is as comprehensive 143 as possible we collect not only the urls in doiorg pubmed and pmc but also the 144 url to the article in its publishers website where possible these latter urls are very 145 important since they are widely used in tweets to collect them we utilize doiorg 146 redirections to avoid incorrect tweet counts due to duplicate tweets we used a simple 147 deduplication process after the tweet object retrieval moreover the use of the unshrtn 148 library to expand the short urls from tweet texts ensures that our measurements 149 derive from all available url instances of each publication record no matter how they 150 were shortened by users or twitter the following limitations should be taken into consideration with respect to the data 152 while we take effort to include as many articles as possible there are many cases where 153 our source data do not provide any pmids or pmcids as a consequence no data for 154 these articles are collected and they are not included in the bip4covid19 dataset our data are available in files following tsv format allowing easy import to various 165 database management systems and can be conveniently opened and edited by any text 166 editor or spreadsheet software we plan to update the data regularly incorporating any 167 additions and changes from our source datasets as well as to expand the tweet counts 168 based on all available data for 2020 additionally we plan to incorporate any further 169 sources on coronavirus related literature that may be released and which will index the 170 literature based on pmids andor pmcids the contents of the bip4covid19 dataset may be used to support multiple 172 interesting applications for instance the calculated scores for each impact measure 173 could be used to rank articles based on their impact to help researchers prioritise their 174 reading in fact we used our data to implement such a demo as previously described additionally the rank scores may be useful for monitoring the research output impact of 176 particular sub-topics or as features in machine learning applications that apply data 177 mining on publications related to coronavirus we presented bip4covid19 an openly available dataset providing impact scores for 180 coronavirus related scientific publications our dataset can be potentially useful both 181 8 such references to future articles are often observed in citation data due to various reasons hence a common practice is to remove them 6  for researchers in need to prioritize their reading as well as for in various applications 182 eg applications using impact scores as machine learning features we have 183 additionally built on our dataset providing a web interface that allows for the ordering 184 of coronavirus literature based on the various impact measures finally our dataset is 185 regularly updated   the imex coronavirus interactome an evolving map of coronaviridae-host molecular interactions l perfetto c pastrello n del-toro m duesbury m iannuccelli m kotlyar l licata b meldal k panneerselvam s panni n rahimzadeh s ricard-blum l salwinski a shrivastava g cesareni m pellegrini s orchard i jurisica hh hermjakob p porras   severe acute respiratory syndrome or sars emerged as a life-threatening viral disease of unknown origin in late 2002 in the guangdong province of southern china caused by the sars-cov-1 virus 1 the severe acute respiratory syndrome coronavirus 2 sars-cov-2 is a related virus responsible for the current outbreak of coronavirus disease 2019 covid-19 2 as of june 2020 over 76 million people globally have been shown to be infected with the virus with more than 426000 deaths directly attributed to its effects john hopkins university httpscoronavirusjhuedumaphtml the covid-19 pandemic has resulted in massive scientific efforts attempting to fight the disease and understand the biology of the virus this has derived in enormous challenges for the research scientist when attempting to find and select information relevant to specific areas of viral biology and pathology in order to aid the scientific community and expedite drug and vaccine development multiple data curation efforts have been undertaken to perform a critical assessment of the literature and represent different aspects of the virus and the disease in a structured and computationally accessible manner one recent example of such efforts is the covid-19 disease map 3 a community effort to capture the intricate aspects of sars-cov-2 biology as reusable and interoperable pathway maps so they can be used in systems biology and modelling pipelines identification of virus-host interactions and the analysis of the topological structure of a relevant molecular interaction network is a necessary step in enabling an understanding of the cellular mechanisms involved in a biological process such as viral infection of a cell a detailed map of the interactions between human and pathogen proteins will aid a more complete awareness of the mechanisms of infection and subsequent viral replication assembly and release and may help to identify novel drug targets or assist in rapid and more accurate repurposing of existing drugs for treating or preventing infection further to this such networks can be used to study data such as changes in the transcriptome or proteome of a virally infected cell when compared to normal co-regulated genes or proteins which also co-cluster in this network may indicate that these entities are involved in the same biological process or are members of the same functional complex for such a network to be of value to the researcher a certain amount of meta-data needs to be provided which enables the assessment of network quality and data types these data need to be supplied in a standardized and computer-accessible format which allows for scoring filtering and selection the imex consortium 4 has been providing such data for over 15 years supplying experimental details using controlled vocabulary terms captured using a detailed curation model all aspects of an interaction experiment are described including host organism interaction detection and participant identification methodologies and full details of the constructs such as binding domains and the effects of site-directed mutations current membership of the imex consortium includes the intact 5 mint 6 dip 7 uniprot 8 matrixdb 9 and iid 10 data resources who collaborate to provide the users with a single consistent viral-host dataset to work with when a novel virus emerges as was the case with sars-cov-2 in 2019 the study of closely related species such as sars-cov-1 and other coronaviruses may help with this process giving the scientific community time to produce species-relevant data for this reason the network includes data on all coronaviruses available in the scientific literature whilst primarily consisting of protein-protein interaction data the network also contains interactions with lipids glycosaminoglycans and rnas again curated to imex standards the data is fully open under a cc-by 40 license and downloadable from the intact website and ftp in psi-mi standard tab-delimited and xml-based formats a brief description can be found under wwwebiacukintactresourcesdatasets and a collection of interactive network representations of the dataset at the time of writing is available at httpwwwndexbioorgnetworkset4c2268a1-a0f0-11ea-aaef-0ac135e8bacf the dataset will be expanded and updated with every intact release as of june 2020 the dataset contains 1778 unique interacting molecule pairs represented in 2212 binarized interactions extracted from 86 publications 5 of which are pre-prints from biorxiv the dataset can be downloaded from the intact ftp site in psi-mi standard xml-based formats psi-mi xml 25 ftpftpebiacukpubdatabasesintactcurrentpsi25datasetscoronaviruszip and 30 ftpftpebiacukpubdatabasesintactcurrentpsi30datasetscoronaviruszip it can also be browsed on the intact webpage wwwebiacukintactqueryannotdatasetcoronavirus where it is available for download in additional formats such as the tab-delimited psi-mi-tab 27 a brief dataset description can be found under wwwebiacukintactresourcesdatasetscoronavirus the data can be searched in the imex psicquic 11 service and on both the imex consortium webpages wwwimexconsortiumorg and via the virusmentha 12 browser at httpsvirusmenthauniroma2it interactive network representations for unique molecule pairs and full evidence details can be found in httpsdoiorg1018119n9mp4s and in httpsdoiorg1018119n9rc8f as with all data hosted in intact the dataset is freely available under a cc by 40 license the data refers mostly to ppis 1674 interactions plus some interactions involving different rnas 67 interactions or small molecules 37 interactions while data on 70 organisms are included suppl table 1a most interactions refer to sars-cov-2 and sars-cov-1 -human interactions 992 and 351 unique interactions respectively imex consortium curators have collated interaction evidence from scientific articles and pre-prints using the following selection criteria
the publication contains interactions involving proteins from any virus member of the coronaviridae family ncbi taxon id 11118 this includes not only sars-cov-1 and sars-cov-2 but also mers-cov and members of the family that infect other mammalsthe publication contains interactions of human proteins with established relevance for sars-cov-2 life cycle eg ace2 interactions have been included in the datasetevery interaction described in these publications is curated and included in the dataset even if their relevance to covid-19 might seem limited this results in the inclusion of data from apparently irrelevant species eg yeast that is of interest from the phylogenetic and evolutionary point of viewpre-prints are considered and if deemed appropriate curated when containing sars-cov-2 data an exception to imex practice of representing only peer-reviewed research this reflects the interest these data have engendered during the pandemic these datasets are clearly marked as pre-publication and will be re-curated if necessary when published the imex curation model captures the details of specific constructs used for the detection of interactions this allows for the representation of different construct-associated features such as specific mutations affecting interaction outcome 13 or sequence regions that are associated with binding most of these in-depth studies are centered around the spike-ace2 interaction for sars-cov-2 and sars-cov-1 the only other sars-cov-2 mutation reported so far is nsp5  3c-like proteinase pcys145ala catalytically inactive mutant which exhibits an interaction profile different from the canonical form in an ap-ms study 14 there are a few other mutations reported for sars-cov-1 and in other members of the coronaviridae family but it is clear that specific variation effects on coronavirus-related interactions is an area requiring extensive exploration as shown in figures 1b and 1c fragment constructs have been used in more studies than mutations although some of these might have been designed for convenience reasons eg constructs that are easier to express in heterologous systems interactive network representations highlighting mutations and binding regions can be found in httpsdoiorg1018119n9w590 and in httpsdoiorg1018119n90w3w regarding interaction data generation approaches the experimental setup used to detect an interaction is summarized by three key fields in the imex curation model the method used to determine that an interaction is happening interaction detection method the method used to determine which molecules are involved participant detection method and the biological environment in which the interaction occurs host organism each of these is described by an appropriate controlled vocabulary term making the data readily searchable in the coronavirus dataset the overwhelming majority of data were generated by affinity-purification combined with mass-spectrometry approaches performed in hek293t or hela cells suppl table 1b this type of data produces sets of potential interacting partners preys associated with a bait of interest but does not directly identify binary interacting partners also the data needs to be automatically expanded into binaries for its representation in tabular or network graph form this accounts for a large proportion of spoke-expanded binary relationships found in the dataset 1310 out of 2212 interactions 59 are expanded binaries while only 43 of the binary interactions found in the full intact database are expanded binaries most content has been curated after declaration of the covid-19 pandemic on 11032020 figure 1 the data growth timeline shows three jumps in interaction numbers two for sars-cov-2 and one for sars-cov-1 due to curation of high-throughput ht studies not available for other coronaviridae figure 1a this is also reflected in the pattern of data growth regarding detailed information about mutations and binding regions figure 1a the plots also illustrate that at this point there are more studies dealing with detailed interaction data for sars-cov-1 a situation that will likely change as more sars-cov-2 studies are performed interaction data is derived from both small-scale studies focused on one or just a few interactions low-throughput lt and large-scale screenings able to detect hundreds or thousands of interactions in a single experiment ht the extreme urgency in the study of sars-cov-2 has resulted in a strong dominance of high-throughput data for this species in comparison with the other members of coronaviridae reported in the dataset figure 2a additionally sars-cov-2 small-scale studies are mainly focused around the ace2-spike interaction due to its relevance for virion recognition and infection for the remaining interactions most sars-cov-2 data comes from two studies gordon et al 14 and li et al 15 both focused on affinity purification techniques combined with mass-spectrometry detection of interacting candidates ap-ms in hek293t cells transfected with sars-cov-2 proteins the studies are distinct showing virtually no overlap figures 2bd and different degree distribution patterns figure 2e gordon et al shows an unexpected pattern of fully isolated components likely due to stringent target selection lack of overlap between different high-throughput interaction datasets is a long-recognized phenomenon since different experimental approaches are better suited to detect interactions featuring specific physico-chemical characteristics and protein abundances among other parameters 1618 even in this case where the approach used is broadly similar they likely reflect methodological differences ap-ms datasets are very sensitive to protein abundances and affinities as well as to the selection and orientation of tags and expression systems additionally strong differences can arise during the selection of bona fide interactors where multiple strategies can be used in order to clean up spurious detections the lack of common interactions between the different studies focused on sars-cov-2 suggest that more of these systematic experiments are needed to increase reliability of biological conclusions extracted from this type of data exploring the biological context of sars-cov-2 interactors suggests that the two ht studies are complementary pathway enrichment figure 3 and supplementary table 2 finds commonalities on expected pathways related with cell cycle response to stress and infectious disease and dna and rna synthesis and processing key pathways such as innate immune response or cytokine signaling are only found in one ht study along with several intracellular signaling and metabolic routes only sars-cov-1 shows appreciable enrichment in lung tissue while none of the sars-cov-2 sets reach the 5x enrichment threshold suggested by jain  tuteja 19 supplementary figure 1 supplementary table 3 finally sars-cov-2 interactors from both studies are found as components of histone deacetylase exosome and atp-ase transmembrane complexes supplementary table 4 including those represented in only one of the studies we see an abundance of complexes involved in endosome and exosome generation mitochondrial metabolism protein production ca2-dependent cell signaling and cell cycle control development of the imex coronavirus dataset complements other curation efforts that have been initiated in the light of the pandemic the dataset has been used by the disgenet database 20 for contextual annotation with related diseases httpswwwdisgenetorgdownloadssection9 also members of the imex consortium are involved in the covid-19 disease map initiative httpscovidpagesunilu where interaction information from the dataset is guiding covid-19-related pathway curation as an example the list of pmids from imex coronavirus dataset has been used to screen papers containing causal interactions to build covid-19 causal network perturbed during sars-cov2 infection by the signor 20 resource 21 httpssignoruniroma2itcovid and to select the gordon et al human interactors to integrate in the signor 20 network we are also involved in a parallel effort curating sars-cov-1 and sars-cov-2 related protein complexes in the complex portal wwwebiacukcomplexportal linking available experimental interaction evidence when possible this initiative is especially relevant because as previously stated coronaviruses increase the number of functional proteins produced by the viral genome by post-translational cleavage of long polypeptide transcripts the functionality andor stability of these proteins is further increased through the formation of protein complexes all of which have been catalogued into the complex portal to date 12 complexes have been identified in each strain formed by viral-viral protein interactions including homomeric assemblies such as the dimeric sars-cov-2 main protease complex cpx-5685 other reference entities such as the sars-cov-2 spike - human ace2 receptor complex cpx-5683 have also been created to enable their identification in large -omics datasets all complexes have been annotated with gene ontology terms describing their role in the virus lifecycle which again will assist in the analysis of large omics-derived datasets ongoing work includes the addition of the mers virus complexosome to the available data accurate and detailed representation of biological insight into public databases is a fundamental source of data for scientific discovery even more so in a situation of accelerated research work such as the current pandemic our curation of molecular interactions related to coronaviridae enables a systematic perspective of this data and greatly increases its interoperability from our overview analysis of the data available so far we can highlight how the sars-cov-2 data seems to be both biologically relevant and thus informative for research on the disease and strongly preliminary so full consideration for its inherent incompleteness should be given when using it the imex consortium is expanding the resource as new data becomes available striving to provide the most accurate possible picture of the coronaviridae interactome all stats and details shown on this manuscript are derived from the intact database release of 2020-04-30 complex portal data used for human complexes overlap checks was also obtained from the same data release specific analyses include the following datasets sars-cov-2 - all human targets of sarscov-2 viral proteins sars-cov-1 - all human targets of sars-cov-1 viral proteins gordonlt - all human targets of sars-cov-2 viral proteins derived by gordon et al plus selected sars-cov-2 low throughput studies lilt - all human targets of sars-cov-2 viral proteins derived by li et al plus selected sars-cov-2 low throughput studies the selected low-throughput studies added to gordon and li studies represent just two representative interactions that were not detected in either study ace2-spike and bsg-spike so they were not analysed separately formatted subsets are available in supplementary table 5 analyses were done using r 400 22 and the r datatable package 23 venn diagrams were created using ggvenndiagram 03 24 all other plots were created using ggplot2 330 24 and wesanderson 25 packages analysis was performed using tissueenrich 19 180 r package on human protein targets using protein atlas 26 expression data background was the entire proteome as listed in the human protein atlas website wwwproteinatlasorg only hits with a fold change above zero are shown log10 p-value was zero for all tissues full data is available in supplementary table 3 pathway enrichment analysis was performed using pathdip 4 27 api in r with literature curated set only pathways with false discovery rate 001 bh-method were considered as the majority of enriched pathways 75 for sars-cov-1 80 for sars-cov-2 72 for gordonlt and 76 for lilt were from reactome database 28 we further organized them using level one reactome ontology to create the figure all enriched pathways are listed in the supplementary table 2 and pathways present in multiple sets are highlighted in the overlap tab  covidiagnosis-net deep bayes-squeezenet based diagnostic of the coronavirus disease 2019 covid-19 from x-ray images ferhat ucar deniz korkmaz   coronavirus outbreak continues to surprise the world to date over one million people across the two hundred countries have been infected according to the last updates of the world health organization who approximately sixty thousand confirmed deaths among the cases are reported 1 humanity had not faced a pandemic through the history that spreads rapidly all over the earth if a defined brand new virus is able to spread from person to person while infecting the contacts easily with a sustained and efficient way then it is called a pandemic the novel coronavirus 2019-ncov fulfills all those definitions strongly at the end of the year 2019 wuhan city of china cradled the first case of the novel coronavirus now from europe to america its deadly effects threaten the whole world the who named the 2019-ncov epidemic disease on february 11 2020 as coronavirus disease covid-19 2019-ncov is a new member of the severe acute respiratory syndrome coronavirus family sars-cov and labeled as sars-cov-2 2 with its spike surface for binding to receptors see figure 1
 sars-cov-2 presents the covid-19 with the symptoms of fever sore throat and following pneumonia with severe acute respiratory distress 3 for all that the respiratory symptoms are not in a specific form there are so many isolated cases ie the existence of the covid-19 may not appear at the first clinical symptoms 4 the rapid spreading nature of the coronavirus and the serious respiratory effects to humans make the diagnosis of the covid-19 an urgent situation 5 today health specialists use the reverse transcription polymerase chain reaction rt-pcr test for the detection of the nucleic acid forms stem from the sars-cov-2 in the process the respiratory specimens such as oropharyngeal swabs or nasopharyngeal sampling are collected and the very important issue when doing this is the receipt place of the specimens the operation is categorically open to malfunctions by the expert mistakes 6 besides the operation procedure the pcr test is a time-consuming process because a patient has to be isolated in non-suitable circumstances for hours until getting the test results in addition these types of tests have a low detection rate of between 30  50 hence most of the times they need to be repeated to make a confirmation 7 to be able to procure an atmosphere where the patients could get quick treatment and care is a crucial task in the fight of the covid-19 because of the fast-spreading essence of the pandemic patients apply to the health center in batches at this point the need for rapid diagnosis methods is a very important issue for monitoring the sars-cov-2 infections to diagnose there is another option of visualization using the radiological images for instance chest x-rays or computed tomography ct former studies prove that covid-19 causes abnormalities that are visible in the chest x-rays and ct images in the form of ground-glass opacities with a strong suggestion a diagnostic with radiological images could be a first step in monitoring the covid-19 9 although the radiological images based diagnostic is a faster way and also it has some advances over the pcr testing in terms of the detection rate in earlier stages of the covid-19 the backbone of the system is the need of experts in comprehending the images intrinsically artificial intelligence ai based diagnostic options can encourage the experts to gain a rapid and accurate explication over the x-ray images on the way of the detection of the covid-19 10 for this motivation there are several studies in the literature including the analysis conducted on ai-based diagnostic of the covid-19 with the help of the radiological images 11 12 13 14 15 in 15 the authors propose a transfer learning model that processes a dataset including the ct images of the covid-19 infected patients they obtain a test accuracy of 793 the study 14 indicates a three-class model that can distinguish the covid-19 influenza-a viral-based pneumonia and healthy cases the segmentation-based study reaches the 867 accuracy value with the ct images dataset in 13 the authors propose a rapid ai development cycle using a deep learning-based ct image analysis heretofore the mentioned studies in the literature use non-public datasets through developing a deep learning-based diagnostic of the covid-19 the studies 11 12 provides public datasets including the covid-19 x-ray images of infected patients in 11 the authors propose a combined public dataset besides a deep learning model called covid-net for the detection of covid-19 covid-net architecture relies on a tailored convolutional neural network cnn model which uses the chest x-rays as inputs the authors reach a test accuracy of 924 with restricted covid-19 class images in our study we use the same dataset of the 11 to be able to outperform the existing covid-net accuracy in detecting the covid-19 in addition there are several more studies that we can consider in covid-19 detection using chest x-rays 16 17 18 19 with a detailed pre-processed dataset our study captures the flag with a specially designed deep learning model the usage of deep learning models in medical image processing and analysis is a challenging topic in the ai field 10 20 in 21 the authors propose a cnn model for pneumonia detection the authors of the study 22 propose a vessel extraction from the fundus images in 23 an expert system is proposed for brain tumor detection in high-resolution brain magnetic resonance images to this end in our study we use a specially designed deep learning model called squeezenet first proposed in 24 the proposed deep learning model for the diagnostic of the covid-19 is based on squeezenet architecture as because it has a smaller structure compared to the well-known pre-trained network designs 25 26 in this study we introduce a covid-19 detection ai model covidiagnosis-net based on deep squeezenet with bayes optimization with a view to obtain a higher accuracy rate the hyper-parameter optimization of the deep learning models is a crucial task 27 the backbone of the proposed model ie the dataset is a public dataset that is detailed in 11 differently from the study 11 we perform a multi-scale augmentation process to overcome the imbalance problem of the proposed public dataset since the focus is on covid-19 diagnosis we perform a detailed offline augmentation process for the limited number of covid-19 infected chest x-rays of the patients with the help of the offline well-defined augmentation process and bayes-squeezenet our proposed diagnostic model for covid-19 outperforms the covid-net 11 while reaching a test accuracy of 0983 in building our model we first perform a detailed augmentation after obtaining the augmented dataset the data split is generated on the shuffled database to form the train validation and test datasets we manage the training process of the deep squeezenet while performing a bayes optimization with the validation phase at the same time through the training the best model is determined and the final network design is tested with the separate test dataset package on through those developments the proposed deep bayes  squeezenet obtains a higher detection rate in the diagnosis of the covid-19 using the chest x-ray images herein we can describe the contributions of our proposed model as listed below1presents a novel model for the rapid diagnostic of the covid-19 based on deep bayes-squeezenet called covidiagnosis-net2overcomes the imbalance problem of the public dataset a multi-scale offline augmentation is performed3proposes an easy to implement deep learning network for embedded and mobile systems that could aid the health experts for a stable diagnosis of the covid-19 in the control of the current epidemic
 the composition of the rest of the article is as follows section 2 describes the materials and methods with details of the proposed deep bayes-squeezenet along with the model components ie squeezenet architecture bayesian optimization and dataset description section 3 presents the explanation of what we design in experiments with evaluation criteria findings and a comparison sub-section to draw the big picture of where our study stands among the other state-of-the-art methods finally section 4 briefs a conclusion of the study the overall architecture of the deep bayes-squeezenet based rapid diagnostic system is presented in figure 1 the proposed system is composed of three main stages as offline augmentation of the raw dataset training of the bayesian optimization-based squeezenet model and decision-making of the network with the testing phase the proposed method classifies the three-class x-ray images labeled as normal no infection pneumonia bacterial or none-covid viral infection and covid covid-19 viral infection in the first stage the offline augmentation method is utilized to the raw input x-ray images due to their uneven sample distributions this method is preferred for smaller classes with fewer sample numbers in order to increase the size of the classes by a transformation factor after the augmentation the augmented dataset is divided into three subsets as train validation and test sets train and validation sets are set as the input of the training and optimization stage the test set is used for the testing input in the training and optimization stage the squeezenet convolution network is utilized and it uses the squeeze and expands layers of the fire modules to construct a smaller and more effective cnn architecture squeezenet is a pre-trained cnn model and it is pre-trained on the ilsvrc-12 challenge imagenet dataset 30 31 this supporting dataset is completely different from x-ray images and the squeezenet model needs to be fine-tuned to classify the covidx classes in order to obtain the best decision-making model the cnn network is optimized with the bayesian-based method which is a sequential design strategy during training a validation error is used to update the optimization process finally the best squeezenet model is obtained and used for the decision-making process with the test set the obtained best network model classifies the infection classes and classification performances are determined squeezenet is a convolution network that executes better performance than alexnet with 50x fewer parameters 23 24 30 squeezenet consists of fifteen layers with five different layers as two convolution layers three max pooling layers eight fire layers one global average pooling layer and one output layer softmax the architecture of the network is given in figure 2
 as shown in figure 2 kk notation represents the receptive field size of the filters s denotes the stride size and l is the feature map length respectively the input of the network has 227227 dimensions with rgb channels the input images are generalized by convolution and max pooling is applied convolution layer convolutes between the weights and small regions in the input volumes with 33 kernels each convolution layer performs an element-wise activation function as the positive part of its argument squeezenet utilizes from the fire layers which constructed of squeeze and expansion phases between the convolution layers the output tensor scale and input of the fire are consistent the squeeze phase uses the filter of size 11 whereas expansion uses the filters of size 11 and 33 firstly the input tensor hwc passes through the squeeze and the number of convolution is equal to c4 of the number of input tensor channels after the first phase the data passes through the expansions and depth of the data is expanded to c2 of the output tensor depth both squeeze and expansion phases are connected to the relu units the squeeze operation compresses the depth and expansion increases the depth by keeping the same feature size finally expansion outputs are stacked in the depth dimension of input tensor with concatenate operation figure 3
summarizes the fire layer and sub-operations assuming fm and c define the feature maps and channels the output layer fy of the squeeze operation with the kernel w can be expressed as 321fyfm11fmc1cwcfxcfm1
 here fyrn and wrc1fm2 the squeeze outputs can be defined as a weighted combination of the feature maps of the different tensors in the network max pool layers execute a down-sampling operation along the spatial dimensions and global average pool convert the feature maps of the classes into one value at the end of the network softmax activation function gives the multiclass probability distributions 
table 1
presents the detailed layer configuration of the squeezenet architecture the motivation for designing the squeezenet architecture in covid-19 diagnosis is that the network provides three main advantages 23 24 1 the network is more efficient because it has fewer parameters 2 applications developed for this network are easy to move and require less communication 3 it has a model size of less than 5 mb and it is easy to implement to embedded systems hyperparameters have a key role in both machine learning and deep learning algorithms inasmuch as those parameters are tightly managing the acts of the training algorithms and they affect the performance of the models significantly therefore the optimization of hyperparameters is a crucial task especially when it comes to deep learning in medical image processing in general there exist two ways of hyperparameters optimization called manual and automatic searching the manual searching as the name suggests looks for the hyperparameters by hand hence manual searching requires expertise unfortunately when dealing with big data and so many model parameters for tuning even expertise may be insufficient 27 33 to handle the difficulties of manually searching automatic searching alternatives take place in the literature grid search and random search algorithms can be considered in this topic nevertheless there are still problems remaining in both methods such as the curse of dimensionality and unavailability of the highly efficient performance with the time-consuming operations 27 34 tuning of hyperparameters is such an optimization problem that the objective function of it is latent and unknown in other words it is a black-box function as its name suggests stemming from the bayesian theorem the bayesian optimization is an efficient algorithm dealing with such kind of optimization problem 27 35 bayesian optimization relies on a typical kind of approximation dealing with an unknown function requires an approximation with the help of some known samples ie prior knowledge it is like the concept of the posteriori probability here the food of the algorithm is observations generated by the model evaluations in which the outputs of the online learning this means that in bayesian optimization we need a training process during the training the model will trace a function that we only have its knowledge from the learned data in the center of the bayesian optimization algorithm the main purpose is to obtain the related hyperparameters that make learning outline maximum 36 in mathematical expression we can consider a global maximization or minimization problem of the black box unknown functionf2xargmaxxxfx
 here x stands for a searching space ofx caused by the nature of the bayes theorem 35 bayesian optimization calculates the posteriori probability pdl of a model d with the aid of the learned data l posteriori probability is proportional to the likelihood pld of observations l and the multiplication of the prior probabilitypd3pdlpldpd
 equation 3 reflects the main behavior of the bayesian optimization 27 in brief bayesian optimization searches for the best model amid many of them at this point one can recall the cross  validation method however it is very hard to find the best model in many samples of pre-listed hundreds of alternatives thus bayesian optimization accelerates the operation by reducing the computational cost and we do not need expertise to guess the outputs though 37 the algorithm combines the prior distribution of thefxfunction with the samples of the prior knowledge to obtain the posteriors those posteriors calculate the value which describes the maximization point of thefx herein the criterion of the maximization process is the expression called acquisition function we introduce a pseudo-code format of bayesian optimization via algorithm 1 table in the algorithm n1i-1xnynn1i-1 reflects the training dataset which includes i-1 observations of the ffunction in the flow we can clarify the two basic parts of the algorithm 1 it updates the posterior distribution and 2 it maximizes the acquisition function bayesian optimization process continues repeatedly until the defined maximum iteration value is reached alternatively it can also be quitted when it catches a threshold value which is the difference between the actual value and the obtained optimal value 27 36in the proposed deep bayes-squeezenet model the most important hyperparameter of the deep network design called initial learning rate is optimized beside the l2-regularization and the momentum values we also provide a validation dataset to be able to track the validation error object function in the online training please find the experiment details of the bayesian optimization in section 3 as declared before the general detection method for covid-19 disease is the rt-pcr testing that identifies sars-cov-2 rna from sputum or nasopharyngeal swab however rt-pcr testing has a long time complex process and it is very troublesome 6 another detection method is chest radiography imaging due to the abnormalities in chest x-ray images of patients infected with covid-19 2 11 therefore we have selected a distinctive and public dataset including chest x-ray images to respond to the need for a rapid disease diagnosis system 11 in order to compose a special covid-19 dataset two different publicly available datasets were combined as covid chest x-ray dataset 12 and kaggle chest x-ray pneumonia dataset 38 the obtained covidx dataset 11 consists of a total of 5949 posteroanterior chest radiography images for 2839 patient cases the dataset includes 1583 normal 4290 pneumonia and 76 covid-19 infection cases in the pneumonia samples diseases were caused by none-covid-19 viral and bacterial effects considering the number of cases there are a total of 1203 uninfected normal patients 1591 pneumonia cases with none-covid-19 and 45 covid-19 patient cases the dataset includes three classes and figure 4
shows a batch of images that are randomly selected from class samples the images are transformed to rgb with 8-bit depth and have variable pixel-based resolution values the main purpose of the selection of covidx dataset is that it is public available so it is accessible for researchers and to be extensible therefore further studies based on this database may be more helpful in the diagnosis and treatment of covid-19 cases in the classification process of both classical machine learning and deep learning algorithms the imbalance ratio of the class distribution of the dataset has a huge impact on the performances of the models in the study 39 the authors conduct systematic research on how imbalance data affects the classification performance of cnn the findings of the study point out a detrimental effect of the imbalanced class distribution on classification performance in our dataset there are very few covid-19 class images compared to the other classes to overcome this unfavorable situation we perform a detailed offline augmentation over the covid-19 class images in our dataset firstly we obtain the mirrored version of the original images by flipping each image then the listed augmentation technics are applied to both original and flipped images1
noise adding gaussian noise to images2
shear shearing the images in affine form3
brightness decrease decreasing the brightness of the images by subtracting 30 from every pixel4
brightness increase increasing the brightness of the images by adding 30 to every pixel
 as figure 5
describes we obtain twelve different images from a single image with the aid of the augmentation techniques and their combinations the same operations depicted in figure 5 are also implemented to the flipped images which are the mirrored versions of the original images at the end of the day we gain twenty-four different images for a single image thus the number of images in the covid-19 class is augmented offline in the pre-processing of the dataset resulting in an acceptable amount figure 6
shows a sample image both in original and flipped mirrored version in order to evaluate the quantitative performance of the proposed method such evaluation metrics accuracy acc correctness cor completeness com specificity spe f1 score and matthew correlation coefficient mcc are statistically computed from the confusion matrix acc measures the classification performance cor gives the rate of the truly classified x-ray images among the classes while com defines the truly detected negative images spe represents the correctly classified the rate of opposite disease classes f1 is a harmonic average and gives the combination of cor and com mcc measures the quality of the classification performance according to the confusion matrix the selected evaluation metrics are defined as4accntpntnntpnfpntnnfn
5corntpntpnfp
6comntpntpnfn
7spentnntnnfp
8f12corcomcomcor
9mccntpntn-nfpnfnntpnfpntpnfnntnnfpntnnfn
 here ntpntnnfpnfn define the number of correctly classified diseases number of correctly classified opposite classes number of incorrectly classified diseases and number of the misclassified diseases respectively the classification procedure proves and determines the robustness effectiveness and generalization ability of the proposed method using the aforementioned evaluation metrics in the experimental setup firstly we perform an offline augmentation to the raw covidx dataset after the pre-processing the augmented dataset is divided into three packages as training validation and testing sets the triple split of the dataset packages is formed as 80 for training 10 for validation and 10 for testing training and validation datasets are designed for the bayesian optimization-based online learning structure as because of the bayesian contribution of our model it needs a validation result to minimize the objective function error after the bayesian optimization-based online training process we reach the best network model to implement the testing phase with a separate test dataset the obtained best model is evaluated all the input images are resized to 227227 pixel size and transformed to rgb with 8-bit depth in the meantime all the dataset packages are shuffled to overcome the negative effect of the overfitting thus we reach a robust decision-making performance for the classification of the infected patient cases in the training process mini-batch size is given as 32 and all images are normalized with the mean subtracting operation 
table 2
shows the class distribution of the raw and augmented dataset in the pre-processing we achieve 1536 images after the augmentation of covid-19 class since other classes have sufficient images each we perform the augmentation to just covid class we also provide a balanced dataset fixing the all class image numbers to 1536 samples to gain a robust training performance of the model briefly our offline augmentation model enhances the covid class approximately 20 times in our proposed model we improve the existing dataset by increasing the covid class images proposed deep bayes-squeezenet includes the bayesian optimization in the training stage with validation process the objective function of the optimization process is given in figure 7
 it can be seen that function evaluation ends with 35 iterations because of the model saturation at the end of the 10th iteration the minimum observed objective is achieved to construct the best model the optimized parameters ie initial learning rate initiallearnrate momentum and l2 regularization are listed in table 3
along with iterations model result run time and observed - estimation values of the objective function during the optimization process it is clearly seen that after catching five different best models bayesian optimization points out the model of the 10th iteration as selected best model after the training process the obtained best model parameters are used in the proposed deep bayes-squeezenet network and highlighted in table 3 in order to evaluate the effectiveness of our augmentation improvement we first present the raw dataset results here our aim is to prove the negative effect of the imbalance distributions in the raw dataset over the performance it should be noted that we tune the squeezenet with the best model parameters for a regular training process the re-training and testing processes are performed with the train and test packages of the related dataset please see table 2 figure 8
demonstrates the confusion matrix of the test process of the re-trained squeezenet in the confusion matrix presentation accuracies and errors of each row and columns are given as the percentage value in the lower and right cells respectively the accuracy rates of each column show us the correctness value of each class and the accuracy values for each row state the single accuracy values of the classes as shown in figure 8 the false classification rate appears majorly within normal and pneumonia pneumonia class achieves nearly the perfect classification whereas covid class has 70 accuracy within 10 test samples in the normal class distribution 141 samples are misclassified as pneumonia this situation shows the negative effect of the imbalanced distribution of the dominant pneumonia class in table 4
 we can see the detailed classification results of the squeezenet for the raw dataset the obtained results show that normal class has the lower values of acc com and f1 as 3889 3889 and 5583 respectively the model with the raw dataset just reaches the 7637 overall accuracy and 7000 single accuracy value of the covid class while the highest accuracy is presented by pneumonia class the lower values of mcc and spe in pneumonia point out poor classification performance the second phase of our experiments is performing the augmented dataset testing process in this phase the proposed deep bayes-squeezenet model which is obtained by the bayesian optimization approach is validated with the separate test dataset it should be noted that the obtained best model is trained with the augmented dataset to overcome the above-mentioned imbalance effects as well as achieving a rapid system for covid-19 diagnosis with a robust and sustainable structure figure 9
presents the confusion matrix of the test phase here we can see a tremendous performance boosting on all classes and overall accuracy the deep bayes-squeezenet model catches all the covid samples in the x-rays perfectly there are just eight misclassified samples among 459 test samples the error rate of the normal class is 2 while it is 33 in pneumonia in addition the most misclassification rate is presented by pneumonia we can interpret the detailed test results from table 5
 in the decision-making system covid class reaches the perfect classification rate as showing the 100 test accuracy and completeness values f1 and mcc values also prove that it is obtained a stable classification the overall accuracy is 9826 with a com of 9826 it draws a picture that our model is well-trained and robust although pneumonia accuracy decreases to 9673 compared to former experiments all other performance criteria of the related class are boosted and exhibit an effective prediction the classification performance of the normal class is visibly enhanced and it reaches to 9804 acc value in order to analyze the performance comparison between the experiments of the raw dataset and the augmented one we report the increase rates of the performance values as in figure 10
 the sharp bounce is experienced in the normal class by a boosting of 25 times as it is the focus of our model when we concentrate on the performance of the covid then we detect 25 times boosting in pneumonia class there is a decreasing percentage of 206 considering just the accuracy value the overall accuracy rate has also a performance increasing at the rate of 2866 the overall accuracy rate has also a performance increase at a rate of 2866 for a detailed visual analysis we provide a class activation mapping images as shown in figure 11
 class activation mapping is a way of generating visual explanations of the predictions of deep learning models misclassified or unreasonable predictions sometimes can rely on reasonable explanations by the aid of the class activation mapping we can investigate useful knowledge of the prediction regions activation mapping also defines the bias regions in the training images the first column of figure 11 defines the original input images while the second column includes the heat map images of the predicted samples when all the class activation mappings are examined in figure 11 a  c the probability values of the predictions are nearly 100 according to the heat maps of the images the trained network distinguishes the classes with an acceptable feature mapping to form an outlier example figure 11 d presents a misclassified class sample ie normal class is confused with pneumonia the probability values are 025 and 074 for normal and pneumonia respectivelyfig 12
 the proposed deep bayes-squeezenet with its low model size is easy to implement in hardware deployments as shown in table 6
 the model size of the proposed network is less than 7731 times lower compared to the alexnet which is the inspiration of the squeezenet architecture the overall experimental results show that proposed model has a significant and robust performance value over the covid-19 patient cases this study proposes a complete and compact solution using the chest x-ray images for rapid diagnosis the coronavirus disease 2019 was announced as an outbreak by who on february 11 2020 1 due to the covid-19 outbreak the early diagnosis of this disease has become a key topic for clinicians and radiologists in the world the ai techniques regarding the image classification approaches can help in early diagnose of the disease considering ai cnn methods achieve better and faster results compared to the traditional diagnosis methods in this paper a rapid robust and efficient covid-19 diagnosis method which is namely deep bayes-squeezenet is proposed the proposed method performs the x-ray images into multiclass as normal pneumonia and covid in order to evaluate the proposed cnn model the general performance comparison of our study with the state-of-art methods is given in this section in the model evaluations the related studies depend on the multiclass classification of the chest x-ray images with various ai techniques table 7
shows the comparison results with the related studies uses the same or similar datasets li and zhu 16 propose a densenet based covid-xpert architecture classifying the three-class chest x-ray images they use transfer learning and obtain an overall accuracy of 0889 wang and wong 11 present covid-net design to the diagnosis of the covid-19 and in their study the main model is based on the tailored cnn machine-driven design is used to improve the model architecture the overall accuracy com and cor metrics of 11 can be listed as 0923 0887 and 0913 respectively the authors also share and collect the covidx dataset used in our study afshar et al 17 introduce a deep learning model based on a capsule network using a four-class dataset their model produces a 0957 overall accuracy farooq and hafeez 18 present a resnet based framework in a four-class dataset with augmentation the model accuracy has remained as 0962 chowdhury et al 19 explain a bundle structure that includes various deep learning models using four different chest x-ray datasets amid the performance metrics that table 8
gives our model outperforms similar studies that use chest x-rays in the diagnosis of the covid-19 although it seems that some of the performance values have been achieved the same with the study 19 the whole performance of the proposed method in our study is better than it in table 8 the performance values of the listed studies are given in terms of covid-19 class accuracy while chowdhury et al 19 have the same overall accuracy with our study the covid-19 class accuracy stays behind the proposed method farooq and hafeez 18 obtain the same accuracy of covid-19 class but our study outperforms it in the overall accuracy in addition the test dataset of the study includes just eight samples of covid-19 to the best of our knowledge the proposed model reveals the excellent classification performance for the covid-19 diagnosis with chest x-rays the proposed model has a great advantage of owning a practical network architecture with a robust and stable operation with its nature of including fewer parameters our network is more favorable for embedded systems among existing deep learning models a rapid diagnosis method has a key role in the control of infectious diseases and pandemic situations like the up to date covid-19 some limitations of the rt-pcr nucleic acid-based test modules reveal a need for fast alternative methods to be able to serve the front-line experts to make them reach a quick and accurate diagnosis in this study we propose an ai-based decision-making system including the recognition of input x-ray images under the roof of a very practical deep learning model this study is an important attempt including an easy to implement deep learning model which has an accuracy performance of 983 among normal pneumonia and covid cases and 100 for the single recognition of covid-19 among other classes in these difficult days of the global covid-19 pandemic our model has a strong potential to build a tool design for covid-19 monitoring we would like to note that that the rt-pcr test method to detect the sars-cov-2 is still important however it is proved that there are also undeniable shortcomings along with the rt-pcr test method which can be listed as follows 1 its possible methodology lacks 2 strict dependence on the level of the disease timing 3 the possibility for collecting the specimens in mistaken localizations and 4 its response time delay 6 7 in our model working with a deep learning-based practical structure the early stage detections of the covid-19 cases could be done to manage and control the pandemic disease in medical image processing while deep learning methods are preferred in many areas it is becoming more and more important especially in the interpretation of radiological images as such our model which is extremely satisfactory even with its initial results opens the door for the implementation of a comprehensive product that can work mobile and appeal to the end-user backbone of our model is the deep bayes-squeezenet decision-making system for the covid-19 diagnosis from x-ray images squeezenet with much less model size is a state-of-the-art deep learning model which is inspired by the well-known alexnet with its practical structure and generalization performance the squeezenet is preferable in the embedded applications we improve the squeezenet structure with bayes optimization algorithm to build a robust and sustainable learning model bayesian optimization helps us to build a best-performed model with a validation dataset the diagnosis system is trained using the public dataset proposed in 11 with its augmented form a separate test which is independent of train and validation sets performs the experiments our experimental results also present the performance boosting of the augmentation contribution to the dataset pre-processing thus model training can be performed with a rich image set of x-rays of covid-19 after comprehensive literature research the up to date studies which use the same or similar public datasets are detected and we evaluate our model with those the proposed diagnosis model for covid-19 using the x-ray images the deep bayes-squeezenet outperforms its competitors we believe that with increased training dataset it is expected to get higher results in further works we aim to plan our model to be able to work mobile appealing to the health care experts for diagnosis of the covid-19 in addition the possibility of presenting this diagnostic system as a solution for other medical image processing cases will also be explored the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper  covid-19 pandemic and the unprecedented mobilisation of scholarly efforts prompted by a health crisis scientometric comparisons across sars mers and 2019-ncov literature milad haghani michiel bliemer c j  during the current century each major coronavirus outbreak has triggered a quick and immediate surge of academic publications on this topic the spike in research publications following the 2019 novel coronavirus covid-19 outbreak however has been like no other the global crisis caused by the covid-19 pandemic has mobilised scientific efforts in an unprecedented way in less than five months more than 12000 research items have been indexed while the number increasing every day with the crisis affecting all aspects of life research on covid-19 seems to have become a focal point of interest across many academic disciplines here scientometric aspects of the covid-19 literature are analysed and contrasted with those of the two previous major coronavirus diseases ie severe acute respiratory syndrome sars and middle east respiratory syndrome mers the focus is on the cooccurrence of key-terms bibliographic coupling and citation relations of journals and collaborations between countries certain recurring patterns across all three literatures were discovered all three outbreaks have commonly generated three distinct and major cohort of studies i studies linked to the public health response and epidemic control ii studies associated with the chemical constitution of the virus and iii studies related to treatment vaccine and clinical care while studies affiliated with the category i seem to have been the first to emerge they overall received least numbers of citations compared to those of the two other categories covid-19 studies seem to have been distributed across a broader variety of journals and subject areas clear links are observed between the geographical origins of each outbreak or the local geographical severity of each outbreak and the magnitude of research originated from regions covid-19 studies also display the involvement of authors from a broader variety of countries compared to sars and mrs  on december 31 2019 an official case of a novel respiratory diseases of the category of coronaviruses named covid-19 was reported in wuhan china marking the beginning of what proved to be one of the direst and most devastating viral outbreaks in the modern history sohrabi et al 2020  this was immediately followed by an unprecedented and swift response of the academic community to address various dimensions of this health crisis and prompted an avalanche of scholarly publications on this topic golinelli et al 2020  haghani et al 2020  kagan et al 2020  in less than five months more than 12000 publications on this topic have already been indexed by scopus with the number increasing figuratively every day in considerable increments torres-salinas et al 2020  and this figure does not even include many more publications available in various repositories including cord-19 colavizza et al 2020  in the form of preprints awaiting peer review by their respective journals such explosion of research on a single topic and the off-the-charts surge in the rate of publications are arguably unprecedented trends in the history of scholarly publications an article published by science on may 13 2020 referred to this phenomenon as one that is among the biggest explosions of scientific literature ever brainard 2020  it highlighted how in the face of this phenomenon it has become extremely challenging for scientists to stay abreast of the latest developments this has made the importance of research synthesis more tangible than ever and has even resulted in the development of several computational research mining tools for this very topic utilising methods such as artificial intelligence ai among such efforts is a research synthesis powered by ai algorithms which has harvested datapoints from a large number the cord-19 articles and categorised them brainard 2020  though the impact of the covid-19 health crisis has marked it as a rather unique milestone in the history disease outbreaks the world prior to this was not a stranger with coronavirus disease outbreaks mcintosh 1974  myint 1994  cavanagh 2005  lim et al 2016  chen et al 2020  prior to 2020 two major outbreaks of this family of viruses had already been reported with at least one of them carrying the official label of a global pandemic  on november 16 2002 the first case of the severe acute respiratory syndrome sars disease was reported in the guangdong province in southern china which by 2003 swiftly spread from continent to continent prompting the world health organisation to declare it as a pandemic in fact sars is known to be the first pandemic of the 21st century cherry and krogstad 2004  nearly ten years later on june 13 2012 the first case of the middle eastern respiratory syndrome mers disease was discovered in jeddah saudi arabia these two constituted the two most major coronavirus outbreaks until covid-19 came along similar to covid-19 though at a much smaller scale each of these previous outbreaks generated a literature of their own kostoff and morse 2011  in the face of the flood of scholarly outputs on covid-19 and along with the conventional review and research synthesis studies chang et al 2020  chen et al 2020  cortegiani et al 2020  scientometric colavizza et al 2020 and bibliometric methods bonilla-aldana et al 2020  hossain 2020 have also gained traction in documenting and analysing the rapid developments of this literature chahrour et al 2020  dehghanbanadaki et al 2020  haghani et al 2020  kumar 2020  le bras et al 2020  here in this work the literatures of these three major coronavirus diseases are disentangled and analysed in a comparative way and from scientometric perspectives the aim is to discover possible similarities and discrepancies across these three segments of the coronavirus literature and to discover whether there are recurring patterns in terms of magnitude temporal evolution and the shape of these three literatures that were each developed in response to a disease outbreak the main focus of the analyses is on keyword co-occurrences bibliographic coupling and citation relations of sources and collaborations between countries to compare the scientometric aspects of the studies on sars mers and covid-19 three separate datasets of publications on these three topics were retrieved from scopus through three separate search strategies the decision on which general database to use eg web of science wos or scopus was mainly made based on the number of indexed covid-19 studies as the sector of the literature that is currently emerging compared to the literatures on sars and mers that are better established at the time of the data retrieval wos had indexed slightly less than 5000 research items on covid-19 while the number of items in scopus neared 12000 given the fact that the scopus database was considerably more up to date in that area this database was set as the main source of data extraction in this work therefore for the sake of consistency the data for sars and mers were also extracted from scopus the search strategies were devised in a way to minimise the possible overlap between the datasets on sars mers and covid-19 and to disentangle the three datasets to the most possible extent preliminary inspection of the literature on each three topics determined a set of distinct keywords that would return the target literature with reasonable specificity and sensitivity in each search key terms associated with the other literatures were combined with the logical operator and not in order to avoid the overlap the lower bound of the time span for each search was set with consideration of the year where the first outbreak of each virus took place the query string associated with each dataset are as follows sars  title-abs-key    severe acute respiratory syndrome or sars  and  coronavirus   or  sars virus or sars disease or severe acute respiratory syndrome disease or severe acute respiratory syndrome virus or sars-cov  and not title- or severe acute respiratory syndrome-2 or mers or middle east respiratory syndrome    and pubyear  2001 mers  title-abs-key    middle east respiratory syndrome or mers  and  coronavirus   or  mers-cov or mers virus or mers disease or middle east respiratory syndrome virus or middle east respiratory syndrome disease   and not title-abs-key   ncov or covid-19 or covid19 or sars-cov or sars-cov-2 or sars or severe acute respiratory syndrome    and pubyear  2011 covid-19 title-abs- key  covid-19 or covid19 or coronavirus disease 2019  or 2019  and pubyear  2018   the search was last time updated on 24 may 2020 where it returned 5907 items on sars  1752 items on mers and 11859 items on covid-19 figures 1 2 and 3 show the distribution  of the studies on sars mers and covid-19  respectively across subject areas figure 4 a also shows the composition of the covid-19 literature in terms of the document types demonstrating that only nearly 50 of the studies on this topic have so far been in the form of full-length articles while letters notes reviews and other document formats constitute a large portion ie nearly half of the literature on this topic at the time of this investigation full records of the three datasets on sars mers and covid-19 were retrieved in csv excel format from scopus all on the same day this included the citation information bibliographic information abstract and keywords funding details and the references the scopus restriction of maximum 2000 document to export posed challenges for the retrieval of the sars and covid-19 datasets whose size were bigger than 2000 documents for the sars dataset the challenge was circumvented by further limiting the search to specific years in separate bundles in a way that the size of each bundle was less than 2000 items therefore allowing us to export the items of each bundle separately the extraction of the covid-19 dataset however posed a further layer of complication given that nearly all studies of covid-19 have been published in one year ie 2020 therefore the year of publication could not be used as a criterion to form a set of mutually exclusive smaller-size exportable bundles for this literature to decompose the search outcome to bundles of 2000 documents or less the following strategy was adopted the document type was used to initially limit the search to mutually exclusive non-overlapping categories first the search was limited to review or short survey or erratum or conference paper or data paper this formed a set of 1267 documents which was extracted in one single export see figure 4 a for details of the number of items within each document type category subsequently the search was set back to original and was limited to notes 1067 items and then to editorial 1270 items with each set of these two subsets being smaller than 2000 they were exported separately there were 2564 documents of letter type this set was further decomposed to two mutually exclusive subsets based on the publication stage criterion 1539 article in press and 1025 final and was retrieved in two separate exports for the remaining 5691 article documents the following strategy was devised of the 5691 items 2944 were article in press and 2747 were final first the 2944 article in press items were considered the list of those studies was sorted as first author a-z and the first 2000 items were extracted in one export then the list was sorted as first author z-a and the first 944 items were exported similar strategy was utilised to extract the remaining 2747 final documents a supplementary search was also conducted on the general topic of coronaviruses using the string title-abs-key  coronavirus  and pubyear  1985 which returned 24620 documents on the same day only the data related to the number of documents by year was extracted for this search the increase continued though at a slower rate to 2004 and was then followed by a gradual decline till 2012 the 2012 mers outbreak triggered another spike in the number of publications on coronaviruses though not as large as that of the sars the intensification of attention to this topic this time lasted for about three years till 2015 before another decline began the spike of coronavirus studies prompted by the covid-19 outbreak however seem to have been occurring at a completely different scale which can be deemed unprecedented in the history of coronavirus studies the number of studies emerged in the first five months of 2020 nears an equivalent of the 70 of the total sizer of coronavirus literature during more than 50 years  in figure 4 c the temporal distribution of the sars mers and covid-19 studies have been shown separately according to the three datasets explained earlier note that the quantities associated with sars and mers are represented by the left vertical axis whereas that of the covid-19 is represented by the right vertical axis with a scale ten times bigger than the scale of the left axis the history of previous coronavirus research has suggested that the number of studies will likely keep rising for at least a few years before it peaks but given the unprecedented magnitude of research and the explosive rate of publications since the begging of 2020 it would be interesting to observe whether this pattern would repeat itself and whether the peak would occur at an earlier or later stage compared to those of the previous outbreaks a question whose answer will only be determined by time  the co-occurrence of keywords associated with the sars mers and covid-19 literature were analysed using vosviewer van eck and waltman 2010 each analysis was performed on the separate set of data associated with the literature of interest the maps of keyword cooccurrences associated with sars mers and covid-19 literatures are provided in figures 5 6 and 7 respectively the minimum number of occurrences for the keywords to be included in the map was set to 5 in all three cases the unit of analysis has also been set to all keywords that includes both author and index keywords and the method of counting was full counting figures a1 and a2 in the appendix illustrate the map associated with the sars literature overlaid respectively with the average year of publication and average number of citations associated with the studies where these keywords have occurred figure a3 and a4 present the counterpart outputs for the mers literature analysis figure a5 is a heatmap of covid-19 keyword co-occurrence and figure a6 overlays the covid-19 keywords map in figure 7 with the colour-coding of the average number of citations given that almost all studies of covid-19 are 2020 items the colour-coding related to the average publication year was forgone in regard to this literature maps of term occurrences based on the analysis of the title and abstract of studies on sars mers and covid-19 have also been presented in figures 7 8 and 9 respectively while the below analysis focuses mainly on the interpretation of the keyword maps similar patterns are by-and-large observable through analysis of the title and abstract terms of these studies with respect to each of the three literatures three distinct clusters of keywords were identifiable these clusters showed certain patterns of commonality across the three datasets each map presents a distinct cluster of keywords that seem to be associable to the studies related to public health emergency management and the prevention of epidemic this would be the red cluster in figure 5 sars the green cluster in figure 6 mers and the red cluster in figure 7  here this is referred to as cluster i in this cluster one can observe terms such as those associated with general public health including wold health organisation public health public health service global health as well as those associated with disease outbreaks including emergency health risk epidemics pandemic outbreak viral diseases virus infection communicable disease transmission travel terms representing measures of emergency severity also appear in this cluster including mortality fatality morbidity infection risk this cluster also includes terms that are linked to the prediction of disease propagation these are terms such as mathematical model modelling simulation statistical model and prediction that have commonly occurred in this cluster the cluster includes terms affiliated with measures of disease control and spread prevention such as socialpatient isolation quarantine hygiene handwashing prevention infection control population surveillance mass screening face mask contact tracing the cluster also represents keywords that attributable to public policy making and social protection such as health care planning health care policy health care quality leadership disaster planning polices the cluster i of keywords also have distinctly and commonly across all three datasets represented keywords that are attributable to the studies on mental health impact of the epidemic these are keywords such as mental health service psychiatry psychology mental stress anxiety fear mental disease these studies have often used methods such as questionnaires and surveys that have commonly reflected in this cluster across the three literatures issues surrounding the safety of medical facilities and medical staff also appear to have been addressed mainly by studies whose keywords are attributable to this cluster these studies have generated keywords such as health care personnel nurses medical staff hospital health care facility personal protective equipment that are distinctly observable in cluster i of keywords across all three datasets the economic aspects of the epidemics also seem to have been addressed particularly by covid-19 as reflected in cluster i of the covid-19 literature these have been reflected in terms such as economics economic aspect which have occurred frequently enough in covid-19 studies for them to appear distinctly on the map the trace of such cohort of studies is however not as clearly identifiable based on the sars and mers maps as is it with respect to the covid-19 literature this could be explained by the greater magnitude of the societal impact of covid-19 outbreak compared to sars and mers the names of the countries and regions have almost invariably appeared in cluster i across all three datasets in certain cases the country names that have occurred most are those from which the outbreaks originated or those that suffered most from the impact of the outbreak for example saudi arabia appears quite distinctly on the cluster i of the mers dataset similarly the occurrence of the names of south-east asian countriesregions such as china hong kong taiwan singapore on the cluster i of the sars map or the term wuhan on the cluster i of the covid-19 map are quite notable the occurrence of the name of the countries also could be a reflection of the early studies with respect to each outbreak that have addressed the local impactsspread of the outbreaks on their own society on the issue of early studies the terms letters editorial and review which have intentionally been kept on the maps seem to also have distinctly occurred in cluster i of each literature which is another indication that this cluster includes early studies that appeared at a time where the amount of data and clinical trials were insufficient for full-length articles an inspection of the figures a1 and a3 does in fact confirm this hypothesis at least in association with the sars and mers literature that the cluster i of keywords represent studies that on average emerged early during the developments of their respective literatures figures a2 a4 and a6 that have illustrated the colour-coding of the average number of citations on the maps also show that although cluster i is associated with the early studies that generally predated studies of the two other clusters and although it represents the largest variety of topics compared to the two other clusters it is also associated with studies that on average been the recipient of a lesser number of citations when compared to the two other clusters this pattern also appears to have commonly occurred across all three datasets a second cluster of keywords associated with each of the three literatures were also discovered that is attributable to the studies on the chemistry and physiology of the virus or viral pathogenesis or in other words the chemical constitution of the virus knight 1954  a part of virology that investigates the biological processes and activities of viruses that take place in infected host cells and result in the replication of a virus for the sars map in figure 5  as well as the covid-19 map in figure 7  this would be the green cluster whereas for the mers map  figure 6  this cluster is red according to the maps the most distinct terms associated with this cohort of virology studies on sars mers and covid-19 are terms such as virus protein virus entry chemistry metabolism physiology pathology cell line virusviral proteins molecular models virus genome virus rna virus replication mutation and enzyme activity as this sector of studies often use animal models terms such as animal cell animal experiment controlled study mice and mouse have frequently appeared in cluster ii associated with each of the three literatures in reflection of the fact that these cohort of studies ultimately seek drug design in addition to generic common terms such as drug designpotencystructuresynthesis the names of the specific potential drugs that have been investigated in relation to each disease have appeared in this cluster this includes terms such as hydroxychloroquine or remdesivir on the covid-19 map an inspection of the maps overlaid with the average year of publications for sars and mers in figures a1 and a3 in the appendix suggests that on average this cohort of studies are generally the last to emerge in the published domain compared to the two other major clusters but they receive relatively high citations on average according to figures a2 a4 and a6 a third and relatively smaller cluster of keywords was commonly identifiable in relation to each three literatures this cluster has been visualised in blue colour across all three maps of keyword co-occurrence the studies represented by this cluster of keywords here referred to as cluster iii appear to have been more closely focused on the developments of antibodies and vaccines the terms treatment treatment outcome disease severity antiviral therapy prognosis drug safety prospectiveretrospective study immunology immunotherapy innate immunity immune response virusviral vaccines virusviral antibody are notable across these studies terms affiliated with studies related to treatments and clinical care of respiratory disease patients also appear in this cluster this includes terms such as artificial ventilation intensive care unit as well as symptom and organ terminologies associated with each disease terms such as fever headache diarrhea lung injury coughing liver injury kidney terms affiliated with cohort analysis studies have appeared in this cluster of the maps associated with each literature this is reflected in terms such as female male child infant young adult adult age middle aged pregnant pregnancy this pattern of the cohort analysis keywords appearing in cluster iii is particularly common across the mers and covid-19 studies for sars these terms have largely appeared in the red cluster at the border between the red and blue clusters  bibliographic coupling of the studies on sars mers and covid-19 were analysed at the level of their sourcesjournals figure 8 9 and 10 show the maps of journal bibliographic coupling associated with sars mers and covid-10 literatures respectively the node sizes are proportional to the number of documents published by the corresponding sources and the thickness of the links are proportional to the degree of bibliographic couplings between the sources connected by each link the minimum number of documents associated with each nodejournal to appear on the map has been set to 10 no minimum strength was set for links to be visualised on the map a first-glance comparison shows that while the maps associated with sars and mers are well connected connections across the covid-19 map are rather sparse both the sars and mers maps include three major and distinct clusters of bibliographically coupled journals in addition to one minor and smaller cluster these clusters show relatively strong degrees of inter-connectivity whereas this feature is not shared by the covid-19 map the observation is understandable in light of the fact that the sars and mers literatures are relatively well established and have each been under development over a period of several years whereas the covid-19 literature is an emerging field and newly published studies do not seem to be sharing many references as of yet the comparison also suggests that the covid-19 studies are generally scattered across a broader variety of journals and subject areas as opposed to the sars and mers publications that seem to have been concentrated across a smaller set of specialty journals this is also consistent with our observations from figures 1-3 showing that studies of covid-19 are scattered across a broader variety of subject areas compared to the sars and mers literature though not shown in figure 3  due to the respective values being smaller than 1 journals in the following subject areas that are deemed minor areas in relation to covid-19 literature have each published a relatively considerable number of studies on this topics a phenomenon that is not necessarily common with respect to the literature of other coronaviruses arts and humanities 110 items 1  where the most active journal has been social anthropology 24 items covering topics such as climate change reactions bychkova 2020 or legal voids linked to declared states of emergency karaseva 2020 economics econometrics and finance 84 items with economic and political weekly 36 items being the most active journal of that category covering topics such as food supply chains reardon et al 2020  economic stimulus packages mulchandani 2020 or reverse migration dandekar and ghai 2020 physics and astronomy 77 items where chaos solitons and fractals 16 items has been the most active publication outlet covering topics such as mathematical models for forecasting the outbreak barmparis and tsironis 2020 bekiros and kouloumpou 2020 boccaletti et al 2020  ndarou et al 2020  postnikov 2020  ribeiro et al 2020  zhang et al 2020  energy 67 items with international journal of advanced science and technology 44 items being the most active journal in that category covering topics such as flexible work arrangement in manufacturing sedaju et al 2020  material sciences 57 items with acs nano 10 items being the most active outlet in that category covering topics such as 3-d printed protective equipment wesemann et al 2020  decision sciences 23 items with lancet digital health 8 items and transportation research interdisciplinary perspectives 4 items being the most active outlets in that category covering topics such as the effect of social distancing on travel behaviour de vos 2020 or the implementation of drive-through and walk-through diagnostic testing lee and lee 2020 earth and planetary sciences 22 items with indonesian journal of science and technology 8 items being most active in that domain covering topics such as the deployment of drones in sending drugs and patient blood samples anggraeni et al 2020  the analyses of journal citations also showed similar patterns of scatter and relatively unclear clusters in relation to the covid-19 literature compared to well-defined clusters of journal citations for sars and mers literatures consistent with the previous observation with respect to journal bibliographic coupling the covid-19 literature seems to be also much less cohesive in terms of its journal citation networks when compared to the sars and mers literatures as discussed earlier in relation to bibliographic couplings this could also be partly explained by the fact that covid-19 papers are scattered across a more diverse range of journals and broader variety of subject categories for the maps of journal citation relations presented in figures 11 12 and 13 in figures a10-a14 in the appendix the nodes of the bibliographic coupling maps have been colour-coded by the average year of publications and the average citations per document associated with the journals that each node represent except for the covid-19 map that has only been overlaid with the average citations according to these maps emerging infectious diseases and the lancet have been a major source of publications for early studies on both sars and mers this pattern for the lancet seems to have extended to covid-19 studies as well as this journal has published a substantial portion of early studies on this topic for sars the strong representation of chinese medical journal and chinese journal of microbiology and immunology among the journals that published early studies are notable a pattern that could be explained by the geographical origin of the sars outbreak such pattern is to a less obvious extent observable in regard to the mers literature through representations of outlets such as eastern mediterranean health journal and saudi medical journal on the bibliographic coupling map associated with this literature by colours associated with relatively early publications in  collaborations of authors aggregated at the level of the countries were also analysed with respect to the sars mers and covid-19 literatures outputs of the analysis are presented in figures 14 15 and 16 for sars mers and covid-19 respectively in each map the size of nodes each corresponding with a country are proportional to the number of published documents with an author affiliated with the institutes of those countries the links connecting the nodes indicate co-authorships between authors residing in the countries while the thickness of the links represent the strength ie frequency of the co-authorships the colour assigned to each node represents the average number of citations that documents authored by the countries have received the minimum number of documents for country names to appear on the maps has been set to 5 comparison across the three maps of co-authorships shows a pattern of author involvement from the regions where each viral outbreak originated studies authored by researchers affiliated with chinese institutes are well represented in all three cases but clearly more so with respect to the sars and covid-19 literature diseases whose first cases were recorded in china the involvement of chinese authors is relatively less notable in relation to the mers studies whose origin was in the middle east instead with respect to the mers literature it appears that authors affiliated with institutes in saudi arabia have been exceptionally overrepresented in the publications this is also to a lesser extent the case with egypt being notably presented on the mers map of the country co-authorships  has a very well spread and rather more evenly distributed network of collaborations with countries across the world when compared with its network of collaboration on sars and mers while its strongest collaboration has been with the united states the names of many other countries appear on its network with no particular country standing out distinctly italy as a country that was highly affected by the viral outbreak has been exceptionally well represented on this map with a very strong link of collaboration with the united states followed by united kingdom at a smaller scale this pattern of unique over-representation has to a lesser extent extended to iran spain france and brazil as other countries also severely affected by the covid-19 outbreak at early stages of the global spread the sars studies with involvements of the authors affiliated with the institutes in the netherlands have on average received the highest number of citations and this is followed by authors from germany as two countries whose authors have both published considerable number of documents and received high number of citations at the same time this pattern was to some extent repeated in relation to the mers literature with studies from the netherlands germany and united kingdom having received on average highest number of citations for studies published on covid-19 studies from china have so far stood out in terms of both the magnitude of research activities and the average number of citations the map of country co-authorships associated with the sars literature the map of country co-authorships associated with the mers literature the map of country co-authorships associated with the covid-19 literature outbreaks of infectious diseases have often shown a pattern of generating a quick surge of publications on their respective topics such that they often create an entirely new literature over a short amount of time olijnyk 2015 tian and zheng 2015  by all measures however the influx of research publications that began to emerge following the 2019 novel coronavirus outbreak outsizes those of the previous cases in the history of coronaviruses and perhaps arguably in the history of infectious diaereses tian and zheng 2015  this has certainly marked a new milestone in the timeline of research on coronaviruses which dates back to 1968 almeida et al 1968  according to the editor-in-chief of the journal of virology as quoted in an article of the scientist magazine jarvis 2020 this surge of research outputs has been to the extent that has inundated established coronavirus researchers and domain experts with peer review requests to an extent that they are unable to cope parallel to such intensified efforts in the research peer review and editorial fronts widespread efforts are underway in synthesising summarising and visualising these rapid developments a pattern that has also been observedthough at much smaller scales-in relation to the previous epidemics of viral diseases kostoff and morse 2011  in line with these endeavours this work also aimed at quantifying and analysing scientometric aspects of the the involvement of authors from various countries on the publications linked to these three diseases seem to be distinctly correlated with the regions where the outbreaks originated with authors from china for example being much more strongly represented on sars and covid-19 studies two diseases whose origin of outbreaks were attributed to this country middle eastern countries on the other hand are exceptionally represented in the mers literature the questions of where the covid-19 literature is headed how big it will grow in the next coming years at what point in time the rate of publications on this topic are going to slow down if ever and how widely this literature is going to spread across journals and subject categories are only a few examples of questions that will be determined by time these may also be influenced down the line by possible highly sought medical discoveries in relation to vaccine and treatment development or lack thereof but given the current rate at which scholarly outputs are emerging and given the extent of studies projects and trials that have already been conceived on this topic around the world and also given the seemingly long-lasting and farreaching consequences of this global emergency which have impacted on aspects of life it will probably not be so soon before we observe a decline in covid-19 research interest the map of keyword co-occurrence for sars overlaid with the colour-coding of the average year of publication the map of keyword co-occurrence for sars overlaid with the colour-coding of the average citation number   covid-19 pandemic and the unprecedented mobilisation of scholarly efforts prompted by a health crisis scientometric comparisons across sars mers and 2019-ncov literature milad haghani michiel bliemer c j  during the current century each major coronavirus outbreak has triggered a quick and immediate surge of academic publications on this topic the spike in research publications following the 2019 novel coronavirus covid-19 outbreak however has been like no other the global crisis caused by the covid-19 pandemic has mobilised scientific efforts in an unprecedented way in less than five months more than 12000 research items have been indexed while the number increasing every day with the crisis affecting all aspects of life research on covid-19 seems to have become a focal point of interest across many academic disciplines here scientometric aspects of the covid-19 literature are analysed and contrasted with those of the two previous major coronavirus diseases ie severe acute respiratory syndrome sars and middle east respiratory syndrome mers the focus is on the cooccurrence of key-terms bibliographic coupling and citation relations of journals and collaborations between countries certain recurring patterns across all three literatures were discovered all three outbreaks have commonly generated three distinct and major cohort of studies i studies linked to the public health response and epidemic control ii studies associated with the chemical constitution of the virus and iii studies related to treatment vaccine and clinical care while studies affiliated with the category i seem to have been the first to emerge they overall received least numbers of citations compared to those of the two other categories covid-19 studies seem to have been distributed across a broader variety of journals and subject areas clear links are observed between the geographical origins of each outbreak or the local geographical severity of each outbreak and the magnitude of research originated from regions covid-19 studies also display the involvement of authors from a broader variety of countries compared to sars and mrs  on december 31 2019 an official case of a novel respiratory diseases of the category of coronaviruses named covid-19 was reported in wuhan china marking the beginning of what proved to be one of the direst and most devastating viral outbreaks in the modern history sohrabi et al 2020  this was immediately followed by an unprecedented and swift response of the academic community to address various dimensions of this health crisis and prompted an avalanche of scholarly publications on this topic golinelli et al 2020  haghani et al 2020  kagan et al 2020  in less than five months more than 12000 publications on this topic have already been indexed by scopus with the number increasing figuratively every day in considerable increments torres-salinas et al 2020  and this figure does not even include many more publications available in various repositories including cord-19 colavizza et al 2020  in the form of preprints awaiting peer review by their respective journals such explosion of research on a single topic and the off-the-charts surge in the rate of publications are arguably unprecedented trends in the history of scholarly publications an article published by science on may 13 2020 referred to this phenomenon as one that is among the biggest explosions of scientific literature ever brainard 2020  it highlighted how in the face of this phenomenon it has become extremely challenging for scientists to stay abreast of the latest developments this has made the importance of research synthesis more tangible than ever and has even resulted in the development of several computational research mining tools for this very topic utilising methods such as artificial intelligence ai among such efforts is a research synthesis powered by ai algorithms which has harvested datapoints from a large number the cord-19 articles and categorised them brainard 2020  though the impact of the covid-19 health crisis has marked it as a rather unique milestone in the history disease outbreaks the world prior to this was not a stranger with coronavirus disease outbreaks mcintosh 1974  myint 1994  cavanagh 2005  lim et al 2016  chen et al 2020  prior to 2020 two major outbreaks of this family of viruses had already been reported with at least one of them carrying the official label of a global pandemic  on november 16 2002 the first case of the severe acute respiratory syndrome sars disease was reported in the guangdong province in southern china which by 2003 swiftly spread from continent to continent prompting the world health organisation to declare it as a pandemic in fact sars is known to be the first pandemic of the 21st century cherry and krogstad 2004  nearly ten years later on june 13 2012 the first case of the middle eastern respiratory syndrome mers disease was discovered in jeddah saudi arabia these two constituted the two most major coronavirus outbreaks until covid-19 came along similar to covid-19 though at a much smaller scale each of these previous outbreaks generated a literature of their own kostoff and morse 2011  in the face of the flood of scholarly outputs on covid-19 and along with the conventional review and research synthesis studies chang et al 2020  chen et al 2020  cortegiani et al 2020  scientometric colavizza et al 2020 and bibliometric methods bonilla-aldana et al 2020  hossain 2020 have also gained traction in documenting and analysing the rapid developments of this literature chahrour et al 2020  dehghanbanadaki et al 2020  haghani et al 2020  kumar 2020  le bras et al 2020  here in this work the literatures of these three major coronavirus diseases are disentangled and analysed in a comparative way and from scientometric perspectives the aim is to discover possible similarities and discrepancies across these three segments of the coronavirus literature and to discover whether there are recurring patterns in terms of magnitude temporal evolution and the shape of these three literatures that were each developed in response to a disease outbreak the main focus of the analyses is on keyword co-occurrences bibliographic coupling and citation relations of sources and collaborations between countries to compare the scientometric aspects of the studies on sars mers and covid-19 three separate datasets of publications on these three topics were retrieved from scopus through three separate search strategies the decision on which general database to use eg web of science wos or scopus was mainly made based on the number of indexed covid-19 studies as the sector of the literature that is currently emerging compared to the literatures on sars and mers that are better established at the time of the data retrieval wos had indexed slightly less than 5000 research items on covid-19 while the number of items in scopus neared 12000 given the fact that the scopus database was considerably more up to date in that area this database was set as the main source of data extraction in this work therefore for the sake of consistency the data for sars and mers were also extracted from scopus the search strategies were devised in a way to minimise the possible overlap between the datasets on sars mers and covid-19 and to disentangle the three datasets to the most possible extent preliminary inspection of the literature on each three topics determined a set of distinct keywords that would return the target literature with reasonable specificity and sensitivity in each search key terms associated with the other literatures were combined with the logical operator and not in order to avoid the overlap the lower bound of the time span for each search was set with consideration of the year where the first outbreak of each virus took place the query string associated with each dataset are as follows sars  title-abs-key    severe acute respiratory syndrome or sars  and  coronavirus   or  sars virus or sars disease or severe acute respiratory syndrome disease or severe acute respiratory syndrome virus or sars-cov  and not title- or severe acute respiratory syndrome-2 or mers or middle east respiratory syndrome    and pubyear  2001 mers  title-abs-key    middle east respiratory syndrome or mers  and  coronavirus   or  mers-cov or mers virus or mers disease or middle east respiratory syndrome virus or middle east respiratory syndrome disease   and not title-abs-key   ncov or covid-19 or covid19 or sars-cov or sars-cov-2 or sars or severe acute respiratory syndrome    and pubyear  2011 covid-19 title-abs- key  covid-19 or covid19 or coronavirus disease 2019  or 2019  and pubyear  2018   the search was last time updated on 24 may 2020 where it returned 5907 items on sars  1752 items on mers and 11859 items on covid-19 figures 1 2 and 3 show the distribution  of the studies on sars mers and covid-19  respectively across subject areas figure 4 a also shows the composition of the covid-19 literature in terms of the document types demonstrating that only nearly 50 of the studies on this topic have so far been in the form of full-length articles while letters notes reviews and other document formats constitute a large portion ie nearly half of the literature on this topic at the time of this investigation full records of the three datasets on sars mers and covid-19 were retrieved in csv excel format from scopus all on the same day this included the citation information bibliographic information abstract and keywords funding details and the references the scopus restriction of maximum 2000 document to export posed challenges for the retrieval of the sars and covid-19 datasets whose size were bigger than 2000 documents for the sars dataset the challenge was circumvented by further limiting the search to specific years in separate bundles in a way that the size of each bundle was less than 2000 items therefore allowing us to export the items of each bundle separately the extraction of the covid-19 dataset however posed a further layer of complication given that nearly all studies of covid-19 have been published in one year ie 2020 therefore the year of publication could not be used as a criterion to form a set of mutually exclusive smaller-size exportable bundles for this literature to decompose the search outcome to bundles of 2000 documents or less the following strategy was adopted the document type was used to initially limit the search to mutually exclusive non-overlapping categories first the search was limited to review or short survey or erratum or conference paper or data paper this formed a set of 1267 documents which was extracted in one single export see figure 4 a for details of the number of items within each document type category subsequently the search was set back to original and was limited to notes 1067 items and then to editorial 1270 items with each set of these two subsets being smaller than 2000 they were exported separately there were 2564 documents of letter type this set was further decomposed to two mutually exclusive subsets based on the publication stage criterion 1539 article in press and 1025 final and was retrieved in two separate exports for the remaining 5691 article documents the following strategy was devised of the 5691 items 2944 were article in press and 2747 were final first the 2944 article in press items were considered the list of those studies was sorted as first author a-z and the first 2000 items were extracted in one export then the list was sorted as first author z-a and the first 944 items were exported similar strategy was utilised to extract the remaining 2747 final documents a supplementary search was also conducted on the general topic of coronaviruses using the string title-abs-key  coronavirus  and pubyear  1985 which returned 24620 documents on the same day only the data related to the number of documents by year was extracted for this search the increase continued though at a slower rate to 2004 and was then followed by a gradual decline till 2012 the 2012 mers outbreak triggered another spike in the number of publications on coronaviruses though not as large as that of the sars the intensification of attention to this topic this time lasted for about three years till 2015 before another decline began the spike of coronavirus studies prompted by the covid-19 outbreak however seem to have been occurring at a completely different scale which can be deemed unprecedented in the history of coronavirus studies the number of studies emerged in the first five months of 2020 nears an equivalent of the 70 of the total sizer of coronavirus literature during more than 50 years  in figure 4 c the temporal distribution of the sars mers and covid-19 studies have been shown separately according to the three datasets explained earlier note that the quantities associated with sars and mers are represented by the left vertical axis whereas that of the covid-19 is represented by the right vertical axis with a scale ten times bigger than the scale of the left axis the history of previous coronavirus research has suggested that the number of studies will likely keep rising for at least a few years before it peaks but given the unprecedented magnitude of research and the explosive rate of publications since the begging of 2020 it would be interesting to observe whether this pattern would repeat itself and whether the peak would occur at an earlier or later stage compared to those of the previous outbreaks a question whose answer will only be determined by time  the co-occurrence of keywords associated with the sars mers and covid-19 literature were analysed using vosviewer van eck and waltman 2010 each analysis was performed on the separate set of data associated with the literature of interest the maps of keyword cooccurrences associated with sars mers and covid-19 literatures are provided in figures 5 6 and 7 respectively the minimum number of occurrences for the keywords to be included in the map was set to 5 in all three cases the unit of analysis has also been set to all keywords that includes both author and index keywords and the method of counting was full counting figures a1 and a2 in the appendix illustrate the map associated with the sars literature overlaid respectively with the average year of publication and average number of citations associated with the studies where these keywords have occurred figure a3 and a4 present the counterpart outputs for the mers literature analysis figure a5 is a heatmap of covid-19 keyword co-occurrence and figure a6 overlays the covid-19 keywords map in figure 7 with the colour-coding of the average number of citations given that almost all studies of covid-19 are 2020 items the colour-coding related to the average publication year was forgone in regard to this literature maps of term occurrences based on the analysis of the title and abstract of studies on sars mers and covid-19 have also been presented in figures 7 8 and 9 respectively while the below analysis focuses mainly on the interpretation of the keyword maps similar patterns are by-and-large observable through analysis of the title and abstract terms of these studies with respect to each of the three literatures three distinct clusters of keywords were identifiable these clusters showed certain patterns of commonality across the three datasets each map presents a distinct cluster of keywords that seem to be associable to the studies related to public health emergency management and the prevention of epidemic this would be the red cluster in figure 5 sars the green cluster in figure 6 mers and the red cluster in figure 7  here this is referred to as cluster i in this cluster one can observe terms such as those associated with general public health including wold health organisation public health public health service global health as well as those associated with disease outbreaks including emergency health risk epidemics pandemic outbreak viral diseases virus infection communicable disease transmission travel terms representing measures of emergency severity also appear in this cluster including mortality fatality morbidity infection risk this cluster also includes terms that are linked to the prediction of disease propagation these are terms such as mathematical model modelling simulation statistical model and prediction that have commonly occurred in this cluster the cluster includes terms affiliated with measures of disease control and spread prevention such as socialpatient isolation quarantine hygiene handwashing prevention infection control population surveillance mass screening face mask contact tracing the cluster also represents keywords that attributable to public policy making and social protection such as health care planning health care policy health care quality leadership disaster planning polices the cluster i of keywords also have distinctly and commonly across all three datasets represented keywords that are attributable to the studies on mental health impact of the epidemic these are keywords such as mental health service psychiatry psychology mental stress anxiety fear mental disease these studies have often used methods such as questionnaires and surveys that have commonly reflected in this cluster across the three literatures issues surrounding the safety of medical facilities and medical staff also appear to have been addressed mainly by studies whose keywords are attributable to this cluster these studies have generated keywords such as health care personnel nurses medical staff hospital health care facility personal protective equipment that are distinctly observable in cluster i of keywords across all three datasets the economic aspects of the epidemics also seem to have been addressed particularly by covid-19 as reflected in cluster i of the covid-19 literature these have been reflected in terms such as economics economic aspect which have occurred frequently enough in covid-19 studies for them to appear distinctly on the map the trace of such cohort of studies is however not as clearly identifiable based on the sars and mers maps as is it with respect to the covid-19 literature this could be explained by the greater magnitude of the societal impact of covid-19 outbreak compared to sars and mers the names of the countries and regions have almost invariably appeared in cluster i across all three datasets in certain cases the country names that have occurred most are those from which the outbreaks originated or those that suffered most from the impact of the outbreak for example saudi arabia appears quite distinctly on the cluster i of the mers dataset similarly the occurrence of the names of south-east asian countriesregions such as china hong kong taiwan singapore on the cluster i of the sars map or the term wuhan on the cluster i of the covid-19 map are quite notable the occurrence of the name of the countries also could be a reflection of the early studies with respect to each outbreak that have addressed the local impactsspread of the outbreaks on their own society on the issue of early studies the terms letters editorial and review which have intentionally been kept on the maps seem to also have distinctly occurred in cluster i of each literature which is another indication that this cluster includes early studies that appeared at a time where the amount of data and clinical trials were insufficient for full-length articles an inspection of the figures a1 and a3 does in fact confirm this hypothesis at least in association with the sars and mers literature that the cluster i of keywords represent studies that on average emerged early during the developments of their respective literatures figures a2 a4 and a6 that have illustrated the colour-coding of the average number of citations on the maps also show that although cluster i is associated with the early studies that generally predated studies of the two other clusters and although it represents the largest variety of topics compared to the two other clusters it is also associated with studies that on average been the recipient of a lesser number of citations when compared to the two other clusters this pattern also appears to have commonly occurred across all three datasets a second cluster of keywords associated with each of the three literatures were also discovered that is attributable to the studies on the chemistry and physiology of the virus or viral pathogenesis or in other words the chemical constitution of the virus knight 1954  a part of virology that investigates the biological processes and activities of viruses that take place in infected host cells and result in the replication of a virus for the sars map in figure 5  as well as the covid-19 map in figure 7  this would be the green cluster whereas for the mers map  figure 6  this cluster is red according to the maps the most distinct terms associated with this cohort of virology studies on sars mers and covid-19 are terms such as virus protein virus entry chemistry metabolism physiology pathology cell line virusviral proteins molecular models virus genome virus rna virus replication mutation and enzyme activity as this sector of studies often use animal models terms such as animal cell animal experiment controlled study mice and mouse have frequently appeared in cluster ii associated with each of the three literatures in reflection of the fact that these cohort of studies ultimately seek drug design in addition to generic common terms such as drug designpotencystructuresynthesis the names of the specific potential drugs that have been investigated in relation to each disease have appeared in this cluster this includes terms such as hydroxychloroquine or remdesivir on the covid-19 map an inspection of the maps overlaid with the average year of publications for sars and mers in figures a1 and a3 in the appendix suggests that on average this cohort of studies are generally the last to emerge in the published domain compared to the two other major clusters but they receive relatively high citations on average according to figures a2 a4 and a6 a third and relatively smaller cluster of keywords was commonly identifiable in relation to each three literatures this cluster has been visualised in blue colour across all three maps of keyword co-occurrence the studies represented by this cluster of keywords here referred to as cluster iii appear to have been more closely focused on the developments of antibodies and vaccines the terms treatment treatment outcome disease severity antiviral therapy prognosis drug safety prospectiveretrospective study immunology immunotherapy innate immunity immune response virusviral vaccines virusviral antibody are notable across these studies terms affiliated with studies related to treatments and clinical care of respiratory disease patients also appear in this cluster this includes terms such as artificial ventilation intensive care unit as well as symptom and organ terminologies associated with each disease terms such as fever headache diarrhea lung injury coughing liver injury kidney terms affiliated with cohort analysis studies have appeared in this cluster of the maps associated with each literature this is reflected in terms such as female male child infant young adult adult age middle aged pregnant pregnancy this pattern of the cohort analysis keywords appearing in cluster iii is particularly common across the mers and covid-19 studies for sars these terms have largely appeared in the red cluster at the border between the red and blue clusters  bibliographic coupling of the studies on sars mers and covid-19 were analysed at the level of their sourcesjournals figure 8 9 and 10 show the maps of journal bibliographic coupling associated with sars mers and covid-10 literatures respectively the node sizes are proportional to the number of documents published by the corresponding sources and the thickness of the links are proportional to the degree of bibliographic couplings between the sources connected by each link the minimum number of documents associated with each nodejournal to appear on the map has been set to 10 no minimum strength was set for links to be visualised on the map a first-glance comparison shows that while the maps associated with sars and mers are well connected connections across the covid-19 map are rather sparse both the sars and mers maps include three major and distinct clusters of bibliographically coupled journals in addition to one minor and smaller cluster these clusters show relatively strong degrees of inter-connectivity whereas this feature is not shared by the covid-19 map the observation is understandable in light of the fact that the sars and mers literatures are relatively well established and have each been under development over a period of several years whereas the covid-19 literature is an emerging field and newly published studies do not seem to be sharing many references as of yet the comparison also suggests that the covid-19 studies are generally scattered across a broader variety of journals and subject areas as opposed to the sars and mers publications that seem to have been concentrated across a smaller set of specialty journals this is also consistent with our observations from figures 1-3 showing that studies of covid-19 are scattered across a broader variety of subject areas compared to the sars and mers literature though not shown in figure 3  due to the respective values being smaller than 1 journals in the following subject areas that are deemed minor areas in relation to covid-19 literature have each published a relatively considerable number of studies on this topics a phenomenon that is not necessarily common with respect to the literature of other coronaviruses arts and humanities 110 items 1  where the most active journal has been social anthropology 24 items covering topics such as climate change reactions bychkova 2020 or legal voids linked to declared states of emergency karaseva 2020 economics econometrics and finance 84 items with economic and political weekly 36 items being the most active journal of that category covering topics such as food supply chains reardon et al 2020  economic stimulus packages mulchandani 2020 or reverse migration dandekar and ghai 2020 physics and astronomy 77 items where chaos solitons and fractals 16 items has been the most active publication outlet covering topics such as mathematical models for forecasting the outbreak barmparis and tsironis 2020 bekiros and kouloumpou 2020 boccaletti et al 2020  ndarou et al 2020  postnikov 2020  ribeiro et al 2020  zhang et al 2020  energy 67 items with international journal of advanced science and technology 44 items being the most active journal in that category covering topics such as flexible work arrangement in manufacturing sedaju et al 2020  material sciences 57 items with acs nano 10 items being the most active outlet in that category covering topics such as 3-d printed protective equipment wesemann et al 2020  decision sciences 23 items with lancet digital health 8 items and transportation research interdisciplinary perspectives 4 items being the most active outlets in that category covering topics such as the effect of social distancing on travel behaviour de vos 2020 or the implementation of drive-through and walk-through diagnostic testing lee and lee 2020 earth and planetary sciences 22 items with indonesian journal of science and technology 8 items being most active in that domain covering topics such as the deployment of drones in sending drugs and patient blood samples anggraeni et al 2020  the analyses of journal citations also showed similar patterns of scatter and relatively unclear clusters in relation to the covid-19 literature compared to well-defined clusters of journal citations for sars and mers literatures consistent with the previous observation with respect to journal bibliographic coupling the covid-19 literature seems to be also much less cohesive in terms of its journal citation networks when compared to the sars and mers literatures as discussed earlier in relation to bibliographic couplings this could also be partly explained by the fact that covid-19 papers are scattered across a more diverse range of journals and broader variety of subject categories for the maps of journal citation relations presented in figures 11 12 and 13 in figures a10-a14 in the appendix the nodes of the bibliographic coupling maps have been colour-coded by the average year of publications and the average citations per document associated with the journals that each node represent except for the covid-19 map that has only been overlaid with the average citations according to these maps emerging infectious diseases and the lancet have been a major source of publications for early studies on both sars and mers this pattern for the lancet seems to have extended to covid-19 studies as well as this journal has published a substantial portion of early studies on this topic for sars the strong representation of chinese medical journal and chinese journal of microbiology and immunology among the journals that published early studies are notable a pattern that could be explained by the geographical origin of the sars outbreak such pattern is to a less obvious extent observable in regard to the mers literature through representations of outlets such as eastern mediterranean health journal and saudi medical journal on the bibliographic coupling map associated with this literature by colours associated with relatively early publications in  collaborations of authors aggregated at the level of the countries were also analysed with respect to the sars mers and covid-19 literatures outputs of the analysis are presented in figures 14 15 and 16 for sars mers and covid-19 respectively in each map the size of nodes each corresponding with a country are proportional to the number of published documents with an author affiliated with the institutes of those countries the links connecting the nodes indicate co-authorships between authors residing in the countries while the thickness of the links represent the strength ie frequency of the co-authorships the colour assigned to each node represents the average number of citations that documents authored by the countries have received the minimum number of documents for country names to appear on the maps has been set to 5 comparison across the three maps of co-authorships shows a pattern of author involvement from the regions where each viral outbreak originated studies authored by researchers affiliated with chinese institutes are well represented in all three cases but clearly more so with respect to the sars and covid-19 literature diseases whose first cases were recorded in china the involvement of chinese authors is relatively less notable in relation to the mers studies whose origin was in the middle east instead with respect to the mers literature it appears that authors affiliated with institutes in saudi arabia have been exceptionally overrepresented in the publications this is also to a lesser extent the case with egypt being notably presented on the mers map of the country co-authorships  has a very well spread and rather more evenly distributed network of collaborations with countries across the world when compared with its network of collaboration on sars and mers while its strongest collaboration has been with the united states the names of many other countries appear on its network with no particular country standing out distinctly italy as a country that was highly affected by the viral outbreak has been exceptionally well represented on this map with a very strong link of collaboration with the united states followed by united kingdom at a smaller scale this pattern of unique over-representation has to a lesser extent extended to iran spain france and brazil as other countries also severely affected by the covid-19 outbreak at early stages of the global spread the sars studies with involvements of the authors affiliated with the institutes in the netherlands have on average received the highest number of citations and this is followed by authors from germany as two countries whose authors have both published considerable number of documents and received high number of citations at the same time this pattern was to some extent repeated in relation to the mers literature with studies from the netherlands germany and united kingdom having received on average highest number of citations for studies published on covid-19 studies from china have so far stood out in terms of both the magnitude of research activities and the average number of citations the map of country co-authorships associated with the sars literature the map of country co-authorships associated with the mers literature the map of country co-authorships associated with the covid-19 literature outbreaks of infectious diseases have often shown a pattern of generating a quick surge of publications on their respective topics such that they often create an entirely new literature over a short amount of time olijnyk 2015 tian and zheng 2015  by all measures however the influx of research publications that began to emerge following the 2019 novel coronavirus outbreak outsizes those of the previous cases in the history of coronaviruses and perhaps arguably in the history of infectious diaereses tian and zheng 2015  this has certainly marked a new milestone in the timeline of research on coronaviruses which dates back to 1968 almeida et al 1968  according to the editor-in-chief of the journal of virology as quoted in an article of the scientist magazine jarvis 2020 this surge of research outputs has been to the extent that has inundated established coronavirus researchers and domain experts with peer review requests to an extent that they are unable to cope parallel to such intensified efforts in the research peer review and editorial fronts widespread efforts are underway in synthesising summarising and visualising these rapid developments a pattern that has also been observedthough at much smaller scales-in relation to the previous epidemics of viral diseases kostoff and morse 2011  in line with these endeavours this work also aimed at quantifying and analysing scientometric aspects of the the involvement of authors from various countries on the publications linked to these three diseases seem to be distinctly correlated with the regions where the outbreaks originated with authors from china for example being much more strongly represented on sars and covid-19 studies two diseases whose origin of outbreaks were attributed to this country middle eastern countries on the other hand are exceptionally represented in the mers literature the questions of where the covid-19 literature is headed how big it will grow in the next coming years at what point in time the rate of publications on this topic are going to slow down if ever and how widely this literature is going to spread across journals and subject categories are only a few examples of questions that will be determined by time these may also be influenced down the line by possible highly sought medical discoveries in relation to vaccine and treatment development or lack thereof but given the current rate at which scholarly outputs are emerging and given the extent of studies projects and trials that have already been conceived on this topic around the world and also given the seemingly long-lasting and farreaching consequences of this global emergency which have impacted on aspects of life it will probably not be so soon before we observe a decline in covid-19 research interest the map of keyword co-occurrence for sars overlaid with the colour-coding of the average year of publication the map of keyword co-occurrence for sars overlaid with the colour-coding of the average citation number   review on machine and deep learning models for the detection and prediction of coronavirus ahmad waleed salehi preety baglat gaurav gupta   viruses are a submicroscopic agent which are made up of genetic substances inside of a protein coating and it can be found anywhere such as air water and soil a virus can cause different infectious types of diseases such as common flu cold and warts they also cause severe illnesses such as acquired immunodeficiency syndrome aids ebola smallpox 1 etc viruses are akin to hijackers they enter normal cells and from those cells to replicate themselves this process destroys or damages the cells and makes us sick and this kind of virus is called pandemic virus various viruses attack and harm certain cells of the human body such as our respiratory system liver or blood when we get a virus it will not always affect us to get ill from the virus the immune system of us may be capable to fight with it for most epidemiologic infections the only method to help is the treatment of symptoms as you should wait for the immune system to confront against or fight off the virus for viral infections the antibiotics cannot work there are antiviral drugs and medicines to cure or heal some of the viral infections the vaccines help to prevent us from getting various viral diseases a pandemic is an occurrence of a disease or wide-scale outbreak of any infectious disease which can rapidly increase mortality and morbidity rate on a large-scale over a large area even crossing the international boundaries which can disturb a large population of the world and that can cause a significant social economic and political disturbance previous studies indicate that the chances of pandemics have risen in the past centuries 2 this pandemic outbreak can happen when a novel virus turns out to be capable of circulating or spreading rapidly globally wherein an epidemic can be in a specific region or city in one and other cases there is a serious illness that can rapidly spread out from one to another individual quickly evidence shows that the death rate of the epidemic is generally lesser than a pandemic outbreak for example the unfavorable pandemic in the history that caused the death of more than a hundred million people was the spanish flu 3 coronavirus has rapidly spread to almost all countries of the world to date coronavirus is a big family of viruses which causes illness or sickness starting from the very common cold to very more acute or severe disease sars-cov-2 showed about 80 identity of sars-cov-1 and 50 to the middle east respiratory syndrome mars-cov from the genetic sequence and both have their origin from bats according to the phylogenetic reports and genetic sequence coronavirus is adequately similar to sars- cov most likely this disease has spread from bats as proof supports that this virus has a high grade of homology of ace2 angiotensin converting enzyme 2 receptor from a variety of animal species 4 the common sign of the coronavirus infection consists of high and consistent fever persistent dry cough respiratory syndromes such as breathing difficulties and breath shortness 5 the coronavirus infection has spread like a fire and transitioned into a globally pandemic in which as if now know medical researchers found any therapeutic vaccine or drug because of this serious issue it is very significant to identify the diseases at its early stage and quickly isolate people who are infected from the non-affected population the coronavirus should be the starting of an exciting time or decade in science and medicine because with the development and improvement of numerous digital technologies they can be applied and used to tackle different diseases and major clinical problems these are the technologies that include big-data analytics iot internet of things blockchain ai which uses machine learning ml and deep learning dl etc these are the advanced technologies that are being used in many aspects of healthcare especially ai and machine learning systems to predict the outcomes and understanding healthcare trends 6 and how ai-based technology and deep learning can help and enhance in the diagnosis of this coronavirus by using a different kind of radiographical images ct and x-ray scans of the chest can be used in diagnosing the pneumonia there are ai-based automated ct or x-ray images analysis techniques for the detecting monitoring and quantifying of coronavirus and to find out the patients from the healthy person have developed 7 as of 14 april 2020 the total number of confirmed cases is 1 776 867 the confirmed number of deaths is 111828 and the overall number of countries affected by this virus is 213 as reported by who the usa china italy spain are the most affected countries initially in european countries the very fast and worst affected country was italy where the public health departments hospitals emergency medical systems are struggling to deal with the surge of affected patients the fatality rate of the coronavirus was increasing in the outbreak region of china the first 17 deaths reported by the national health commission of china was on 22nd january 2020 and 56 deaths on 25th january 2020 4 from the initial case in dec 2019 to the rise of new incidents outside the city of wuhan by 13th january 2020 41 number of cases or incidents were confirmed the epidemiologic analysis has shown which already in this early stage human-to-human space and transmission have been happened by close interaction on 13 january 2020 in thailand the first case which was outside china had been reported this was caused by ax resident of wuhan who traveled to thailand and 19 cases were reported in january from outside wuhan city in beijing city 8 furthermore 858 of 37 269 confirmed cases had either lived in or traveled to wuhan or had close contact with persons who had been to wuhan the figure 1
depicted above shows the outbreak of the disease till 9th april 8 the coronavirus varies widely from the range of infection asymptomatic to severe condition and serious pneumonia along with the high number of fatality rates the main disease control centers of the chinese reported that the majority of the number of infected people were classified as a symptom of mild to moderate condition 138 as severe condition and only 47 classified as critical condition the total rate of mortality for coronavirus confirmed cases were detected higher in males as compared to the female patients and high risk of death in both cases with rising age the maximum fatality rate of the aged started from the 80 and above 10fig 2
 it was very important to control the epidemic and lift the quarantine but only if specific measures could have been taken as much as possible usage of facial masks and restricting public contact is the most effected precaution that people can take until the vaccine is not available apart from these who announced other very significant measures to be taken by everyone such as frequent hand washing use of disinfectants avoid from touching the eye face nose and mouth use gloves to name a few which are adopted worldwide to reduce the probability of 2nd wave of this pandemic in the recent report of who 78 of infected cases were because of household spread or transmission and in the recent clinical report 5 of hospital-acquired infections from patients and medical staff until the vaccine is made available the studies recommended that keeping a reduction in person to person contact less than the pre-quarantine level is very important to contain the spread of the virus the study has shown that from the previous experience coronavirus symptoms are mainly highlighted by checking the travel history rather than radiography of chest the treatment and detection for the isolation of coronavirus patient cases at an early stage are very important all hard works are being made to decrease the fast spreading of the disease and to provide time for better preparing of healthcare systems and also to the general public it is important to clearly characterize coronavirus to guide and help general public health recommendations and to build vaccines therapeutics and timely diagnostics lastly with the improvement and advancement of internet communication it increases the availability and broadcasting of knowledge the internet has the possibilities in terms of development and the spreading of fake news or misinformation so the government sector must be responsible for providing and delivering the accurate information and clarifying misinformation to help the general public to face this coronavirus infection the below figure depicts the percentage of the coronavirus symptoms in this article we have selected publications for review of literature that have assessed machine and deep learning algorithms applied in medical images to solve a clinical problem and compared each algorithm regarding their output and performance machine learning is the branch of ai which is based on how the system can learn from previous data recognize patterns and to make decisions with minimum human intervention for example algorithms includes support vector machine svm logistic regression clustering etc deep learning dl is a subset of machine learning and in terms of medical images it can be defined as a computational model 20 21 which is composed of several processing layers to learn data representation and extract the features with multiple abstraction levels an example of a deep learning type is cnn 11 in 12 has proposed a deep learning-based technique called deep transfer learning which can predict patients with coronavirus disease automatically it uses images of chest x-ray obtained from patients with coronavirus and from a healthy person dataset of 50 patients with coronavirus images of x-ray were taken from a shared github repository and 50 x-ray images of healthy humans have taken from a repository in kaggle in this study results showed that the pre-trained model resnet50 yielded 98 accuracy among the other three models they have also stated that we believe in our findings and this can help the public health care workers to make decision in clinical practice because of its high performance and accuracy also for patients with coronavirus early prediction of the infection can avoid the rapid spreading of the disease this paper study 13 presented automated techniques are used for classifying the x-ray of chest into pneumonia and the class of disease-free by using nine architectures of deep learning which includes the followingbaseline cnndensenet201vgg16 vgg19inceptionresnetv2 inceptionv3xceptionresnet50mobilenetv2
 experiments have been conducted using a ct scan and x-ray dataset that includes 5856 images with 1583 normal and 4273 pneumonia and the performance was evaluated using different performance metrics the result shows that the mobilenetv2 inceptionresnetv2 and resnet50 have given more accurate results which is more than 96 in addition they have suggested that with the bigger datasets and more sophisticated techniques of feature extraction based upon deep learning image segmentation may improve the performance in 14 established a deep learning paradigm for the screening of coronavirus patients at an early stage the main aim of this paper is to distinguish coronavirus from influenza-a viral pneumonia and normal cases with the use of ct images ct samples have taken from three hospitals designated to coronavirus from china zhejiang province the total number of 618 samples was collected which includes 219 from 110 coronavirus patients 224 samples of ct from patients with the viral pneumonia influenza-a and ct samples of 175 healthy people the experiments result of this study shown an overall 867 accuracy from the viewpoint of ct cases as a whole they demonstrated that it can be a promising accompanying diagnostic tool for the clinical frontline doctors as in 15 introduced a new approach of deep learning called covidx-net to help radiologists to automatically diagnose patients with coronavirus using x-ray images this technique is built on seven deep cnn classifiers which consists of densenet121 vgg19 resnetv2 inceptionresnetv2 xception inceptionv3 and mobilenetv2 in this model they have successfully experimented and evaluated the result which is based on 80 training phase 20 testing phase of x-ray images the vgg19 and the dense cnn densenet models have shown a better and similar functioning of automated classification of coronavirus with 089 f1-scores for normal and 091 for coronavirus respectively x-ray images of the public dataset used in this work which consists of 50 images divided into two classes as normal cases of 25 and positive coronavirus images of 25 another paper implements an automated detection system of coronavirus diseases using deep learning convolutional neural network and chest x-ray images in this paper they have implemented three different cnn based models such as resnet50 inception- resnetv2 and inceptionv3 for the identification of coronavirus cases using x-ray images the dataset of 50 coronavirus patients has been taken from the open-source github repository dr joseph cohen and 50 images of normal chest x-rays have been selected from a repository in kaggle chest x-ray images the overall performance out of three models shows that the best results obtained from the pre-trained model of resnet50 with an accuracy rate of 98 and the other two models accuracy rate from inception-resnetv2 and inceptionv3 are 87 and 97 in future work they have mentioned that the different cnn models classification performance can be tested by adding the greater number of images in the dataset 7 in this study 16 they present earlier detection of coronavirus using ct scan images by using machine learning methods and the dataset is taken from societa-italiana di radiologia medica-e interventistica which belongs to the 53 coronavirus cases and data involving ct abdominal images of 150 and 150 ct images then patch regions of the images were cropped and four different subsets of the patch were created the feature extraction techniques used in this study are the followinggrey -level co-occurrence matrixlocal directional patterngrey-level size-zone matrixgrey-level run-length matrixlast-discrete wavelet transform
 the svm classifier was applied to classify the extracted features and the best classification outcomes were obtained from glszm future extraction techniques with an accuracy rate of 9968 in future work more classification and segmentation studies should be done on coronavirus disease by increasing the dataset as in 17 used different deep learning-based techniques for coronavirus detection diseases by using x-ray images different classification models based on svm using deep features of various deep learning architectures as followalexnetvgg16 vgg19googlenetresnet18 resnet50 resnet101inceptionv3inception resnetv2densnet201xceptionnet
 are used for detecting the coronavirus patients for experimental work they have been conducting they use an x-ray images dataset that includes 50 images with 25 coronavirus cases from github dr joseph cohen and 25 normal cases from kaggle x-ray images of pneumonia the results show that svm  resnet50 obtained high accuracy fpr9552 f1 score 9552 mcc  9141 and kappa 9076 for detecting the coronavirus patients as compare to the other models various ml and dl-based algorithms and techniques used for classification of the novel disease known as coronavirus 2019 have been studied and reviewed different papers are studied in which the majority of the related paper used different deep learning architecture according to the literature review it is demonstrated that deep learning with convolutional neural networks might have remarkable impacts on the automatic detection and automatic extraction of highly essential features from chest images which is related to the diagnosing of coronavirus the below figure 3 figure 4
demonstrated the accuracy of different models using ct x-ray images used for the detection of this virus as in the following table on the next page we have categorized each paper with their what technique data modality data source they have used and the result they obtained respectively with the given references for the treatment of coronavirus patients there is no effective drug present until now and due to the rapid increase in the number of cases of coronavirus patients effectual medicinal approach is urgently needed to treat the patients worldwide 18 earlier detection and prediction of coronavirus cases can decrease the spread of diseases 7 worldwide patients of coronavirus data will be a great source and useful for the researchers working on ai and ml to develop an automatic diagnostic tool therapeutic strategy against coronavirus patients and for the same type of pandemics for the coming future 18 19 in this study we have summarized different ai-based approaches and the current situation of spreading coronavirus according to the results and studies reviewed it is indicated that using convolutional neural networks which is the deep learning-based technique might have significant effects on automatic tools in terms of detection distinguishing and extraction of essential features automatically from the x-ray images that are related to coronavirus diagnosis due to the early stage of coronavirus there are still some limitations of the related studies that can be conquered or overcome in the future researches specifically an in-depth analysis requires a lot more patient data particularly those patients suffering from the novel coronavirus so the main point of focus for research work in the future could be distinguishing the patients indicating mild symptoms instead of pneumonia symptoms whereas these symptoms might not be visualized on x-rays more accurately or even not at all to be visualized as more authentic dataset will be available in the near future a more accurate ai-based prediction model can be formed we are working in this direction to procure a more authentic dataset as on today after reviewing all the papers mentioned in table 1
 we are working to develop cnn based deep learning methods for the prediction of coronavirus patients ai-based approaches are very useful for the automatic detection of coronavirus patients using x-ray and ct images the important point is to increase the number of datasets for coronavirus patients and using advance deep learning algorithms to achieve better performance for the detection and prediction of coronavirus also in spite of the fact that proper treatment or cure cannot be determined only from an x-ray or ct image these techniques would be useful as an initial screening of the patients appropriate use of quarantine actions should be implemented on the positive samples until a specific treatment and a complete examination procedure is followed  visualising covid-19 research pierre le bras azimeh gharavi david robb a ana vidal f stefano padilla mike chantler j  lebras p d robb a s padilla m chantler j  the world has seen in 2020 an unprecedented global outbreak of sars-cov-2 a new strain of coronavirus causing the covid-19 pandemic and radically changing our lives and work conditions many scientists are working tirelessly to find a treatment and a possible vaccine furthermore governments scientific institutions and companies are acting quickly to make resources available including funds and the opening of large-volume data repositories to accelerate innovation and discovery aimed at solving this pandemic in this paper we develop a novel automated theme-based visualisation method combining advanced data modelling of large corpora information mapping and trend analysis to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and research resources we apply this method on two recently released publications datasets dimensions covid-19 dataset and the allen institute for ais  the results reveal intriguing information including increased efforts in topics such as social distancing cross-domain initiatives eg mental health and education evolving research in medical topics and the unfolding trajectory of the virus in different territories through publications the results also demonstrate the need to quickly and automatically enable search and browsing of large corpora we believe our methodology will improve future large volume visualisation and discovery systems but also hope our visualisation interfaces will currently aid scientists researchers and the general public to tackle the numerous issues in the fight against the covid-19 pandemic   our online system demonstrates the hierarchical topic visualisation of the dimensions covid-19 dataset the principal panel on the left shows the at-a-glance overview of all the resources in the dataset the panels on the right visualise the sub-topics when selecting a main topic the topic descriptions the trend analysis and the drill-down to available resources moreover the system includes a search capability for the bottom-up discovery of known areas and support for advanced users the interactive visualisation is available on our research lab pages httpstrategicfuturesorgtopicmapscovid-19dimensionshtml as the scientific community grapples with the search for solutions to the covid-19 pandemic it also needs to accelerate the pace of innovation 5  globally research funding organisations are seeking to further drive an acceleration in research with additional short duration calls schemes to re-purpose existing funding and opening research resources eg 1 2  with this need for faster-paced research comes a need for timely processing assimilation and appreciation of the continually growing and evolving body of research literature that is originating rapidly from the global research effort recent studies have examined the growing literature on covid-19 and employed various methods and techniques to analyse it haghani et al 15 use bibliometric analysis with heat maps pie charts and bar graphs to describe covid-19 research areas and their relative importance others combine bibliometrics with text mining hossain 16 does this using bibliometrics combined with text mining for word co-occurrence and factorial analysis of the top keywords visualised in network diagrams and dendrograms similarly to hossain aguado-corts  castao 4 use bibliometrics and concurrence of keywords with network diagrams to visualise the analysis fister et al 10 use association rule text mining 3  then analyse word relationships and employ bar chart and word cloud visualisations wang et al 24 create an application which uses distantly supervised named entity recognition 25 and facilitates text queries it visualises query results with a doughnut chart one study by domingo-fernandez et al 9 took a subset of the literature focusing on drug targets generated a network graph by manually annotating evidence text from the corpus with biological expression language bel and explored the network graph with web applications built for the task lastly ahamed  samad 5 use a method of topic analysis where topics are identified using betweenness centrality measurement 12  subgraphs are then generated for the influential topics and they examine several topics in detail through these subgraphs in this paper we introduce a covid-19 research knowledge visualisation designed to help researchers and research strategists to get an at-a-glance overview of the research landscape  figure 1  this overview is laid out by topics and it is valuable to a identify connections between research areas b recognise trends over time by visualising research volume in specific topics and c find available resources from these large volume datasets of research we believe our approach to visualise the relationships between the concepts in the covid-19 research offers a more suitable pipeline for a frequently updateable visualisation we use latent dirichlet allocation lda 6 to build a topic model from the research corpus titles and abstracts in a similar manner to padilla et al 21  lda allows control of the number of topics and thus the ability to choose a suitable level of abstraction for both overview and detailed sub-topics the topics can be interrogated intuitively to reveal more detail and further visualised with a word cloud and a trend chart showing how the volume of research in that topic has changed over time our method which we describe in section 2 uses a pipeline perfected over recent years allowing a rapid and efficient generation of a newly updated visualisation as the corpus grows and develops thus allowing efficient and timely updating of the visualisation we first describe in this paper the methodology we use to create our visualisation for analysing the covid-19 research landscape and present the open visualisation itself providing the web address where it can be accessed then with the aid of screenshots from our interactive visualisations of two research corpora we describe four research trends which illustrate the benefits that our techniques bring to the understanding of covid-19 and coronavirus research in summary the contributions of this paper are 1 we explore a novel automated theme-based visualisation methodology combined with data modelling of large corpora of covid-19 resources 2 we develop covid-19 research information mapping and trend analysis to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and resources  in this section we present our methodology for visualising research information of a large volume in particular we use topic modelling to abstract thousands of research documents into a smaller hierarchical set of themes we then estimate the trends of these topics and group these into simplified bubble treemaps to create semantic overviews figure 2 illustrates this automated process our methods and visualisations are automatically generated from research corpora this automated approach is particularly suited for rapidly evolving knowledge datasets as it requires little oversight manipulation and can avoid common delays frequently encountered in manual classifications and processes we have chosen to first work on the dimensions covid-19 research dataset 22 as it curates from multiple sources the details of publications submitted in the last four months investigating the covid-19 outbreak we extracted the publications title and abstract from our experience we have found this information is sufficient for generating meaningful representative topics figure 3 in red in total 17 015 publications were retrieved from the 16 th version of this dataset dated from the 23 rd of april 2020 moreover our process can scale to any corpus input and we have also  we produced a two-level hierarchical topic model to allow for fast visual discovery cognitive processing and cognitive retention of the topics in users we did so by separately modelling 30 topics and 200 topics from the same publication data dimensions using mallets implementation of gibbs sampling for latent dirichlet allocation lda 6 14 18  care was taken to process the datasets appropriately this includes removing general stop words lemmatising terms and choosing the most beneficial parameters for the implementation to do so we used knowledge from previous work 17 and followed recommendations from boyd-graber et al 7  each sub-topic 200 was then assigned to the one main topic 30 with the least cosine distance between their document vectors because the cord-19 dataset contained more historical publications eg sars and mers we used larger numbers of topics to model 50 main topics and 400 sub-topics we believe it is valuable to understand the evolution of the themes while topic modelling offers the ability to abstract thousands of documents into a more digestible set of themes it is also essential to represent the time trajectory of the resources to enable researchers to compare current trends of research quickly using the publication date information and the topic weights in each publication we were able to construct the distribution of each topic over time this distribution details per topic and across dates the sum of this topics weights for publications of that date 1  these trend charts are displayed when a user interacts with a topic as shown in figure 1 for the social-measure-intervention sub-topic 1 it was noted that in the dimensions dataset some publications where scrapped with only the information 2020 as a result they were defaulted to the 1 st of january making this date showing an unusual volume of publications to simplify the reading of trend charts we have excluded this date  we use a novel implementation of bubble treemaps to visualise our sets of topics 13  we choose this representation as it allows us to show a the topics relative similarities with the placement of bubbles b the importance of the topic with the size of bubbles and c clusters of topics to facilitate cognitive recognition and memory we first computed the hierarchical topic similarities using the cosine distance between the topics document vectors and agglomerative clustering with a complete linkage criterion 23  then the bubble treemap placement algorithm on this hierarchy allowed us to position similar topics next to one another we used the sum of each topic weights across publications to set the bubble sizes showing the topics relative importance in the dataset to represent our two-level topic hierarchy we first mapped the main topic model the resulting visualisations for both datasets are shown in figure 3  then for each main topic we grouped the associated sub-topics and mapped these groups independently from one another it resulted as a set of sub-maps each accessible from their main topic finally this automated methodology allowed us to quickly abstract and visualise the content of thousands of research publications on covid-19 and coronaviruses in the next sections we present these visualisations and highlight compelling findings within them we present the bubble map overview of covid-19 research in figure 4  because it focuses specifically on the latest publications regarding covid-19 this map uses the dimensions datasets described in the previous section while this visualisation depicts eight clusters 2  we can identify three major regions in this map 1 on the top right-hand side corner we see topics dealing with patients treatment and care these cover domains such as common symptoms and clinical trials of possible treatments for covid-19 modelling of the disease hospital management transmission research and mental health concerns 2 on the left-hand side of the map the topics seem to represent domains like virology and micro-biology it is in this region that we find research investigating the actual virus sars-cov-2 its genome testing drugs effects and possible vaccines 3 at the bottom of the map we find bigger topics all with regards to the pandemic and its consequences this includes modelling the pandemic and discussion about its economic political and societal impacts in the next section we present four detailed analyses indicative of the emerging and rapidly changing focuses this pandemic has unfold virology vaccine antiviral health and treatment research gratefully constitute the core of the scientific response to covid-19 this unprecedented situation has however also seen the emergence of research in many other fields by modelling these topics and mapping them in semantic layouts we were able to discover exciting themes navigate this research and highlight interesting trends three months after first being reported by march 2020 this pandemic has launched an unprecedented global response to slow down its spread for many countries this response meant limiting human contacts and possible transmission to the maximum hence introducing social distancing we examine the importance of this issue in research as illustrated in figure 5a  which shows the sub-topic social-measure-intervention predominating other sub-topics in the main topic measure-model-social we can also measure the rapid ascent of this theme from february to march and april 2020 in figure 5b  corresponding to the point when many countries followed chinas suite and put lockdown measures in action to get a broader perspective we have searched for similar topics in the cord-19 dataset which comprise older research on coronaviruses from 1951 there we find that social distancing is only mentioned along covid figure 6a  furthermore while recent papers constitute 25 of the whole corpus we can see that the trend for this sub-topic only shows an increase in 2020 figure 6b  lower line it could be in part due to covid-19 being categorised in this sub-topic however checking the other labels in the sub-topic figure 6c  we only find a few occurrences of covid related terms spread epidemic pandemic compared to a larger number of labels describing social distancing measure lockdown quarantine intervention mitigation mobility it reflects that this is the first time in peacetime that a lockdown of this c although we could expect the topic trend to be caused by the term covid we see that most labels of that topic focus on distancing scale has been imposed to slow the exponential growth in transmission rate we found that the research categorised in these topics can provide a resource to understand and optimise social distancing measures such as a drawing models of the costs and benefits of these measures against their effectiveness b analysing the impact of asymptomatic carriers c suggesting effective exit strategies or d predicting long term social and travel behaviours measures such as social distancing have also triggered society to adapt and find new ways of continuing its activities we discuss these societal impacts below and how scientists address them topic modelling is a technique that allows us to analyse large amounts of information to get an overview of all emerging issues in text corpora as such we can gain access to often-overlooked topics or themes in the dimensions dataset for example we can find a group of sub-topics under education-health-state all with regards to the societal impacts of this pandemic we show these topics in figure 7a  although a large portion of publications describes a pandemic crisis or challenge we find that they also address issues such as public health and individual rights mental health education and economy responding to any outbreak requires policymakers to modify or enact new public health laws the covid-19 pandemic however has seen an even greater and more urgent public health response due to the high transmission rate of the virus research has therefore emerged to investigate the impact of these measures on individual rights 19  one common concern is individual liberty faced with forced social distancing and isolation another is medical privacy versus the prevention of virus spread ie by reporting and disclosing patients name to their employers or large-scale surveillance and contact tracing this latter example is even large enough to have its own sub-topic where technological vocabulary meets with ethics and privacy  figure 7b  as we have seen before social distancing has been a significant consequence of this pandemic it is therefore natural for researchers to explore the effects of such measures we have found three major fields of research in our topic maps the first concerns mental health and seems to follow two paths on the one hand there is research on the effectiveness of new approaches for pursuing existing therapies remotely either one-on-one or group support meetings to manage addiction on the other hand we recognise studies focusing on the overall populations stress anxiety and depression facing the pandemic as well as concerns towards the mental strain on medical and care staff working endlessly to help a with crises and challenging times emerge new issues regarding rights education or mental health b contact tracing and surveillance can enable better prevention but needs strong ethical considerations c mental health can be challenging in difficult times it is therefore crucial to understand and address such issues patients the importance of that field has made it one of the main topics in the dimensions dataset from which we show the detailed submap in figure 7c  the second addresses the new challenges for education with schools and universities closing to protect their students and prevent the virus from spreading teachers and administrations have to adapt to provide learning resources remotely it is then necessary to study the effectiveness of large-scale home-schooling as well as new online teaching methods for elementary higher and university levels on a more direct response to the outbreak we have also found a shift in medical training where advanced medical students are offered to start their internship early to help treat patient and relieve stress on the other medical staff finally there has also been significant concerns around the economic impacts of this pandemic present and future in particular we see economists and actuarial scientists analysing its damaging effects on the world market stock prices countries local economies moreover they are trying to provide solutions to such issues we also found that research into modern digital currencies and banking has been carried out to understand their potential as a solution to slow and prevent the progression of the virus looking at terms such as pneumonia and sars-cov-2 probably highlights best the scientific communitys response to a crisis of this sort looking at the topic treatment-coronavirus-pneumonia and its large sub-topic about pneumonia figure 8a  we find that this theme has peaked in february only to decline in the following two months figure 8b  we suspect that it reflects the first stage of a new disease outbreak medical staff report on the observation and study of similar symptoms rising in number of cases 8  meanwhile temporarily only known as 2019-ncov the virus responsible for this epidemic is sparingly used in publications it is only by march when it is named by the coronaviridae study group csg as sars-cov-2 20  that we see a net increase of its mention in publication figure 8c  taking over the initial reports on pneumonia at the same time looking at terms like simulation reveals the massive effort that has gone into developing better computational models to inform government policy many scientists have been working against the clock to use the limited available data to make predictions about the evolution of the pandemic the interface we developed can help speed up progress by helping scientists find relevant related publications in figure 9 we show an excerpt of the top a the sub-map of topic treatment-coronavirus-pneumonia shows how treating pneumonia has been as critical as preventing and controlling the epidemic b looking at the trend for topics around pneumonia we see it peaks in february before slowly decreasing a the sub-map of topic epidemic-model-time b an excerpt from the top document list we highlight in blue a relevant publication that would be difficult to discover just from its title fig 9  the cord-19 dataset interface allows researchers to discover relevant related work that might otherwise be difficult to find from the publication titles alone documents for the sub-topic data-model-predict within the topic epidemic-model-time and we highlight in blue one particular publication which would be otherwise difficult to find due to the lack of epidemic-related keywords in its title a close examination of the topics in the pandemic region of the main topic map see figure 4  allows us to visualise the trends in the volume of publication which share topics including particular countries upon closer inspection we found that we can track the progress of the pandemic around the world by following the volume of publications we show the evolution of these trends in figure 10  in figure 10a  we see how both main and sub-topics featuring wuhan and china display a trend showing publication volumes peaking in march and dropping back in april then if we examine the sub-topics of the model-case-number main topic we can see that the publication volumes trend for the sub-topic featuring korea japan iran and italy rose and levelled off figure 10b  for the europe sub-topic italy-spain-france-germany the publication volumes rose and have kept rising lastly the india sub-topic shows publication volumes which are just beginning to rise using topic modelling on large corpora combined with trend analysis we can see how scientists have followed the progress of the pandemic through their publications the interface moreover can help predict the trends in other countries and discover exciting movements in the relevance of other various research topics of the pandemic c publications around the outbreak in europe italy spain france and germany have grown steadily from march into april lower line d publications around the outbreak in india only appeared in april lower line fig 10  the trend analysis allows us to trace the virus spread through research publications note that fig 10a shows both main and sub-topics about wuhan and china fig 10b 10c and 10d show sub-topics nested under the main topic case-model-number in this paper we have presented one of the first works combining analysis and visualisation of large-volume literature datasets to highlight the impact of covid-19 on many research communities we show that it is possible to integrate advanced statistical topic modelling techniques into a visualisation pipeline which quickly a abstracts thousands of publication entries into smaller themes b extracts trend information and c produces at-a-glance semantic visual overviews of rapidly changing corpora this method its techniques and interfaces can help scientists browse search and access knowledge faster and stay abreast of evolving themes we have presented analysis using topic and visual information through different themes to summarise interesting aspects of the information inside the large volume of research literature this analysis highlights a the development of research regarding social distancing for the first time in 70 years b insights into cross-domain initiatives to understand the consequences of this unprecedented situation c the evolution in medical topics and d the unfolding of the pandemic through publications we hope the methods and findings may be useful as a reference guide for similar systems to stimulate new ideas and directions of research and to help in the fight against this pandemic the datasets used in this paper can be found here  httpsdimensionsfigsharecomarticlesdimensionscovid-19publicationsdatasetsandclinicaltrials11961063  httpswwwkagglecomallen-institute-for-aicord-19-research-challenge we have made available the visualisation interfaces at the following addresses  httpstrategicfuturesorgtopicmapscovid-19dimensionshtml  httpstrategicfuturesorgtopicmapscovid-19cord19html  tracking the twitter attention around the research efforts on the covid-19 pandemic zhichao fang rodrigo costas  the outbreak of the covid-19 pandemic has been accompanied by a bulk of scientific research and related twitter discussions to unravel the public concerns about the covid-19 crisis reflected in the science-based twitter conversations this study tracked the twitter attention around the covid-19 research efforts during the first three months of 2020 on the basis of nearly 14 million twitter mentions of 6162 covid-19-related scientific publications we investigated the temporal tweeting dynamic and the twitter users involved in the online discussions around covid-19-related research the results show that the quantity of twitter mentions of covid-19-related publications was on rising scholarlyoriented twitter users played an influential role in disseminating research outputs on covid-19 with their tweets being frequently retweeted over time a change in the focus of the twitter discussions can be observed from the initial attention to virological and clinical research to more practical topics such as the potential treatments the countermeasures by the governments the healthcare measures and the influences on the economy and society in more recent times  the global spread of the covid-19 pandemic an infectious disease caused by the pathogen severe acute respiratory syndrome coronavirus 2 sars-cov-2 has already unleashed an unprecedented impact on public health economy and human society worldwide mckee  stuckler 2020  as of june 2 2020 it is reported by the world health organization who that there have been over 61 million confirmed cases of covid-19 globally carrying a mortality of approximately 6 1 on january 30 2020 the who officially declared that the covid-19 outbreak constitutes a public health emergency of international concern pheic 2 making it become the sixth pheic in the 21 st century after the 2009 h1n1 pandemic the 2014 polio declaration the 2014 ebola virus disease west africa the 2016 zika virus epidemic and the 2018 ebola virus disease kivu harmer et al 2020  in response to this ongoing public health emergency scientists around the world have been contributing their expertise to the understanding and potential treatments of the novel coronavirus disease leading to an explosion of research outputs covering a range of subject fields callaway et al 2020  the strong concerns about this public health crisis arose within not only the scientific community but also the social media landscape millions of people are talking about the coronavirus on social media yammine 2020  particularly on twitter where there are massive conversations around a variety of topics related to covid-19 chen et al 2020  amongst these conversations up-to-date research progress made by scientists is one of the most important elements reflecting the twitter attention paid toward scientific discoveries in fighting the pandemic against this background through the lens of scholarly twitter metrics which focus on the recorded events of acts on twitter related to scholarly documents or scholarly agents haustein 2019  this study aims to disclose the focus and dynamic of public concerns about relevant research efforts during the time of pheic specifically in the case of the covid-19 pandemic as one of the most prevalent social media platforms the role that twitter plays in scholarly communication has been widely investigated in previous studies sugimoto et al 2017  on the whole twitter mention data outperform most other altmetric indicators in terms of not only the data coverage but also the accumulation speed after publication haustein et al 2015  making it possible to measure the twitter reception of research outputs in a relatively short period of time considering the above advantages the potential of twitter metrics in research evaluation has already been discussed from both conceptual and practical perspectives wouters et al 2019  moreover captured twitter attention toward scientific publications has been analyzed for identifying the focus of interest of twitter users on some specific subject fields or research topics for example robinson-garcia et al 2019 mapped twitter attention distributing within the field of microbiology and found that topics about translational medicine future prospects and challenges and bacterial outbreaks are prominent with regard to their twitter mentions received haunschild et al 2019 made comparisons for a set of climate change publications among the networks created based on the author keywords and the tweet hashtags finding that most tweeted research topics are those about the consequences of climate change for humans taking the area of big data as a case study lyu and costas 2020 observed that hashtags embedded in the twitter mentions of big data research are mainly about technologies showing a similar technical orientation as author keywords in publications these existing studies indicate that detected twitter attention toward scientific research opens a window for tracking broader public concerns beyond academia about specific topics the study of twitter activities around scholarly outputs can be used to analyze broader perspectives on science-society interactions in what has been labeled as heterogeneous couplings  these being relevant to characterize and study how in a pandemic scientific results are being received and communicated among very diverse audiences the outbreak of covid-19 is not merely an urgent threat to global health but also a significant challenge to the current scientific system on the one hand being confronted with such emergency deficiencies existing in the scholarly communication system have further been proven to be obstacles in the way to a more open and efficient scholarly environment larivire et al 2020  on the other hand the transformation from the traditional status of journals to faster online publishing channels has been facilitating the explosion of covid-19-related scientific literature resulting in an over-abundance of scientific information brainard 2020 torres-salinas et al 2020  to delineate the massive research progress there are a range of studies investigating covid-19-related literature from several scientometric perspectives such as the coverage of publications in diverse scholarly databases kousha  thelwall 2020  the identification of hot topics haghani et al 2020  the international collaboration patterns and the funding sources fry et al 2020 zhang et al 2020  etc amongst these existing studies some of them have a focus on the social media performance of covid-19-related publications for instance colavizza et al 2020 took relatively exhaustive investigations to the cord-19 database namely the covid-19 open research database which captures global research on covid-19 and coronavirus family of viruses  and concluded that the cord-19 database covers a wide range of research on viruses in general but not only on covid-19 and coronavirus in particular they found that cord-19 publications published in 2020 especially those on topics of pressing relevance are disproportionately popular on social media torres-salinas et al 2020 focused on the uptake of open access on covid-19-related literature and found that nearly 675 of publications in their dataset are openly accessible moreover they confirmed the advantage of open access publications in obtaining social media attention over non open access publications based on the relationships between early citations in the database dimensionsai and social media mentions kousha and thelwall 2020 observed a high degree of convergence between covid-19-related publications shared in the social web and citation counts suggesting that altmetrics twitter counts in particular might be helpful for quickly filtering useful new documents from the daily flood of covid-19-related literature taken together although the coverage and performance of covid-19-related publications in social media have been analyzed in many existing studies with diverse data sources in contrast there are less explorations into what and how the public discuss about covid-19-related research progress on social media in response focusing on the twitter attention toward covid-19-related research during the time of pheic the main objectives of this study are two-fold the first one is to unravel the tweeting patterns around covid-19-related research from the perspectives of the temporal accumulation and engaging twitter users and the second one is to trace the evolution of the focus of the twitter discussions around covid-19-related research over time we addressed the following specific research questions rq1 how are the covid-19-related scientific publications mentioned on twitter over time since the outbreak rq2 by drawing on the twitter users profile descriptions who are the actors tweeting covid-19related research what kinds of twitter users are more influential in terms of being retweeted rq3 based on the titles what are the main research topics of covid-19-related publications which research topics attract higher levels of twitter attention rq4 on the basis of the full tweet texts and hashtags what are the topics raised by the twitter users when discussing the covid-19-related publications how does the focus of interest of their twitter discussions change over time ever since the outbreak of covid-19 an increasing number of academic publishers and organizations have been gathering and compiling global relevant research and making them open access for the sake of public concerns our meta-dataset stemmed from two databases one is the database of covid-19-related publications established and updated by the who 3 the other one is the file made available by dimensionsai which contains all covid-19-related published articles preprints datasets and clinical trials from dimensionsai 4 on april 4 2020 a total of 8163 distinct scientific publications were extracted from these two data sources consisting of 3478 publications tracked by the who and 6338 publications provided by dimensionsai 1653 publications are overlapped the bibliometric information of the above set of publications were retrieved through the dimensionsai api on april 4 2020 as well on the whole 6582 of them accounting for 806 are indexed by dimensionsai with the doi or pubmed id matched since we focus on the research efforts occurred in the first three months of 2020 from january 1 2020 to march 31 2020 publications published beyond this time window were filtered out in this study doi created date namely the date on which a doi was created was collected through the crossref api to serve as the more precise proxy of publication date fang  costas 2018  while the doi created date is unavailable publication date recorded by dimensionsai was used as the alternative 5 based on the publication date a total of 6162 publications were selected as the final dataset in order to track twitter attention toward covid-19-related publications twitter mention data of selected publications recorded by altmetriccom were queried through the api between april 4 and april 7 2020 based on the list of tweet ids responded by the altmetriccom api we further collected detailed twitter mention information with the twitter api on april 7 2020 finally after excluding unavailable twitter mentions caused by the deletion of tweets and the suspension or protection of twitter accounts  a total of 4195 publications accounting for 681 were found to have some twitter mention data accrued in the same time window as the publication date of selected publications involving with 1374231 distinct tweets posted by 655494 unique twitter users on the basis of tweet objects information responded by the twitter api 6 specifically the retweeted replied and quoted relationships of each tweet twitter mentions were classified into two main categories original tweet including regular original tweet and reply tweet and retweet including simple retweet and quote tweet table 1 lists the concepts of these types of tweets together with their presence in our dataset simple retweets contribute most of the twitter mentions of covid-19-related publications accounting for 761 note that in this study original tweet and retweet refer to the two main categories since except for simple retweet the other three sub-categories contain original contents from the twitter users tweets with original contents analyzed in the text analysis parts of this study considered regular original tweets reply tweets and quote tweets 3 see more information about the covid-19-related database of who at httpswwwwhointemergenciesdiseasesnovel-coronavirus-2019global-research-on-novel-coronavirus-2019ncov accessed 2020-05-20 4 see more information about the file provided by dimensionsai at httpswwwdimensionsainewsdimensions-is-facilitating-access-to-covid-19-research accessed 2020-05-20 5 amongst 6582 publications indexed by dimensionsai 6300 of them have a doi created date recorded by crossref as their publication date while the remaining 282 publications were assigned with the dimensionsai publication date 6 see more introduction to tweet objects in the json data responded by the twitter api at httpsdevelopertwittercomendocstweetsdata-dictionaryoverviewtweet-object accessed 2020-05-20  a reply tweet is a response to a tweet it is generated by clicking the reply icon from a tweet typing in the compose box and clicking the reply button yes 101078 73 simple retweet a simple retweet is a re-posting of a tweet by simply using twitters native retweet functionality without comments attached it is generated by clicking the retweet icon from a tweet and selecting retweet directly no 1046058 761 a quote tweet is a re-posting of a tweet by using twitters native retweet functionality with original comments attached it is generated by clicking the retweet icon from a tweet and selecting retweet with comment to add comments yes 77664 57 note table 1 only briefly introduces the methods of generating different types of tweets by manual operation on twitter it should be noted that tweets can be created programmatically by apis as well see more introduction to the concepts and methods of generating different categories of tweets at httpshelptwittercomenusing-twitter accessed 2020-05-20 moreover because one can reply to a tweet with quoting other tweets in the reply tweet it is possible for a tweet to be classified as a reply tweet and a quote tweet at the same time note that in this study to avoid the overlap in the calculation of numbers of this kind of tweets with dual sub-categories they were only counted as reply tweets because the following quoting behavior is resulted by the replying action happened at first in addition to the category assigned detailed information responded by the twitter api were also appended to each tweet for further analysis including the full tweet text the post date ie the date on which the tweet was posted the hashtags used the number of retweets received and the language of tweet text etc as well as the information of twitter users such as the profile descriptions to show overall pictures of the covid-19 dataset and the twitter discussions around them vosviewer van eck  waltman 2010 was utilized as the visualization tool for text mining and constructing cooccurrence networks of terms extracted from the titles of publications the descriptions of twitter users and the tweet texts respectively as well as for creating hashtag coupling 7 networks besides the socalled overlay visualization of vosviewer was employed to exhibit additional information on top of the generated base maps such as the average number of twitter mentions received by terms the average number of retweets received by twitter users and the post date of twitter mentions and their appendant hashtags for measuring how recently the terms and hashtags in twitter mentions were posted by users the post month and post day of tweets were used to create the post date score with post month serving as the integer part and post day the decimal part eg the post date score of a tweet posted on february 1 is 201 the higher the post date score the more recently the tweet was posted the post date score of each tweet was also applied to its used terms and hashtags in order to examine the extent to which the tweet texts are different from the corresponding titles of publications we cleaned the full texts of tweets with original contents in english at first by removing the embedded urls handle names of mentioned twitter users and the -sign as well as the -sign of hashtags the texts of hashtags were kept because they often contain specific meaning in this study the text similarity between titles of publications and related cleaned tweet texts was measured by cosine similarity score cosine similarity scores were calculated for each pair of texts by using the python scikit-learn package pedregosa et al 2011  the higher the cosine similarity score the more similar the two pieces of texts it should be noted that in the analyses involved with full tweet texts we only took into consideration the 209003 tweets with original contents in english language which account for the majority about 637 in the sub-categories of all tweets with original contents as depicted in table 1  tweet texts in other languages were not included in the text analysis part in this study coinciding with the four proposed research questions this section characterizes the twitter attention toward covid-19-related research from four perspectives the first one studies the temporal accumulation patterns of tweets mentioning covid-19-related research over time with a focus on the top-10 most tweeted publications the second perspective shows the profile characteristics of twitter users who have participated in sharing covid-19-related research to the twitter landscape the third perspective presents an overall picture of research topics of covid-19-related publications and identifies those with higher levels of twitter attention received the last perspective analyzes the discussions around covid-19-related research on twitter based on both full tweet texts and hashtags used to disclose twitter users focus of interest and its evolution over time since the reported outbreak of covid-19 scientists from around the world have made remarkable contributions to the understanding of this novel infectious disease which is represented by the increasing number of related publications as shown in figure 1 a the quantity of covid-19-related publications presents a rising tendency in the first three months of 2020 particularly in late march around 681 of covid-19-related publications in our dataset have been mentioned by twitter users at least once figure 1 b shows the temporal distribution of the twitter mentions it is obvious that intensive twitter attention toward covid-19-related research started to rise on january 21 2020 and reached the first peak on january 25 following the wuhan citys lockdown on january 23 in february when the outbreak was temporarily confined to china twitter attention toward covid-19-related research maintained a relatively stable state however along with the global outbreak in full swing and the mounting research outputs in march twitter attention steeply increased especially after march 15 when the global confirmed cases outside of china exceeded those reported in china for the first time to provide a more detailed analysis about how covid-19-related publications were shared and discussed after they have been published we selected the top-10 most tweeted publications from the dataset as a case study table 2 lists the information of these ten publications ranked by their number of twitter mentions accrued the publication entitled the proximal origin of sars-cov-2 whose main conclusion is that sars-cov-2 is not a purposefully manipulated virus from laboratories andersen et al 2020  attracted the most attention on twitter in our observation time window followed by research related to clinical treatments public health countermeasures clinical characteristics and transmissibility of the virus etc the twitter mentions of the top-10 publications were depicted in figure 2 based on their post date twitter mentions of different publications are highlighted by different colors in each bar according to their proportion in total twitter mentions posted in that day the publication date of each publication is annotated on the top of the corresponding bar to show how fast the twitter attention was accumulated after they were published in general twitter attention toward a specific highly tweeted publication concentrated within the first few days after its publication for example publication 4 as a preprint ranking in the top ten which has been withdrawn by the authors attracted a high level of twitter attention in the very early stage of the coronavirus crisis compared to other publications most of the twitter mentions of publication 4 occurred in the first two days after its publication similar patterns can be observed for most other highly tweeted publications moreover due to the existence of advanced online version some publications have already accumulated a certain amount of twitter attention before their publication date recorded eg publication 9 and 10 in a word in the case of the top-10 most tweeted publications twitter users were following covid-19-related research in next to no time since their appearance besides the twitter attention concentrated in the first few days and then gradually faded away this finding is in line with what has been observed in a previous study based on a more generalized dataset shuai et al 2012  in which twitter mentions were also found to have relatively short delays and narrow time spans  in our dataset there are 161390 unique users who have posted at least one tweet with original contents about covid-19-related publications by whom the research outputs were initially brought into the twitter landscape to reveal the characteristics of the users by drawing on their profile descriptions written in english figure 3 a illustrates the co-occurrence network of terms that users used to describe themselves size of nodes is proportional to their frequency of occurrence for clear visualization the 269 most relevant terms were kept thereby algorithmically generating four clusters cluster 1 red cluster mainly contains terms that belong to personal description about the private and personal life like husband father and mom etc cluster 2 blue cluster is formed by terms implying users interest and faith like music dog and christian etc cluster 3 green cluster is comprised by terms referring to academic role in the scientific community such as professor phd student and university etc at last cluster 4 yellow cluster includes terms that express users attitude and position on twitter like endorsement like and retweet etc in general the clustering is consistent to a large extent with the main textual patterns of twitter users descriptions found in the study conducted by daz-faes et al 2019 which is based on a more universal dataset consisting of a random sample of 200000 twitter users who have ever tweeted at least one scholarly output according to their results terms extracted from profile descriptions are clustered into four communities as well including personal description academic role business and practice role and attitude and position the difference of involved twitter users between the universal situation and the covid-19 case is that users with business and practical roles are less active while those highlighting their interest and faith and academic roles become more predominant in the case of covid-19 indicating that research progress taken place on covid-19 might attracted a higher proportion of general users and scientific researchers after summing up the number of retweets received by all tweets with original contents each twitter user considered in figure 3 a were assigned with their own total number of obtained retweets as a result in figure 3 b terms were scored by the average number of retweets that twitter users have received terms related to academic roles have predominately higher retweet scores suggesting that twitter users describing themselves with academic roles are more likely to get retweeted in the case of covid-19 especially for senior researchers and users from the biomedical and health sciences-related fields such as epidemiology immunology and cardiology therefore during the covid-19 crisis scholarly-oriented twitter users with expertise seem to be playing an influential role in updating and disseminating science-based information on twitter  in order to show an overview of the research topics that authors focused on figure 4 on the basis of the total number of twitter mentions that each publication has accumulated terms were scored by the average number of twitter mentions accrued as shown in figure 4 b as to each research direction there exist some research topics with relatively intensive twitter attention for example in the field of epidemiology the actions taken by the governments against the coronavirus pandemic which have made a significant impact on peoples daily lives are of interest to twitter users like quarantine and lockdown in the field of virology twitter users paid quite a lot of attention to the exploration of the animal source of the novel virus as scientists did mallapaty 2020  making origin and related genomic structure terms the most tweeted topics in this field in addition chloroquine and hydroxychloroquine the two widely-used anti-malarial drugs attracted a great deal of attention as well because of their controversial reported efficacy and safety in inhibiting the novel coronavirus in vitro and in different clinical trials gautret et al 2020 liu et al 2020 molina et al 2020  however on the whole topics in clinical research got higher levels of twitter attention as the topics directly related to the severity of the pandemic and the health of humans clinical characteristics mortality ncov infection risk factor and some other research topics have been of great concern by the public while sharing covid-19-related research on twitter users might express their own opinions on or beyond the tweeted research to quantitatively reflect the extent to which science-based twitter conversations are different from the titles of publications figure 5 shows the results of the calculation of cosine similarity scores between publication titles and related full tweet texts there are around 453 of tweets with original contents added showing a big dissimilarity with the mentioned publications titles with the cosine similarity scores falling between 0 and 01 over 74 of twitter mentions have the values of similarity lower than 05 implying that most twitter users conducted some commenting rather than just repeating the titles of the publications this finding is further confirmed by the gap between the average length of full tweet texts and that of publication titles in each statistical bin of similarity scores most twitter mentions are formed by longer texts than titles indicating the extension of contents made by twitter users while sharing scientific information different from the devoid of original thought of tweet contents observed in previous studies based on the samples from the fields of dentistry and big data lyu  costas 2020 robinson-garcia et al 2017  during the covid-19 crisis twitter users were more inclined to set forth their original opinions and attitudes when tweeting about recent scientific outcomes  given that twitter users generally enlarged the contents of their tweets beyond the publication titles in this part we further explored what they talked about around covid-19-related research and how the focus of their discussions changed over time figure 6a shows the co-occurrence network of terms used in the tweets with original contents in english size of nodes is proportional to the occurrence frequency of each term as the corpus of full tweet texts is much larger than that of publication titles we kept more terms in the map to present a relatively complete landscape of science-based twitter conversations the 601 most relevant terms were retained for clear visualization thus generating five clusters of the main topics in the twitter discussions related to covid-19-related research terms in cluster 1 red cluster reveal the public concerns about the clinical characteristics and transmission risks of the infectious disease caused by the virus besides some specific symptoms eg fever and cough and the mortality the risk of being infected was also seriously concerned for different age groups particularly for childkid these terms were frequently occurred with wuhan the initial city struggled with the novel coronavirus terms in cluster 2 yellow cluster reflect the discussions about clinical trials and potential treatments that have been conducted to treat or prevent covid-19 including some general terms like treatment trial and drug etc as well as more specific therapies like chloroquine and hydroxychloroquine the two controversial drugs as aforementioned which have caused a wave of misinformation due to the unproven effectiveness in treating covid-19 erku et al 2020  as the basis of knowing more details of the causative coronavirus terms about its genomic structure and probable origins were also frequently mentioned by twitter users in cluster 3 blue cluster including not only the jargons of genomics eg receptor and ace2 but also the comparisons of similarity with other viruses like hiv based on which twitter users discussed about several animal sources supposed to be the probable origins of the coronavirus in the nature cluster 4 purple cluster includes a sequence of terms about the virus persistence under different circumstances especially on various surfaces eg cardboard plastic and copper and in high temperature and high humidity last but not least terms related to the governmental measures and social responses constitute cluster 5 green cluster which contains a range of topics reflecting the tremendous influences that covid-19 have exerted on human society such as the countermeasures applied by the governments across countries and regions and the related issues emerged in education eg school and economy to track the evolution of the focus of twitter discussions as shown in figure 6 b we assigned each term with the average post date score in general terms in cluster 1 and 3 that related to the clinical characteristics and genomic structure are in lighter colors because they were mentioned by twitter users in the relatively earlier stage of the coronavirus outbreak in contrast terms in cluster 2 4 and 5 are in darker colors suggesting that with the spread of the covid-19 pandemic the focus of the twitter discussions transferred from clinical and virological research to topics on how to treat and prevent the infectious disease how do the governments face the public health emergency and how does the pandemic affects society considering that hashtags are deemed as concept symbols indicating particular concepts in relation to the mentioned publications haustein et al 2016  just like author keywords to scientific publications haunschild et al 2019  hereby figure 7 a shows the hashtag coupling network of 219 english hashtags that have been used at least 50 times in total 8 there are four clusters generated based on these hashtags hashtags in cluster 1 green cluster are mainly about the coronavirus outbreak in china and virological research cluster 2 yellow cluster includes hashtags of some medical technologies and hashtags labelling the pubmed updates hashtags in cluster 3 red cluster highlights some political and misinformation debates in which chloroquine and hydroxychloroquine are included finally hashtags in cluster 4 blue cluster indicates the global spread of the outbreaks in many countries and a series of healthcare measures that were called on to prevent the virus transmission similarly figure 7 b shows the overlay visualization of the hashtag coupling network scored by the post date it is obvious that hashtags located in the lower right part hold higher post date scores than those in the upper left part which means that in the early stage of the coronavirus outbreak the pandemic in china was frequently discussed by twitter users as well as the basic virological research on the novel virus however along with the global spread the focus of twitter attention transferred to other countries suffering from the crisis by the meantime twitter users started to attach hashtags of healthcare measures such as flattenthecurve socialdistancing and stayhome to underline the importance of taking actions to stop the virus  the global outbreak of covid-19 has triggered an avalanche of scientific research and public discussions thereby generating a so-called infodemic referring to an over-abundance of information related to the epidemic that increases the risk of misinformation 9 which is also concerned by twitter users in the light of their use of hashtags pointing controversial misinformation as emphasized by xie et al 2020  it is of great importance to study information behaviors during global health crises against this background we focused on the information behaviors of sharing and discussing covid-19-related research on twitter by employing nearly 14 million twitter mentions as the traces of broader public engagement with covid-19-related publications we tracked the changing public concerns about the novel pheic during the first three months of 2020 since the initial outbreak of covid-19 at the very end of 2019 in china there have been a growing torrent of new scientific publications on this infectious disease and the causative novel coronavirus brainard 2020  as one of the most significant windows for the public to get better understanding of the unprecedented pandemic continuous research progress has widely attracted twitter attention from the public causing a wealth of twitter mention data that connect the science landscape and the social media landscape for the 6162 covid-19-related publications in our dataset 681 of them have been mentioned on twitter at least once the value of twitter coverage is much higher than those found in previous surveys based on larger-scale multidisciplinary samples 133 by costas et al 2015  215 by haustein et al 2015  implying that the twitter attention to covid-19-related research is much larger than usual alongside the pandemic of covid-19 entered a new stage with rapid spread in countries outside of china in the middle of march 2020 bedford et al 2020  the twitter attention toward research progress increased drastically in late march accordingly overall the tweeting patterns around covid-19-related research comply with the observed general tweeting patterns from the perspectives of both temporal accumulation and involved twitter users through the lens of the top-10 most tweeted publications we found that twitter attention accumulated soon after the publication and concentrated in the following first few days which is in accordance with the general temporal accumulation patterns of scholarly twitter mentions shuai et al 2012  based on the users profile descriptions the composition of twitter users with interest in discussing covid-19-related research was found to be similar with the general communities of twitter users who shared scholarly literature daz-faes et al 2019 twitter users primarily described themselves from the angles of personal description interest and faith academic roles and attitude and position in terms of being retweeted twitter mentions originated from users with academic roles got substantially more retweets especially for senior researchers and users describing themselves as being related to the field of biomedical and health sciences therefore in the case of covid-19 sciencebased information posted by academically related users are more influential in the twitter dissemination network based on our dataset topics extracted from the titles of covid-19-related publications were clustered into three main research directions including epidemiological research virological research and clinical research in each direction there are some research topics with higher levels of twitter attention received same as what has been observed by colavizza et al 2020  in general research topics of pressing relevance received more twitter attention especially for those in relation to clinical features infection treatments and countermeasures different from the strong concordance between scholarly contents and related tweet contents found in other fields lyu  costas 2020 robinson-garcia et al 2017  the twitter discussions around covid-19-related research show a higher degree of engagement according to the dissimilarity between publication titles and related full tweet texts instead of simply repeating the titles twitter users expanded the tweet contents with their own opinions in the science-based twitter conversations the expanded twitter discussions are mainly about five topics related to the covid-19 pandemic including the clinical characteristics and transmission risks the clinical trials and potential treatments the genomic structure and probable origins the virus persistence under different circumstances and the governmental measures and social responses on the one hand these topics frequently mentioned by twitter users have a strong connection to the scientific discoveries on the other hand twitter users talked about covid-19-related research with particular focus on its impacts on the real world covering a range of concerns about children education economy and society besides there is a change trend of the focus of the twitter discussions across the above topics over time at the early stage when the covid-19 pandemic was confined to china most twitter discussions were associated with the research on the virological and clinical characteristics however over time the focus of the twitter discussions transferred to the pursuit of effective treatments the countermeasures by the governments and the strong influences on social and economic activities this finding was further confirmed by the usage of hashtags with the hashtags related to china and basic virological research dominated at the beginning and those about global outbreaks potential treatments and healthcare measures stood out latterly in light of these results during the age of worldwide pandemic it appears that understanding the origin and features from the virological and clinical aspects was the first priority for the public concerns before a global outbreak while along with the aggravation of situation and the improvement of cognition the public concerns transferred to a broader scope beyond academia particularly about the progress in treatment the countermeasures by the governments the healthcare measures for self-protection and the influences on society the long-term impacts made by covid-19 are still continuing in future research we will continue to study the evolution of the twitter attention toward covid-19-related research to unveil the differences of local public concerns we plan to respectively explore the twitter attention from different geographic locations and in different language contexts there are some limitations that should be acknowledged in this study first we relied on the bibliometric information provided by dimensionsai for most analyses so only covid-19-related publications with doi or pubmed id indexed by dimensionsai were taken into account while those without available identifiers or not indexed by dimensionsai were not included in our analyses second due to the limitation of bibliometric information that we retrieved from dimensions only the titles were analyzed as the representatives of scholarly contents of covid-19-related publications while other bibliometric information like abstract keywords and full texts were left out lastly in the analyses of full tweet texts and hashtags we only considered the tweets written in english although english tweets account for the majority of the twitter discussions they might only reflect the opinions owned by english-speaking users this study tracked the twitter attention surrounding the covid-19-related research efforts during the first three months of 2020 to disclose the public concerns about the covid-19 pandemic embedded in the science-based conversations on twitter twitter mentions of covid-19-related publications presented a rising tendency in parallel with the increasing number of research outputs especially in late march 2020 amongst the twitter users sharing covid-19-related research academically related users with relevant expertise played an influential role in disseminating research outputs on twitter according to their higher levels of retweets received in general research topics with pressing relevance attracted more twitter attention such as those about clinical characteristics infection treatments and countermeasures with time elapsing the focus of the twitter discussions around covid-19-related research evolved from virological and clinical research findings to the potential treatments the countermeasures by the governments the healthcare measures for self-protection and some broader discussions about the social influences showing a dynamic of the public concerns during the age of public health emergency overall our results support the idea that the analysis of the twitter engagement around the covid-19-related research efforts can provide evidence of other types of societal perceptions around the pandemic therefore the study of the social media response to the scholarly response can be a useful approach to inform the existence and development of broader concerns around the pandemic  lest we forget a dataset of coronavirus-related news headlines in swiss media alireza ghasemi amina chebira  we release our covid-19 news dataset containing more than 10000 links to news articles related to the coronavirus pandemic published in the swiss media since early january 2020 this collection can prove beneficial in mining and analysis of the reaction of the swiss media to the covid-19 pandemic and extracting insightful information for further research we hope this dataset helps researchers and the public deliver results that will help analyse the pandemic and potentially lead to a better understanding of the events  the covid-19 pandemic started in switzerland on february 25 th 2020 when the first infection was officially reported in the italian-speaking canton of ticino 1  2  soon the pandemic spread around the country on all cantons and switzerland became one of the most infected countries on a per-capita basis 3  4  the swiss government started putting in place various measures to control and suppress the pandemic gatherings were limited and later totally banned following by closure of all except essential business and finally closing land borders with neighbouring countries 5  these measures helped control the spread of the virus and significantly decreased the number of active and daily new cases in switzerland with the success confirmed government started gradually lifting the established restrictions from late april 6  finally the june 15 th re-opening land borders with the neighbouring countries marked the end of the pandemic in switzerland at least for the first wave 7  since the first recorded case of the covid-19 virus in switzerland and far before as it was gaining attention around the world the swiss media started covering the topic from various aspects including the everyday news about the state of the country the immediate effects and longer-term consequences of the pandemic given the multi-lingual and multicultural nature of switzerland interesting analyses can be accomplished to see how the media coverage of the pandemic has been managed and what topics in respect to the pandemic have been important to the swiss media and hopefully by proxy to the swiss public in order to help the research community and the public be able to analyse and seek answers to the above questions we at elca decided to release our covid-19 news dataset containing more than 10000 links to news articles related to the coronavirus pandemic published in the swiss media since early january 2020 we hope this dataset helps researchers make insightful analyses on the reaction of the swiss public to the pandemic and deliver results that help shape a better response in the prospective future cases we tried to cover the most popular swiss newspapers and news websites therefore we chose a total of 10 news sources in german five in french three in italian and also two english-speaking swiss news websites in order to make the data more accessible to to researchers outside switzerland table i  we backdated our data collection to late 2019 and started scanning front pages of the selected news sources in consecutive days extracting headlines of the articles initially articles with any of the following keywords in the title were deemed coronavirus-related  pandem to account for different spellings of the concept in different languages this inevitably leads to false negatives in order to reduce such false negatives we read at a later stage the synopsis of the article and searched for the keywords also in the body yielding more positive results the distribution of the languages in the dataset is depicted in figure 1  the first article we could find in the swiss media has been published on january 8 th in the french-speaking news portal 20 minutes titled a new coronavirus appears in china 8 we have made a web application to simplify exploring and browsing the data and reading the collected news articles the web application is available at httpscovidnewsdatasetherokuappcom we explained in this article our swiss covid-19 dataset and how it has been collected we publish the dataset hereby for public use along with an online visualisation application to help explore and look at the news articles of swiss media during the pandemic in switzerland we hope this dataset proves useful in analysis of the pandemic era and the public response to it in switzerland  recovery a multimodal repository for covid-19 news credibility research xinyi zhou emilio ferrara reza zafarani   as of june 4 th  the covid-19 pandemic has resulted in over 64 million confirmed cases and over 380000 deaths globally 1 governments have enforced border shutdowns travel restrictions and quarantines to flatten the curve 2  the covid-19 outbreak has had a detrimental impact on not only the healthcare sector but also every aspect of human life such as education and economic sectors 10  for example over 100 countries have imposed nationwide even complete closures of education facilities which has lead to over 900 million learners being affected 2 statistics indicate that 33 million americans applied for unemployment benefits in the week ending on march 21 th and the number doubled in the following week before which time the highest number of unemployment applications ever received in one week was 695000 in 1982 7 along with the covid-19 pandemic we are also experiencing an infodemic of information with low credibility regarding covid-19 3 hundreds of news websites have contributed to publishing false coronavirus information 4 individuals who believe false news articles claiming that for example eating boiled garlic or drinking chlorine dioxide an industrial bleach can cure or prevent coronavirus might take an ineffective or extremely dangerous action to protect themselves from the virus 5 given this background research is motivated to combat this infodemic hence we design and construct a multimodal repository recovery to facilitate reliability assessment of news on covid-19 we first broadly search and investigate 2000 news publishers as past literature has indicated there is a close relationship between the credibility of news articles and their publication sources 22 in total 2029 news articles on coronavirus are finally collected in the repository along with 140820 tweets that reveal how these news articles are spread on the social network the main contributions of this work are summarized as follows first we construct a repository to support the research that investigates 1 how news with low credibility is created and spreads in this covid-19 pandemic and 2 ways to predict such fake news the manner in which the ground truth of news credibility is obtained allows a scalable repository as annotators need not label each news article that is time-consuming and instead they can directly label the news site second recovery provides multimodal information on covid-19 news articles basically for each news article we collect its news content and social context information revealing how it spreads on social media which covers textual visual temporal and network information third we conduct extensive experiments using recovery which includes analyzing our data data statistics and distributions and providing baseline performances for predicting news credibility using recovery data these baselines allow future methods to be easily compared to baselines are obtained using either news content alone or combined with social context information within a framework of supervised machine learning the rest of this paper is organized as follows we first detail how the data is collected in section 2 the statistics and distributions of the data are presented and analyzed in section 3 experiments that use the data to predict news credibility are designed and conducted in section 4 whose results can be used as benchmarks finally we review the related dataset in section 5 and conclude in section 6 the overall process that we collect the data including news content and social media information is presented in figure 1  to facilitate scalability news credibility is assessed based on the credibility of the media site that publishes the news article based on the process outlined in figure 1  we will further detail how the data is collected answering the following three questions 1 how to identify reliable or unreliable news sites mainly releasing real news or fake news which we address in section 21 having determined such news sites 2 how do we crawl covid-19 news articles from these sites and what news components are valuable for collection section 22 and given covid-19 news articles 3 how can we track their spread on social networks section 23 to determine a list of reliable and unreliable news sites we primarily rely on two resources newsguard and media biasfact check newsguard 6 newsguard is developed to review and rate news websites its reliability rating team is formed by trained journalists and experienced editors whose credentials and backgrounds are all transparent and available on the site the performance credibility of each news website is assessed based on the following nine journalistic criteria 1 does not repeatedly publish false content 22 points 2 gathers and presents information responsibly 18 points 3 regularly corrects or clarifies errors 125 points 4 handles the difference between news and opinion responsibly 125 points 5 avoids deceptive headlines 10 points 6 website discloses ownership and financing 75 points 7 clearly labels advertising 75 points 8 reveals whos in charge including possible conflicts of interest and 5 points 9 the site provides the names of content creators along with either contact or biographical information 5 points where the overall score of a site is between 0 to 100 0 indicates the lowest credibility and 100 indicates the highest credibility a news website with a newsguard score higher than 60 is often labeled as reliable otherwise it is unreliable newsguard has provided ground truth for the construction of news datasets such as nela-gt-2018 11 for studying misinformation media biasfact check mbfc 7 mbfc is a website that rates factual accuracy and political bias of news medium the fact-checking team consists of dave van zandt the primary editor and the website owner and some journalists and researchers more details can be found on its about page mbfc labels each news media as one of six factual-accuracy levels based on the fact-checking results of the news articles it has published more details can be found on its methodology page i very high ii high iii most factual iv mixed v low and vi very low such information has been used as ground truth for automatic fact-checking studies 1 what are our criteria referenced by newsguard and mbfc our criteria for determining reliable and unreliable news sites are  reliable a news site is reliable if its newsguard score is greater than 90 and its factual reporting on mbfc is very high or high  unreliable a news site is unreliable if its newsguard score is less than 30 and its factual reporting on mbfc is below mixed our search towards news medium with high credibility is conducted among news articles listed in mbfc 2000 to find news medium with low credibility we search in mbfc and the newly released coronavirus misinformation tracking center 5 of news-guard which provides a list of websites publishing false coronavirus information ultimately we obtain a total of 61 news sites from which 22 are the sources of reliable news articles eg national public radio 8 and reuters 9  and the remaining 39 are sources to collect unreliable news articles eg human are free 10 and natural news 11  the full list of sites considered in our repository is also available at httpcoronavirus-fakenewscom note that several fake news medium are not included such as 70 news conservative 101 and denver guardian since they no longer exist or their domains have been unavailable also note that to achieve a good trade-off between dataset scalability and label accuracy we determine more extreme threshold a reliable news 12 b unreliable news 13 figure 3  examples of news articles collected scores 30 and 90 compared to the initial one provided by news-guard 60 in this way the selected news sites share an extreme reliability or unreliability to reduce the number of the false positive and false negative of news labels in our repository ideally each news article published on a reliable site is factual and on an unreliable site is false figure 2 illustrates the credibility distributions of reliable and unreliable news sites it can be observed from the figure that for reliable news most of them get a full mark on newsguard and are labeled as highly factual by mbfc very high is rare for all sites listed in mbfc in contrast unreliable news sites share an average newsguard score of 15 and a low factual label by mbfc similarly very low is rarely given on mbfc to crawl covid-19 news articles from selected news sites we first determine whether the news article is about covid-19 the process is detailed in section 221 next we detail how the data is crawled and the news content components that are included in our repository in section 222 to identify news articles on covid-19 we use a list of keywords  covid-19 and  coronavirus news articles whose content contains any of the keywords caseinsensitive are considered related to covid-19 these three keywords are the official names announced by the who on february 11 th  where sars-cov-2 standing for severe acute respiratory syndrome coronavirus 2 is the virus name and coronavirus and covid-19 are the name of the disease that the virus causes before the who announcement covid-19 was previously known as the 2019 novel coronavirus  14  which also includes the coronavirus keyword which we are considering we merely consider official names as keywords to avoid potential biases or even discrimination in articles collected furthermore a news media article that is credible or pretends to be credible often acts professionally and adopts the official names of the diseasevirus compared to those articles that use biased andor inaccurate terms false news pretending to be professional is more detrimental and challenging to detect which has become the focus of current fake news studies 22 examples of such news articles are illustrated in figure 3  python library 15 the content of each news article corresponds to twelve components c1 news id each news article is assigned a unique id as the identity c2 news url the url of the news article the url helps us verify the correctness of the collected data it can also be used as the reference and source when repository users would like to extend the repository by fetching additional information c3 publisher the name of the news media site that publishes the news article c4 publication date the date in yyyy-mm-dd format on which the news article was published on the site which provides temporal information to support the investigation of eg the relationship between the misinformation volume and the outbreak of covid-19 over time c5 author the authors of the news article whose number can be none one or more than one note that some news articles might have fictional author names author information is valuable in evaluating news credibility by either investigating the collaboration network of authors 14 or exploring its relationships with news publishers and content 20  c6-7 news title and bodytext as the main textual information c8 news image as the main visual information which is provided in the form of a link url note that most images within the news page are noise -they can be advertisements images belonging to other news articles due to the recommender systems embedded in news sites logos of news sites andor social media icons such as twitter and facebook logos for sharing hence we particularly fetch the mainheadtop image for each news article to reduce noise c9 country the name of country where the news is published c10 political bias each news article is labeled as one of extreme left left left-center center right-center right and extreme right that is equivalent to the political bias of its publisher news political bias is verified by two resources allsides 16 and mfbc both which rely on domain experts to label media bias and c11-12 newsguard score and mbfc factual reporting as the original ground truth of news credibility which has been detailed in section 21 we first use twitter search api 17  the general statistics on our dataset is presented in table 1  the dataset contains 2029 news articles most of which have both textual and visual information for multimodal studies 2017 18 21 and have shared on social media 1747 the proportion of reliable versus unreliable news articles is around 21 hence due to class imbalance compared to accuracy rate auc or f 1 scores should be a better evaluation metric when using the collected data to predict news credibility note that the number of users who spread reliable news 78659 pluses that of users spreading unreliable news 17 323 is greater than the total number of users including in the dataset 93761 which indicates that users can both engage in spreading reliable and unreliable news articles next we visualize the distributions of data featuresattributes distribution of news publishers figure 4 shows the number of covid-19 news articles published in each extremely reliable or extremely unreliable news site there are six unreliable publishers with no news on covid-19 hence they are not presented in the figure we keep these publishers in our repository as the data will be updated over time and these publishers may publish news articles on covid-19 in the future news publication dates the distribution of news publication dates is presented in figure 5  where all articles are published in 2020 we point out that from january to may the number of covid-19 news articles published is significantly exponentially increased the possible explanation for this phenomena is three-fold first from the time that the outbreak was first identified in wuhan china december 2019 8 to may 2020 the number of confirmed cases and deaths caused by sars-cov-2 have grown exponentially globally 1 meanwhile the virus has become a world topic and has triggered more and more discussions on a world-wide scale second some older news articles are no longer available which has motivated us to timely update the dataset third the keywords we have used to identify covid-19 news articles are the official ones a b c n ew s b us in es s in si de r c b s n ew s c n b c c hi ca go s un -t im es f iv et hi rt ye ig ht lo s a ng el es d ai ly n ew s n at io na l p ub lic r ad io n p r  p b s n ew sh ou r p ol iti co r eu te rs s la te t he a tla nt ic t he d et ro it n ew s t he m er cu ry n ew s t he n ew y or k t im es t he n ew y or ke r t he v er ge t he w as hi ng to n p os t u s a to da y w as hi ng to n m on th ly 19 some news articles published in january are also collected as before the who announcement covid-19 was known as the 2019 novel coronavirus  which also includes one of our keywords coronavirus  we have detailed the reasons behind our keyword selection in section 221 news authors and author collaborations figure 6 presents the distribution of the number of authors contributing to news articles which is governed by a long-tail distribution most articles have less than five authors instead of including the real or virtual names of the authors some articles provide publisher names as authors considering such information has been available in the repository we leave the author information of these news articles blank ie their number of authors is zero furthermore we construct the coauthorship network shown in figure 7  it can be observed from the network that node degrees also follow a power-law-like distribution among 1095 nodes authors over 90 of them have less than or equal to two collaborators figures 8 and 9 reveal textual characteristics within news content including news title and bodytext it can be observed from figures 8 that the number of words within news content follows a long-tail power-low-like distribution with an average value of 800 and a median value of 600 on the other hand figure 9 provides the word cloud for the entire repository as the news articles collected share the same covid-19 topic some relevant topics and vocabularies have been naturally and frequently used by the news authors such as coronavirus 6465 covid 5413 state 4432 test 4274 health 3714 pandemic 3427 virus 2903 home 2871 case 2676 and trump 2431 that are illustrated with word font size scaled to their frequencies country distribution figure 10 reveals the countries that news and news publishers belong to it can be observed that in total six countries usa russia uk iran cyprus and canada are covered where us news and news publishers constitute the majority of the population figure 11 is the distribution of political bias of news and news medium publishers it can be observed from the figure that for both news and publishers the distribution for those exhibiting a right bias including extreme right right and right-center is more balanced compared to those exhibiting a left bias including extreme left left and left-center news spreading frequencies figure 12 shows the distribution of the number of tweets spreading each news article the distribution exhibits a long tail -over 80 of news articles are spread less than 100 times while a few have been shared by thousands of tweets news spreaders the distribution of the number of spreaders for each news article is shown in figure 13  it differs from the distribution in figure 12 as one user can spread a news article multiple times as for social connections of news spreaders the distributions of their followers and friends are respectively presented in figures  14 and 15  where the most popular spreader has over 40 million followers or 600000 friends in this section several methods that often act as baselines are utilized and developed to predict covid-19 news credibility using recovery data hoping to facilitate future studies these methods baselines are first specified in section 41 the implementation details of experiments are then provided in section 42 finally we present the performance results for these methods in section 43 broadly speaking all developed methods fall under a traditional supervised machine learning framework where features are manually engineered to represent news articles see section 411 and then classified by a well-trained classifier such as a random forest classifier see section 412 we design and extract the following three feature groups in our experiments liwc features liwc is a widely-accepted psycholinguistic lexicon given a news story liwc can count the words in the text falling into one or more of 93 linguistic eg self-references psychological eg anger and topical eg leisure categories 12  based on which 93 features are extracted here we consider in a total of eight features for each news article 1 the timestamp at which the news was published 2 the number of news authors 3 4 the mean and median number of collaborators of the news authors 5-7 the number of words in news title bodytext and the entire content and 8 the number of news images compared to liwc features that merely focus on news textual information title and bodytext this group of features comprehensively investigates most of the components of news content that are included in the repository social attributes six features are extracted from the available social attributes of each news article in the repository 1 the frequency of the news being spread ie the number of corresponding tweets 2 the number of news spreaders 3-6 the mean and median number of followers or friends of news spreaders in current fake news research often a random classifier is used as one of the baselines 22  which randomly labels a news article as reliable or unreliable with equal probability we further use multiple common supervised learners classifiers in our experiments logistic regression lr nave bayes nb k-nearest neighbor k-nn random forest rf decision tree dt support vector machines svm and xgboost xgb 4  the overall dataset is randomly divided into training and testing datasets with a proportion of 8020 as the dataset has an unbalanced distribution between reliable and unreliable news articles 21 we evaluate the prediction results in terms of precision recall and the f 1 score each performance outcome is obtained by averaging five experimental results repeated with different random seeds on the dataset division all classifiers are trained with default hyperparameters prediction results are presented in table 2  it can be observed from the table that when predicting news credibility using news content alone attribute features are more representative compared to liwc features attribute features can perform best with an f 1 score of 0772 with a random forest classifier and liwc features perform best with an f 1 score of 0708 using xgboost furthermore using both news content and social information to predict news credibility can further improve the performance achieving an f 1 score of 08 related datasets can be generally grouped as i covid-19 datasets and ii fake news and rumor datasets covid-19 datasets as a global emergency 15  the outbreak of covid-19 has been labelled as a black swan event and likened to the economic scene of world war ii 10  with this background a group of datasets have emerged whose contributions range from real-time tracking of covid-19 to help epidemiological forecasting eg 5 and 19  and collecting scholarly covid-19 articles for literaturebased discoveries eg  to tracking the spreading of covid-19 information on twitter eg 3  20 httpswwwsemanticscholarorgcord19 specifically researchers at johns hopkins university jhu develop a web-based dashboard 21 to visualize and track reported cases of covid-19 in real-time the dashboard is released on january 22 nd  presenting the location and number of confirmed covid-19 cases deaths and recoveries for all affected countries 5  another dataset shared publicly on march 24 th is constructed to aid the analysis and tracking of the covid-19 epidemic which provides real-time individual-level data eg symptoms date of onset admission and confirmation and travel history from national provincial and municipal health reports 19  intended to mobilize researchers to apply recent advances in natural language processing nlp to generate new insights in support of the fight against covid-19 allen institute for ai has contributed a free and dynamic database of more than 128000 scholarly articles about covid-19 named cord-19 to the global research community 20 on the other hand chen et al 3 release the first large-scale covid-19 twitter dataset the dataset updated regularly collects covid-19 tweets that are posted from january 21 st and across languages fake news and rumor datasets existing fake news and rumor datasets are collected with various focuses these datasets may i only contain news content that can be full articles eg nela-gt-2018 11  or short claims eg fever 16  ii only contain social media information eg credbank 9  where news refers to user posts or iii contain both content and social media information eg liar 17 and fakenewsnet 13  specifically nela-gt-2018 11 is a large-scale dataset of around 713000 news articles from february to november 2018 news articles are collected from 194 news medium with multiple labels directly obtained from newsguard pew research center wikipedia opensources mbfc allsides buzzfeed news and politifact these labels refer to news credibility transparency political polarizations and authenticity fever dataset 16 consists of 185000 claims and is constructed following two steps claim generation and annotation first the authors extract sentences from wikipedia and then the annotators manually generate a set of claims based on the extracted sentences then the annotators label each claim as supported refuted or not enough information by comparing it with the original sentence from which it is developed on the other hand some datasets focus on user posts on social media for example credbank 9 comprises more than 60 million tweets grouped into 1049 real-world events each of which is annotated by 30 human annotators while some contain both news content and social media information for instance collecting both claims and fact-check results labels ie true mostly true halftrue mostly false and pants on fire directly from politifact wang establishes the liar dataset 17 containing around 12800 verified statements made in public speeches and social medium the aforementioned datasets only contain textual information valuable for nlp research with limited information on how fake news and rumors spread on social networks which motivate the construction of fakenewsnet dataset 13 the dataset collects verified real or fake full news articles from politifact 1056 and gossipcop 22140 respectively the dataset also tracks news spreading on twitter to fight the coronavirus infodemic we construct a multimodal repository for covid-19 news credibility research which provides textual visual temporal and network information regarding news content and how news spreads on social media the repository balances data scalability and label accuracy to facilitate future studies benchmarks are developed and their performances on predicting news credibility using the data available in the repository are presented we find that using news content andor social attributes available in the repository we can achieve an f 1 score of 077 when news has not yet spread on social media ie only news content is available and an f 1 score of 081 can be achieved when it has been shared by social media users we point out that the data could be further enhanced 1 by including covid-19 news articles in various languages such as chinese russian spanish and italian as well as the information on how these news articles spread on the popular local social media for those languages eg sina weibo china countries speaking but not limited to these languages have all been suffering heavy losses in this pandemic and have shown different characteristics in their spreading in the physical world 22  which would be invaluable when investigating the relationship between the spread of the virus in the physical world and that of its related misinformation on social networks furthermore 2 extending the dataset by introducing the ground truth of for example hate speech clickbaits and social  building a pubmed knowledge graph background and summary jian xu sunkyu kim min song minbyul jeong donghyeon kim jaewoo kang justin rousseau f xin li weijia xu vetle torvik i yi bu chongyan chen islam ebeid akef daifeng li ying ding  pubmed  is an essential resource for the medical domain but useful concepts are either difficult to extract or are ambiguated which has significantly hindered knowledge discovery to address this issue we constructed a pubmed knowledge graph pkg by extracting bio-entities from 29 million pubmed abstracts disambiguating author names integrating funding data through the national institutes of health nih exporter collecting affiliation history and educational background of authors from orcid   and identifying fine-grained affiliation data from mapaffil through the integration of the credible multi-source data we could create connections among the bio-entities authors articles affiliations and funding data validation revealed that the biobert deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the f1 score by 051 with the author name disambiguation and achieving a f1 score of 9809 pkg can trigger broader innovations not only enabling us to measure scholarly impact knowledge usage and knowledge transfer but also assisting us in profiling authors and organizations based on their connections with bio-entities the pkg is freely available on figshare httpsfigsharecoms6327a55355fc2c99f3a2 simplified version that exclude pubmed raw data and tacc website httpertaccutexasedudatasetsped full version facilitating the effortless development of applications such as finding experts searching bioentities analyzing scholarly impacts and profiling scientists careers methods the bio-entity extraction component has two models 1 an ner model which recognizes the named entities in pubmed abstracts based on the biobert model 7  and 2 a multi-type normalization model which assigns unique ids to recognize biomedical entities 1 named entity recognition ner the ner task recognizes a variety of domain-specific proper nouns in a biomedical corpus and is perceived as one of the most notable biomedical text mining tasks in contrast to previous studies that have built models based on long shortterm memory lstm and conditional random fields crfs 1213  the recently proposed bidirectional encoder representations from transformers bert 14 model achieves excellent performance for most of the nlp tasks with minimal task-specific architecture modifications the transformers applied in bert connect the encoders and decoders through self-attention for greater parallelization and reduced training time bert was designed as a general-purpose language representation model that was pre-trained on english wikipedia and bookscorpus consequently it is incredibly challenging to maintain high performance when applying bert to biomedical domain texts that contain a considerable number of domain-specific proper nouns and terms eg brca1 gene and triton x-100 chemical bert required refinement so biobert-a neural network-based high-performance ner model-was developed its purpose is to recognize the known biomedical entities and discover new biomedical entities first in the ner component the case-sensitive version of bert is used to initialize biobert second pubmed articles and pubmed central articles are used to pre-train bioberts weights the pre-trained weights are then fine-tuned for the ner task while finetuning bert biobert we use wordpiece tokenization 15 to mitigate the out-of-vocabulary issue wordpiece embedding is a method of dividing a word into several units eg immunoglobulin divided into i mmuno g lo bul in and expressing each unit this technique is effective at extracting the features associated with uncommon words the ner models available in biobert can predict the following seven tags iob2 tags ie inside outside and begin 16  x ie a sub-token of wordpiece cls ie the leading token of a sequence for classification sep ie a sentence delimiter and pad ie a padding of each word in a sentence the ner models were fine-tuned as follows 8   experts in healthcare and medicine communicate in their own languages such as snomed ct icd-10 pubchem and gene ontology these languages equate to gibberish for laypeople but for medical minds they are an intricate method of transporting important semantics and consensus capable of translating diagnoses medical procedures and medications among millions of physicians nurses and medical researchers thousands of hospitals hundreds of pharmacies and a multitude of health insurance companies these languages eg genes drugs proteins species and mutations are the backbone of quality healthcare however they are deeply embedded in publications making literature searches increasingly onerous because conventional text mining tools and algorithms continue to be ineffective given that medical domains are deeply divided locating collaborators across domains is arduous for instance if a researcher wants to study ace2 gene related to covid-19 he or she would like to know the following which researchers are currently actively studying ace2 gene what are the related genes diseases or drugs discussed in these articles related to ace2 gene and with whom could the researcher collaborate this is a strenuous position to be in and the aforementioned problems diminish the curiosity directed at the topic many studies have been devoted to building open-access datasets to solve bio-entity recognition problems for example kai et al 1 used a conditional random field classifier-based tool to recognize the named entities from pubmed and pubmed central bell et al 2 performed a large-scale integration of a diverse set of bio-entities and their relationships from both bio-entity datasets and pubmed literature although these open-access datasets are predominantly about bio-entity recognition researchers have also been interested in extracting other types of entities and relationships in pubmed including the mapping of author affiliations to cities and their geocodes 3 4  author name disambiguation 5 and and author background information collections 6  although the focus of previous research has been on limited types of entities the goal of our study was to integrate a comprehensive dataset by capturing bio-entities disambiguated authors funding and fine-grained affiliation information from pubmed literature figure 1 illustrates the bio-entity integration framework this framework consists of two parts 1 bio-entity extraction which contains entity extraction named entity recognition ner and multi-type normalization and 2 integration which connects authors orcid and funding information the process illustrated in figure 1 can be described as follows first we applied the highperformance deep learning method bidirectional encoder representations from transformers for biomedical text mining biobert 7 8 to extract bio-entities from 29 million pubmed abstracts based on the evaluation this method significantly outperformed the state-of-the-art methods based on the f1 score by 051 on average then we integrated two existing highquality author disambiguation datasets author-ity 5 and semantic scholar 9  we obtained the disambiguated authors of pubmed articles with full coverage and quality of 9809 in terms of the f1 score next we integrated additional fields from credible sources into our dataset which included the projects funded by the national institutes of health nih 10  the affiliation history and educational background of authors from orcid 6  and fine-grained region and location information from the mapaffil 2016 dataset 11  we named this new interlinked dataset pubmed knowledge graph pkg pkg is by far the most comprehensive up-to-date high-quality dataset for pubmed regarding bio-entities articles scholars affiliations and funding information being an open dataset pkg contains rich information ready to be deployed where k represents the indexes of seven tags b i o x cls sep pad p is the probability distribution of assigning each k to token i and  is the final hidden representation which is calculated by biobert for each token i h is the hidden size of    is a weight matrix between k and  k represents the number of tags and is equal to 7 and b is a k-dimensional vector that records the bias on each k the classification loss l is calculated as follows where represents the trainable parameters and n is the sequence length first a tokenizer is applied to words in a sentence on a dataset with labels in the conll format 17  the wordpiece algorithm is then applied to the sub-words of each word consequently biobert is able to extract diverse types of bio-entities furthermore an entity or two entities with frequently-occurring token interaction will be marked with more than one entity type span 262 for all pubmed abstracts based on the calculated probability distribution we are able to choose the correct entity type when entities are tagged with more than two types according to the probability-based decision rules 8  2 multi-type normalization because an entity may be referred to by several synonymous terms synonyms and a term can be polysemous if it refers to multiple entity types polysemy we require a normalization process for the extracted entities however it is a daunting challenge to build a single normalization tool for multiple entity types because there exist various normalization models that depend on the type of entity we addressed this issue by combining multiple ner normalization models into one multi-type normalization model that assigns ids to extracted entities table 1 illustrates the statistics of the proposed normalization model the multi-type normalization model is based on a normalization model per entity type table 1  to improve the number of normalized entities we added the disease names from the polysearch2 dictionary 76001 names of 27658 diseases to the sieve-based entity linking dictionary 76237 names of 11915 diseases we also added the drug names from drugbank 25 and the us food and drug administration fda to the tmchem dictionary because there are no existing normalization model for species we normalized species based on dictionary lookup using tmvar 20 we created a dictionary of mutations with normalized mutation names in which a mutation with several names was assigned to one normalized name or id despite a rigorous effort to create global author ids eg orcid and researcherid most articles in pubmed particularly those before 2003 the year in which the field orcid was added into pubmed provide limited author information with respect to last name first initial and affiliation only for first authors before 2014 author information is not effective metadata to be used directly as a unique identifier because different people may have the same names and the names and affiliations of an individual can change over time and is essential for identifying unique authors in recent decades researchers have made several attempts to solve the and problem using three types of methods the first type of method relies on manual matching of articles with authors by surveying scientists or consulting curricula vitae cvs gathered from the internet 28  although this type of method ensures high accuracy a considerable amount of investment in labor is required to collect and code the data which is impractical for huge datasets the second type of method uses publicly-accessible registry platforms such as orcid or google scholar to help researchers identify their own publications which produces a source of highly accurate and low-cost accessible disambiguation of authorship for large numbers of authors however registries cover only a small proportion of researchers 29 30  which introduces a form of survivor bias into samples the third type of method uses an automated approach to estimate the similarity of author instance feature combinations and identify whether they refer to the same person the features for automated and include author name author affiliation article keywords journal names 31  coauthor information 32  and citation patterns 33  automated methods typically rely on supervised or unsupervised machine learning in which the machine learns how to weigh the various features associated with author names and where to assign a pair of author names either to the same author or to two different authors 34 35  this type of method can potentially avoid the shortcomings of the previous two types moreover automated methods have been improved to a high level of accuracy after years of development for pubmed automated methods are the optimal choice because they can overcome the shortcomings of the other two methods while simultaneously providing high-quality and results for the entire dataset several scholars have disambiguated the authors using automated methods although the evaluations of these results have exhibited different levels of accuracy and coverage limitations we believe that integrating them with due diligence can yield a highquality and dataset with full coverage of pubmed articles according to our investigation a high-quality pubmed and dataset with complete coverage can be obtained through the integration of the following two existing and datasets 1 author-ity the author-ity database uses diverse information about authors and publications to determine whether two or more instances of the same name or of highly similar names on different papers represent the same person according to the and evaluation based on the method discussed in the section technical validation the f1 score of author-ity is 9816 which is the highest accuracy result that we have observed however this dataset only covers authors before 2009 2 semantic scholar it trains a binary classifier to merge a pair of author names and use the pair to create author clusters incrementally according to the and evaluation based on the method discussed in the section technical validation the f1 score of semantic scholar is 9694 which is 122 lower than that of author-ity however it has the most comprehensive coverage of authors because the author-ity dataset has a higher f1 score than the semantic scholar dataset we selected the authors unique id of the author-ity dataset as the primary andid andid is limited by time range containing pubmed papers before 2009 however we supplemented authors after 2009 using the and result from semantic scholar the following steps were applied step 1 allocate the authors unique id to each author instance according to the authority and results such that authors from the author-ity dataset before 2010 have unique author ids step 2 for authors that have the same semantic scholar andid but never appear in the author-ity dataset we generated a new andid to label them for example author pietranico r published two papers in 2012 and 2013 and had two corresponding author instances because all papers that pietranico r published were after 2009 they were not covered by author-ity and therefore had no andid allocated by author-ity however the authors disambiguated correctly by semantic scholar were allocated unique andids in semantic scholar to maintain the consistency in labeling we generated a new andid continuing and-ids of author-ity to label these two author instances as disambiguated by semantic scholar step 3 for author instances with a unique andid in semantic scholar and in which authors at least one had the same author-ity andid we allocated the author-ity andid to all author instances as their unique id for example maneksha s published three papers in 2007 2009 and 2010 and the first two author instances had unique author-ity andid however the last one had no author-ity andid because it was beyond the time coverage of the author-ity dataset nevertheless based on the and results of semantic scholar the three author instances had an identical andid therefore the last author instance with no authority andid could be labeled with the same id as the other two author instances in addition to bio-entity extraction by biobert and and we made a considerable effort to integrate pubmed by extending multi-source data into pkg which exploited the mapping connections between andid and the pubmed identifier pmid to build relationships between different objects to provide a comprehensive overview of the pubmed dataset these integrated data includes the funding data from nih exporter the affiliation history and educational background of authors from orcid and the fine-grained region and location information from the mapaffil 2016 dataset the entities and their associated relationships are depicted in figure 2  nih exporter provides data files that contains research projects funded by major funding agencies such as the centers for disease control and prevention cdc the nih the agency for healthcare research and quality ahrq the health resources and services administration hrsa the substance abuse and mental health services administration samhsa and the us department of veterans affairs va furthermore it provides publications and patents citing support from these projects it consists of 49 data fields including the amount of funding for each fiscal year organization information of the pis and the details of the projects according to our investigation nih-funded research accounts for 807 of all grants recorded in pubmed the nih exporter dataset contains a unique piid for each scholar who received nih funding between 1985 and 2018 and his or her pmids of the published articles through the mapping of pmids in nih exporter to pmids in pubmed 1n connections between the pi and the article have been established paving the way for investigating the article details of a specific pi and vice versa furthermore by mapping pi names last name first initial and affiliation to author names that were listed in articles supported by the pis projects a 11 connection between the pi and the andid was established providing a way to obtain pirelated article information regardless of whether the article was labeled with a project id according to its website orcid is a nonprofit organization helping to create a world in which all who participate in research scholarship and innovation are uniquely identified and connected to their contributions and affiliations across disciplines borders and time 36  it maintains a registry platform for researchers to actively participate in identifying their own publications information about formal employment relationships with organizations and educational backgrounds orcid provides an open-access dataset called orcid public dataset 2018 6  which contains a snapshot of all public data in the orcid registry associated with an orcid record that was created or claimed by an individual as of october 1 2018 the dataset includes 7132113 orcid ids of which 1963375 have educational affiliations and 1913610 have employment affiliations as a result of the proliferation of orcid identifiers pubmed has used orcid identifiers as alternative author identifiers since 2013 37  using the aforementioned two steps we could map orcid records to the pubmed authors our first step was to map the author instances in pubmed to an orcid record based on the feature combinations of article doi and author name last name and first initial because the doi is not a compulsory field for pubmed we appended the feature combinations of article titles journals and author names to map the records between the two datasets the result contained many 11 connections between a disambiguated author of pubmed and an orcid record furthermore 11 connections between andid and orcid id and 1n connections between andid and background information education and employment were established the mapaffil 2016 dataset 3 resolves pubmed authors affiliation strings to cities and associated geocodes worldwide this dataset was constructed based on a snapshot of pubmed which included the medline and pubmed-not-medline records acquired in the first week of october 2016 affiliations were linked to a specific author on a specific article prior to 2014 pubmed only recorded the affiliation of the first author however mapaffil 2016 covered some pubmed records that lacked affiliations and were harvested elsewhere such as from pmc nih grants the microsoft academic graph and the astrophysics data system all affiliation strings were processed using mapaffil to identify and disambiguate the most specific place names the dataset provides the following fields pmid author order last name first name year of publication affiliation type city state country journal latitude longitude and federal information processing standards fips code the mapaffil 2016 dataset does have a limitation because it does not cover the pubmed data after 2015 covering 629 affiliation instances in pubmed consequently we performed an additional step to improve the fraction of coverage we collected authors who published their first article before 2016 and continued publishing articles after 2015 by their andids the new affiliation instances of the author after 2015 succeeded their corresponding finegrained affiliation data from the affiliation instances before 2016 fraction of affiliation instance coverage increased to 842 if the author did not change affiliation we also applied an up-todate open-source library affiliation parser 4 to extract additional fine-grained affiliation fields from all affiliation instances including department institution email zip code location and country  we built pkg with bio-entities extracted from pubmed abstracts and results of pubmed authors and the integrated multi-source information this dataset is freely available on figshare 38  it contains seven comma-separated value csv files named authorlist bioentitiesmain bioentitiesmutation affiliations researcheremployment researchereducation and nihprojects the details are presented in table 3  pubmed raw data are not included into figshare file set because the amount of pubmed raw data is too large and they are not generated or altered by our methods pubmed raw data can be freely downloaded from pubmed website 39  we also provide download link httpertaccutexasedudatasetsped which contains both the pubmed raw data and pkg dataset to facilitate the application of pkg dataset csv file containing projects from nih exporter and mapping relation between piid pmid and andid note in file authorlist about 13 million 115 author instances cannot be disambiguated because they do not exist in authority or semantic scholar dataset therefore their andid field values were set to zero the statistics of all five types of extracted entities are presented in table 4  each data field is self-explanatory by its name and fields with the same name in other tables follow the same data format that can be linked across tables tables 5-11 illustrate the field name format and short description of fields for each data file listed in table 3  updating pkg is a complex task because it is subject to the update of different data sources and requires significant computation in the future we hope to refresh pkg quarterly based on pubmed updated files and updated datasets from other sources we may also develop an integrative ontology to integrate all types of entities to validate the performance of the bio-entity extraction we established bert and the stateof-the-art models as baselines then we calculated the entity-level precision recall and f1 scores of these models as evaluation metrics the datasets and the test results of biomedical ner are presented in table 12  in table 12  we report the precision p recall r and f1 f scores of each dataset the highest scores are in boldface and the second-highest scores are underlined sachan et al 2017 48 reported the scores of the state-of-the-art models for the ncbi disease and bc2gm datasets presented in table 10  moreover the scores for the 2010 i2b2va dataset were obtained from zhu et al 50  and scores for the linnaeus and species-800 datasets were obtained from giorgi and bader 2018 51  according to table 12  bert which is pre-trained on the general domain corpus was highly effective on average the state-of-the-art models outperformed bert by 228 in terms of the f1 score however biobert obtained the highest f1 score in recognizing genesproteins diseases and drugschemicals it outperformed the state-of-the-art models by 051 in terms of the f1 score on average we used the multi-type normalization model to assign unique ids to synonymous entities table 13 presents the performance of the multi-type entity normalization model there are empty cells in the table because gnormplus and tmvar 2 0 did not report their accuracies the sieve-based entity linking model only reported its accuracy and sr4gn only reported its f1 score the authors of tmchem did not report the normalization performance of tmchem independently so there were no performance data for drugchemical table 13  with respect to genes and proteins there were 75 different species in the bc3 gene normalization bc3gn test set but gnormplus focused only on seven of these species consequently gnormplus achieved a considerably lower f1 score of 366 on the multispecies test set bc3gn than on the human species test set bc2gn for mutations tmvar 20 achieved f1 scores close to 90 on two corpora osirisv12 and the thomas corpus the validation of author disambiguation remains a challenge because there is a lack of abundant validation sets we applied a method using the nih exporter-provided information on nihfunded researchers to evaluate the precision recall and f1 measures of the author disambiguation 59  nih exporter provides information about the principal investigator id piid for each scholar who received nih funding between 1985 and 2018 because applicants established a unique piid and used the piid across all grant applications these piids have extremely high fidelity nih exporter also provides article pmids as project outputs which can be conveniently used as a connection between piids and andid we confirmed the bibliographic information of the nih-funded scientists who received nih funding during the years 1985-2018 our and evaluation steps were as follows first we collected project data for the years 1981-2018 in nih exporter including 304782 piid records and the corresponding 331483 projects secondly we matched the projects to articles acknowledging support by the grant which were also recorded in the nih exporter dataset we matched 214956 of the projects to at least one article and identified 1790949 articles funded by these projects some of these projects 116527 did not match articles and were excluded because the nih occasionally awards a project to a team that includes more than one pi we eliminated the 13154 records that contained multiple pis because they could result in uncertain credit allocation consequently our relevant set of pis decreased to 147027 individuals associated with 1749873 articles and 201802 projects we then connected nih piids from nih exporter to andids using the article pmids and author pis last name plus the initials as a crosswalk this step resulted in 1400789 unique articles remaining associated with 109601 piids and 107380 andids finally we computed precision p based on the number of articles associated with the most frequent andid-to-piid matched over the number of all articles associated with a specific andid 60  furthermore we computed recall r based on the number of articles associated with the most frequent piid-to-andid matched over the number of all articles associated with a particular piid 60  figure 3 summarizes the precision recall and f1 calculations table 14 illustrates the precision recall and f1 scores for author-ity semantic scholar and our integrated and result as presented in table 14  after integrating the and results of author-ity and semantic scholar we obtained a high-quality integrated and result that outperformed semantic scholar by 115 in terms of the f1 score and had more comprehensive coverage until 2018 than author-ity until 2009 the evaluation results of and might be slightly overestimated the pis of nih grants usually have many publications over a long period and might be more likely to have rich information such as affiliations and email addresses about publications therefore it should be easier to acquire higher performance on and tasks than that of new entrants who published fewer papers and may lack of sufficient information for and furthermore approximately 115 of the author instances cannot be disambiguated since they do not exist in the author-ity or semantic scholar and results which further slightly reduces the performance of and results theoretically however the semantic scholar and results and the and integration are evaluated based on the same baseline dataset with author-ity in this section and the evaluation of author-ity performance using a random sample of articles indicates reliable high quality the recall of the author-ity dataset is 988 the lumping putting two different individuals into the same cluster of the author-ity dataset affects 05 of the clusters and the splitting assigning articles written by the same individual to more than one cluster of the author-ity dataset affects 2 of the articles 5  consequently we believe these factors have a limited impact on and performance networking and collaboration have been associated with faculty promotions in academic medical centers 61  barriers exist for identifying researchers working on common bio-entities to facilitate collaboration it is a challenge even at a single academic institution to identify potential collaborators who are working on the same bio-entities this has led to many institution-specific projects profiling the faculty associated with the topics that they are studying 62 63 64 65 the challenge is exacerbated when we search across multiple institutions researchers academic institutions and the pharmaceutical industry often face the challenge of identifying researchers working on a specific bio-entity a traditional bibliographic database specializes only in returning an enormous number of related articles for particular keyword or term searches bio-entity profiling for researchers offers an advantage over this traditional approach by identifying specific connections between bio-entities and disambiguated authors in which bio-entity profiling for researchers can directly locate the core specialists whose research is focused on these bio-entities furthermore a bipartite authorentity network projection analysis can identify a specific authors neighborhood with similar research interest which is crucial for community detection and collaborative commendation we sought to use the pkg dataset to understand the trends over time of researcher-centric and bio-entity-centric activity by the following use cases 1 researcher-centric for stephen silberstein md a neurologist and expert in headache research 2 calcitonin gene-related peptide cgrp a target of inhibition for one of the newest therapeutics in migraine treatment and 3 bipartite author-entity projection network analysis for coronavirus a disease that causes respiratory illness with symptoms such as a fever cough and difficulty breathing for researcher-centric and bio-entity-centric activities we collected 455 articles with dr silberstein as an author and 7877 articles on cgrp in the pkg dataset from 1970 to 2018 and extracted the bio-entities from these articles several publications and bio-entities were used for profiling the career of dr silberstein several publications and the authors distribution were used for profiling cgrp for bipartite author-entity projection network analysis we collected 9778 articles on coronavirus in the pkg dataset from 1969 to 2019 for dr silberstein 539 bio-entities including 342 diseases 142 drugs 24 genes 17 species and 14 mutations were extracted from 455 articles as depicted in figure 4 a headache and migraine were his two most studied diseases reaching 21 and 19 articles respectively in 2004 we trended his research over time on triptans starting with sumatriptan cgrp began to emerge in his publications starting in 2015 we noted the five researchers that have collaborated with dr silberstein through his career and map with pkg their collaborations interactions and institutions over time visualizing the profiles of individual researchers can help to understand the trends in their topics of interest and collaboration patterns to enable an understanding of collaboration factors that may be associated with academic success or scientific discovery as we demonstrated with a previous analysis of the repurposing of aspirin 66-67  we observe research on cgrp starting at approximately the same time as the research on triptans for the treatment of migraines research on the pathophysiology of migraines identified a central role of the neuropeptide calcitonin gene-related peptide cgrp which is thought to be involved with the dilation of cerebral and dural blood vessels release of inflammatory mediators and the transmission of pain signals 68  research on the mechanism of the action of triptansserotonin receptor agonists-has led to an understanding that they normalize elevated cgrp levels which among other mechanisms has led to an improvement in migraine headache symptoms consequently papers in high-impact journals have called for identifying molecules and the development of drugs to directly inhibit cgrp 69  which has since led to the development of cgrp inhibitors as a new class of migraine treatment medications a total of 28223 disambiguated authors and 5379 distinct bio-entities of coronavirus articles were used to construct author-bio-entity bipartite network figure 5 illustrated the bipartite network  figure 5a  and its author projection  figure 5b  and bio-entity projection  figure  5 c in figure 5a  the author vertices are blue and the bio-entity vertices are pink a link between a bio-entity and an author exists if and only if this bio-entity has been researched by that author connections between two authors or between two bio-entities are not allowed the edge weight is set as the number of papers an author published that mention a bio-entity in figure 5 b and 5c the edge weight is set as the number of common neighbors for the author and bio-entity respectively vertices are marked with different colors to show their community attribution for example the disease sars have been frequently studied by author baric r s yuen kwok-yung and zheng bo-jian in addition to sars baric r s is also interested in coronavirus infection and hbv infection figure 5 b depicts the common research interest relationship between authors strong connections between authors may indicate that they collaborated multiple times such as chan kwok hung and yuen kwok-yung who published 69 papers together these connections may also indicate author pairs that have similar research interests but never collaborated such as baric r s and yuen kwok-yung which is crucial for the collaborative commendation similarly those connections between bio-entities in figure 5 c indicate that they have been studied by authors with similar research interests which can be further applied to discover the hidden relations between bio-entities  the eect of large-scale anti-contagion policies on the coronavirus covid-19 pandemic solomon hsiang daniel allen sbastien annan-phan kendon bell ian bolliger trinetta chong hannah druckenmiller andrew hultgren luna huang yue emma krasovich peiley lau jaecheol lee esther rolf jeanette tseng tiany wu manaaki  whenua -landcare research  governments around the world are responding to the novel coronavirus covid-19 pandemic 1 with unprecedented policies designed to slow the growth rate of infections many actions such as closing schools and restricting populations to their homes impose large and visible costs on society in contrast the benefits of these policies in the form of infections that did not occur cannot be directly observed and are currently understood through process-based simulations 234 here we compile new data on 936 local regional and national anti-contagion policies recently deployed in the ongoing pandemic across localities in china south korea iran italy france and the united states us we then apply reduced-form econometric methods commonly used to measure the eect of policies on economic growth to empirically evaluate the eect that these anti-contagion policies have had on the growth rate of infections in the absence of any policy actions we estimate that early infections of covid-19 exhibit exponential growth rates of roughly 45 per day we find that anti-contagion policies collectively have had significant eects slowing this growth although policy actions in the us appear to be too recent to have a substantial impact since the magnitude of these eects grows over time our results suggest that similar policies may have dierent impacts on dierent populations but we obtain consistent evidence that the policy packages now deployed are achieving large and beneficial health outcomes we estimate that to date current policies have already prevented or delayed on the order of eighty-million infections these findings may help inform whether or when ongoing policies should be lifted or intensified and they can support decision-making in the over 150 countries where covid-19 has been detected but not yet achieved high infection rates 5 2  the 2019 novel coronavirus 1 pandemic is forcing societies around the world to make consequential policy decisions with limited information after containment of the initial outbreak failed attention turned to implementing large-scale social policies designed to slow contagion of the virus 6 with the ultimate goal of slowing the rate at which life-threatening cases emerge so as to not exceed the capacity of existing medical systems in general these policies aim to decrease opportunities for virus transmission by reducing contact among individuals within or between populations such as by closing schools limiting gatherings and restricting mobility such actions are not expected to halt contagion completely but instead are meant to slow the spread of covid-19 to a manageable rate these large-scale policies are developed using epidemiological simulations 2 4 7 8 9 10 11 12 13 14 15 16 17 and a small number of natural experiments in past epidemics 18 however the actual impacts of these policies on infection rates in the ongoing pandemic are unknown because the modern world has never experienced a pandemic from this pathogen nor deployed anti-contagion policies of such scale and scope it is crucial that direct measurements of policy impacts be used alongside numerical simulations in current decision-making populations in almost every country are now currently weighing whether or when the health benefits of anti-contagion policies are worth the costs they impose on society for example restrictions imposed on businesses are increasing unemployment 19 travel bans are bankrupting airlines 20 and school closures may have enduring impacts on aected students 21 it is therefore not surprising that some populations hesitate before implementing such dramatic policies particularly when these costs are visible while their health benefits -infections and deaths that would have occurred but instead were avoided or delayed -are unseen our objective is to measure this direct benefit specifically how much these policies slowed the growth rate of infections we treat recently implemented policies as hundreds of dierent natural experiments proceeding in parallel our hope is to learn from the recent experience of six countries where the virus has advanced enough to trigger largescale policy actions in part so that societies and decision-makers in the remaining 180 countries can access this information immediately to exhibit almost perfect exponential growth 7 14 22 the rate of this exponential growth may change daily and is determined by epidemiological factors such as disease infectivity and contact networks as well as policies that induce behavior changes 7 8 22 we cannot experimentally manipulate policies ourselves but because they are being deployed while the epidemic unfolds we can measure their impact empirically we examine how the growth rate of infections each day in a given locality changes in response to the collection of ongoing policies applied to that locality on that day we employ well-established reduced-form econometric techniques 23 24 commonly used to measure the eect of policies 25 26 or other events eg wars 27 or environmental changes 28  on economic growth rates similarly to early covid-19 infections economic output generally increases exponentially with a variable rate that can be aected by policy or other conditions unlike process-based epidemiological models 7-9 12 22 29 30 the reduced-form statistical approach to inference that we apply does not require explicit prior information about fundamental epidemiological parameters or mechanisms many of which remain unknown in the current pandemic rather the collective influence of these factors is empirically recovered from the data without modeling their individual eects explicitly see methods prior work on influenza 31 for example has shown that such statistical approaches can provide important complementary information to process-based models to construct the dependent variable we transform location-specific sub-national time-series of infections into first-dierences of their natural logarithm which is the per day growth rate of infections see methods we use data from first-or second-level administrative units and data on active or cumulative cases depending on availability see appendix section 2 we then employ widely-used panel regression models 23 24 to estimate how the daily growth rate of infections changes over time within a location when dierent combinations of large-scale social policies are enacted see methods our econometric approach accounts for dierences in the baseline growth rate of infections across locations due to dierences in demographics socio-economic status culture or health systems across localities within a country it accounts for systemic patterns in growth rates within countries unrelated to policy such as the eect of the work-week it is robust to systematic under-surveillance and it accounts for changes in procedures to diagnose positive cases see methods and appendix section 2 the reduced-form statistical techniques we use are designed to measure the total magnitude of the eect of changes in policy without attempting to explain the origin of baseline growth rates or the specific epidemiological mechanisms linking policy changes to infection growth rates see methods thus this approach does not provide the important mechanistic insights generated by process-based models however it does eectively quantify the key policyrelevant relationships of interest using recent real-world data when fundamental epidemiological parameters are still uncertain we estimate that in the absence of policy early infection rates of covid-19 grow 45 per day on average implying a doubling time of approximately two days country-specific estimates range from 2523 per day p 005 in china to 6504 per day p 0001 in iran although an estimate only using data from wuhan the only chinese city where a meaningful quantity of pre-policy data is available is 55 per day p 0001 growth rates in south korea italy france and the us are very near the 45 average value figure 2a  these estimated values dier from the observed growth rates because the latter are confounded by the eects of policy in the early stages of most epidemics a large proportion of the population remains susceptible to the virus and if the spread of the virus is left uninhibited by policy or behavioral change exponential growth will continue until the fraction of the susceptible population declines meaningfully 7 29 this decline results from members of the population leaving the transmission cycle due to either recovery or death 29 at the time of writing the minimum susceptible population fraction in any of the administrative units analyzed is 994 of the total population lodi italy 1445 infections in a population of 230000 this suggests that all administrative units in all six countries would likely be in a regime of uninhibited exponential growth if policies were removed today consistent with predictions from epidemiological models 2 18 32 we find that the combined eect of all policies within each country reduces the growth rate of infections by a substantial and except in the us statistically significant amount  figure 2b  for example a locality in italy with a baseline growth rate of 038 national avg that deployed all policy actions used in italy would be expected to lower its daily growth rate by 018 to 020 in general the estimated total eects of policy packages are large enough that they can in principle oset a large fraction of or even eliminate the baseline growth rate of infections-although in several countries many localities are not currently deploying the full set of policies our estimate for the total growth eect of all us policies is quantitatively substantial -025 but not statistically significant us estimates are highly uncertain due to the short period of time for which data are available and because the time elapsed since these actions may be too short to observe a significant impact in china where policies have been enacted for over seven weeks we observe that policy impacts have grown over time during the first three weeks of deployment -011 to -033  in all other countries except china we only estimate an average eect for the entire interval of observation due to the short temporal length of the sample the estimates above describe the superposition of all policies deployed in each country ie they represent for each country the average eect of policies on infection growth rates that we would expect to observe if all policies enacted anywhere in the country were implemented simultaneously in a region of the country we also estimate the eects of individual types of policies or clusters of policies that are grouped based on their similarity in goal eg closing libraries and closing 5  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 museums are grouped or timing eg policies that are generally deployed simultaneously in a certain country in many cases our estimates for these eects are statistically noisier than the estimates for all policies combined presented above because we are estimating multiple eects simultaneously thus we are less confident in individual estimates and in their relative rankings estimated eects dier between countries and policies are neither identical nor perfectly comparable in their implementation across countries or in many cases across dierent localities within the same country nonetheless overall we estimate that almost all policies likely contribute to slowing the growth rate of infections figure 2c  except two policies social distancing in france and italy where point estimates are slightly positive small in magnitude and not statistically dierent from zero we combine the estimates above with our data on the timing of hundreds of policy deployments to estimate the total eect to date of all policies in our sample to do this we use our estimates above to predict the growth rate of infections in each locality on each day given the policies in eect at that location on that date  figure 3  blue markers we then use the same model to predict what counterfactual growth rates would be on that date if all policies were removed  figure 3  red which we refer to as a no policy scenario the dierence between these two predictions is our estimated eect that all anti-contagion policies actually deployed had on the growth rate of infections on that date we estimate that since the beginning of our sample on average all anti-contagion policies combined have slowed the average daily growth rate of infections 0166 per day 0015 p  0001 in china 0276 0066 p  0001 in south korea 0158 0071 p  005 in italy 0292 0037 p  0001 in iran 0132 0053 p  005 in france and 0044 0059 p  045 in the us taken together these results suggest that anti-contagion policies currently deployed in the first five countries are achieving their intended objective of slowing the pandemic broadly confirming epidemiological simulations we estimate that anti-contagion policies have not yet had a substantial nor significant impact suppressing overall infection growth rates in the us at a particular moment in time the total number of covid-19 infections depends on the growth rate of infections on all prior days thus persistent decreases in growth rates have a compounding eect on total infections at least until a shrinking susceptible population slows growth through a dierent mechanism to provide a sense of scale and context for our main results in figures 2 and 3 we integrate the growth rate of infections in each locality from figure 3 to estimate total infections to date both with actual anti-contagion policies and in the no policy counterfactual scenario to account for the declining size of the susceptible population in each administrative unit we couple our econometric estimates for the eects of policies to a simple susceptible-infected-removed sir model of infectious disease dynamics 7 22 see methods this allows us to extend our projections beyond the initial exponential growth phase of infections a threshold which our results suggest would currently be exceeded in several countries in the no policy scenario our results suggest that ongoing anti-contagion policies have already substantially reduced the 6 number of covid-19 infections observed in the world today  figure 4  our central estimates suggest there would be roughly 74-million more cumulative cases in china 5-million more in south korea 12-million more in italy 26-million more in iran 650000 more in france and 20000 more in the us had these countries never enacted any anti-contagion policies since the start of the pandemic the relative magnitudes of these impacts partially reflects the intensity and extent of policy deployment eg how many localities deployed policies and the duration for which they have been applied several of these estimates are subject to large uncertainties see intervals in figure 4  overall our results indicate that large-scale anti-contagion policies are achieving their intended objective of slowing the growth rate of covid-19 infections because infection rates in the countries we study would have initially followed rapid exponential growth had no policies been applied our results suggest that these ongoing policies are currently providing large health benefits for example we estimate that there would be roughly 621 the current number of infections in south korea 36 in italy and 153 in iran if large-scale policies had not been deployed during the early weeks of the pandemic consistent with process-based simulations of 2 4 10 11 12 14 17 29 our empirical analysis of existing policies indicates that seemingly small delays in policy deployment likely produce dramatically dierent health outcomes while the quantity of currently available data poses challenges to our analysis our aim is to use what limited data exist to estimate the first-order impacts of unprecedented policy actions in an ongoing global crisis as more data become available empirical research findings will become more precise and may capture more complex interactions for example this analysis does not account for potentially important interactions between populations in nearby localities 7 33 nor the structure of mobility networks 3 4 10 12 17 34 nonetheless we hope the results we are able to obtain at this early stage of the pandemic can support critical decision-making both in the countries we study and in the other 150 countries where covid-19 infections have been reported based on our results from china where the most post-policy time has elapsed and where a relatively uniform set of policies were imposed during a narrow window of time it appears that roughly three weeks are required for policies to achieve their full eect in other countries these temporal dynamics are more di cult to disentangle with currently available data in part because less post-policy data is available and also because countries continue to deploy new policies making it more challenging to precisely measure the lagged eects of earlier policies future work should investigate these timing changes after more time has passed and new data become available a key advantage of our reduced-form top down statistical approach is that it captures the real-world behavior of aected populations without requiring that we explicitly model all underlying 7  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted march 27 2020  mechanisms and processes this property is useful in early stages of the current pandemic when many process-related parameters remain unknown however our results cannot and should not be interpreted as a substitute for process-based epidemiological models specifically designed to provide guidance in public health crises rather our results complement existing models for example by helping to calibrate key model parameters we believe both forward-looking simulations and backward-looking empirical evaluations should be used to inform decision-making here we have focused our analysis on large-scale social policies specifically to understand their impact on infection rate growth within a locality however contact tracing international travel restrictions and medical resource management along with many other policy decisions will play key roles in the global response to covid-19 our results do not speak to the e cacy of these other policies our analysis accounts for some known changes in the availability of testing for and changes in testing procedures however it is likely that other unobserved changes in patterns of testing could aect our results for example if growing awareness of covid-19 caused an increasing fraction of infected individuals to be tested over time then unadjusted infection growth rates later in our sample would be biased upwards because an increasing number of policies are active later in these samples as well this bias would cause our current findings to understate the overall eectiveness of anti-contagion policies it is also possible that changing public information during the period of our study has some unknown eect on our results if individuals alter their behavior in response to new information unrelated to anti-contagion policies such as news reports about covid-19 this could alter the growth rate of infections and thus aect our estimates because the quantity of new information is increasing over time if this information reduces infection growth rates it would cause us to overstate the eectiveness of anti-contagion policies we note however that if public information is increasing in response to policy actions then it should be considered a pathway through which policies alter infection growth not a form of bias investigating these potential eects is beyond the scope of this analysis but it is an important topic for future investigations lastly we note that the results presented here are not su cient on their own to determine which anti-contagion policies are ideal for individual populations nor whether the social costs of individual policies are larger or smaller than the social value of their health benefits computing a full value of health benefits also requires understanding how dierent growth rates of infections and total active infections aect mortality rates as well as determining a social value for all of these impacts furthermore this analysis does not quantify the sizable social costs of anti-contagion policies a critical topic for future investigations  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020  we have provided a brief summary of our data collection processes here see appendix section 2 for more details including access dates epidemiological and policy data for each of the six countries in our sample were collected from a variety of in-country data sources including government public health websites regional newspaper articles and wikipedia crowd-sourced information the available epidemiological and policy data varied across the six countries and preference was given to collecting data at the most granular administrative unit level the country-specific panel datasets are at the region level in france the state level in the us the province level in south korea italy and iran and the city level in china below we describe our data sources china we acquired epidemiological data from an open source github project 1 that scrapes time series data from ding xiang yuan we extended this dataset back in time to january 10 by manually collecting o cial daily statistics from the central and provincial hubei guangdong and zhejiang chinese government websites we compiled policies by collecting data on the start dates of travel bans and lockdowns at the city-level from the 2020 hubei lockdowns wikipedia page 2  the wuhan coronavirus timeline project on github 3  and various other news reports as we suspect that most chinese cities have been treated by at least one anti-contagion policy due to their reported trends in infections we have dropped cities where we cannot find a policy deployment date to avoid miscategorizing the policy status of cities we manually collected and compiled the epidemiological dataset in south korea based on provincial government reports policy briefings and news articles we compiled policy actions from press releases from the korean centers for disease control and prevention kcdc the ministry of foreign aairs local governments websites and news articles iran we used epidemiological data from the table new covid-19 cases in iran by province 4 in the 2020 coronavirus pandemic in iran wikipedia article which have been compiled from the data provided on the iranian ministry of health website in persian we relied on news media reporting and two timelines of pandemic events in iran 5 6 to collate policy data italy we utilized epidemiological data from the github repository 7 maintained by the italian department of civil protection dipartimento della protezione civile for policies we primarily relied on the english version of the covid-19 dossier chronology of main steps and legal acts taken by the italian government for the containment of the covid-19 epidemiological emergency written by the department of civil protection dipartimento della protezione civile 8  france we used the region-level epidemiological dataset provided by frances government website 9 and supplemented it with scraped number of confirmed cases by region on frances public health website which is updated daily 10 we obtained data on frances policy response to the covid-19 pandemic from the french government website 11 press releases from each regional public health site 12 and wikipedia 13  we used state-level epidemiological data from the github repository 14 associated with the interactive dashboard from johns hopkins university jhu for policy responses we relied on a number of sources including the us center for disease control cdc individual state health departments as well as various press releases from county and city-level government or media outlets policy data policies in administrative units were coded as binary variables where the policy is coded as either 1 after the date that the policy was implemented and before it is removed or 0 otherwise for the aected administrative units there were instances when a policy implementation only aected a portion of the administrative units eg half of the counties within the state in an attempt to accurately represent the locality and impact of policy implementation policy variables were weighted by the percentage of population within the administrative unit that was treated by the policy the most recent estimates available of population data for countries administrative units were used see the population data section in the appendix additionally in order to standardize policy types across countries we mapped country-specific policies to one of our broader policy categories used as variables in our analysis in this exercise we collected 130 policies for   cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  we collected information on cumulative confirmed cases cumulative recoveries cumulative deaths active cases and any changes to domestic covid-19 testing regimes for our regression analysis figure 2  we use active cases when they are available for china and south korea and cumulative confirmed cases otherwise we document quality control steps in detail in appendix section 2 notably for china and south korea we acquire more granular data than the the data hosted on the john hopkins university jhu interactive dashboard 15  we confirm that the number of confirmed cases closely match between the two data sources see appendix figure a2  to conduct the econometric analysis we merge the epidemiological and policy data to form a single data set for each country reduced-form approach the reduced-form econometric approach that we apply here is a top down approach that describes the behavior of aggregate outcomes y in data here infection rates this approach can identify plausibly causal eects 23 24 induced by exogenous changes in independent policy variables z eg school closure without explicitly describing all underlying mechanisms that link z to y and without observing intermediary variables x eg behavior that might link z to y nor other determinants of y unrelated to z eg demographics denoted w let f  describe a complex and unobserved process that generates infection rates y process-based epidemiological models aim to capture elements of f  explicitly and then simulate how changes in z x or w aect y this approach is particularly important and useful in forwardlooking simulations where future conditions are likely to be dierent than historical conditions however a challenge faced by this approach is that we may not know the full structure of f  for example if a pathogen is new and many key biological and societal parameters remain uncertain crucially we may not know the eect that large-scale policy z will have on behavior xz or how this behavior change will aect infection rates f  alternatively one can dierentiate equation 1 with respect to the k th policy z k  which describes how changes in the policy aects infections through all n potential pathways mediated by x 1   x n  usefully equation 2 does not depend on w if we can observe y and z directly and estimate y z k with data then intermediate variables x also need not be observed 15 httpsgithubcomcssegisanddatacovid-19  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  nor modeled the reduced-form econometric approach 23 24 thus attempts to measure y z k directly exploiting exogenous variation in policies z model active infections grow exponentially during the initial phase of an epidemic when the proportion of immune individuals in a population is near zero assuming a simple susceptible-infected-recovered sir disease model eg ref  22  the growth in infections during the early where i t is the number of infected individuals at time t is the transmission rate new infections per day per infected individual is the removal rate proportion of infected individuals recovering or dying each day and s is the fraction of the population susceptible to the disease the second equality holds in the limit s  1 which describes the current conditions during the beginning of the covid-19 pandemic the solution to this ordinary dierential equation is the exponential where the growth rate g  and t 1 are the initial conditions taking the natural logarithm and rearranging we have anti-contagion policies are designed to alter g through changes to  by reducing contact between susceptible and infected individuals holding the time-step between observations fixed at one day t 2 t 1  1 we thus model g as a time-varying outcome that is a linear function of a time-varying where  0 is the average growth rate absent policy policy t is a binary variable describing whether a policy is deployed at time t and  is the average eect of the policy on growth rate g  t is a mean-zero disturbance term that captures inter-period changes not described by policy t  using this approach infections each day are treated as the initial conditions for integrating equation 4 through to the following day we compute the first dierences logi t  logi t 1  using active infections where they are available otherwise we use cumulative infections noting that they are almost identical during this early period except in china where we use active infections we then match these data to policy variables that we construct using the novel data sets we assemble and apply a reduced-form approach to estimate a version of equation 6 although the actual expression has additional terms detailed below 12  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 estimation to estimate a multi-variable version of equation 6 we estimate a separate regression for each country c observations are for sub-national units indexed by i observed for each day t because not all localities began testing for coivd-19 on the same date these samples are unbalanced panels to ensure data quality we restrict our analysis to localities after they have reported at least ten cumulative infections we estimate a multiple regression version of equation 6 using ordinary least squares we include a vector of sub-national unit-fixed eects  0 ie varying intercepts captured as coe cients to dummy variables to account for all time-invariant factors that aect the local growth rate of infections such as dierences in demographics socio-economic status culture or health systems 24 we include a vector of day-of-week-fixed eects to account for weekly patterns in the growth rate of infections that are common across locations within a country we include a separate singleday dummy variable each time there is an abrupt change in the availability of covid-19 testing or a change in the procedure to diagnose positive cases such changes generally manifest as a discontinuous jump in infections and a re-scaling of subsequent infection rates eg see china in figure 1  eects that are flexibly absorbed by a single-day dummy variable because the dependent variable is the first-dierence of the logarithm of infections denote the vector of these testing dummies  lastly we include a vector of p c country-specific policy variables for each location and day these policy variables take on values between zero and one inclusive where zero indicates no policy action and one indicates a policy is fully enacted in cases where a policy variable captures the eects of collections of policies eg museum closures and library closures a binary policy variable is computed for each then they are averaged so the coe cient on these variables are interpreted as the eect if all policies in the collection are fully enacted in some cases for italy and the us policy data is available at a more spatially granular level than infection data eg city policies and state-level infections in the us in these cases we code binary policy variables at the more granular level and use population-weights to aggregate them to the level of the infection data thus policy variables may take on continuous values between zero and one with a value of one indicating that the policy is fully enacted for the entire population for each country our general multiple regression model is thus where observations are indexed by country c sub-national unit i and day t the parameters of interest are the country-by-policy specific coe cients  pc  we verify that our residuals  cit are approximately normally distributed appendix figure a1  and we estimate uncertainty over all parameters by clustering our standard errors at the day level 23 this approach non-parametrically 13  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 accounts for arbitrary forms of spatial auto-correlation or systematic misreporting in regions of a country on any given day it generates larger estimates for uncertainty than clustering by i when we report the eect of all policies combined eg figure 2b  we are reporting the sum of coe cent estimates for all policies p pc p1  cp  accounting for the covariance of errors in these estimates when computing the uncertainty of this sum note that our estimates of  and  0 in equation 7 are robust to systematic under-reporting of infections a major concern in the ongoing pandemic due to the construction of our dependant variable if only a fraction of infections are being reported such that we observe  i rather an actual infections i then the left-hand-side of equation 7 will be and is therefore unaected by the under-reporting thus systematic under-reporting does not aect our estimates for the eects of policy  there are some country-specific adjustments to equation 7 due to idiosyncratic dierences between samples in china we code policy parameters using weekly lags based on the date that the policy is first implemented in locality i as discussed in the main text this is done to understand the temporal dynamics of the response to policy in the one country where policy has been enacted the longest and in the most consistent way weekly lags are used because the incubation period covid-19 is thought to be 5-6 days 4 econometrically this means the eect of a policy implemented one week ago is allowed to dier arbitrarily from the eect of a policy implemented two weeks ago etc these eects are all estimated simultaneously also in china we omit day-of-week eects because there is no evidence to suggest they are present in the data -this could be due to the fact that the outbreak of covid-19 began during a national holiday and workers never returned to work in iran we estimate a separate eect of policies implemented in tehran that is allowed to dier from the eect in other locations by creating tehran-specific dummy variable that is interacted with both policy variables this is implemented because of the stark and significantly dierent eect of policies in tehran relative to eects in other parts of the country daily growth rates of infections to estimate the instantaneous daily growth rate of infections if policies were removed we obtain fitted values from equation 7 and compute a predicted value for the dependent variable when all p c policy variables are set to zero thus these estimated growth rates no policy cit capture the eect of all locality-specific factors on the growth rate of infections eg 14  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  demographics day-of-week-eects and adjustments based on the way in which infection cases are reported this counterfactual does not account for changes in information that are triggered by policy deployment since those should be considered a pathway through which policies aect outcomes as discussed in the main text when we report an average no policy growth rate of infections figure 2a  it is the average value of these predictions for all observations in the original sample location-and-day specific counterfactual predictions  no policy cit  accounting for the covariance of errors in estimated parameters are shown as red markers in figure 3  to provide a sense of scale for the estimated cumulative benefits of eects shown in figure 3  we link our reduced-form empirical estimates to the key structures in a simple sir system and simulate this dynamical system from the start of the pandemic to the present in each country the system is defined as the following where s is the susceptible population and r is the removed population here is a time-evolving parameter determined via our empirical estimates as described below accounting for changes in s becomes increasingly important as the size of cumulative infections i t  r t  becomes a substantial fraction of the local subnational population which occurs in some no policy scenarios our reduced-form analysis provides estimates for the growth rate of active infections  for each locality and day in a regime where s  1 thus we know able to obtain individuals are coded as recovered when they no longer test positive for covid-  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 19 whereas in the classical sir model this occurs when they are no longer infectious we adopt the average of these two medians setting  052 we use medians rather than simple averages because low values for i induce a long right-tail in daily estimates of and medians are less vulnerable to this distortion we then use our empirically based reduced-form estimates of both with and without policy combined with equations 8-11 to project total cumulative cases in all countries shown in figure 4  we simulate infections and cases for each administrative unit in our sample beginning on the first day for which we observe 10 or more cases for that unit using a time-step of 4 hours we estimate uncertainty by resampling from the estimated variance-covariance matrix of all parameters 5 who novel coronavirus covid-19 situation httpsexperiencearcgiscom experience685d0ace521648f8a5beeeee1b9125cd accessed 2020-03-19 6 with new state decrees 1 in 5 americans to be ordered indoors httpswwwnytimes com20200320worldcoronavirus-news-usa-worldhtmlactionclickmodule spotlightpgtypehomepage accessed 2020-03-20 7 chowell g sattenspiel l bansal s  viboud c mathematical models to characterize early epidemic growth a review physics of life reviews 18 66-97 2016   cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 22 ma j estimating epidemic exponential growth rate and basic reproduction number infectious disease modelling 2020 the lancet 395 689-697 2020  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020   cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  or timing eg policies that are deployed simultaneously in a given country to reduce the number of estimated parameters eects are all estimated simultaneously within a country for china we simultaneously estimate separate eects for each week after the policy was implemented eg china week 2 is the change in daily growth rates caused by policies implemented 8-14 days prior  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020 infections based on the observed timing of all policy deployments within each country blue and in a scenario where no policies were deployed red the dierence between these two predictions is our estimated eect of actual anti-contagion policies on the growth rate of infections small markers are daily estimates for sub-national administrative units vertical lines are 95 ci large markers are national average values for all sub-national units in our sample on that day black circles are observed changes in loginf ections averaged across the same administrative units predictions are only for observations in our sample and we omit observations before sub-national units report ten cumulative cases to focus our analysis on the impact of new policies we omit data from china after march 5 2020 because policies began to be rolled back during this period  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 1011012020032220040642 doi medrxiv preprint figure 4  estimated cumulative covid-19 infections with and without anti-contagion policies the predicted cumulative number of covid-19 infections based on each countrys actual policy deployments blue and in the no policy counterfactual scenario red sub-national infection growth rates from figure 3 are integrated adjusting for sir system dynamics in each sub-national unit see methods shaded areas show uncertainty based on 1000 simulations where estimated parameters are resampled from their joint distribution dark  inner 70 of predictions light  inner 95 black circles show the cumulative number of reported infections observed in the data in both scenarios the sample is restricted to units we analyze in figures 2 and 3  note that infections are not projected for administrative units that never report infections in the data but which plausibly would have experienced infections in a no policy scenario the jump in infections in france on march 2 2020 occurs due to administrative units entering the sample  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020   cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  table a1  number of policies tabulated by administrative divisions of each country table a1  policy data have been collected at various levels of administrative divisions in each country adm0 represents the country level and higher adm numbers indicate smaller administrative subdivisions each policy is counted at the highest level of specificity of the regions where the policy is applied for example if a country has ten regions at the adm1 level and a policy is applied across five of those regions the policy is counted as five separate adm1 policies rather than a single adm0 policy 1  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  figure a1  error distributions for estimated growth rates of covid-19 cases by country figure a1  these plots show the error structure for each country-specific econometric model used to predict the daily growth of active or cumulative covid-19 cases under the countrys actual policy regime as compared to the counterfactual world where no policies were enacted see the full model under the methods -econometric analysis section as well as the results in figure 3 of the main paper 2  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 3  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  figure a2  as an additional check we compared the cumulative number of confirmed cases from a handful of regions in our collated epidemiological dataset to the same statistics from the 2019 novel coronavirus covid-19 2019-ncov data repository by the johns hopkins center for systems science and engineering jhu csse we conducted this comparison for the two countries that we had the 1 most data for and at two different administrative levels in china we aggregate city level data up to the province level and in korea we aggregate provincial level data up to the country level the numbers tracked each other for the entire time series we have collected thus far this section describes the data acquisition and processing procedure for both epidemiological and policy data used in this paper the sources for both types of data come from a variety of in-country data sources which include government public health websites regional newspaper articles and wikipedia crowd-sourced information we have supplemented this data with international data compilations a list of the epidemiological and policy data compiled for this analysis can be found here  the epidemiological datasets and sources used in this paper are described below the main health variables of interest 1  cumconfirmedcases  the total number of confirmed positive cases in the administrative area since the first confirmed case 2  cumdeaths  the total number of individuals that have died from covid-19 3  cumrecoveries  the total number of individuals that have recovered from   cumhospitalized  the total number of hospitalized individuals 5  cumhospitalizedsymptom  the total number of symptomatic hospitalized individuals 6  cumintensivecare  the total number of individuals that have received intensive care 7  cumhomeconfinement  the total number of individuals that have been self-quarantined in their homes as a result of a positive test 8  activecases  the number of individuals who currently still test positive on the date of the observation 9  activecasesnew  the number of new cases since the previous date 10  cumtests  the total number of tests includes both positive and negative results conducted in an administrative unit additional metadata accompanying the health outcome variables 1  date  the date of observation 2  adm0name  the iso3 code to which this observation belongs 3  adm1name  the name of the adm1 region to which this observation belongs 1 httpsgithubcomcssegisanddatacovid-19 4  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  4  adm2name  if the dataset contains observations at the adm2 level then this is the name of the adm2 region to which this observation belongs 5  adm 1 2 id  any alphanumeric id scheme to identify different administrative units eg fips code 6  lat  the latitude of the centroid of the administrative unit 7  lon  the longitude of the centroid of the administrative unit 8  policiesenacted  the number of active policies that are in place for the administrative unit as of that date this variable is not population weighted 9  testingregime  a categorical variable used to identify when an administrative region or country changed their covid-19 testing regime this is zero-indexed with the ordering only indicating chronological progression there is no external meaning to regime 2 vs regime 1 vs regime 0 and there is no consistency enforced for coding across countries for example if china changes their testing regime twice all observations prior to the first regime change would be coded  testingregime 0 all observations in between the two changes would be coded  testingregime 1 and all observations after the second change would be coded  testingregime 2 data imputation in instances where health outcome observations are missing or suffer from data quality issues we have imputed to fill in the missing values imputed health outcome variables are denoted by  healthoutcomeimputed  for the majority of our analyses we do not use imputed data france is the exception where we impute two days of missing data we do this to ensure we have variation in policy variables for use in the analysis we impute by 1 taking the natural log of the non-missing observations pertaining to that health outcome variable 2 linearly interpolating over the missing dates for that health outcome variable 3 exponentiating the interpolated values back into levels and rounding to the nearest integer we have collated a city level time series health outcome dataset in china for 339 cities from january 10 2020 to present-day for data from january 24 2020 onwards we relied on the public dataset ding xiang yuan dxy that 2 reports daily statistics across chinese cities since dxy only publishes the most recent cross-sectional statistics and not the historical data we used the time series dataset scraped from dxy in an open source github project  the web scraper program checks for updates at least once a 3 2 httpsncovdxycnncovh5viewpneumonia 3 httpsgithubcomblankerldxy-covid-19-data 5  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 day for the statistics published on dxy and records any changes in the number of cumulative confirmed cases cumulative recoveries or cumulative deaths we assumed that no updates to the statistics meant there had been no new cases we dropped a small number of cases that had been recorded but not assigned to a specific city many of these cases are imported ones from other cities we also dropped confirmed cases in prison populations we assumed the spread of covid-19 in prisons was not affected by the implementation of city-level lockdowns or travel ban policies for city level health outcomes prior to january 24 2020 we manually collected official daily statistics from the central and provincial hubei guangdong and zhejiang  chinese government websites we did not collect city level health outcomes recorded prior to january 24 2020 in provinces that had fewer than ten confirmed cases at that date we made this decision since our analysis dropped observations with fewer than ten cumulative confirmed cases to prevent noisy data during the early transmission phase from disproportionately biasing the estimated results after merging the two datasets we conducted a few quality checks 1 we checked that cumulative confirmed cases cumulative recoveries and cumulative deaths were increasing over time in instances when cumulative outcomes decreased over time we assumed that the recent numbers were more reliable and treated the earlier number of cumulative cases as missing this was often due to data entry errors or cases where patients that were reported to have been diagnosed with covid-19 but were later found out to actually have tested negative the magnitude of these errors was relatively small we filled in any missing data with the imputation methodology described in the health data overview section 2 we validated our city level dataset by aggregating observations up to the provincial level and comparing the time trends from the aggregated dataset to that of the provincial dataset collated by johns hopkins university we confirmed that the two datasets matched very closely see figure a2 8 panel a as of the time of writing the criteria for being diagnosed with covid-19 had changed twice in china 9 on february 13 2020 china recategorized patients who exhibited symptoms as determined through a chest scan as part of the confirmed cases count even if they had not tested positive in the pcr test this was due to concerns that the pcr test had relatively high false negative rates on february 20 2020 china reversed this decision we included this information in the dataset because it could have potentially changed the levels and short-term growth rates of the number of confirmed cases we have collated a regional level time series health outcome dataset in france from february 15 2020 to present-day we used the number of confirmed covid-19 cases by rgion from frances government website the 10 sources listed for this dataset were the french public health website the ministry of solidarity and 11 health french newspapers that reported government information and regional public health 12 13 websites given that this dataset was not published on a daily basis we supplemented it by scraping 14 the number of confirmed cases by rgion on the french public health website which has been updated every day 15  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 have been reporting the number of confirmed cases on a daily basis for these provinces we recorded this published health data given that the province of gangwon-do does not report provincial level health data we refer to the 21 daily number of new cases reported by each of its counties 22 23 taebaek-si sokcho-si and samcheok-si  as a result we manually collected the number of new not explicitly publish the number of cumulative confirmed cases however they did publish patient-level data including the date of when patients had tested positive for these provinces we constructed the measure of cumulative confirmed cases by counting the number of daily confirmed cases and adding it to the previous dates total most provinces did not publish the number of deaths instead we checked the daily policy briefings posted on the government homepages mentioned in the footnotes and manually collected mortality data in instances when mortality data were not found in the briefings we obtained the mortality data from other official sources such as through social media sources eg facebook and blogs maintained by local governments lastly we supplement these sources with mortality data reported in news articles we collected information on testing regime changes from the homepage of the korean center for disease control and prevention kcdc in the press release menu the kcdc uploaded daily briefing announcements which contained information on testing criteria and changes to the testing regime 39 8  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  initially the south korean government only tested people who 1 demonstrated respiratory symptoms within 14 days after visiting wuhan south china seafood wholesale market and 2 those who had pneumonia symptoms within 14 days after returning from wuhan 40 as the outbreak spread the kcdc broadened the criteria for testing starting january 28 2020 the agency isolated 1 those who had fever or respiratory symptoms upon returning from hubei province and 2 those who had symptoms of pneumonia upon returning from mainland china  we coded 41 42 this as the first change in the testing regime the second testing regime change occurred on february 4 2020 when the kcdc announced that people who had had any routine contacts with confirmed cases were required to self quarantine for a 14-day period the agency defines two categories of contacts close contacts and routine contacts the former is defined as a person who has been within two meters of in the same room as or exposed to any respiratory secretions of an infected individual the latter refers to whether the individual conducted any activity in the same place and time as the infected person prior to this regime change kcdc separated those two cases and applied different quarantine policies starting february 4 2020 any routine contacts were also required to be self-quarantined 43 shortly thereafter south korea aggressively expanded the scope of their testing starting february 7 2020 the kcdc broadened the definition of suspected cases to 1 anyone who developed a fever or respiratory symptoms within 14 days after returning from china 2 anyone who developed a fever or respiratory symptoms within 14 days after being in close contact with a confirmed case and 3 anyone suspected of contracting covid-19 based on their travel history to affected countries and their clinical symptoms moreoverthe kcdc announced that the test would be free for all suspected cases and 44 40 httpswwwcdcgokrboardboardesmida20501000000bid0015listno365654actview 41 httpswwwcdcgokrboardboardesmida20501000000bid0015listno365874actview 42 nb the kcdc english website explains the testing regime change in a more condensed format any citizens identified with a fever or respiratory symptoms and have visited wuhan will be isolated and tested at a nationally designated isolation hospital and any foreigners staying in korea will be conducted in cooperation with police httpswwwcdcgokrboardboardesmida30402000000bid0030actviewlistno365888tagn page1 43 httpwwwmohwgokrreactalsal0301vwjspparmenuid04menuid0403page1conts eq352662 44 httpswwwcdcgokrboardboardesmida30402000000bid0030actviewlistno366114tagn page1 nb the date of this press release is february 8 2020 but the definition of suspected cases was effective starting from february 7 2020  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 confirmed cases as a result of these efforts kcdc announced that they would begin to test 3000 45 people daily a marked increase from only 200 people a day 46 the kcdc revised their guidelines on february 20 2020 in order to test more people their press release stated suspected cases with a medical professionals recommendation regardless of travel history will get tested additionally those who are hospitalized with unknown pneumonia will also be tested lastly anybody in contact with a diagnosed individual will need to self-isolate and will only be released when they test negative on the thirteenth day of isolation 47 as the number of patients grew rapidly the kcdc decided to focus on more vulnerable groups in their february 29 2020 press release the agency stated the kcdc has asked local government and health facilities to focus on tests and treatment especially targeting those aged 65 and those with underlying conditions who need early detection and treatment this change was coded as our last testing regime change in the dataset 48 we have collated a regional and provincial level time series health outcome dataset in italy from february 24 2020 to present-day this data came from the github repository maintained by the italian department of civil protection  dipartimento della protezione civile  health outcomes included the number of confirmed cases the number of deaths the number of recoveries and the number of active cases these figures have been updating daily at 5 or 6 pm central european time the regional level dataset was pulled directly from  dati-regionidpc-covid19-ita-regionicsv  and the provincial level dataset was pulled from  dati-provincedpc-covid19-ita-provincecsv  the testing regime change in italy occurred when the director of higher health council announced on february 26 2020 that covid-19 testing would only be performed on symptomatic patients as the majority of the previous tests performed were negative cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  we have collated a provincial level time series health outcome dataset in iran from february 19 2020 to present-day the iranian government had been announcing its new daily number of covid-19 confirmed cases at the provincial level on the ministry of healths website this data has been compiled daily in the table new covid-19 cases in iran by province located in the 2020 coronavirus pandemic in iran article 49 on wikipedia we spot-checked the data in the wikipedia table against the iranian ministry of health announcements using a combination of google translate and a comparison of the numbers in the 50 51 announcements which were written in persian script to the persian numbers on march 6 2020 the ministry of health announced a national coronavirus plan which included 52 contacting families by phone to identify potential cases along with the disinfecting of public places the plan was to begin in the provinces of qom gilan and isfahan and then would be rolled out nationwide on march 13 2020 the government announced a military-enforced home isolation policy throughout the nation this announcement included nationwide disinfecting of public places while 53 a follow-up announcement of the march 6 high testing regime stating its complete rollout was not found the march 13 announcement did reference the implementation of the public spaces component of the earlier plan across the country we thus assumed that the high testing regime had also been fully rolled out on march 13 2020 we have collated a state level time series health outcome dataset in the united states from january 22 2020 to present-day the data comes from the github repository associated with the johns hopkins university jhu interactive dashboard dong du  gardner 2020 lancet as of the time of writing the data are available here  the repository and dashboard are updated essentially in real-time at least daily  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  to determine the testing regime we used estimated daily counts of the cumulative number of tests conducted in every state as aggregated by the largely crowdsourced effort named the covid tracking project  covidtrackingcom  we estimated the total number of tests as the sum of confirmed positive and negative cases for some states and some days there have been no negative case counts in which case we utilize just the confirmed positive cases we also ensured that the confirmed number of positive cases agreed with the counts in the jhu dataset we programmatically filtered for possible testing regime changes by filtering for any consecutive days during which the testing rate increased at least 250 from one day to the next and where this jump was an increase of at least 150 total tests over one day after visually inspecting the candidates we removed detected testing regime changes for north carolina and connecticut as these states did not demonstrate spikes in their testing rate but rather a more gradual and steady rate in the increase of testing nb the last download from covidtrackingcom was march 19 1930 pst we have been updating the process and the removal of detected testing regime changes periodically so this may change the policy events datasets and sources used in this paper are described below for each country the relevant country-specific policies identified were then mapped to a harmonized policy categorization used across all countries the policy categories are coded as binary variables where  policyvariable   0 before the policy is implemented in that area and  policyvariable   1 on the date the policy is implemented and for all subsequent dates until the policy is lifted the main policy categories identified across the six different countries fall into four broad classes 1 restricting travel a  travelbanlocal   a policy that restricts people from entering or exiting the administrative area eg county or province treated by the policy b  travelbanintlin  a policy that either bans foreigners from specific countries from entering the country or requires travelers coming from abroad to self-isolate upon entering the country c  travelbanintlout  a policy that suspends international travel to specific foreign countries that have high levels of covid-19 outbreak d  travelbancountrylist  a list of countries for which the national government has issued a travel ban or advisory this information supplements the policy variable  travelbanintlout  e  transitsuspension  a policy that suspends any non-essential land- rail- or water-based passenger or freight transit 2 distancing through cancellation of events and suspension of educationalcommercialreligious activities a  schoolclosure  a policy that closes school and other educational services in that area b  businessclosure  a policy that closes all offices non-essential businesses and non-essential commercial activities in that area non-essential services are defined by area c  religiousclosure  a policy that prohibits gatherings at a place of worship specifically targeting locations that are epicenters of covid-19 outbreak see the section on korean policy for more information on this policy variable d  workfromhome  a policy that requires people to work remotely this policy may also include encouraging workers to take holidaypaid time off e  eventcancel  a policy that cancels a specific pre-scheduled large event eg parade sporting event etc this is different from prohibiting all events over a certain size f  nogathering  a policy that prohibits any type of public or private gathering whether cultural sporting recreational or religious depending on the country the policy can prohibit a gathering above a certain size in which case the number of people is specified by the  nogatheringsize  variable g  nodemonstration  a policy that prohibits protest-specific gatherings see the section on korean policy for more information on this policy variable h  socialdistance  a policy that encourages people to maintain a safety distance often between one to two meters from others this policy differs by country but includes other policies that close cultural institutions eg museums or libraries or encourage establishments to reduce density such as limiting restaurant hours 3 quarantine and lockdown a  poscasesquarantine  a policy that mandates that people who have tested positive for covid-19 or subject to quarantine measures have to confine themselves at home the policy can also include encouraging people who have fevers or respiratory symptoms to stay at home regardless of whether they tested positive or not b  homeisolation  a policy that prohibits people from leaving their home regardless of their testing status for some countries the policy can also include the case when people have to stay at home but are allowed to leave for work-or health-related purposes for the latter case when the policy is moderate this is coded as  homeisolation  05 4 additional policies a  emergencydeclaration  a decision made at the citymunicipality county stateprovincial or federal level to declare a state of emergency this allows the affected area to marshal emergency funds and resources as well as activate emergency legislation b  paidsickleave  a policy where employees receive pay while they are not working due to the illness  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 optional policies in the cases when the aforementioned policies are optional we denote this as  policyvariableopt  population weighting of policy variables in the cases when only a portion of the administrative unit eg half of the counties within the state are affected by the implementation of the policy we weight the policy variable by the percentage of population within the administrative unit that is treated by the policy this is denoted as  policyvariablepopwt  and the value that this variable can take on is a continuous number between 0 and 1 sources for the population data are detailed in a later section we obtain data on chinas policy response to the covid-19 pandemic by culling data on the start dates of travel bans and lockdowns at the city-level from the 2020 hubei lockdowns wikipedia page the wuhan coronavirus timeline project on github and various news reports to combat the spread of covid-19 the chinese government imposed travel restrictions and quarantine measures starting with the lockdown of the city of wuhan the origin of the pandemic on january 23 2020 immediately following the wuhan lockdown neighboring cities followed suit banning travel into and out of their borders shutting down businesses and placing residents under household quarantine the same policy measures were implemented in cities across china for the next three weeks some lockdowns occurred during the national chinese new year holiday january 24 -30 2020 when schools and most workers were on break on january 27 2020 china extended the official holiday to february 2 2020 while many additional provinces delayed resuming work and opening schools for even longer the chinese new year holiday is analogous to containment policies such as school 56 closures and restrictions on non-essential work we do not specifically estimate the effect of this holiday extension as most cities were in lockdown during the extended holiday and a lockdown is a more restrictive containment measure a lockdown requires all residents to stay home except for medical reasons or essential work and only allows one person from each household to go outside once every one to five days exact policy varied by city we obtain data on frances policy response to the covid-19 pandemic from the french government website press releases from each regional public health site and wikipedia 54 httpsenwikipediaorgwiki2020hubeilockdowns 55 httpsgithubcompratityawuhan2020-timeline 56 httpswwwchina-briefingcomnewschina-extends-lunar-new-year-holiday-february-2-shanghai-february -9-contain-coronavirus-outbreak 14  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 the french government website contains a timeline of all national policy measures each regional 57 public health agency  lagence rgionale de sant  in france posts press releases with information on the policies the rgion or dpartements within the rgion will implement to mitigate the spread and impact of the covid-19 outbreak the wikipedia page on the 2020 coronavirus pandemic in france 58 has collated information on the major policy measures taken in response to the starting february 29 2020 france banned mass gatherings of more than 5000 people nationwide while some major sporting events were cancelled and a handful of schools closed to mitigate the spread of the virus as more covid-19 cases were confirmed during the following week additional sporting events were canceled more schools decided to close and certain cities and dpartements limited mass gatherings to no more than 50 people excluding shops business restaurants bars weddings and funerals some rgions closed early childhood establishments eg nurseries daycare centers and prohibited visitors to elderly care facilities on march 8 2020 france banned mass gatherings of more than 1000 people nationwide other schools cities and dpartements followed suit with additional school closures and limiting mass gatherings on march 11 2020  france prohibited all visits to elder care establishments starting march 16 2020  france closed all schools nationwide we have coded various policies that cancel events and large gatherings as such any cancellations of professional sporting and other specific pre-scheduled events as the policy variable  eventcancel  the  nogathering  policy variable represents policy measures that banned all events or mass gatherings of a certain size eg no gatherings of over 1000 people the  socialdistance  policy variable includes measures preventing visits to elder care establishments closures of public pools and tourist attractions and teleworking plans for workers we obtained data on south koreas policy response to the covid-19 pandemic from various news sources as well as press releases from the korean centers for disease control and prevention kcdc the ministry of foreign affairs and local governments websites the policy variables coded in the dataset are  businessclosure   businessclosureopt   emergencydeclaration   nodemonstration   religiousclosure   schoolclosure   socialdistanceopt   travelbanintlinopt   travelbanintloutopt  and  workfromhomeopt  the kcdc announced on february 28 2020 that health-related public facilities were recommended to be closed hence the policy variable  businessclosure  was coded as one from the announcement 60 date even though it was technically a recommendation we did not code this policy as an optional 15  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 the first travel ban in our dataset since level 2 alerts are issued relatively rarely such as during a significant demonstration or military coup as a result we coded the level 2 alert due to into the dataset for the policy analysis the policy variable  workfromhomeoptional  indicates when kcdc began recommending that people work from home on march 15 2020 the kcdc press release stated since contact with confirmed cases in an enclosed space increases the possibility of transmission it is recommended to work at home or adjust desk locations so as to keep a certain distance among people in the office more detailed guidelines for local governments and high-risk working environments will be distributed soon 98 we have obtained data on italys policy responses to the covid-19 pandemic primarily from the english version of the covid-19 dossier chronology of main steps and legal acts taken by the italian government for the containment of the covid-19 epidemiological emergency written by the 99 department of civil protection  dipartimento della protezione civile  most recently updated on march 12 2020  this dossier details the majority of the municipal regional provincial and national policies rolled out between the start of the pandemic to present-day we have supplemented these policy events with news articles that detail which administrative areas were specifically impacted by the additional policies the first major policy rollout was on february 23 2020 when 11 municipalities across two provinces in northern italy were placed on lockdown these policies included closing schools cancelling public and private events and gatherings closing museums and other cultural institutions closing non-essential commercial activities and prohibiting the movement of people into or out of the municipalities the second major policy rollout was on march 1 2020 when two provinces and three regions in northern italy were placed on partial lockdown these policies also included closing schools cancelling public and private events and gatherings closing museums closing non-essential commercial activities as well as limiting the number of people at places of worship restricting operating hours of bars and restaurants and encouraging people to work remotely 19  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020 the third major policy roll-out was on march 5 2020 when all schools across the country were closed the fourth major policy roll-out was on march 8 2020 when the region of lombardy and 13 provinces in northern italy were placed on lockdown these policies included the cancellation of public and private events and gatherings closing of museums encouraging people to work remotely limiting the number of people at places of worship restricting opening hours of bars and restaurants mandating quarantine of people who tested positive for covid-19 prohibiting the movement of people into or out of the affected area and restricting movement within the affected area to only work-or health-related purposes commercial activities were still allowed as long as they maintained a safety distance of one meter apart per person within the establishment all civil and religious ceremonies including weddings and funeral ceremonies were suspended during this same policy roll-out the rest of the country faced less stringent policies cancelling of public and private events closing of museums and requiring restaurants and commercial establishments to maintain a safety distance of one meter apart per person within the establishment the fifth major policy roll-out was announced on march 9 2020 and went into effect on march 10 2020 when lockdown policies applied to northern italy were rolled out to the entire country lastly on march 11 2020 the lockdown was changed to also cover the closing of any non-essential businesses and further restricted people from leaving their home for irans policy response to the covid-19 pandemic we relied on news media reporting as the primary source of policy information mostly due to translation restrictions we also relied on two timelines of pandemic events in iran to help guide the policy search 100 101 the first major outbreak in iran was connected to a major shia pilgrimage in the city of qom that brought shiite pilgrims from iran and throughout the middle east where they came to kiss the fatima masumeh shrine it is possible that the disease was brought to qom by a merchant traveling from wuhan china in addition it is believed that the iranian government knew of the covid-19 outbreak 102 prior to its february 21 2020 parliamentary elections but downplayed the risks associated with the disease as not to suppress voter turnout given concerns that a low turnout would reflect poorly on its legitimacy the disease initially centered in qom and neighboring tehran spread rapidly 103 throughout the country 100 httpswwwthinkglobalhealthorgarticleupdated-timeline-coronavirus 101 httpsenwikipediaorgwiki2020coronaviruspandemiciniran 102 httpswwwnewyorkercomnewsour-columnistshow-iran-became-a-new-epicenter-of-the-coronavirus-o utbreak 103 httpswwwnewyorkercomnewsour-columnistshow-iran-became-a-new-epicenter-of-the-coronavirus-o utbreak 20  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 1011012020032220040642 doi medrxiv preprint the k-12 and higher education level business closures have also been recommended or enforced such that employees should work from home unless their work is considered essential to the greater public eg health care grocery stores to support employees working remotely or staying home when sick a number of states have also mandated paid sick leave for those who are affected by  free testing has also been implemented in certain states so that anyone experiencing symptoms or has been exposed to the virus can now get tested for free 111 we coded various policies that cancel events and large gatherings as follows the cancellation of large events specifically the election postponement in louisiana is categorized as  eventcancel  the separate  nogathering  policy variable represents policy measures that banned all events or mass gatherings of a certain size ie no gatherings over a certain number of people where this number has varied by region the  socialdistance  category includes measures that prevent visits to elderly care facilities close public facilities such as libraries and require workers to work remotely the  emergencydeclaration  encompasses the declarations of a state of emergency at the city county state and federal level this declaration allows the affected area to immediately marshal emergency funds and resources and activate emergency legislation while also giving the public an indication of the gravity of the situation in order to construct population weighted policy variables and to determine the susceptible fraction of the population for disease projections under the realized and the no policy counterfactual scenarios we obtained the most recent estimates of population for each administrative unit included in our analysis the sources of that population data are documented below city-level population data have been extracted from a compiled dataset of the 2010 chinese city statistical yearbooks we matched the city level population dataset to the city level covid-19 epidemiology dataset as the two datasets use slightly different administrative divisions we only matched 295 cities that exist in both datasets and grouped the remaining 39 cities in our compiled epidemiology dataset into other for prediction purposes cities grouped into other because of mismatches have a total population of 114000000 or 85 of the total population in china dpartement -level populations are obtained from the national institute of statistics and economic database httpswwwinseefrfrstatistiques2012713tableau-tcrd004tab1departements  we used the most up to date estimation of the population in france as of january 2020 110 httpswwwcdcgovcoronavirus2019-ncovcommunitylarge-eventsmass-gatherings-ready-for-covid-19html 111 httpsappropriationshousegovsitesdemocratsappropriationshousegovfilesfamilies20first20summarypdf 22  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted march 27 2020  httpsdoiorg 101101 10 2020   significant expression of furin and ace2 on oral epithelial cells may facilitate the efficiency of sars-cov-2 entry mei zhong bing-peng lin hong-bin gao andrew young j xin-hong wang chang liu kai-bin wu ming-xiao liu jian-ming chen jiang-yong huang learn-han lee cui-ling qi  lin-hu ge li-jing wang  leading to a sustained epidemic spread with 2000000 confirmed human infections including 100000 deaths covid-19 was caused by sars-cov-2 and resulted in acute respiratory distress syndrome ards and sepsis which brought more challenges to the patients treatment the s-glycoprotein which recognized as the key factor for the entry of sars-cov-2 into the cell contains two functional domains an ace2 receptor binding domain and a second domain necessary for fusion of the coronavirus and cell membranes furin activity exposes the binding and fusion domains is essential for the zoonotic transmission of sars-cov-2 moreover it has been reported that ace2 is likely to be the receptor for sars-cov-2 in addition furin enzyme and ace2 receptor were expressed in airway epithelia cardiac tissue and enteric canals which considered as the potential target organ of the virus however report about the expression of furin and ace2 in oral tissues was limited in order to investigate the potential infective channel of new coronavirus in oral cavity we analyze the expression of ace2 and furin that mediate the new coronavirus entry into host cells in oral mucosa using the public single-cell sequence datasets furthermore immunohistochemical staining experiment was performed to confirm the expression of ace2 and furin in the protein level the bioinformatics results indicated the differential expression of ace2 and furin on epithelial cells of different oral mucosal tissues and the proportion of furin-positive cells was obviously higher than that of ace2-positive cells ihc experiments revealed that both the ace2-positive and furin-positive cells in the target tissues were mainly positioned in the epithelial layers partly expressed in fibroblasts which further confirm the bioinformatics results conclusions based on these findings we speculated that sars-cov-2 could effectively invade oral mucosal cells though two possible routes binding to the ace2 receptor and fusion with cell membrane activated by furin protease our results indicated that oral mucosa tissues are susceptible to sars-cov-2 which provides valuable information for virus-prevention strategy in clinical care as well as daily life  in december 2019 an increasing number of patients with pneumonia caused by unknown pathogen emerged in wuhan hubei china with clinical presentations greatly resembling viral pneumonia 1 2 3  a novel coronavirus which was named as corona virus by the world health organization who was detected from lower respiratory tract samples via deep sequencing analysis 1  as of 15th april 2020 the number of coronavirus infections approaches more than two million people and kill more than a hundred thousand 4  the present outbreak of the 2019 coronavirus strain has been labeled as a global pandemic by who 5  the covid-19 outbreak has posed critical challenges for the public health research and medical communities similar to those of patients with sars and mers the clinical manifestations of covid-19 at illness onset are fever dry cough and myalgia the patients may suffer from dyspnea shortness of breath and then respiratory failure and even death in later stages 2 6 7  previous findings indicated that the ace2 plays an important role in cellular entry thus ace2-expressing cells may act as target cells and are susceptible to covid-19 infection 2 8 9 10  besides ace2 receptor was proven to be expressed in 72 human tissues and is abundantly present in the epithelia of the lung and small intestines 11  studies also found ace2 expression in the epithelial layer of nasal and oral mucosa however this conclusion was based on the bulk rna datasets the cell type and number of cells expressing ace2 in each tissue could not be estimated as a co-expressed membrane endopeptidase of ace2 receptor furin play a central role in the human immunodeficiency virus type-1 hiv-1 entering into the host cells through cleaving its envelope glycoprotein 12  genomic characterization of the sars-cov-2 severe acute respiratory syndrome coronavirus 2 have revealed that furin enzyme could activate the specific site of its spike protein 13  meanwhile li h et al revealed that a putative furin-cleavage site reported recently in the spike protein of sars-cov-2 may facilitate the virus-cell fusion which can explain the virus transmission features 14  in addition ma y and co-authors found that presence of furin cleavage site which is absent in other coronavirus during the course of sars-cov-2 infection may cause some distinct clinical symptoms from sars-cov infection 15  similar results were also reported in li xs study from china 16  hence the expression and distribution of ace2 and furin in human body were the key factors for the coronavirus to invade host cells furthermore online single-cell sequence datasets analysis unveiled that furin was expressed in coronaviruss potential target organs such as lung heart nose rectum colon intestine and ileum 14  however information on the expression and distribution of ace2 and furin in oral cavity were limited single-cell rna sequencing scrna-seq technique which provides an avenue to understand gene regulation networks metastasis and the complexity of intratumoral cell-to-cell heterogeneity at the single-cell level has diverse applications on bioinformatics stem cell differentiation organ development whole-tissue subtyping and tumor biology 17 18 19  it captures cellular differences underlying tumor biology at a higher resolution than regular bulk rna-seq and has revolutionized studies of cancer mechanisms and personalized medicine  20   for better understanding the potential infection risk in oral cavity we explored whether the ace2 and furin were expressed and their expressing cells composition and proportion in different oral tissues based on the public scrna-seq profiles from geo public databases then ihc analysis was used to further confirm the results in the protein level these results will provide a theoretical basis for clarifying the possible invading path and damage of sars-cov-2 in oral tissues single-cell rna-seq datasetgse103322 including the gene expression and cell type annotation from human oral squamous cell carcinoma tissue oscc were downloaded from the geo database httpswwwncbinlmnihgovgeo single-cell rna-seq datasets gse103322 were downloaded from the geo database datasets of five patients which have the closest histological features with normal tissue were selected for futher analysis the scrna-seq data sets of gse103322hnumitabletxt was used to subsequently analyze based on r package seurat version 362 to control quality we removed cells with  50 genes and as well as the cells with mitochondrial content higher than 5 besides the genes detected in  3 cells were filtered out in the function createseuratobject subsequently the data was log-normalized using the function normalizedata with the default parameters the findvariablegenes function was used to determine the highly expressed variable genes followed by pca dimensionality reduction by runpca function pca components which p value was less than 005 were selected to further analyze using two-dimensional t-distributed stochastic neighbor embedding tsne the setting of kparam was 20 in the function findneighbors and the setting of the resolution was 05 in the function findclusters finally the cell types were assigned based on their canonical markers gene expression of different cell types was displayed by the functions of dotplot and vlnplot the significant level was set as 005 twelve normal tissues of oral mucosa three buccal and gingival tissues two lip tongue and palatal tissues respectively were taken from 12 different patients with clinically diagnosed fibrous epithelial polyp or benign tumor 11 21  whose average age was 53 written informed consent was obtained for each participant and the study was in accordance with the declaration of helsinki and the guidelines approved by the clinical ethics committee of stomatology hospital of guangzhou medical university tissue morphology was evaluated in haematoxylin and eosin-stained sections by a qualified pathologist the selected tissues were fixed in 10 neutral formalin solution and embedded in paraffin the paraffin sections 4 m in thickness were deparaffinized in xylene rehydrated in a graded series of alcohol then incubated in citrate buffer ph 60 for 5 min at 120c and the endogenous peroxidase was blocked by 03 h2o2 for 10 min subsequently they were dried and incubated anti-ace2 cat no ab15348  abcam cambridge cb uk antibody overnight at 4 c the next day the hrp-conjugated secondary antibody was added to the sections peroxidase activity was developed by using dab for 8 min and counterstained with hematoxylin the expression levels of ace2 were evaluated by two experimenters a qualified pathologist analyzed the staining for structures positive for ace2 the stain intensity of ace2 and furin was semiquantitatively graded according to the previous methods 22  statistical significance was determined with a one-way anova followed by bonferronis post-hoc test for multiple group comparisons or students t-test for two group comparisons for all tests p  005 or  001 was considered statistically significant or very significant given that sars-cov-2 invade host cells via ace2 receptors we detected the ace2-expressing cell types by analyzing single-cell rna sequencing data 1843 individual cells from the oral mucosa of five patients were analyzed using the unsupervised graph-based clustering we found that at least thirteen distinct cell clusters existed in the oral tissues  fig 2a  as ace2 had been proven to be one of the major receptors of covid-2019 in human body we explored the online datasets to find out the expression level of ace2 in oral mucosa as expected data from geo indicated that ace2 receptor expression level was relatively higher in epithelial cells cluster 1 4 5 fig 3a 3b  while very little cellular expression of ace2 was found in fibroblasts cluster 2 then we calculated the percent of ace2-expressing cells in the dataset fig 3c  we find that in oral tissue the percentage of ace2-positive cells was 22 among them 92 were epithelial cells it has been indicated that furin activity is essential for the transmission of sars-cov-2 to determine the cell types of expressing furin we analyzed the furin mrna expression using tsne method violin and scatter plot showed that furin was highly expressed in epithelial cells follow by fibroblasts t-cells and endothelial cells of the oral mucosal tissues but hardly expressed in b cells fig 4a 4b  meanwhile the proportion of furin-expression cells was approximately 10 among them epithelial cells make up more than 55 fig 4c  to further determine the protein expression level of ace2 in the oral mucosa ihc experiments were performed the first remarkable finding was that ace2 was present in epithelial cells in all the tissues studied fig 5  results indicated that the expression level of ace2 protein was significantly higher in the lip tongue and buccal mucosa especially the epithelial cells in the basal layer although the mrna expression level is not as high fig 3  the ace2-postive cells in the gingival and palate tissue were limited the corresponding gingiva and palate epithelial cells showed weak positive ace2 staining also we found that ace2 expressed in fibroblasts and endothelial cells these results indicate that oral epithelial cells were the potential targets of sars-cov-2 furin mrna expression has been described mainly in oral epithelial cells through scrna-seq technique to determine whether this differential expression of furin was maintained at the protein level oral tissues from different sites were screened as shown in fig 6  moderate to mild immunostaining in the cytoplasm of epithelial cells can be seen in five normal oral tissues surprisingly except for the positive cells in the basal layer of oral epithelium the spinous layer in all examined tissues also turned out with large numbers of furin-positive cells in addition the percentage of furin-positive cells in lip tongue and gingiva was higher than that of buccal and palate mucosa these results indicated that sars-cov-2 might be able to infect oral tissues through the route of furin-mediated fusion with oral mucosal cell the amazing spreading speed of sars-cov-2 and the large number of infected people have quickly overwhelmed the public health services in the world and have become a global public health problem 23  as mentioned before ace2 receptor in cell membrane and furin clearage site of sars-cov-2 were the key factors that allowed the virus to invade the host 14 15  thus clarifying the distribution of ace2 and furin at the protein and gene levels in tissues is essential for understanding the pathogenic mechanism of sars-cov-2 many studies have shown that ace2 receptor was highly expressed in respiratory epithelium kidney cardiovascular system and testis 24  some scholars also found that ace2 was expressed in oral tissues with bulk rna sequencing data which could not accurately reflect the expression of ace2 on the single cell level 21 25  chen et al benefited from the single-cell sequencing data to further proved the expression of ace2 receptor in tongue buccal mucosa and gingiva 11  however these studies were based on public databases and were not analyzed together and further validation of protein levels was lacking in the current study we firstly characterized the ace2 protein expression profiles in five different oral tissues through immunohistochemical staining experiment results indicated that ace2 was mainly present in epithelial cells in all included tissues especially in buccal mucosa lip and tongue previous study discovered the similar result 21  overall these results demonstrated that compared with other oral sites buccal mucosa lip and tongue might be more susceptible to sars-cov-2 in addition virologist gary whittaker of cornell university whose team published another structural analysis of the coronavirus s protein on biorxiv on february 18th argues that the furin cutting site allows the virus to enter cells in a very different way than sars and this may affect the stability and spread of the virus 26  other research teams have also identified this activation site suggesting that it may enable the virus to spread effectively between humans 13  it has been demonstrated that sars-cov-2 could fuse with cell membrane and then entry into the target cells  27  28   researchers have found furin proteases in many tissues including the lungs liver and small intestine 29  however limited reports were seen regarding the expression of furin in oral cavity and whether the new coronavirus could spread by oral cavity are not fully understood thus we investigated the furin enzyme in both gene and protein level from five different oral sites and the results would be valuable for the prevention of the covid-19 spreading previous analysis about the sars-cov-2 receptor in the sing cell level were based on the complete oscc database named gs103322 which was significantly different from the genetic profiles of normal oral tissues 30  in order to accurately analyzing the expression status of furin in the gene level datasets from five oscc patients which were graded as 1 and t1n0 stage were selected originated from the online platform in the present research the obtained single cell rna-seq data were processed and analyzed with r software as expected furin were found abundantly enriched in oral mucosa cells particularly in epithelial cells and the proportion of furin-positive cells was higher than that of ace2-positive cells except for the expression profiles in the gene level furin protein expression in oral cavity was crucial for further clarifying the impact of sars-cov-2 on the mouth previous reports had demonstrated that furin protein was expressed in metastasizing osccs 31  besides furin was expressed in normal epithelium and up-regulated in sccs from three different organs 22  but the relative report of furin protein expression in normal oral cavity was limited for further validating the results in the gene level furin immunohistochemistry was therefore performed on conventional sections for the first time to assess its expression in oral mucosa obtained from five different sites it is noteworthy that moderate positive expression of furin protein were observed in tongue giniva and lip while weakly positive in the buccal and palatal tissues as a result we speculated that sars-cov-2 could fuse with the furin enzyme on the cell membrane the oral mucosal cells then entry into the host cells alternatively furin could be recognised as the potential drug target of suppressing the infection of new coronavirus strikingly the scrna-seq data analysis and ihc tests results indicated that ace2 and furin were both abundantly expressed in oral mucosa especially on oral epithelial cells which were similar to the previous reports 32 33 34  in other word two key proteins of sars-cov-2 entering the target cell in different routes were both present in oral mucosa which implied the potential attack and spread of sars-cov-2 in the oral tissues moreover the live sars-cov-2 was consistently detected in the self-collected saliva of 917 of patients till 11 days after hospitalization and it could be transmitted via saliva directly or indirectly even among asymptomatic infected patients 35  thus it is suggested that all the healthcare workers should focus on taking effective measures to prevent the virus from spreading via the oral cavity and potential impairment of oral tissues should be evaluated after infected with sars-cov-2 giving more evidence to the prevention and treatment of covid-19 we should also pay close attention to protecting the oral tissues of all people preventing the virus from spreading through the mouth in the current study we investigated the expression of other endopeptidases in various oral mucosa cells using the public single-cell sequence datasets as shown by voilin plot in supplemental files the high expression of ace2 sheddase adam17 adam10 indicated the higher membrane fusion activity virus uptake and virus replication in the oral mucosa besides the endopeptidases such as capn1 and capns1 were abundantly enriched in the oral mucosal cells large proportion of endopeptidases positive cells in the different oral sites provided opportunities for sars-cov-2 infection and rapid replication in the oral cavity 15  these results were in accordance with the previous study which indicated the residence of sars-cov-2 in nasal and throats swab samples suggesting the strong spread ability in oral cavity 36  in conclusion our results systematicly investigated the expression profiles of furin enzyme and ace2 receptor in oral cavity at the gene and protein level for the first time furthermore significant expression of furin and ace2 were discovered in oral epithelial cells implying the possible two transmission routes of the coronavirus through oral mucosa which provides a new insight to the future prevention strategy in clinical cares as well as daily life more evidence is still needed to reinforce these findings all human participants signed the informed consent before collecting the samples each of the human studies was reviewed and approved by the institution ethics committee of stomatology hospital of guangzhou medical university not applicable the datasets supporting the results of this article are included within the article and its additional files the authors declare that they have no competing interests immunohistological staining against ace2 was used to determine whether ace2 express in the normal buccal lip palate tongue and gingival tissues from the human the lower panels demonstrate the appearance of basal layer and epithelial cells at higher magnification as indicated by black squares in upper panels ace2 was highly expressed in basal layer epithelial cells of these five oral tissues scale bars  50  m immunohistological staining against furin was used to determine whether furin expressed in the normal buccal mucosa lip palate tongue and gingival tissues from the patients the lower panels demonstrate the appearance of basal layer and epithelial cells at higher magnification as indicated by black squares in upper panels furin was highly expressed in spinous and basal layer cells while furin-positive fibroblasts and endothelial cells in tongue buccal and palate tissues were limited scale bars  50  m table s1  patients and samples included in dataset scatter plot showed the distribution profile of adam10 adam17 tmprss2 sppl2b capn1 and capns1 in the oral mucosal cells   kai shen tong yang enyu li peter sun lin zuo jiayue hu yiwen mo weiwei zhang haonan zhang  timely creditable and fine-granular case information is vital for local communities and individual citizens to make rational and datadriven responses to the covid-19 pandemic this paper presents covidnet a covid-19 tracking project associated with a large scale epidemic dataset which was initiated by 1point3acres to the best of our knowledge the project is the only platform providing real-time global case information of more than 4124 sub-divisions from over 27 countries worldwide with multi-language supports the platform also offers interactive visualization tools to analyze the full historical case curves in each region initially launched as a voluntary project to bridge the data transparency gap in north america in january 2020 this project by far has become one of the major independent sources worldwide and has been consumed by many other tracking platforms 4 9 the accuracy and freshness of the dataset is a result of the painstaking efforts from our voluntary teamwork crowd-sourcing channels and automated data pipelines as of may 18 2020 the project website has been visited more than 200 million times and the covidnet dataset has empowered over 522 institutions and organizations worldwide in policy-making and academic researches all datasets are openly accessible for noncommercial purposes at httpscoronavirus1point3acrescom via a formal request through our apis  starting from december 2019 or earlier the outbreak initially detected and reported in wuhan hubei china due to a novel type of coronavirus the severe acute respiratory syndrome coronavirus 2 sars-cov-2 has been rapidly spreading firstly across regions in china and other east-asian countries and then since late february to nearly all the continents in the world as of may 15 there have been more than 442 million cases confirmed across 225 countries and regions associated with 302 thousands deaths declared as a pandemic by world health organization on march 11 the coronavirus outbreak has brought severe challenges to not only local medical systems especially in underdeveloped areas but our society as a whole governments across the world have been taking various measures at different levels in response to the pandemic to be able to make scientific and data-driven decisions local communities rely on timely and accurate epidemic data to understand the spread and the trend of the covid-19 outbreak case information in cities andor provinces is especially valuable to promptly adjust local policies in response to the rapid change of the pandemic situation at the same time nearly every individuals daily life has been severally affected by the crisis to minimize the ramification of community spread it is of extreme importance to provide the public with transparent and accurate local information to guide their daily life decisions overall there is a huge need for timely creditable and fine-granular data which can be easily accessed via a single platform on january 31 our team initiated a project as a data website in response to the ongoing covid-19 emergency the original purpose of the project was to provide real-time case information in north america to compensate the delayed official reporting at the time in less than three months the platform has grown into a global covid-19 tracker which includes geographic information of covid-19 spread with sub-division level breakdown for more than 27 countries and territories up to now we have become one of the very few completely independent reporting sources of covid-19 case data integration as the united states has become the epidemic center of the world since late march where a realtime data collection across the country was unavailable from most official channels we have been the original data provider to the platforms like 4 9 and the us centers for disease control and prevention cdc since march 2020 to provide a full picture about covid-19 we have also collaborated with another project team 1 to integrate testing and hospitalization statistics in the us and delivered a richer visualization of local pandemic status combined with our real-time case data our covid-19 dataset named as covidnet offers the full historical case trends with a fine-granular regional breakdown the name addresses the hierarchical structure of the geographical network the data is embedded in covidnet is constantly been updated to include the most up-to-date case information in real-time to achieve data accuracy real-time update and worldwide coverage the covidnet features  the data is collected from only reliable sources various quality control assurances have been applied  the data is updated in real-time with the effort of crowdsourcing and automated data collection  the data is collected with fine geographical granularity worldwide we have been providing data both to the public and academic institutes for pure research purposes as of may 18 2020 the project website has been visited more than 200 million times and has empowered over 522 institutions and organizations we have built a convenient application programming interface to access the covid-net dataset and to assist the worldwide battle against covid-19 we would like to encourage more users with both governmental and academic backgrounds to take advantage of our data collection the rest of the paper is organized as follows section 2 provides an overview of the 1point3acres covidnet project on both datasets and visualization we discuss our data collection practice in details in section 3 north america data and section 4 global data we elaborate our quality control mechanism in section 5 and introduce the rich set of interactive visualization tools in section 6 in section 7 we explain briefly about how to access our datasets related projects and platforms are mentioned in section 8 we summarize our work in section 9 and discuss various ways that covidnet could aid the battle against covid-19 a disclaimer is highlighted in the end emphasizing the role of the covidnet project and associated contents since the beginning of the covid-19 outbreak there has been overwhelming related information from numerous resources a major challenge therefore is to integrate the scattered information on a single platform with consistent quality and credibility the 1point3acres project focuses on the following three aspects in collecting and presenting the covidnet data  data accuracy and consistency we extract information from local health authorities and trustful media reports media reports are used when official data is significantly delayed and are cross-checked with official data afterwords no data from other tracking platforms is used in covidnet to eliminate loops of references  timely update since the launch of our project covidnet data has been updated in nearly real-time crowd-sourcing has been implemented to ensure timely updates this sets us apart from official channels like who 7 and us cdc whose updates are delayed by days to our best knowledge most other non-governmental platforms 4 6 have also experienced a 1 -2 days delay by far  worldwide data with regional breakdown covidnet provides case information with finer geographical granularity in over 27 countries and we are still expanding the coverage after the initial launch with county-level case data in the us and canada we have received numerous feedback from local authorities and residents on how it helped local communities in decision making which motivated us to bring the finer data granularity to more countries impacted by covid-19 our covidnet dataset provides real-time epidemic information in three major categories confirmed deceased and recovered cases when they are publicly available 1  the complete history since the outbreak in each local region is available in the covidnet dataset with a flood of available covid-19 data we offer a suite of interactive analysis and visualization tools to provide the general public more insights about the current situation of the covid-19 pandemic we focus on presenting both the temporal trends ie epidemic curves as well as the geographical distribution of the case spread see section 6 for details the interactive tools are available in four different languages  geographical distribution the covid-19 case distribution is presented in various ways including epidemic maps doughnut charts and tabular views figure 3 showcases a few examples of visualization world state and county level epidemic maps are provided for user exploration the tabular view can be customized to rank by different case dimensions such as the infection rate the death rate etc  temporal epidemic curves temporal trends of the outbreak are captured in various epidemic curves including basic time-series lines and burn down charts users can choose and compare curves of different regions with our interactive tools one such example is shown in figure 4  the united states has been the epidemic center since march 2020 and the most challenging area to integrate creditable case information in real-time due to data inconsistency at different levels of the public health system our platform was initially launched to confront this challenge and the severe delay by official authorities this section describes our data collection and validation practice during different stages of the outbreak amid a timely reliable and county-level covid-19 dataset before discussing the data collection practice in detail we highlight the specific features of the covidnet dataset in the us and canada in addition to section 21 the tracking in north america started on january 21 2020 when the first case was officially confirmed in the us the data covers 3169 sub-country-level regions across the north america 3  all data is collected from publicly available sources including both local health official announcements and reliable media reports and is integrated from 1064 distinct websites in addition to case information in the covidnet our project also includes testing locations and statistics thanks to the covid tracking project 1  the epidemic situation underwent several stages in north america since january 2020 each with unique challenges in data collection and validation we now elaborate different stages of the data collection practice along with the evolution of the covid-19 outbreak 311 initial stage january to late february the first north america covid-19 case was officially reported on jan 21 5 and the total cases had remained at a low level until late february 4  during this period both federal and local official health departments had not developed a systematic reporting schedule each individual case however received plenty of media coverage with both geographic and demographic details the most timely data source had been local media reports the covidnet project was initially launched in jan 31 2020 as a platform for real-time case data aggregation of related media report 5  since active crowed-sourcing was the most effective way to track the sparsely emerging cases we organized a volunteer team working 247 for real time updates and focused on data accuracy we recorded each single confirmed case with all accessible data features including geo-location demographic information infection cause and a summary of news report in additional to per-case information the reference links to original news were also attached to each record in our dataset and were visible to all users on the website this helped us account for double counting and to cross validate against later official reports 312 expanding stage late february to end of march the epidemic outbreak in north america started expanding geographically since late february while numbers were still closely tracked by local media the task of integrating all available sources manually by volunteers had become more and more challenging health official departments also started to actively release case information but yet to provide a real-time data with limited capacity in addition to active searching for media reports by volunteers we offer an issue report submission form to all users through which anyone can provide useful information back to us the submission form categorizes information reports into 9 different classes new case recover case death case error report feature request breaking news further details testing location and question whenever a delay of our data or an error was noticed a user could directly report the issue to us with trustful links the reputation we built up during the initial stage has rewarded us with a large number of visits and users which in turn provides us informative feedback frequently as of may 14 2020 we have received and resolved 16240 issue reports from our users which have been a significant part of the data source after an issue gets reported by users a volunteer would be assigned to manually check the provided information via media or official sites and compare the report with all currently recorded cases within the same county 6 313 rapid increasing stage since april the total number in the us surpassed 10000 on march 27 2020 which significantly elevated the difficulty for data collection and validation while local media remained to be a valuable data source in regions with large increasing numbers there appeared to be also a delay even in media reports on the other hand more local health departments started to develop official announcement schedules generally from 1 to 3 times per day therefore while still accumulating local media reported data in real-time we attempted to adapt our collection process to include most up-to-date official health reports which would prevent a possible delay in the data integration procedure we now give a brief introduction about the construction of this pipeline by using us health systems as an example there is a three-level hierarchy structure of the official public health system the overall country-level cdc the state-level health departments and local county-level health departments with a bottom-to-up data flow the cdc database has experienced the most delay which is also the exact reason to launch a project like ours as for state-level and county-level reports the situation has been mixed some state health departments have tracked local cases closely and hence provided a trustful source for update checking while others are in general behind county-level statistics there are in total 3243 counties in the united states to monitor all local reports manually is impossible for the volunteer team we therefore initiated an automated data fetching pipeline which enables a 247 checking for updates from local health department sites any fetched data update would be assigned to the volunteer team for a second-round check as official reports potentially contain several types of noise which we discuss in detail in section 5 only verified updates would be finally recorded in the database we want to address the three important facts below about our automated official-data checking pipeline 1 we collect both official health authority reports and media report to stay close to real-time 2 while most official health authority provide case data to public it remains of significant importance to integrate all local data into a complete dataset 3 we account for potential noise in data from local health officials and implement various quality assurance steps before adding into our database this automation pipeline has been integrated into our workflow since april 2020 and has become one of the dominant components of our data collection process we used the cloud collaboration service provided by airtable our team members can simultaneously work on the same tabular data there are three major tabular formats we have used  expanded tabular et format we document every case sometimes a small cluster of cases into a single row to provide a per-case view to our user we also provide a summary of the original report for each case-cluster in this et format all information including the reference link to the source is presented directly to users for the full transparency  compact tabular ct format given a table content topic eg recovered case table and a fixed geographic granularity we assign each row with a region and each column with a date the number in a cell therefore indicates the accumulative counting in the row-labeled region on the columnlabeled date and each row represents a time series of data in a specific region while ct is easy to consume compared to et the ct format cannot easily summarize the full set of references and losses certain demographics information we keep such dimensions only internally visible for quality control purpose  statistic assistant sa format this type of table is used to present statistics of currently collected data we usually assign each row a region which could be of countryequivalence level state-equivalence level or county-equivalence level different columns are used to represent different information categories associated with each region like region name total confirmed number fatality rate contact of the local health official sa is mostly used for tabulated presentation and other interactive maps on our project web-page during initial stage section 3  most mainstream covid-19 data platforms 3 4 6 7 8 9 10 offer case information at country-level with only exceptions for china by 3 and north america eg by our project and subsequent platforms that aggregate our data 4 9  international organizations like who 7 and ecdc aggregate worldwide data but are often delayed without sub-division regional breakdown creating a platform with consistent freshness credibility and granularity for worldwide data is the key to present the full picture of the covid-19 spread which however poses a great challenge determined to confront this challenge we have exploited a combination of manual data collection crowd-sourcing channels and automated data pipelines to solve the problem as of may 18 2020 the covidnet dataset has covered data in 1055 state-level regions and 3169 county-level regions of more than 27 countries and territories while collecting the most updated data in a timely fashion we also retrieved all the case history since the outbreak within each region we choose to only present daily aggregated numbers in covidnet in each region for consistency we note that some health departments have provided richer information than a daily aggregated number a full list of countries can be found in appendix we prioritize countries with the most confirmed cases and attempt to cover major countries in all continents of the world we highlight that this is an ongoing effort to expand to as many countries as our resource permits subdivision naming convention we adopt the iso-3166 standard 7 to normalize provinces or states names in covidnet official names in english and local languages are both available local health authorities reports are used as data source for consistent credibility 8  while the data reporting systems differ across countries they mostly fall into the following two categories the first class of countries offers open access to the full historical data starting from the outbreak within this paradigm case information is provided in either the per-patient level or per-region daily statistic level examples are the public repository of italy 9 and the open data api of colombia 10  we transform all data into 7 httpswwwisoorgiso-3166-country-codeshtml2012iso3166-2 8 the only exception is north america discussed in section 3 9 presidenza del consiglio dei ministri official github repository httpsgithubcom pcm-dpccovid-19 10 el portal web de datos abiertos del ministerio de tecnologas de la informacin y las comunicaciones httpswwwdatosgovcoensalud-y-protecci-n-socialcasospositivos-de-covid-19-en-colombiagt2j-8ykrdata per-region-day statistics and discard detailed demographics while this simplifies the full history retrieval it requires systematical quality control due to the modification of historical data and changes of reporting criteria for instance the official data repository of spain changed confirmed case definition and data fields in late april which led to mismatches in our time-series within a short time window 11 we apply alarms whenever a suspicious decrease in the curve gets observed which is then assigned to manual check by volunteers the second class of countries reports only the most recent case data through a data-accessing interface in most locales the historical data is collectively archived and stored in various formats pdf csv json or retrievable from official daily reports examples include the official covid report by korean cdc 12 and the south africa covid news portal 13  we collect all archived information and create an integrated dataset as in such data-accessing systems there lacks a consistent way to track edits in historical data we hence only retrieve the historical data once occasionally we failed to trace back all the historical data archive we resort to local non-official data platforms to back fill the earlier case data 14  we check all the official data sources every 2 hours and update the most recent data in our database accordingly for countries that provide open access to the full case history we also implement scheduled daily checking for modifications of official historical data the timestamp associated with each record is in agreement with the local time of the publishing authority 15  given different stages of the outbreak the data accessibility and publishing channels are constantly evolving in each country for instance official data is provided in per-patient level in the initial stage in most countries and slowly evolves into an aggregated format during the outbreak stage we combine our data pipeline along with the volunteers manual effort to ensure the data quality and consistency section 3 highlights this issue in the north america as an example the data quality control qc is always the first priority of the covidnet project as both local communities and researchers might make critical decisions based on the data note that we only have access to the case information released by health officials andor media reports and the quality of the original case data is beyond our capability we control the covidnet dataset to accurately reflect the information provided by local health authorities this section focuses on specific quality control challenges in a rapidly evolving pandemic and the practices we adopted in the voluntary teamwork as discussed in section 41 our data comes from distinct reporting systems in different countries while each paradigm requires a specific quality model the general principle is to start with the data with the finest granularity andor with the full history for instance when per-case data are provided eg open data projects in columbia 16 and philippine 17 health ministries the pipeline would aggregate from such per-case dataset to get daily case statistics and geographical distributions following the same principle when health officials release and update the entire aggregated daily history eg italy and spain we update the entire time series whenever edits are made officially we dedicated our effort mostly in health reporting systems with a federated administration structure the key challenge presented with such a system is the asynchronized natural of data from different levels taking the united state as an example county health departments release case statistics following their own schedules while state officials may update at a lower frequency typically 1-3 times a day at any given time aggregated county-level data may differ from state-level ones in extreme cases state-level reports could be delayed by days this asynchronized nature has brought difficulty for qc the situation gets further complicated when an unassigned category is presented quality control practice in federated systems as discussed in section 3 our database records the finest granularity and has included the full set of references either publicly visible in the early stage or internally traceable in the current stage all volunteers follow the same protocol to determine whether an inconsistency is due to human mistakes or not by investigating data from different levels of health departments and also internal comments on the website a note would be left to users when presented numbers differ from official statistics we also created an inconsistency diary maintained by the volunteer team as an internal reference to track discrepancies and periodically revisit issues that persist we acknowledge that any case data is subject to edit and change in a rapidly growing pandemic situation and have assumed potential noise in the official data collected while most of noise would be corrected eventually to present timely update we have categorized several noise mechanisms and implemented quality assurance accordingly  suspicious jump in case number an issue would be created when a large jump in case numbers appears 18  and updates in the sub-region would be temporarily suspended typically for 2 to 6 hours to allow for potential subsequent corrections in official data  decrease in accumulated numbers most number decreases would be considered as history corrections 19  when this happens if the full official historical data is accessible we update the dataset according to the most recent official time series in systems where only the current statistics is available we adjust the most recent historical case data to maintain the non-decreasing property of each time series 20  in a nutshell continuous update mechanism is instrumental in reducing noise in the covidnet dataset we also implemented dailyscheduled checks and cross validations with official and other independent platforms many of the noises have been identified and corrected thanks to the manual efforts of our volunteer team unlike many other platforms the covidnet project has been conducted by a fully decentralized volunteer team this presents a unique challenge to the data control quality repeated entry of the same data in the decentralized team multiple volunteers might be working on the same update overlapping work or failing to recognize existing duplicated records duplicated records to prevent overlapping work our volunteers will first check potentially related open updates before their own updates duplicated records was prominent in the early stage of covidnet project when we used et format if accumulated numbers were available in the reports the volunteer would directly check the total number in the specific area and prevent duplicated cases if accumulated numbers were unavailable the volunteer would check within all existing records especially case-clusters sharing the similar attributes as described in the report and leave out duplicated reports uncoordinated deployment all volunteers can deploy data updates to the front-end website this was designed earlier to avoid delayed data presentation to the public however when one attempts to deploy a correctly completed number change other members might be in the middle of updating and validating case information in a different region for instance on april 15 2020 when a volunteer attempted to modify the confirmed number in okaloosa texas from 102 to 103 the data entry experienced a transit state as 102103 which was quickly corrected by the volunteer however during the short transit stage there happened to be a website deployment which resulted in an abrupt number jump by more than 100000 and then got corrected within minutes after the subsequent deployment 21 these type of issues are not typical data errors but due to the decentralized deployment setup which is however necessary to prevent delayed website presentations to minimize disadvantages associated with we gradually constructed a list of rules that would be automatically checked before each deployment any deployment would be forbidden unless being manually interfered if one of the following criteria is triggered 1 the number is smaller than the one from the previous day 2 the number of a county-level region increases more than 4000 in a single day 3 the daily increase is more than 300 while the previous days number is larger than 10 4 the daily increase is more than 200 while the previous days number is larger than 50 5 the daily increase is more than 50 while the previous days number is larger than 1000 where all thresholds applied above are deduced empirically the covidnet project offers a rich set of data visualization tools for all users visiting our website as outlined in section 2 these interacted tools are integrated for better presenting both the temporal trends and the geographical distribution about the covid-19 pandemic we discuss the details in this section the geographical distribution of the covid-19 spread is important for communities and individuals to make decisions we visualize the current case statistics via two visualization modules epidemic maps and doughnut charts as showcased in figure 3  for both the whole world and each single country we present a detailed epidemic map which shows the statistics 22 in each state-level region where darker colors indicate a larger number in the area we also implement doughnut chart representation mostly to spot areas associated with largest proportion of the cases the time series of case data often referred to as epidemic curve helps general public to understand the full history and the current stage of the covid-19 spread our project includes both basic trends charts with plain numbers and burn-down charts which provide further insights into the varying condition users can explore these epidemic curves for all sub-divisions in our dataset basic trend charts we provide basic trend charts at daily resolution for the following type of statistics confirmed cases deceased cases positivenegative testing cases positive testing rates and hospitalized cases for both confirmed and deceased curves users can choose to view them in either linear or logarithm scale for example figure 2 shows the cumulative and daily new confirmed cases worldwide analytic trend charts we have implemented the burn down chart to show simultaneously the trends of active cases and recovered deceased cases which provides a clearer picture about the local progress over time see figure 4  we offer several ways for users to compare the covid-19 trends across different countries and sub-divisions over the whole period one such example were shown in figure 5  user can choose the set of countries or sub-divisions they are interested in in addition to 22 statistics includes local populations confirmed numbers and density per million deceased numbers deceased density per million population testing numbers if available and testing density per million population static plots we make available several animations on our project website to show the evolution of the comparative statistics across different regions we would grant open access to covidnet for any non-commercial data usage our dataset is accessible by filling a data request form at httpsairtablecomshrmqs4c6wjpzlcp0 before which the user should have read carefully about terms of data usage at https coronavirus1point3acrescomendata we do prohibit crawling scraping caching or otherwise accessing any content on the platform via any automated means due to the expensive bandwidth consumption which has brought a huge financial burden on the project to continue providing timely information to the general public we welcome only fair access to our dataset and other contents we briefly discuss some other platforms which are also taking efforts in serving covid-19 related epidemic information to the general public dingxiangyuan 3  initiated in early january 2020 the dingxiangyuan dxy project has been among the most popular dashboards for covid-19 information in china they provide daily update by collecting data from all levels of official health departments which has been quite trustful and we have adopted their data as our source in china compared with the case in us the data collection in china has been relatively easy and well-organized as official channels have started a formal reporting schedule since the early stage and no extra media reports were required we have seen the success and the impact brought by dxys integrated data platform which helped local communities in china to response effectively to the outbreak this inspired us to start our own project for north america and eventually as a global tracker we also studied their data presentations which helped us build our own system while their focus has been the condition in china data in the rest part of the world is collected and presented only in country level with a delayed update for data in us due to the reason we explained earlier john hopkins covid-19 dashboard 4  a previous paper 4 described the effort of a team from john hopkins university in a global covid-19 data platform which consumes dataset provided by dxy in china and later ours in us for a global visualization as a completely independent data-collection source our north america dataset has been continuously used by 4 as one of their sources in the county-level breakdown data in the us area unlike our close to real-time update there has been a delay of us data on the jhu dashboard 1 or more days since the end of april 23  last but not least similar as dxy worldwide data is only provided at country level on the jhu dashboard worldmeter covid-19 tracker 10  this is another covid-19 tracker which offers partial regional breakdown in us while providing country-level numbers in the rest of the world for several states in us 24  worldmeter updates information in real-time which has been more up-to-date than the jhu dashboard other related projects the covid tracking project 1 has collected and provided data in testing hospitalization and very recently the demographic distribution in different states in us we have been using their data for both testing and hospitalization visualizations which provide an enriched description about local epidemic conditions especially when the testing does not widely cover the local community the drivethrulocation 1 is another valuable project which collects detailed information on locations providing drive-through covid-19 testing services in us while not closely related to the pure data practice we have collaborated with this project team as testing information is vital for local communities which fits our purpose of serving the public in general this paper delivers a detailed introduction about the 1point3acres covidnet project elaborating both the data collection process and the quality control mechanism we would like to share some lessons we have learnt from the project the real time nature and the exhaustive geographical distribution of covidnet have attracted a large number of users on the platform while we have been trying our best to provide a creditable data integration our practice has also suggested the necessity of constructing an official information integration pipeline to confront potential public health challenges especially in the current era when human interaction has become an essential component of the modern society at the same time as inter-national connections have been much stronger than any previous time global level pandemic information sharing would be of vital importance in both learning from experiences of other countries and assessing a local public health risk level according to interactions among different countries and regions on the other hand when more public challenges have upgraded to the global level we have also seen the strength of data driven approaches in tackling large scale problems with real time update of sub-division level covid-19 information across more than 27 countries the covidnet dataset could benefit everyone in various ways for general public the analytic 23 this had urged us to enhance our own data quality to prevent errors when serving as the most up-to-date independent data platform in us to the public 24 as of may 8 2020 the county-level breakdown is provided for 8 states in us by 10  including nj ca pa fl tx la oh and wa trend charts have provided more intuitive descriptions about the local epidemic situation and would help local communities to make decisions about working and traveling for local governors a comparison with situations and trends in other statescounties would be instructive for evaluating different policy scenarios including restrictive orders and economy reopening for academic community the covidnet dataset could lead to researches in diverse potential directions time series data could be combined with conventional epidemiological models to make prediction about the near future the sub-division breakdown has provided detailed geographical distribution of covid-19 outbreaks and may be used to analyze the impact of different external factors associated with each local region eg weather economy population industries races restrictive-order levels and so on the worldwide dataset together with information on inter-national activities offers the potential to study the global spreading behavior of the disease we welcome and encourage users from all industries utilizing the covidnet to assist the battle against covid-19 we especially look forward to more insights offered from academic researches by investigating the rich dataset provided by covidnet project the covidnet project including all data mapping analysis copyright 2020 1point3acres llc all rights reserved is provided for the public with general information purpose only all information is collected from multiple publicly available sources that do not always agree while we will try our best to keep the information up to date and correct we make no representations or warranties of any kind express or implied about the completeness accuracy reliability with respect to the website or the information we do not bear any legal responsibility for any consequence caused by the use of the information provided we strictly prohibit unauthorized use of the information in commerce or reliance on the information for medical guidance 1point3acres disclaims any and all representations or warranties with respect to the project including accuracy fitness of use and merchantability screenshots of the website are permissible so long as appropriate credit is provided upgraded plan which make the collaboration possible with a large volunteer team we claim no conflict of interests   masked face recognition dataset and application zhongyuan wang guangcheng wang baojin huang zhangyang xiong qi hong hao wu peng yi kui jiang nanxi wang yingjiao pei heling chen yu miao zhibing huang jinbi liang  in order to effectively prevent the spread of covid-19 virus almost everyone wears a mask during coronavirus epidemic this almost makes conventional facial recognition technology ineffective in many cases such as community access control face access control facial attendance facial security checks at train stations etc therefore it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces most current advanced face recognition approaches are designed based on deep learning which depend on a large number of face samples however at present there are no publicly available masked face recognition datasets to this end this work proposes three types of masked face datasets including masked face detection dataset mfdd real-world masked face recognition dataset rmfrd and simulated masked face recognition dataset smfrd among them to the best of our knowledge rmfrd is currently the worlds largest real-world masked face dataset these datasets are freely available to industry and academia based on which various applications on masked faces can be developed the multigranularity masked face recognition model we developed achieves 95 accuracy exceeding the results reported by the industry our datasets are available at httpsgithubcomx-zhangyang real-world-masked-face-dataset  index terms-covid-19 epidemic masked face dataset masked face recognition a lmost everyone wears a mask during the covid-19 coronavirus epidemic face recognition techniques the most important means of identification have nearly failed which has brought huge dilemmas to authentication applications that rely on face recognition such as community entry and exit face access control face attendance face gates at train stations face authentication based mobile payment face recognition based social security investigation etc in particular in the public security check like railway stations the gates based on traditional face recognition systems can not effectively recognize the masked faces but removing masks for passing authentication will increase the risk of virus infection because the covid-19 virus can be spread through contact the unlocking systems based on passwords or fingerprints are unsafe it is much safer through face recognition without touching but the existing face recognition solutions are no longer reliable when wearing a mask to solve above mentioned difficulties it is necessary to improve the existing face recognition approaches that heavily rely on all facial feature points so that identity verification can still be performed reliably in the case of incompletely exposed faces z the state-of-the-art face recognizers are all designed based on deep learning which depend on massive training dataset 1- 5  thus developing face recognition algorithms for masked faces requires a large number of masked face samples at present there is no publicly available masked face dataset and so this work proposes to construct masked face datasets by different means regarding the current popular face masks there are two closely related and different applications namelyfacial mask detection task and masked face recognition task face mask detection task needs to identify whether a person wear a mask as required masked face recognition task needs to identify the specific identity of a person with a mask each task has different requirements for the dataset the former only needs masked face image samples but the latter requires a dataset which contains multiple face images of the same subject with and without a mask relatively datasets used for the face recognition task are more difficult to construct in order to handle masked face recognition task this paper proposes three types of masked face datasets including to the best of our knowledge this is currently the worlds largest real-world masked face dataset fig 1 shows pairs of facial image samples  smfrd in order to expand the volume and diversity of the masked face recognition dataset we meanwhile have taken alternative means which is to put on masks on the existing public large-scale face datasets to improve data manipulation efficiency we have developed a mask wearing software based on dlib library 8 to perform mask wearing automatically this software is then used to wear masks on face images in the popular face recognition datasets presently including lfw 9 and webface 10 datasets this way we additionally constructed a simulated masked face dataset covering 500000 face images of 10000 subjects in practice the simulated masked face datasets can be used along with their original unmasked counterparts fig 2 shows a set of simulated masked face images face-based identification can be roughly divided into two application scenarios uncontrolled and controlled application environments the former mainly refers to public video surveillance situations where face shooting distance view of sight pose occlusion and lighting are all uncertain in these cases the accuracy of face recognition is relatively low moreover the accuracy will be further reduced when wearing a face mask however there are also a large number of controlled application scenarios such as attendance checks in work places security checks at train stations and facial scan payments etc in these situations subjects are usually in a cooperative manner typically approaching and facing up the camera thus high-quality frontal face images are readily acquired so that the masked face recognition task is no longer so difficult even if the mask covers part of the face the features of upper half of the face such as eye and eyebrow can still be used to improve the availability of the face recognition system of course the premise is to exclude mask interference and give higher priority to useful exposed face features our proposed masked face recognition technique has been blessed with two aspects one is the built dataset and the other is the full use of uncovered useful face features we took advantages of the existing public face recognition datasets and combined them with the self-built simulated masked faces as well as the masked faces from actual scenes as the final dataset to train a face-eye-based multi-granularity recognition model in particular we applied different attention weights to the key features in visible parts of the masked face such as face contour ocular and periocular details forehead and so on which effectively addresses the problem of uneven distribution of facial discriminative information as result we promote the recognition accuracy of masked faces from the initial 50 to 95 probably because of the sudden emergence of the covid-19 epidemic at present there are few institutions that apply facial recognition technology to people wearing masks based on our survey sense time technology reported a pass rate of 85 when the person exposes 50 of the nose 11  hanvon technology also reported that the accuracy of masked face recognition is about 85 12  the best result reported so far is from minivision technology with an accuracy of over 90 13  our face-eye-based multi-granularity model achieves 95 recognition accuracy generally masked face recognition technology can be used to identify people wearing masks but it is still not very reliable compared to the regular facial recognition technology which already witnessed an accuracy of over 99 another related task is face mask recognition that is identifying whether a person is wearing a mask as required or not because the task is relatively simple the recognition accuracy is much higher tencent baidu and jingdong all reach a recognition accuracy of more than 99 we built the mfdd rmfrd and smfrd datasets and developed a state-of-the-art algorithm based on these datasets the algorithm will serve the applications of contactless face authentication in community access campus management and enterprise resumption scenarios our research has contributed scientific and technological power to the prevention and control of coronavirus epidemics and the resumption of production in industry furthermore due to the frequent occurrence of haze weather people will often wear masks and the need for face recognition with masks will persist for a long time  one-shot screening of potential peptide ligands on hr1 domain in covid-19 glycosylated spike s protein with deep siamese network nicol savioli  the novel coronavirus 2019-ncov has been declared to be a new international health emergence and no specific drug has been yet identified several methods are currently being evaluated such as protease and glycosylated spike s protein inhibitors that outlines the main fusion site among coronavirus and host cells notwithstanding the heptad repeat 1 hr1 domain on the glycosylated spike s protein is the region with less mutability and then the most encouraging target for new inhibitors drugs the novelty of the proposed approach compared to others lies in a precise training of a deep neural network toward the 2019-ncov virus where a siamese neural network snn has been trained to distingue the whole 2019-ncov protein sequence amongst two different viruses family such as hiv-1 and ebola in this way the present deep learning system has precise knowledge of peptide linkage among 2019-ncov protein structure and differently of other works is not trivially trained on public datasets that have not been provided any ligand-peptide information for 2019-ncov suddenly the snn shows a sensitivity of 83 of peptide affinity classification where 3027 peptides on satpdb bank have been tested towards the specific region hr1 of 2019-ncov exhibiting an affinity of 93 for the peptidylprolyl cis-trans isomerase ppiase peptide this affinity between ppiase and hr1 can open new horizons of research since several scientific papers have already shown that csa immunosuppression drug a main inhibitor of ppiase suppress the reproduction of different cov virus included sars-cov and mers-cov finally to ensure the scientific reproducibility code and data have been made public at the following link httpsgithubcombionick872019-ncov  the 2019-ncov has risen in the city of wuhan in chinas hube as an extraordinary infected human pathogen producing critical respiratory illness and pneumonia on 30 december 2019 three bronchoalveolar lavage samples were obtained from a patient with pneumonia of unfamiliar etiology the complete virus genome sequence of 2019-ncov was acquired and indicated a relationship bat sars-like coronavirus 1  at present numerous coronavirus sequences have been published on gen-bank 4  allowing a direct study of the virus structure that the high mutation and recombination rates of the virus make it difficult to design a wide spectrum inhibitor at a conventional targets 2 3  several approaches are being evaluated the first approach is the virus protease inhibition that prevents the virus from polypeptide filaments to be split and therefore the viral core proteins cannot be built for instance nelfinavir an hiv-1 protease inhibitor to treat hiv was prophesied to be a likely inhibitor of 2019-ncov principal protease by different molecular docking computationalbased 5  the second strategy is targeting the glycosylated spike s protein in the fusion with the entry of the host cell 6  which represents the most promising target for developing new inhibitors for the target site spike s s-hr1 7 8  notwithstanding it is unclear whether 2019-ncov also holds a similar fusion and entry mechanism as that of sars-cov and mers-cov and if true consequently the s-hr1 site can also serve as an important target for the development of 2019-ncov fusionentry inhibitors though it is also unexplored if 2019-ncov also exists other comparable fusion and entry mechanisms with sars-cov and mers-cov 9  despite this all of these deep learning methods are sharpening on the engendering of new molecules that have not yet been clinically tested conversely computational drug repurposing gives an efficient and fast approach to test drugs already available 17  among this repurposing computational approach four molecules have previously been selected to be the main candidates prulifloxacin bictegravir nelfinavir and tegobuvi 18  where prulifloxacin is a synthetic antibiotic of the fluoroquinolone class 19  bictegravir is and an antiretroviral to block the enzyme integrase used in hiv-1 capable of inserting a viral genome into a host one 20  nelfinavir is protease inhibitors well used in the treatment of hiv-1 21  tegobuvi is a non-nucleoside reverse transcriptase inhibitor nnrti with exhibited antiviral activity in patients with genotype 1 chronic hcv infection 22  however none of these molecules has any specific inhibition targeting for the glycosylated spike s proteins specifically protein spike s is made up of two subunits s1 and s2 the s1 subunit binds the cell receptor with its receptor-binding domain rbd followed by a set of conformation changes in the s2 subunit allowing the fusion peptide to enter the cell membrane of the host cell in the s2 region we find another region called heptad repeat 1 hr1 with three hydrophobic grooves that bind to a high region called heptad repeat 2 hr2 forming a structure with six helices 6-hb which helps to bring the two membranes closer together for the final fusion and hence the entrance the rbd of any cov family is a highly mutable region and not suitable for inhibition while the hr region in the s2 subunit is conserved among various hcovs 8  recently several deep learning algorithms have been employed for the research of coronavirus target drugs for example three-dimensional modeling of the virus where 2019-ncov sequences are translated into protein and then within a classification network among ligand and protein as input their inter-action is screened 10 11 12  while generative adversarial approaches were also used for producing novel target molecular structures 14 13 15 16  however the two main limitations of these approaches are i they are mostly with supervised training approach on public datasets in which there is not any specific ground truth ligand for 2019-ncov ii they are based on shallow and non-convolutional networks currently the state of the art in many datasets iii they generate new molecules that have not yet been tested clinically for all these reasons the purposes of this paper are i introduce a new artificial intelligence model that speeds up the research for the promising target ligand and is not based on complex and computationally expensive molecular docking operations ii fast screening of the main antibacterial anticancer and antimicrobial peptides present in satpdb 23 toward the hr1 domain which that as already mentioned describes the more limited mutation spike protein site of 2019-ncov ii moreover introduce a new deep learning method that is not based on a trivial supervised classification of ligand and peptide on public datasets but on one-shot learning approach for specifically studying the glycosylated spike s 2019-ncov protein the work-flow is subdivided into three stages virus genome conversion into protein and subsequent splitting of the protein sequence within peptide filaments text filaments peptide to image conversion and finally the peptide comparisons with a siamese neural network snn the available genome genbank 2019-ncov sequence is used the sequence was obtained from a 41-year-old man hospitalized in the central hospital of wuhan on 26 december 2019 each viral genome structure was determined by the alignment sequence of two characteristic portions of the betacoronavirus family a coronavirus linked with humans sars-cov tor2 genbank accession number ay274119 and a coronavirus correlated with bats bat sl-covzc45 genbank accession number mg772933 1  the genomic sequence was then converted toward the corresponding protein where a sequence of ten protein was extracted in series for forming a protein peptide fig 1   subsequently the protein-peptide is converted into an image of 256  256 pixels fig 2  this method is similar to deepvariant work where the genomics sequence is transformed into an rgb pixel image and then directly processed by a state-of-art convolutions neural network cnn for genotype prediction 29  this enables handle single text strings as images and to use all the advantages of cnn that currently represent the state of the art in multiple tasks on several datasets 30  in particular the smallest variations in the protein sequence can be efficiently recognized by the multiple nonlinear layers of the neural network  a snn 31 is then trained to identify the whole 2019-ncov protein structure versus two distinct family viruses such as ebola 25 and hiv-1 26  where the equivalent genome-to-protein translation has been done though this one-shot learning system reduces the use of specific datasets enabling directly work on the available biological data in other words the network learns to discriminate target examples on its protein domain from other domains with completely different biological characteristics without the explicit use of a specific dataset further is then applied deep snn which i lean generic image functionality to make inference on unknown distributions ii provide a valid approach that explores solutions on the unknown domain without relying on a trivial supervised training dataset ie peptide ligand correspondence iii use several states-of-the-art cnn previously pre-trained on huge datasets the model is then designed with two cnn that take distinct image inputs fig 3  the input of the first network is always a sequence of 2019-ncov peptide while the second can accept either a peptide sequence randomly choose from different virus families such as ebola and hiv-1 or the same 2019-ncov sequence ie as the first network therefore if the two inputs are equivalent ie equal 2019-ncov protein sequence the ground truth target utilized for training the model is one otherwise is zero even though two distinct types of cnn have used a shallow and a deep version notably for the shallow variant alexnet 32 is employed especially a pre-trained version of alexnet on more than a million images from the imagenet 33 database is also applied while for the deep version is used the resnext 34 that is fifty layers deep notably the final convolutional layer of both cnn is flattened into a vector and pair passed to a final layer that calculates their l1 metric within sigmoid output function the model is trained with stochastic gradient descent sdg with nesterov accelerated gradient nag and learning rate 1e  4 in 1000 epochs where the loss function is mean squared error mse while random resized crop random horizontal flip and random vertical flip are made to prevent overfitting during the training phase to train the model a dataset of 1258 images is generated with the corresponding positive proteic example of 2019-ncov 1 and those negative ones of ebola 25 and hiv-1 26  the dataset was subsequently divided into 60 train 20 valid and 20 test respectively the model capacity to distinguish 2019-ncov protein sequences from those of other viral families is estimated within a sensitivity analysis eq 1 where t p are true positive ie 2019-ncov sequence correctly identified as 2019-ncov sequence while f n are the false negative ie 2019-ncov sequence incorrectly identified as ebola or hiv-1 fig 4  validation plot during the training phase as we can see alexnet pre-trained on imagenet 33 shows a better ability to better learn from the earliest epochs compared to the not pre-perinated version instead the deep network resnext shows a more limited ability to adapt and learn probably due to the small dataset available as previously mentioned two different cnns were applied ie alexnet and resnext for getting the best performance on the validation and test data set importantly the small eight layers alexnet has been shown to perform in several tiny clinical datasets 35 and confirmed here to be the best network compared to a deeper version resnext also pre-trained on imagenet 33  made up by fifty layers indeed as pointed in the validation plot fig 4  the pre-trained alexnet shows a more reliable convergence correlated to the not pre-trained one and towards resnext this is probably due to the presence of small dataset to perform with large deep networks that instead show slow convergence ie blu line fig 4  the sensitivity analysis on the test set confirms what was observed during the validation phase whereas the pre-trained alexnet version has more prominent sensitivity compared with its not pre-trained version and resnext after this deep network selection phase the alexnet pre-trained model is chosen to make an inference on 3027 peptides from satpdb 23 to find the closest peptide to targeting hr1 2019-ncov domain site the inference process is performed as follows i the hr1 sequence is cutting in eight sub-peptide of ten protein fig 1 and then each converted into an image fig 2 of 256  256 pixels ii separate satpdb peptide is further converted from text to an image of 256256 pixels fig 2  iii every single peptide is given in input to the first siamese cnn network while any of the eight hr1 sub-peptides is paid to the second siamese cnn sequentially iv finally the final siamese sigmoid outputs between the target satpdb peptide with each of the eight sub-peptides is averaged this inference process shows 93 affinity between ikkt y eeikkt y eeikkt y eeikkt y eeierdw em v peptidylprolyl cis-trans isomerase peptide 24 and the hr1 domain in this article a novel drug repurposing approach has been explored where an snn has been designed to find the best matching peptide inside the hr1 region that represents the less mutability domain of any coronavirus 7 8  the proposed model achieves reasonable performance with an overall sensitivity on a test set of 8329 within two comparison models particularly a shallow pretrained version as alexnet has been applied showed better performance than a deep version one as resnext tab 1 an protein sequence dataset of 1258 images was extracted from 2019-ncov 1  ebola 25 and hiv-1 26 training the snn to classify any protein sequences similar to 2019-ncov compared to other belonging to different viruses family ie hiv-1 ebola whereas the lack of the training set size justifies the greater ability of shallow networks to perform better compared to the more deep one ie resnet however the oneshot learning approach applied here is more suitable than a supervised ligandprotein classification 10 11 12 because i we do not need large train datasets to learn the correct ligands for 2019-ncov ii the neural network focuses the learning only in the interest 2019-ncov protein sequence iii public datasets have no training examples of ligand binding for the specific carnivorous 2019-ncov ie which can create an incorrect classification the inference on 3027 peptides on satpdb 23 had been taken showing the confidence of 93 for peptidyl-prolyl cis-trans isomerase affinity with the hr1 2019-ncov domain the peptidyl-prolyl cis-trans isomerase further known as peptidylprolyl isomerase or ppiase is an enzyme fig 5 that transforms the cis and trans isomers with the amino acid proline 36  the two major families of ppiase are cyps and the fk506-binding proteins fkbps where the cyps are implicated in a broad spectrum of cellular processes including cell signaling protein folding and protein trafficking 37  the affinity between hr1 and ppiase may play an essential role in the identification of ppiase inhibitors as several therapies with csa immunosuppression drugs 38 39  several scientific papers have noted that in cellular culture the reproduction of different cov included sars-cov and mers-cov can be inhibited by csa therapy 40 41 42  a well recognized csa immunosuppression drug is the sirolimus a ppiase inhibitor that has been shown to enhance results of patients with intractable h1n1 pneumonia 43  where recent computational studies show how sirolimus can be also selected as a potential drug for 2019-ncov 44  confirming what has been observed in this work on hr1 and ppiase affinity in order to provide the reproducibility of this work the code and data has been made available to the scientific community at httpsgithubcombionick872019-ncov future work is thus still needed to confirm the association mechanism between hr1 and ppiase through molecular docking studies therefore important for understanding biological mechanisms of ppiase on 2019-ncov   mining twitter data on covid-19 for sentiment analysis and frequent patterns discovery habiba drias yassine drias  a study with a societal objective was carried out on people exchanging on social networks and more particularly on twitter to observe their feelings on the covid-19 a dataset of more than 600000 tweets with hashtags like covid and coronavirus posted between february 27 2020 and march 25 2020 was built an exploratory treatment of the number of tweets posted by country by language and other parameters revealed an overview of the apprehension of the pandemic around the world a sentiment analysis was elaborated on the basis of the tweets posted in english because these constitute the great majority usa gb india on the other hand the fp-growth algorithm was adapted to the tweets in order to discover the most frequent patterns and its derived association rules in order to highlight the tweeters insights relatively to covid-19  coronavirus disease 2019 covid-19 was first detected in wuhan china in december 2019 and has spread worldwide in more than 198 countries within a couple of months this outbreak has upset the world and uncertainties about the future were for the first time evoked several epidemic periods have been observed in the world in the recent years this epidemics phenomenon has grown because of the contagion favored by the globalization nowadays epidemics provoke extreme economic crisis at the scale of countries as well at the individual level people can reach psychosis symptoms because of the contagion and countries may suffer from economic crisis due to people traveling restriction and social isolation in 8  the authors give several battle hints to fight the virus as it travels the world the metaphor of war is used doctors and people the most affected psychologically by the epidemics are the most likely to talk about it on social networks like twitter which have become essential in our daily lives the analysis of internet users behavior has shown that they are mostly either seekers of information from the web or social media facebook twitter microblogs 3  4  15  understanding better this data flow can be of great benefit to people and the industry to make informed and focused decisions on how to meet goals many companies have monitoring activities on social media sites to collect information for example talking on social sites about a city or an airport in a positive way improves its image one of the most popular social networks on the web is twitter tweets are based on the format of micro-blogs and constitute an important source of archive data allowing its export for academic research purposes as part of this work we performed a datamining study on covid-19 by analyzing twitter publications in relation to this disease posted between 23 february 2020 and 03 march 2020 the outcomes could help dressing an inventory for the history of the outbreak and especially how it was apprehended by the world population technically we first collected tweets in relation to the coronavirus using nodexl and stored the recovered tweets in a dataset in a second step the data underwent a preprocessing with a statistical analysis in order to extract knowledge helping their understanding thereafter we conducted a sentiment analysis on the dataset to come up with the feelings of the twitters during the period of study in addition to this effort the most frequent patterns were extracted with the aim to grasp social features about the twitters among the datamining algorithms that can handle such issue fp-growth was selected because of its effectiveness and efficiency the paper is organized as follows in the next section we present related and recent works on tweets analyses and the coronavirus section 3 details the approach of building the tweets dataset section 4 describes the the sentiment analysis algorithm we designed as well as its empirical results section 5 presents the adaptation of fp-growth algorithm to tweets and the experimental results prior to conclude the main contributions covid-19 appeared recently in december 2019 in wuhan china and yet hundreds of publications on this topic updated daily can be found in 16  studies from around the world are performed within a very short period of time as the covid-19 took its toll in almost all countries 14  this fact translates the importance of studying this virus in order to speed up the mastery of the disease and the discovery of a remedy with an impressive number popular articles on covid-19 are widely published on daily classic media relating the disease spread the worrying issues and also helping to cope with the cultural customs and to urge for people isolation to limit the disease propagation this invisible virus is considered as the most dangerous enemy for the humanity and the metaphor of war against it was adopted by certain scientists who advice some military strategies to combat this disease 8  tweets analysis has known an increasing number of efforts these last years most of them are interested in determining features of social interactions of tweeters 2 4 and tweeters behaviors 3 15  also tweets analytics tools have been developed to be applied on numerous subjects 17 there is a recent and rich literature on sentiment analysis sa and its applications to various fields sa has been also investigated for social networking data and is generally used by companies for analyzing the opinion and feelings of the customers about products services and company strategies 5 9 10 11 12  sophisticated datamining tools such as classification clustering and association rules mining were developed 7 13 and used in a very large spectrum of domains association rules mining that are investigated on our tweets data have also known impressive development for use on huge volume and complex data 1 6  for the present work as covid-19 is the subject of the hour for the whole planet we conducted a tweets analytics study on this phenomenon focusing on generating social features of tweeters hoping to achieve important insights as a starting step to handle the study on covid-19 tweets mining we carried out the construction of a dataset we provide metadata for the dataset which merely describes its characteristics such as its title and description keywords or tags publisher date of publication or terms of use in this section we present the different steps performed to achieve a dataset of tweets on covid-19 of a high quality nodexl was exploited to extract tweets related to the coronavirus topic and highlighted by the following hashtags covid-19 covid19 covid coronavirus and corona the crawling data period started on the 27 th of february and ended on the 25 th of march which is spread over four weeks during this phase the world has known tremendous changes especially in its whole organization the huge number of infected and dead people provoked public panic and fear which raised supply shortages not only in pharmaceuticals and ppe but also in food several countries have known a bad public health management and a rapid scalability in world economic crisis has been observed during a very short lapse of time rushes on public market have all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint been seen before certain governments decreed isolation and quarantine for inhabitants an especially for returning travelers all these upheavals impact on the humanity behaviors the aim of this study is to shed light on all these aspects through mining tasks on the extracted collection the latter contains more than a half million tweets written in several languages and sent from many countries programming codes in java language were developed for basic cleaning steps on the collected tweets textual data analysis was first held to eliminate hyperlinks mentions and punctuations stop words such as the articles the prepositions of conjunction time and place and the superfluous and unnecessary words were removed as they do not have impact on the text meaning the keywords are then determined by the stemming algorithm which consists in associating to each remaining word its root let remark that some words were misspelled an example is the word carona which was used by president trump and repeated by others users 8013 times this kind of typos was simply eliminated the created dataset includes at the row level the different tweets and at the column level the different attributes which are described as follows the attributes are all of string type except for the date which follows the monthdayyear format the dataset contains 653 996 tweets with no missing values table 1 gives an overview of a portion of the dataset with 9 attributes the contents of the tweets are truncated for lack of space to insert the figure note that the location is not always provided all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint as a second contribution we performed an analytical study on the constructed dataset in order to mine knowledge allowing an understanding and a mastery of the tweeters insights before performing datamining tasks which help to provide a more comprehensive perception this analysis provides data trends and distributions descriptive statistics data groupings and helps formulating hypotheses this important and costly step is common to several data mining tasks among the latter we focus on association rule generation based on frequent patterns mining using an adapted fp-growth algorithm 7  the experiments of the developed program in java for analytics was executed on the 653 996 tweets and the different outcomes are exhibited in the following subsections the hashtags covid-19 covid19 covid coronavirus and corona that served for the construction of the dataset are not considered since each tweet contains at least one of them figure 1 shows the top 20 hashtags which report some events and situations such as pandemic update outbreak stayathome curfew confinementtotal french words and quarantine people are dealing with on the other hand countries and regions such as china iran wuhan india and italy that were the most affected by the virus are cited note that although india is knowing the phenomenon at the end of the studied period we assume that it is evoked among these countries because of its large population  the sources from which the extracted tweets were published are shown in figure 2  the most used platforms are android iphone and web app with approximatively the same ratio all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity  the 25 top words are shown in figure 3  these terms are eloquent as they are similar to the words expressed by the majority of people in real life we observe four sets of words with approximatively the same respective frequency they are cited in a decreasing order of their occurrence -virus people -flu cases trump test -spread health -work outbreak death update home china hands report world country call confirmed infected week risk panic pandemic we can remark that these words describe all the situation people are overcoming in real life during this hard period and that we can summarize in a few paragraphs which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity  the number of tweets by date experienced the results shown in figure 4  regarding the number of published tweets we find it significant during the period of this study and is changing from day to day we remark that before the advent of this virus in december 2020 in china very few tweets on coronavirus were published these tweets consist in old tweets from the previous coronavirus versions that struck asia in 2013 that were retweeted again by some users figure 5 depicts the boxplot of the data collected during the study period where we see a non-asymmetric distribution of the tweets number and six outliers corresponding the following days 03012020 03122020 03152020 for the lowest numbers of tweets and 03062020 03142020 and 03212020 for the highest numbers of tweets  the first ascertainment about the origin of the tweets is that a significant number of tweeters do not indicate their country therefore the results hereafter are valid only for those tweets with a provenance information figure 6 shows the number of tweets per country where we observe that the majority of the published tweets emanate from usa the second remark is that tweeters belong to several countries from all the continents note that the number of tweets is for some countries as important as their respective demography for instance all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint india is ranked 3th for the number of tweets while it has the second largest population in the world the same observation can be done for nigeria and south africa that count respectively the largest population in africa also the countries the most affected by the virus such as spain france and italy are present in this graph  the language of tweets has also been explored figure 7 gives an insight about this feature we note that english prevails over the other languages this can be explained by the fact that according to the previous statistics the three highest amount of tweets are originated from the usa united kingdom and india where the communication language is english also note that english is scientifically and technologically universal and is written by a large population of other countries spanish is ranked 2 in this graph because it is spoken in several countries like colombia and mexico that appear in the previous figure also the languages of the affected countries with a high degree during this period except china exist all in this graph all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity  we can think about several possible correlations between the parameters identified previously in this subsection we investigate the correlation between the number of real infected cases and the number of tweets published each day the intuition is that the evolution of the number of infected cases urges people to communicate via the social media figure 10 depicts the case of the uk where in the x axis we have the number of tweets per day and in the y axis the number of the new cases tested positive to the coronavirus per day we observe a positive trend for the correlation confirmed by a pearson coefficient equal to 055  in addition to the previous study a sentiment analysis was investigated aiming at capturing the tone of the tweets we know that during the covid-19 period people changed their behaviors and experienced different feelings and emotions than those they have known before their lifestyle has shifted to something new the algorithm described in the next subsection was adopted and implemented to shed insight on tweeters sentiments the sentiment analysis algorithm we propose is based on the lexicon-based approach as input it takes the set of the preprocessed words of the dataset and a sentiment lexicon there are several known sentiment lexicons that can be used for sentiment analysis as examples we cite afinn bing and nrc afinn computes a score from the range -55 to each word and deducts the positive sentiment if the score is positive and the negative sentiment otherwise bing assigns straightly a positive or a negative sentiment to each word nrc considers ten categories of sentiments which are positive negative anger anticipation disgust fear joy sadness surprise and trust and assigns to each word at least one of these categories we chose the nrc lexicon as it provides specific sentiments other than positive and negative this lexicon contains 6468 words distributed over the ten categories the number of words in each category is shown in table 2 which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint the framework of the algorithm is shown in figure 8  after the preprocessing step the list of the words of the whole dataset is determined meanwhile the inverted file of the nrc is built and converted to a hash table initially each category is affected a count set to 0 then the process considers the words one by one and calculates its hash key to access to the index that contains the address of the word in the lexicon once identifying the entry of the lexicon inverted file containing the word it captures the categories and increments the count of each of them once the process is completed the score for each category is computed as the ratio of its corresponding count over the total number of words figure 9 shows the results achieved by the execution of the algorithm that illustrate the tone of the tweets captured by the sentiment analysis we observe that the negative sentiment has a score slightly greater than that of the positive fear has an important score whereas joy and disgust have the lowest although this sad situation the tweeters seem to have trust in gaining the battle against the virus which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity figure 10 details the evolution of the sentiments during the four weeks we notice that the negative sentiment was high in the beginning of the period then it knew a little increase in the second week and since then it is decreasing in an important degree on the contrary the positive sentiment follows the opposite behavior fear has almost the same evolution as the negative sentiment but with less magnitude whereas trust behaves as the positive sentiment with less importance joy is constant and is at the lowest score which translates what people are feeling during these days which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint as tweets can be distributed by country as shown in figure 6  the sentiments analysis can be carried out for each country and each region by merging the tweets of the countries of the region in one dataset examples for such task were performed for the usa and canada separately and the region of north america india has also undergone such treatment note that the sentiments evolution can also be drawn for countries and regions data mining technology proposes a large spectrum of tools to extract interesting and potentially useful patterns from huge and complex volumes of data association rules mining arm is one of those techniques that can be adapted to various domains such as health commerce and industry the generation of arm is achieved within two steps  discovering frequent patterns or itemsets relative to a minimum support  filtering arm from the extracted frequent patterns with respect to a minimum confidence the patterns for the tweets correspond to the most frequent words used in the tweets and hashtags which represent all insights on this social media for the covid-19 since the algorithms for frequent pattern mining fpm are computationally expensive a lot of research has been carried on further improving their effectiveness and efficiency and especially in order to cope with the main drawbacks of the apriori and eclat algorithms the fp-growth algorithm was developed 6  the latter was then selected to be adapted to our tweets dataset the fp-growth algorithm draws its strength from the fact it uses a sophisticated and optimal data structure called fp-tree that condenses only relevant data in a vertical format 6  by scanning the tree the frequent words respecting the minimum support are determined the algorithm for mining frequent tweets keywords is outlined in figure 11  algorithm fp-growth-tweets input d the tweets dataset minsup the minimum support count threshold output subsets of frequent keywords of length 1 2  1 scan the dataset d once and find frequent words single word pattern 2 sort frequent words in frequency descending order to constitute the f-list 3 construct fp-tree by scanning once more d 4 for each path of the tree do a repeat i l  2  ii output all sub-paths non-necessary consecutive of length equal to l with minsup iii increment l 5 end the advantage of the f-list is to eliminate at the beginning those words that do not respect the minimum support then the construction of the fp-tree considers only the f-list words and is consequently a data all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint structure reduced to only relevant information it starts with an empty node then it creates nodes taken sequentially from the f-list and linking them if they belong to the same tweet while scanning the collection of tweets and incrementing their frequency association rules are derived from the frequent patterns calculated by the adapted fp-growth algorithm suppose w1 w2 w3 are words appearing frequently in the tweets dataset with a minimum support equal to minsup then from this subset of words called itemset in the traditional frequent patterns mining algorithms we can generate 4 association rules that are the first rule means that in all tweets the three words exist in the second one the rule is interpreted as whenever w1 exists in a tweet w2 and w3 appears in the same tweet the other rules share the same meaning note that we need to specify another measure for the rule besides the support which is the confidence the confidence of a rule is defined to be the probability whenever an antecedent of the rule is in a tweet the consequent is also in the same tweet it is calculated as the adapted fp-growth algorithm was implemented in java programming language under the following environment windows 10 intel core i5-7300u cpu at 260 ghz 8gb of ram the most frequent words in the tweets published in english and calculated by the algorithm are reported in table 3 in decreasing order of their support we can confirm that these words constitute an important part of the daily vocabulary used by people nowadays in real life which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity similarly the pair of words the most frequent appear in table 4  which shows at the same time examples of association rules ordered with respect to the minimum support on the total number of patterns equal to 19560 these rules are impressively similar to those of the daily life during the pandemic for instance the rule wash hand has the highest confidence which corresponds exactly to what we hear many times a day table 4  examples of generated association rules with 2 frequent patterns table 5 and table 6 show the association rules identified by the algorithm for respectively the 3 and 4 most frequent patterns with a low support but a significant confidence these rules are also eloquent relatively to what people are living these days which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint table 6  examples of generated association rules with 4 frequent patterns within the framework of this work we were interested in investigating the people communication via social media during the terrible starting period of the rapid spread of covid-19 all along the world twitter was selected to undertake the study because of the credibility of its users and also the ease of extracting the tweets as a first contribution a dataset of 653 996 tweets was extracted using nodexl and preprocessed in order to highlight useful insights as a second contribution an exploratory study on the collection of tweets was carried out to yield descriptive statistics a sentiment analysis study followed and interesting results were discussed data mining technologies were afterwards exploited and the fp-growth algorithm was especially adapted to the tweets dataset in order to discover the most frequent patterns in an effective and efficient way the derived association rules highlight our understanding on the tweeters communicating on the covid-19 the global results were impressive and can enrich the information forecasted by the traditional media as a future work other measures such as the lift and the interestingness will be considered for the experiments of the frequent patterns mining with the purpose of improving the outcomes another possible research direction is to explore the way and the acceleration the virus has been spread around the word density-based clustering can be developed for this purpose which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020050820090464 doi medrxiv preprint  title a large-scale covid-19 twitter chatter dataset for open scientific research -an international collaboration juan banda m ramya tekumalla guanyu wang jingyuan yu tuo liu yuning ding gerardo chowell  affiliations juan banda  as the covid-19 pandemic continues its march around the world an unprecedented amount of open data is being generated for genetics and epidemiological research the unparalleled rate at which many research groups around the world are releasing data and publications on the ongoing pandemic is allowing other scientists to learn from local experiences and data generated in the front lines of the covid-19 pandemic however there is a need to integrate additional data sources that map and measure the role of social dynamics of such a unique world-wide event into biomedical biological and epidemiological analyses for this purpose we present a large-scale curated dataset of over 152 million tweets growing daily related to covid-19 chatter generated from january 1st to april 4th at the time of writing this open dataset will allow researchers to conduct a number of research projects relating to the emotional and mental responses to social distancing measures the identification of sources of misinformation and the stratified measurement of sentiment towards the pandemic in near real time  the ongoing covid-19 pandemic began in the form of a cluster of viral pneumonia patients of unknown etiology in the city of wuhan china in december 2019 unfortunately the interventions to contain its spread were not implemented soon enough to limit the spread of the virus within chinas borders while transmission has been dramatically reduced in china through strict social distancing interventions the virus was exported to multiple countries and is now generating sustained transmission in multiple areas of the world including areas with active hotspots of the disease including the united states italy spain and france 1  as of april 4th 1201453 cases have been recorded including 64688 deaths according to the worldometer coronavirus pandemic tracker 2  while the ongoing covid-19 presents with unprecedented challenges to humanity the wider scientific community can only advance science when they have access to openly available data social media platforms like twitter and facebook contain an abundance of text data that can be utilized for research purposes over the last decade twitter has proven to be a valuable resource during disasters for many-to-many crisis communication 3  45  with twitter data it is possible to analyze symptom configurations risk factors origin virus genetics and spread patterns can be studied and monitored 6 7 8  recent studies 910 prove that data sharing improves quality and strengthens research with collaborative efforts providing an opportunity for researchers to continually enhance research ideas and avoid redundant efforts 11 12  we opted to release our data to the public for the greater good when the dataset accumulated 40 million tweets on march 23rd 13  we have been providing updates every two days 14  with a cumulative update every week most recently on march 29th 15  this previous update had over 100 million tweets available for researchers the community response by word of mouth has led to over 2401 views and over 1177 downloads of the resource moreover several international researchers have reached out to contribute data and provide analysis expertise in this release we have incorporated additional data provided by our co-authors expanding the size of the dataset to over 152920832 tweets and added vital data on the early days of the pandemic which was unavailable during the initial data collection effort this shows the value of this kind of data and the engagement of scientists that come together to create extensive resources for the benefit of society aside from providing the full dataset with retweets included we provide a clean version with no retweets for researchers with limited resources to access a lighter version of the dataset furthermore to assist researchers for nlp tasks we provide the top 1000 frequent terms 1000 bigrams and 1000 trigrams the released dataset adheres with fair principles 16  due to twitters terms of service tweet text cannot be shared therefore tweet ids are publicly made available using zenodo 13  the tweet ids can be hydrated using tools like social media mining toolkit or twarc 17 18  the deliverables 13 14 include tweet ids and code to process the tweets please note that the code to process the tweets would work only after the tweets are hydrated we provided the date and time meta-data elements on our dataset for groups wanting to target their research questions to certain days to avoid having to hydrate the whole resource at once we have automated pipelines to continue collecting tweets as the pandemic runs its course and to provide updates every two days on our github repository we are also welcoming any additional data that provides new tweets to our resource the initial versions of this dataset 13 15 only included data collected from the publicly available twitter stream api with a collection process that gathered any available tweets within the daily restrictions from twitter from january to march 11th filtering them on the following 3 keywords coronavirus 2019ncov corona virus we shifted our focus to collect exclusively covid-19 tweets on march 12th 2020 with the following keywords covd19 coronaviruspandemic covid-19 2019ncov coronaoutbreak coronavirus wuhanvirus thus the number of tweets gathered dramatically expanded the dataset please note that the stream api only allows free access to a one percent sample of the daily stream of twitter our methodology relies on python and the tweepy package 19  as in our previous work 20  we recently received another set of 30 million tweets collected from january 27th 2020 to march 27th 2020 from our co-author jingyuan yu and his collaborators with the following keywords coronavirus wuhan pneumonia pneumonie neumonia lungenentzndung covid19 these tweets were collected in the following languages english french spanish and german while our original collection is done for any language available we have fully integrated and deduplicated our collaborators tweet collection with ours thus the numbers and tweets presented in this dataset are of unique tweet identifiers from january 1st to april 4th at the time of writing table 1 represents the monthly number of tweets included in this dataset as previously mentioned the number of collected tweets increased tremendously since starting dedicated collection all our preprocessing scripts utilize components of the social media mining toolkit smmt 17  we make a distinction between our full and clean versions of the dataset the full dataset consists of both tweets and retweets there are several practical reasons to leave the retweets tracing important tweets and their dissemination is one of them a clean version with no retweets is also released intended for nlp researchers we also release extracted frequent terms bigrams and trigrams for this community figure  1 outlines the steps taken to build our dataset as shown in figure 1  we used smmt to listen to the twitter stream api for tweets with the described keywords we then gather all the tweets that have the desired keywords before aggregating them locally our contributors used a similar procedure to gather their tweets and provided us with tab delimited files with their data we processed them to fit our own local format to be able to include them in our dataset after deduplication removal of tweets we have in common and only keep unique tweet identifiers between the datasets we then preprocess the large set of tweets to extract the shareable meta-data of the full dataset tweetid collected date collected time preparing the fulldatasettsvgz file at the same time we also remove tweets that are re-tweeted this is existing tweets that are re-shared by others to create the fulldataset-cleantsvgz file our preprocessing involves cleaning up special characters such as carriage returns removing urls and large blank spaces our preprocessing is rather relaxed as we are leaving all available languages intact to generate the frequent terms and ngrams sets of n-terms that appear constantly together we remove all stop words in english and spanish using the spacy 21  these lists are originally quite large so we only share the top 1000 terms bigrams and trigrams we continue to update our original dataset every two days 14 with major releases every week 13 15 and plan to continue doing this for at least the next 12 months a period that will likely cover the main pandemic period the dataset is made available through zenodo 13  there are 7 files in this repository table 2 details the files formats and their utility the example column consists of a sample line from the files the tweet ids in the dataset can be hydrated using smmt the hydrated tweets would produce a json object for each tweet id it is important to note that when users remove their accounts or individual tweets these get removed and are no longer available for download in such cases we can share the data on request while adhering to the twitter data sharing policy the frequent terms bigrams and trigrams are retrieved from the cleaned version of the dataset the fulldatasettsv consists of all the procured tweet ids the fulldataset-cleantsv contains only original tweets with no retweets while some applications and questions are better served with the full dataset nlp researchers might prefer a clean dataset to have less inflated counts of the n-grams identified  in order to verify the presence of covid-19 chatter in the dataset we performed a text match using spacy matcher to identify the matched keyword in a randomly sampled set of retrieved tweets for all the tweets we performed the following checks 1 a tweet is checked for its originality all the original tweets not retweets are then separated as a clean dataset 2 all the tweet texts are preprocessed preprocessing includes removal of urls and stopwords the words of the text are added to a bag of words and then the top 1000 frequent words are identified 3 in order to validate the total number of tweets we calculated the unique number of tweets generated each day all the details are placed in statisticsfulldatasettsv and statisticsfulldatasetcleantsv in order to use our resource we have provided all the software tools we utilized to preprocess clean and parse the twitter data on our github repository 14  under the processing code directory note that the tweets need to be hydrated first using tools like social media mining toolkit or twarc 17 18  once the tweets are hydrated and a json object has been returned we use the files parsejsonextremepy and parsejsonextremecleanpy to extract the tweet identifier date of creation text language and a few other extra fields this process can be configured by adding which fields from the tweet json object the user wants to extract in the fieldspy file these utilities produce a full and a clean version of the dataset respectively on a tab delimited file this process is optimized to read large files without loading them fully in memory if the user has a system with very large amounts of ram memory we also provide parsejsonlitepy to perform the same task once the json object has been parsed most users will be able to operate on the tweets directly this way we additionally provide the get1gramspy and getngramspy utilities to generate the most frequent terms and bigrams and trigrams respectively as the hydrated tweet json objects are typically quite large we recommend separating them in daily batches to be able to more efficiently process them all our previously mentioned tools take a single file as an input parameter for processing and output a new file in order to combine the results of the ngram generation from multiple files we prove the following tools that take a folder path as input and iterate through all files present combine1gramspy combinengramspy  in order to share the tweet identifiers with other groups we provide the getdatasetpy getdatasetcleanpy files which generate the equivalent files of fulldatasettsv and fulldataset-cleantsv that are presented in this resource in a compressed zip maner lastly dataset statistics can be calculated with getstatspy  by passing the full or clean dataset filename to them all the code used to preprocess and curate the dataset is available on a public github repository  httpsgithubcomthepanacealabcovid19twitter  the code is freely available and the dataset is licensed under creative commons non commercial attribution-sharealike 40 international cc by-nc-sa 40 license and should only be used for research purposes the social media mining toolkit code which was utilized to download the tweets is also available on a public github repository  httpsgithubcomthepanacealabsmmt  in order to use the social media mining toolkit a twitter developer account is required  political polarization drives online conversations about covid-19 in the united states julie jiang emily chen kristina lerman emilio ferrara emilio correspondence  ferrara  since the outbreak in china in late 2019 the novel coronavirus has spread around the world and has come to dominate online conversations by linking 23 million twitter users to locations within the united states we study in aggregate how political characteristics of the locations affect the evolution of online discussions about covid-19 we show that covid-19 chatter in the us is largely shaped by political polarization partisanship correlates with sentiment toward government measures and the tendency to share health and prevention messaging cross-ideological interactions are modulated by user segregation and polarized network structure we also observe a correlation between user engagement with topics related to public health and the varying impact of the disease outbreak in different us states these findings may help inform policies both online and offline decision-makers may calibrate their use of online platforms to measure the effectiveness of public health campaigns and to monitor the reception of national and state-level policies by tracking in real-time discussions in a highly polarized social media ecosystem  as a reflection of the unfolding real-world events for example we illustrate a sharp rise in discussions about health and prevention in march across all states as the pandemic worsened in the us  finally we portray cross-ideological interactions and characterize the role user segregation plays in the context of polarization and information spread we rely on our publicly-available covid-19 twitter dataset collected between january 21 2020 and april 3 2020  this dataset comprises of tweets that contain tracked keywords or were posted by tracked accounts approximately 20 keywords were handpicked and continuously tracked to provide a global and real-time overview of the chatter related to covid-19 since the twitter api matches keywords contained as sub-strings and is not case-sensitive terms such as coronavirusoutbreak will be matched by simply tracking the keyword corona we list examples of tracked keywords in table 1  the official accounts we track include who cdcgov and hhsgov this dataset records over 87 million tweets from 13 million unique users each tweet is categorized as an original tweet a retweet with or without a comment or a reply on a tweet-level we have information regarding the date and the language of the tweet on a user-level we have snapshots of user metadata including their screen name account creation date follower and following count and the self-reported profile location at the time of the tweet collection a small percentage of tweets also come with a place object containing geo-location data latlong of the device used to post for the purpose of geospatial analysis we aim to associate tweets with the us state from which they originate while dc is technically not a state we will refer to it as a state for convenience less than 1 of all tweets in our dataset are officially geotagged by twitter with the place object the majority of geocoding comes from self-reported user profile locations which are available for 65 of the users in our dataset we implemented a fast fuzzy text matching algorithm to detect locations in the us by searching for state names or their abbreviation codes we also searched for the names of the top 25 most populous cities by the latest 2018 population estimates reported by the us census bureau 2020 finally we matched some of the most popular nicknames eg nyc for new york city philly for philadelphia etc with this dictionary we performed iterative string matching we only labeled ambiguous tokens such as washington for washington state or washington dc if there is corroborating evidence in the text for example washington alone was considered inconclusive but would be classified as washington state if it appears alongside seattle out of the 34m tweets with location information 42 originate from the us and 85 of these tweets are associated with some state in table 2  we summarize descriptive statistics of this portion of data which we call the us dataset most of the activity is concentrated in 20 states table 3  which account for 83 of all traffic volume similar to prior geo-based studies on twitter activity in the us cheng et al 2010 conover et al 2013  the majority of tweets in our dataset originate from california texas new york and florida for validation an external human annotator manually verified a stratified random sample of locations tagged as a us state we report a predictive positive value tp  tp fp  of 963 or 982 for the top 20 most active states as such we are confident of the accuracy of locations with inferred geotags as a proxy for a states partisanship level we use the party that controls the state-level legislature as of january 20 2020 national conference of state legislatures states for which the same party holds the legislative chambers and governorship are controlled by the corresponding party and states with different parties holding the legislative chambers and governorship are considered split states the state partisanship for the top 10 states by tweet volume is indicated in table 3  research on inauthentic accounts bots trolls etc suggests that they can play a role in the diffusion of propaganda or incendiary sentiments broniatowski et al 2018 stella et al 2018 subrahmanian et al 2016  recent work on covid-19 suggests that bot-like accounts share significantly more political conspiracies than other users ferrara 2020  following the recommendation of ferrara 2020 we exclude accounts in the top 10 of the bot scores distribution obtained using botometer a bot detection api davis et al 2016  resulting in approximately 15000 accounts and 240000 associated tweets filtered out of the us dataset we aim to identify important hashtags investigate where they are important and how long they are consistently trending from a scan of all hashtags case insensitive present in the dataset we obtain a list of approximately 1 million unique hashtags of which 638 are used more than 10000 times out of these hashtags we identify those that are over-represented in the united states to accomplish this we define the geo-based popularity of a hashtag as the importance ratio ir of hashtags h and location l as follows where    p h l is the probability of observing hashtag h used in tweets geotagged in location l  and  ph is the probability of observing hashtag h used in the full dataset an ir ratio greater than 1 indicates that the hashtag h trends proportionally more regionally in location l than worldwide for the following analyses we limit our discussion to hashtags over-represented ir 1 either nationally across the us or regionally in a us state this effectively filters out most of the generic coronavirus related hashtags eg coronavirus covid19 in total we found 125 hashtags with an ir 1 in the us which we refer to as us  we use states to denote all hashtags in us as well as an additional 305 hashtags with an ir 1 regionally in a us state to understand the content of online discussions we conduct clustering analysis on the 125 hashtags us  trending in the us we de-duplicate synonymous hashtags including misspellings of the same word egcoronarvirus or words with semantically identical meanings to reduce the noise additionally we remove hashtags with ambiguous or overly broad meanings such as america or breaking we report the hashtags we de-duplicated and removed in the supporting information si after filtering we obtain 95 hashtags for clustering analysis our cluster detection method follows a two-step hierarchical paradigm described below we first set out to detect hashtag clusters from the time series of cluster usage by adopting the recently-proposed multidimensional time-series clustering framework named dipm-sc ozer et al 2020  which is specifically designed to cluster social media time series the shift window is set to three days for two reasons the first is to average out inconsistencies due to data collection for example spurious missing data second a manual scan of the daily hashtags with the greatest ir values in the us suggests that some popular topics last at most a few days to find the most optimal k  the number of clusters following the dipm-sc framework ozer et al 2020 we explore the temporal shapes of clustering results we find that 4 k produces the most non-overlapping temporal trajectory peaks after partitioning the hashtags into temporal clusters we employ the louvain community detection algorithm on each cluster blondel et al 2008  to achieve this we build a user-based weighted hashtag usage network      in this section we provide some background information about covid-19 to contextualize our understanding of the twitter activity figure 1 depicts the number of original tweets and retweets that are geotagged as the us each day the contour of the retweet activity resembles an exaggerated version of the original tweet curve which is expected of most twitter activities to facilitate comparison between the twitter activity and covid-19 case counts we also plot the cumulative case counts confirmed cases and deaths in the us compiled by the new york times 2020 the timeline of important events revolving around covid-19 that occurred in the us during the observation period is also displayed taylor 2020 wallach  myers 2020  this list is not intended to be exhaustive but rather paints a holistic and factually accurate picture of some coronavirus events as they unfolded in the us the us twitter activity experienced two peaks in traffic volume interestingly one can observe spikes in twitter activity aligned with changes in infection and death rates most prominently the biggest activity spike occurs on february 29 2020 when the first death due to covid-19 was reported in the us following the second peak user engagement falls to a stable stream of around 100 to 200 thousand tweets per day in our dataset the spread of the virus however during that same period grew exponentially this trend is observed not only at the national level but also at a state-level for every state we now discuss major events that coincided with the twitter traffic peaks in agreement with prior studies on the internet discourse of covid-19 alshaabi et al 2020 cinelli et al 2020  we observe that the twitter activity volume peaked in late january this peak occurred when the world health organization who first declared a global health emergency at the end of january 2020 although the global focus was still on the worsening development of covid-19 cases in china many other countries were seeing their first cases of the disease at this point all cases could be traced back to recent travel history to china and the centers for disease control and prevention cdc maintained that the risk of a coronavirus outbreak in the us was low twitter activity sharply rose at the end of february which was when the coronavirus situation was rapidly worsening in the us we consider this a major turning point for the coronavirus situation in the us in only a few days the white house asked congress for supplemental funding to combat the outbreak the cdc began to suspect the first case of community spread 2020 the first covid-19 death in the us was recorded in seattle washington and the white house imposed further travel restrictions and advisories on foreign accepted article countries many major political events also took place during this timeframe the 2020 conservative political action conference cpac was held from february 26 to 29 and super tuesday for the democratic presidential election primaries took place on march 3 in section  42 we will show that hashtags related to cpac 2020 and super tuesday were trending disproportionately in our dataset these events further drove discussions of politics and covid-19 up on twitter next we gauge the party preference of the majority of residents from each state using the state partisanship table 4 lists the aggregated trending hashtags in the us us  ranked by ir in democratic-leaning states split states republican-leaning states and collectively across the us we highlight hashtags that are tied to a political leader or topic which we further subdivide into hashtags supportive of the trump 2020 re-election campaign left-leaning and other politically relevant hashtags most of the top hashtags are color-coded which indicates that they are political in nature in fact some of the left-leaning hashtags blue are politically inflammatory by directly attacking elected officials two other hashtags not highlighted are also politically relevant medicareforall champions a us health care system reform and familiesfirst references the families first coronavirus response act with the exception of seattle which was the location of the initial outbreak and the first death in the us no other hashtags were explicitly about outbreak locations in the face of a pandemic this suggests that people engaging in online conversations discuss governmental response and policy changes more so than the latest news or covid-19 facts comparing the top hashtags across party lines we observe that the top hashtags in democratic-leaning states are overwhelmingly critical of the federal administration whereas the republican-leaning states generate presidential support and use slogans of the trump 2020 re-election campaign the topics in split control states and holistically in the us are more equally split the characteristics of trending hashtag topics suggest that state partisanship translates well into predicting the polarizing attitudes toward the response to the covid-19 crisis by the current administration this finding indicates a certain disparity in political attitudes between red and blue states while this is not necessarily applicable for every user from those states it serves as evidence partisanship plays a non-negligible role in the polarity of covid-19 conversations on the one hand our results are consistent with a recent survey conducted by green and tyson 2020  which showed that there is a wide partisan gap in views on the current administrations response to the outbreak on the other hand another explanation for the disparity may be attributed to geographical realities contagious epidemics are more likely to spread to transportation hubs or metropolitan areas which in the us are predominately left-leaning indeed barring any erroneous case counts due to test kit insufficiencies republican-leaning states between the coasts had considerably fewer confirmed cases in the early stages of covid-19 spread than democratic-leaning states along the coasts as a result the predominantly democratic states such as california and new york were some of the first to take immediate action to curb the diseases accepted article this article is protected by copyright all rights reserved spread people living in democratic states were impacted early on in terms of not only covid-19 case counts but also an overwhelmed healthcare system reduced economic activity and disruption to daily life which subsequently explains their initial adverse reaction to the crisis combining the digital traces of public discourse with the progression of real-world events we theorize that both the views induced by partisanship and the devastating impacts of reality contributed to polarized and inflammatory reactions of democratic-leaning states for a highly contagious disease such as covid-19 local measures of social distancing and quarantine are effective methods to curb spreading under these circumstances localized communication and information sharing is particularly important retweeting as the main form of rebroadcasting on twitter increases the visibility and spread of content boyd et al 2010  which can be crucial to the timely delivery of local news during a pandemic to examine the dynamics of local communication we focus on retweets that occur within the same state intra-state and retweets that cross state boundaries inter-state for each day t in our dataset we construct a weighted directed retweet network owing to the vast number of users the network is very sparse with many low weight edges thus to reduce the complexity of the network we apply a multi-scale backbone extraction algorithm serrano et al 2009  using a paired t-test on the mean intra-state retweet ratios for each state we conclude that retweet levels after february 29 are significantly greater than the retweet ratios before  3  10 p   the increase in intra-state communication occurs with the rapid expansion of covid-19 in the us interestingly this growth is immediately preceded by a large dip in intra-state communication on february 29 which is attributed to a shared increase in retweets produced from dc orange line the trajectory of the proportion of retweets from dc closely resembles the general us retweet activity trend cf figure 1  in light of our prior analysis we conjecture that the increase in retweets from dc is in part due to the series of political events concerning top us this article is protected by copyright all rights reserved officials located in dc we examine hashtags used in an inter-versus intra-state context to identify the distinctiveness of conversations exchanged within states suppose  t s ch is the number of times a hashtag h is used in a state s under the retweet context interintra t   the probabilities   intra ph and   inter ph of the use of hashtag h given a retweet context is given by where s is the set of all us states and out  s ch is the total appearance of a hashtag h in all the retweets consumed by state s  we rank the top intra-state and top intra-state hashtags in table 5  the results highlight the uniqueness of intra-state communication wherein people are considerably more concerned with spreading actionable messages of health and prevention matters of politics are most likely to be exchanged across states we also note that the top interand intra-hashtag trends are the same for all states regardless of their partisanship indicating the unified characteristics intra-state twitter content we now consider the use of hashtags for content analysis figure 3 illustrates the time-series popularity gain trajectories of the four temporal hashtag clusters detected by dipm-sc as a source of validation the popularity gain trajectories in figure 3 bear close resemblance to the time-series of daily hashtag usage partitioned by their temporal cluster membership which can be found in the si the hashtags belonging to each temporal cluster and semantic sub-cluster are reported in table 6  we removed sub-clusters that have fewer than 4 hashtags to direct our attention to major sub-clusters the first temporal cluster a which spans the first 30 days in our data covers the accumulating interest in the outbreak specifically a1 contains hashtags related to conspiracy theories of the origin of covid-19 the hashtag bioweapon traces back to the debunked fringe theory that the coronavirus was deliberately engineered stevenson 2020  clusters a2 and a3 show how the outbreak is affecting worldwide health and economy as well as how it compares to other diseases such as the flu and hiv temporal cluster b which spikes in popularity in late february corresponds to major political events that took place at the time cpac2020 democratic presidential election debates and super tuesday in b1 in particular b2 exhibits disapproving sentiments directed at the federal administration with hoax being the publics response to the president describing the disease as the democrats new hoax rieder 2020  c peaks shortly after in early march it includes a variety of different topics and is comparatively more spread out over time than other clusters overall it exhibits an increased awareness in public health and prevention c1 concerns for its impact on the economy c3 and this article is protected by copyright all rights reserved a resurgence of the presidents re-election campaign c4 by mid-march as many states begin issuing stay-at-home orders public response shifts unitedly towards urgency public health and covid-19 prevention cluster d during this time the popularity of previous clusters of hashtags most of which politically relevant dropped drastically many of the trending hashtags in cluster d explicitly call attention to actionable preventive measures such as social distancing and hand-washing we also note that most of these hashtags appeared in the top intra-state hashtags in table 5  furthermore the timing of this cluster corresponds with the accumulating engagement in intra-state communication see  43 which serves as corroborating evidence that local measures of health and prevention are the most emphasized issue during this period to confirm the exploding popularity of hashtags in cluster d around mid-march we perform statistical analysis on the usage of cluster d hashtags before and on or after march 12 the day on which cluster ds popularity emerges a paired t-test displays that the average daily usage of each hashtag before march 12 1   590  is significantly lower than the average daily usage after on or after march 12 2   30555  2   10  p  the temporal hashtag clusters reveal that the trending topics of discussion are consistent with the real-world events and the semantic sub-clusters reveal the divided conversations users engage in the first three temporal clusters a b c are either extremely politically charged or concerned with the real-world impact of the pandemic many of the sub-clusters display strongly polarizing sentiments that as we previously showed   42  are related to partisanship moreover we see that sentiments supporting the federal government are persistent and recurring covering a period from late january to early march attitudes against the president on the other hand are only observed briefly but emphatically during the few days in late februaryearly march the last temporal cluster d underscores the publics collective amplifying attention on health and prevention the lack of political content also indicates this shift in focus from politically fueled discussions on covid-19 many of the detected hashtags clusters form polar opposite topics hence we wish to gain insights into the characteristics of users who tweet certain topics for the purpose of cross-ideological interactions analyses among all sub-clusters we select four sub-clusters from table 6 indicated in bold that exhibit polarized dynamics we label them as follows  a1 conspiracy  b2 right-leaning  c4 left-leaningneutral  d2 healthprevention we consider the users geotagged in the us who have tweeted at least one hashtag from the above hashtag sub-clusters with respect to the four sub-clusters a1 and c4 share the biggest fraction of common users users of both sub-clusters a1 and c4 make up for 50 of a1 users and this article is protected by copyright all rights reserved 40 of c4 users together there are 29732 users who used hashtags from a1 or c4 and a comparable total of 28316 users who used hashtags from b2 the hashtag sub-cluster with the biggest user base is d2 healthprevention with 32987 unique users in total nearly 50 of all users geotagged with a us state engaged in the usage of at least one of the four hashtag sub-clusters since the number of users online and the topics of interest vary considerably over time we avoid a direct comparison of users or their hashtag usage overlap instead we estimate cross-ideological interactions by comparing the retweet interaction among users of different hashtags sub-clusters retweeting is commonly understood as a form of approval or endorsement boyd et al 2010  therefore users are much more likely to retweet from someone they share similar ideologies with by removing time-varying factors from the equation we can better capture the ideological differences across user groups we now describe our qualitative methods to measure cross-ideological interactions from the retweet network we can quantify how likely users of one hashtag sub-cluster are to retweet from users of another sub-cluster suppose i u is the set of users who used hashtags from sub-cluster i  let    ij w u u be the number of times a user from i u retweets a user from j u  it follows that the likelihood of i u users retweeting from j u users given how often i u users retweet each other yields from the ratio    a large retweet likelihood ratio     1 ij r u u  indicates that users in i u are more likely to retweet from users in j u than from someone of their own  i u  representing a strong cross-ideological interaction conversely a small retweet likelihood ratio     1 ij r u u  means that users in i u are more likely to retweet from someone within their own group than from an external group j u  signaling a weak cross-ideological interaction this retweet likelihood ratio is not symmetric that is  table 7 show that some hashtag users are more likely to retweet users of other hashtags conspiracy and right-leaning users are more likely to retweet the other group than from their own 140 and 116 by contrast conspiracy and left-leaningneutral users are less likely to retweet each other 086 and 071 however we also observe that users of hashtags in the right-leaning spectrum and in the left-leaning or neutral spectrum are likely to retweet each other both 116 a possible explanation is that many users use ideologically clashing hashtags in the same tweet so to maximize its exposure to users of both sides of the spectrum conover et al 2011  the largest drop in retweet likelihood ratio occurs between users of right-leaning hashtags and users of healthprevention hashtags right-leaning users are 058 as likely to retweet from healthprevention hashtag users and the conspiracy users are even less likely 044 in this article is protected by copyright all rights reserved comparison left-leaning or neutral are more likely than their conservative counterparts to retweet from healthprevention hashtag users 083 overall users of healthprevention hashtags form a much less tight-knit community they are more likely to retweet from users of other hashtag sub-clusters than from their own this suggests that these hashtags come from a more diverse selection of users nonetheless their users are twice as likely to retweet from left-leaningneutral than right-leaning users indicating that they resonate more with left-leaning or neutral users we visualize the retweet network of users of the four hashtag sub-clusters in figure 4  for visualization purposes we randomly sample 20000 nodes users and then plot the largest connected component the graph is then laid out using forceatlas2 jacomy et al 2014 and color-coded according to the communities detected by the louvain method blondel et al 2008  we color nodes from the top three communities which make up for 83 of all nodes figure 4 displays clear segregation of politically opposing communities the red community consists of 77 users who posted conspiracy a1 or conservative c4 hashtags on the opposite side of the plot the blue and yellow communities consist of 75 and 81 of users respectively who posted hashtags that are critical of the federal administration b2 users who used healthprevention d2 hashtags are scattered but they are more represented in the blue and yellow communities 27 and 17 than in the red community 13 this supports our previous finding that users of healthprevention hashtags are more closely tied to left-leaningneutral users to understand the topics of discussion related to the evolution of covid-19 we examined the usage of hashtags in the us from the geospatial and temporal dimensions we also studied the characteristics of interactions among users holding diverging opinions our first observation is that the american public frames the coronavirus pandemic on twitter as a core political issue the vast majority of hashtags relevant to the us directly references key political leaders or major political events that took place given that there is a presidential election taking place in 2020 it is not surprising to see coronavirus at the center of attention in the political agendas however there is an indication of the emergence and spread of conspiracy theories during the early stages of our study period related research also shows that social bots are partially responsible for this ferrara 2020  which could exacerbate the spread of conspiracies online on the other side of the spectrum there exist users who are overly critical of the white houses actions consequently these polarizing discussions on twitter increasingly politicize the issue of covid-19 deviating the publics attention from the core focus that is the pandemic we also illustrate that online dialogue exhibits a clear attitude divide that splits along partisan and ideological lines users from liberal-leaning states frequently tweet content critical of political elites whereas users from conservative-leaning states persistently utilize hashtags in accepted article support of the president correspondingly we observe two segregated communities at the user-level emerging from the user base induced by ideologically distinct hashtag sub-clustersconspiracy right-leaning left-leaningneutral healthpreventionare two polarizing communities largely divided by their political ideology comparing users who tweet right-leaning hashtags with users who tweet left-leaningneutral hashtags the former group is less likely to interact with users who tweet health and prevention hashtags this would suggest that users on the right-leaning and conspiracy thinking side are associated with the aversion in promoting awareness of health and prevention we find evidence in support of this finding in related work a recent survey conducted by uscinski et al 2020 harvardmisinfo revealed that those with a psychological predisposition of conspiracy thinking or those in support of the current administration are likely to believe the virus has been exaggerated another study on partisanship views also concludes that republicans are less likely than democrats to see the pandemic as a serious threat green and tyson 2020  using smartphone location data barrios and hochberg 2020 2020 showed that counties with a higher percentage of conservatives are less likely to adhere to stay home or shelter-in-place orders thus we argue that since these users are more likely to believe the current situation is overstated they are also less inclined to promote health safety and adequate social distancing against the virus since partisan cues guide the evolution of covid-19 beliefs and discussions partisan predispositions have the potential to also impact the efficacy of health campaigns and public policies through observing the topics of discussion we additionally see that elected officials continue to influence conversations about the pandemic granting them the potential to shape public opinion government leaders can utilize these findings to more effectively relay corrective information to their constituents nevertheless as the virus takes a foothold in the us we find that all states collectively steer the conversation to public health and preventive awareness as part of the increasing communication within states arising from this crisis is a shared sense of urgency and strengthened local community engagement implying that people could be growing more receptive to state policy changes in preventing the spread of the virus as many in the united states battle with the lack of adequate health care and economic insecurity the implications of covid-19 are far greater than what can be observed on twitter consequently more in-depth analyses from a social science perspective are warranted for further investigation while this work suggests that the use of natural language processing and network science techniques can provide some insights into covid-19 online discussion we also hope that it will motivate other social scientists and data scientists to apply a diverse toolkit of investigation methods to our publicly-released twitter dataset  n is the number of tweets geo-tagged with the state and  is the fraction of all tweets geo-tagged us we color code hashtags that belong to the same category red trumps 2020 re-election campaign slogans blue politically-charged left-leaning hashtags yellow other politically relevant hashtags n is the average hashtag usage in the given states   role of biological data mining and machine learning techniques in detecting and diagnosing the novel coronavirus covid-19 a systematic review a albahri s rula hamid a jwan alwan k zt al-qays a zaidan a b zaidan b a albahri o a alamoodi h jamal khlaf mawlood e almahdi m eman thabet suha hadi m k mohammed i m alsalem a jameel al-obaidi r ht madhloom   the outbreak of new coronavirus covid-19 infections has caused worldwide concern because this disease has caused illness including illness resulting in death and sustained person-to-person spread in many countries 1 2 covs are a large family of viruses including the middle east respiratory syndrome mers-cov severe acute respiratory syndrome sars-cov 3 4 and the new virus named sars-cov-2 1 in 2012 saudi arabia experienced the outbreak of mers-cov which is responsible for causing mild to moderate colds infection with mers-cov can lead to fatal complications mers-cov is responsible for causing severe acute respiratory illness that leads to death in many cases according to al-turaiki and his group 4 mers-cov symptoms include cough fever nose congestion breath shortness and sometimes diarrhoea unfortunately information on how the virus spreads and how patients are affected is limited 4 fifteen years after the first highly pathogenic human cov caused the sars-cov outbreak another severe acute diarrhoea syndrome-cov devastated livestock production by causing fatal diseases in pigs the two outbreaks began in china and were caused by covs of bat origin 5 6 on february 11 2020 the world health organisation named the ensuing disease covid-19 1 chinese health officials have reported tens of thousands of cases of covid-19 in china with the virus reportedly spreading from person-to-person in several parts of the country covid-19 illnesses where most of them are associated with travel from wuhan have been reported in a growing number of international locations including the united states 1 artificial intelligence ai is gradually changing medical practice with the recent progress in digitised data acquisition machine learning ml and computing infrastructure ai applications are expanding into areas that are previously believed to be only the area of human experts 7 various types of data mining methods have been applied by a few researchers with real cov datasets eg mers-cov based on several types of ml classifiers 8 providing prediction systems that can accurately anticipate and diagnose such virus remains challenging the growth of ai-driven techniques to identify epidemiologic risks in advance will be the key to improving the prediction prevention and detection of future global health risks 9 the main contributions of this study are the exploration of the cov family by reviewing articles on data mining and ml algorithms the acquisition of a clear understanding of its enhancements and how previous research has addressed prediction regression and classification methods this study also aims to collect various information from the literature that are relevant to ml such as application nature the use of ml and data mining algorithms and evaluation methods and accuracy the datasets utilised in the literature are constructed and presented with url sources the motivations challenges and limitations of this approach are examined and recommendations on improving the approach efficiency are provided other important information collected especially on the types of case study used features and classes for cov are explained in separate tables the rest of this paper is organised as follows the second section describes the research methods used in the selected literature the third section presents the literature review the fourth section discusses the results motivations challenges recommendations and limitations case study used and features and classes of cov the fifth section provides a conclusion a comprehensive literature search was conducted in the five mentioned databases for english language citations published from 2010 to 2020 the selection of these indices was because of their sufficient coverage of studies related to our research considering that identified novel cov requires greater attention than other infections this study presented and conducted a three boolean search strategy using various keywords related to pervasive coronavirus eg cov or coronaviridae or coronavirus and keywords related to the detection diagnosis and classification of cov under the concept of ai and ml we used these query techniques to strengthen our search of different ai and ml systems and application studies for cov 
the article is an english journal or conference paperthe main focus is on the development of different artificial intelligence and ml applications systems algorithms methods and techniquesthe development only focuses on the detection diagnosis and classification of adaptive cov
 table 1 summarises the sequences of the boolean search query used in this paper and the results this process was initiated by removing duplicated articles and screening nonduplicated articles by their titles and abstract to check their compatibility with our inclusion and exclusion criteria the relevant articles were subjected to a full reading process for collecting and extracting research data and constructing the review article in all research articles the entire research process was monitored and supervised by a senior author corresponding author to ensure the production of a highly reliable and beneficial research paper given the multidisciplinary topic of this systematic review data extraction and classification of the selected studies including data concerning cov with ai applications especially ml techniques were conducted to evaluate the efficacy of this virus in terms of detection diagnosis and classification throughout ai enhancements data elements were extracted from academic literature and included authors nationalities date of publication number of articles per year and number of articles per database for conferring a comprehensive viewpoint of cov this study discussed the cov and analysed the growth scale of the worldwide epidemic in the context of ai using various data mining and ml algorithms such as classification regression and prediction for each study in the literature this study extracted the important feature names evaluation methods used and state of accuracy for each method brief motivation challenges limitation and recommendation were extracted from the reviewed papers to address the serious public health concern for cov the results of search queries conducted in this study are presented in figure 1 three search queries were accomplished to encompass all databases and their search engine mechanisms during data collection the first result comprised 1305 articles from all five databases the number of duplicated articles in all databases was 66 and the results were 1239 the next process was screening the articles based on the title and abstract followed by the mapping of inclusion and exclusion criteria and the results were 249 articles the final process was the full reading of all articles and the outcome was only 8 articles that met the inclusion and exclusion criteria our understanding of the purposeaim of these studies inspired us to analyse each study depending on two related sequences within the search query that was conducted in this systematic review the first sequence is the article should identify the cov and the second is the utilisation of ml the findings of academic literature search showed that various algorithms are used by previous researchers figure 2 summarises these algorithms and methods the decision tree j48 algorithm was the most frequently used five times naive bayes and support vector machine svm algorithms were each used four times k-nearest neighbour k-nn was utilised two times and the others were each used once figure 3 presents the number of papers included in the academic literature review following the publication year the distribution of scientific papers from 2010 to 2020 is shown below three papers were published in 2016 two papers were published each in 2017 and 2018 only one paper was published in 2019 no paper was published in the other years table 2 illustrates the state-of-the-art cov prediction algorithms table 3 presents the cov dataset descriptions with available sources the analysis indicated many important points that are discussed to identify the research gaps in 11 three ml techniques were applied to the mers-cov dataset to identify the best classification model for binary class and multiclass labels the results showed that the k-nn classifier is the best model for the two-class problems and the decision tree and nave bayes are the best models for multiclass problems in 4 two experiments are applied to the mers-cov infection dataset and the decision tree classifier shows higher prediction proficiency than the other models the experimental results indicated that age and symptoms are the two dominant features for the prediction model and that healthcare staff are likely to survive in 12 a study was conducted in saudi arabia to identify the dominant factors that influence human infection using statistical methods such as univariate and multivariate regression methods the results indicated four dominant features namely disease severity patient age the patient job as a healthcare staff or not and history of chronic disease interaction with camels does not have a high impact on recovery in 8 a study was conducted in saudi arabia between 2013 and 2017 to improve medical diagnosis systems for binary and multiclass problems in mers-cov datasets the experimental results showed that the decision tree classifier achieves the best accuracy for the multiclass labels and the svm classifier obtains the highest accuracy for the two binary class labels in the mers-cov dataset in 13 an emotional recognition system based on ml technique was proposed to understand human reactions to a widespread epidemic of transferrable diseases such as mers the study was conducted using a dataset collected in korea in 2015 and the results indicated the impact of lightening excessive panic in reducing infection in 14 the author investigated people over 50 years old using three ml methods to predict mers cov the results showed that elderly people are more likely to be infected than others in 15 an svm classifier based on sigmoid normal and polynomial iterations was used to analyse the mers and sars proteins the results showed their behaviour similarity and approximate dissimilarities in 16 data mining based on statistical methods was utilised to develop a cloud-based medical system with a high prediction accuracy to prevent mers-cov spread within different regions the dataset comprises the following attributes drug patient and cloud-based user medical record the role of ai in healthcare for enhancing the detection and prediction of numerous viruses and diseases has been previously discussed 9 in this review we aimed to obtain a large number and extensive contributions of published articles regarding the utilisation of ai for the detection and clinical diagnosis of mers-cov and sars-cov however the lack of studies on the recent outbreak of covid-19 indicates the need and opportunity to apply ai for predicting such outbreaks ml technique based on supervised and unsupervised learning provides the opportunity to develop a medical diagnosis system in supervised learning the target class of each sample in the dataset where mers-cov and sars-cov classes are previously identified and the developed system can be adapted to a new disease although mers-cov and sars-cov have similarity within the same cluster they are dissimilar to the objects in other clusters thus the clustering technique based on unsupervised learning is considered an efficient method to cluster the collected datasets as presented in table 3 consequently detection and diagnosis can be remarkably enhanced all studies in this review reported the use of ai techniques such as case-based reasoning and rule-based systems however none of the studies utilised other classification methods such as neural networks reinforcement learning and hybrid classification none of the studies utilised and integrated optimisation techniques such as genetic algorithms and particle swarm optimisation to their systems data mining and ml algorithms used in diagnostic operations primarily rely on classification algorithms including decision tree svm and naive bayes classifiers figure 2 and table 2 notably no study in the literature exploited clustering algorithms for the detection and diagnosis of the cov family this technique can be used as a pre-processing step before feeding data into the classification model to gain valuable insights into the case study data by understanding which groups of disease symptoms fall into when applying these algorithms in this review the sample sizes representing the observations in each dataset are displayed in table 3 in 11 the sample size includes all mers-cov patients in saudi arabia in the second half of 2016 and in 8 the sample size includes the cases from 2013 to 2017 the sample size in 4 represents 1082 records of cases reported from 2013 to 2015 distributed as 633 new case records 231 recovery records and 218 death records the study in 12 includes 836 patient records and 52 patients are reported as dead and only 784 cases are used in 13 the sample size is represented by articles collected from the internet and reported by 153 news media outlets in korea and the comments associated with these articles from day 1 first confirmed case on may 20 2015 to the day 70 in 15 the dataset contains 322 records 92 infected cases and 230 uninfected cases in 16 synthetic data are generated for 02 million users most of the available datasets are related to mers and sars infection cases but no dataset is found for covid-19 because of its novelty to date research areas in the field of ai such as data mining and ml technique-based applications have been rapidly developed because of their large impact on human life in terms of social scientific medical and engineering-based applications accordingly this section presents the motivation of studies conducted on the cov problem to save lives data mining for medical diagnosis systems is efficient and can be utilised to control the spread of mers-cov and protect humans 8 data mining can also be used to estimate and predict the recovery rates from cov infections 4 ml technique can be used to identify and predict the dominant factors affecting the recovery from mers-cov 12 13 thus studies conducted on identifying the best model can help minimise the effects of epidemic diseases 11 the distinction between sars and mers viruses can be considered a challenging task because of the similarities in their symptoms such as breathing problems and high fever 15 medical diagnosis systems play a significant role in identifying patient health conditions with mers-cov symptoms 14 studies such as 16 integrated the gps with their medical diagnosis systems to cluster the population of infected people based on their geographical area covid-19 has spread worldwide and threatened human life accordingly several studies have been conducted to develop an intelligent medical diagnosis system using ai technique to control the effects of this virus however numerous challenges and research limitations have been indicated in the academic literature and need to be addressed in the future 8 some of these challenges are related to mers-cov nature and behaviour because understanding how the virus spreads and how people can be infected caused by the complexity of this epidemic disease is extremely difficult the lack of a large dataset in the academic literature for mers-cov is considered a challenging task for ai researchers because it hinders the understanding of viral patterns and features 4 12 the demand to construct a dataset that can be understood by ml algorithms has increased because the current dataset involves infographic data 11 other challenges are correlated with people and government responses to mers-cov that requires more new monitoring approaches and additional efforts compared with the traditional approach for controlling epidemic diseases 13 another challenge with mers-cov is the large variation in symptoms that are mostly similar to common cold symptoms with many other variations of diseases that may occur in cases but not in others some patients have unique symptoms and others have no symptoms at all activists have generated huge and complex volumes of data that render its analysis impractical and difficult to predict using linear classifiers 14 15 the protection of citizens by the government and health agencies is a significant challenge because no specific vaccine exists for this virus to date and requesting people to undergo medical checkups is difficult 16 this study aimed to mitigate some of the challenges that have been addressed in the academic literature with their recommended solution for future studies studies such as 8 suggested a pre-processing method to solve the missing-value problem that directly impacts the classification accuracy for the mers virus they also suggested the use of an ensemble technique by combining the cosine method with k-nn ml algorithm to improve the classification accuracy to 50 another study 4 recommended increasing the number of samples for the cov dataset and collecting data from patients within the same geographical area by directly communicating with dedicated hospitals and health agencies 13 11 proposed the use of svm classifier for the binary class problem and conducted an empirical study for multiclass problems in mers-cov 17 recommended the use of the r language because of its efficiency and high functionality in supporting ai algorithms that can enable the development of an effective intelligent medical diagnosis system for cov another study 14 indicated the special medical issues related to the female status such as whether she is pregnant which need to be considered in the treatment of cov-infected patients 15 16 suggested the utilisation of the internet of things iot technology for developing a highly dependable medical diagnosis system for covid-19 based on the discussion analysis and details in tables 2 and 3 a case study found in the literature review can be divided into two types namely real and analysis datasets table 4 real datasets consist of a number of real cases of infected and healthy patients within a specific period four studies provided real datasets for patients affected by cov 4 8 11 12 and most datasets in other studies are published and redistributed by the ministry of health website of the kingdom of saudi arabia in analysis datasets standardisation is intended to increase the consistency of review and assessment analysis for mers-cov and sars-cov an organised collection of data is found in four studies for the two viruses in 13 the massive media outlet data were collected during the nationwide outbreak of mers-cov in korea in 2015 in 14 mers-cov cases were recorded from several medical analytical papers focused on the early symptoms of this virus in 15 spike glycol protein sequence data of sars dq4125741 and mers kp2360921 were obtained 16 utilised the gps to represent each mers-cov user on google maps where 5000 users are adopted in r studio through the bnlearn package two attributes namely 1 personal patient information attributes and 2 cov attributes were recognised in the collected studies to explore the effects of cov features and classes on case study datasets used with ml algorithms as shown in table 5 only four studies focused on personal patient information attributes only two studies considered cov attributes and two studies considered the two attributes table 5 shows that the details mostly encompass mers-cov and sars-cov attributes and classes age attribute is considered the most important and dangerous factor in the infected patients because people over the age of 50 are more likely to be at risk and to have this type of virus than others 4 8 12 14 16 gender attribute is an important predictor in four out of eight studies 4 8 11 12 the city was used in three studies 4 8 11 other related attributes are less frequently associated with personal patient information attributes including address sex and name although sob is mentioned in only two studies it is considered the most important concern with mers-cov and sars-cov attributes because the two studies focused on patients affected by the two viruses from a specialised medical perspective the features and classes of the cov family are similar to one another in this context the new epidemic of covid-19 depends on the same features and classes of mers-cov and sars-cov thus extensive research has been conducted to prove a new ai pathway as proof for the above discussion several reliable reports and government news have mentioned that age is the most important feature of patients with covid-19 patients over the age of 50 are susceptible to contract the disease and be exposed to its risks and complications the gender and city of an infected patient are the next concerns the medical team has reported that sob is the most important symptom attribute because it carries a high specificity for covid-19 this systematic review provided an exhaustive overview of integrated ai based on data mining and ml algorithms with the cov family state-of-the-art cov prediction algorithms were presented distinct information such as application nature ml used the evaluation conducted for each study and extracted features and classes with accuracy percentages for the utilised ml algorithms were indicated a set of propositions for the risk recovery of this virus was established to serve as a guide for future research in the context of data mining algorithms despite the increasing rates of death and the number of people affected with cov developments based on ml algorithms to improve cov datasets remain at a redefinition stage especially for covid-19 the shortage of studies in the literature is a real concern and may have serious implications for detecting and minimising the spread of this virus an emerging rapidly evolving situation for the virus must be considered in the viewpoint of ai applications and researchers should provide updated contributions because they are necessary supportive information such as new datasets must be provided and many complex features and classes must be added close cooperation among researchers in the biomedical engineering field and the medical community is necessary to stop the growing public health threat posed by the 2019 cov the goal is to conduct new studies that can guide governments and communities in the early control of the impact of this virus by utilising the features and classes collected in the literature the need for integrated sensor technologies specifically for outdoor scenarios is highly recommended this process is only possible when the technique is interlinked with iot technologies focusing on evaluating and improving ml algorithms for cov datasets with increased efficiency in this context ai software developers in healthcare can develop different software packages to remotely help analyse the extracted features and classes for patients with covid-19  challenges in combating covid-19 infodemic -data tools and ethics kaize ding kai shu yichuan li amrita bhattacharjee huan liu  while the covid-19 pandemic continues its global devastation numerous accompanying challenges emerge one important challenge we face is to efficiently and effectively use recently gathered data and find computational tools to combat the covid-19 infodemic a typical information overloading problem novel coronavirus presents many questions without ready answers its uncertainty and our eagerness in search of solutions offer a fertile environment for infodemic it is thus necessary to combat the infodemic and make a concerted effort to confront covid-19 and mitigate its negative impact in all walks of life when saving lives and maintaining normal orders during trying times in this position paper of combating the covid-19 infodemic we illustrate its need by providing real-world examples of rampant conspiracy theories misinformation and various types of scams that take advantage of human kindness fear and ignorance we present three key challenges in this fight against the covid-19 infodemic where researchers and practitioners instinctively want to contribute and help we demonstrate that these three challenges can and will be effectively addressed by collective wisdom crowd sourcing and collaborative research  coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 sars-cov-2 the world health organization who recently declared the covid-19 outbreak a public health emergency of international concern pheic and a pandemic due to its high morbidity and mortality rates as of april 15 2020 more than 204 million cases have been reported across 210 countries and territories resulting in over 133000 deaths 1  these numbers are continuing to rise and the health systems in many countries are overwhelmed to provide treatment concomitant with the pandemic are many unknowns that create a conducive environment for misinformation fake news political disinformation campaigns scams etc those malicious contents instigate fears or anger capitalize on human vulnerability and exploit human emotion kindness andor wishes for miracles as the coronavirus spreads like fire in the world disinformation machines also accelerate their campaigns on various fronts rendering a new infodemic battlefield social 1 httpsenwikipediaorgwikicoronavirus disease 2019 media platforms such as facebookinstagram twitter and googleyoutube have been abused to disseminate erroneous contents when the whole world is scrambling to fight the covid-19 pandemic governments and who also have to combat an infodemic which is defined as an overabundance of information some accurate and some notthat makes it hard for people to find trustworthy sources and reliable guidance when they need it donovan 2020  the covid-19 infodemic causes confusion sows division incites hatred promotes unproven cures and provokes social panic which directly impacts emergency response treatment recovery and financial and mental health during the difficult time of self-isolation therefore combating the covid-19 infodemic is a challenging yet imperative task in this paper we first present some covid-19 related examples to illustrate the variety and range of infodemic cases in representative categories conspiracy theories and misinformation and scams and security attacks to reinforce the urgency and need for addressing the covid-19 infodemic via scalable and timely solutions we then discuss the essential challenges in designing and developing corresponding ai solutions from three perspectives data computational tools and ethics the last challenge of ethics is particularly easy to overlook when we rush to confront the immediate threats therefore it is important to understand unintended consequences when developing ai solutions to ensure sustainable and healthy use and deployment last we use some current efforts to demonstrate the feasibility of addressing the three challenges in combating the covid-19 infodemic by understanding the challenges and what we have we also appreciate the importance of collaborative research to effectively and efficiently combat the covid-19 infodemic to illustrate what the covid-19 infodemic looks like how expansive active and devastating it is and why it is important to thwart or mitigate its present threats we first present various examples regarding conspiracy theories and misinformation and scam and security attacks with the spread of covid-19 pandemic the world health organization who recently warned of an infodemic of rampant conspiracy theories about the coronavirus those conspiracy theories have appeared in both social media and mainstream news outlets and are often intertwined with geopolitics one example is about how the new coronavirus originated according to a pew research center survey nearly three-in-ten americans believe covid-19 was a bio-weapon made in the lab some top 10 conspiracy theories include sars-cov-2 virus was created as a biologic weapon from a lab gmos are the culprit covid-19 actually doesnt exist and coronavirus is a plot by big pharma lynas 2020 coronavirus misinformation is also flooding the internet through social media text messages and propagated by celebrities politicians or other prominent public figures according to the report in kornbluh and goodman 2020 among outlets that repeatedly share false content eight of the top 10 most engaged-with sites are running coronavirus stories for instance there are plenty of supposed cures on social media that will likely mislead people to risk their lives for quick fixes disregarding the national institutes of health nih warning of many hearsay cures without evidence of curing being effective there are endless claims such as herbs and teas or something of the sort that can prevent the coronavirus recently some wireless towers were damaged in the uk due to a false claim that radio waves sent by 5g technology are causing small changes to peoples bodies that make them succumb to the virus scam spam phishing and malware attacks as more and more people start working or studying from home cyber criminals recently shift focus to target remote workers different attacks such as scam spam phishing and malware which prey on peoples willingness to help fear of supply shortage and moments of weakness have become increasingly active researchers have found that the volume of coronavirus email scams nearly tripled in one week with almost 3 of all global spam now estimated to be covid-19 related during the coronavirus pandemic as state governments and hospitals have scrambled to obtain masks and other medical supplies scammers attempted to sell a fake stockpile of 39 million masks to a california labor union according to the hill miller 2020  hackers are taking advantage of the increased reliance on networks to target critical organizations such as health care groups and members of the public stealing and profiting off sensitive information and putting lives at risk the scale volume and reach of the covid-19 infodemic entails the reliance on ai and machine learning ml algorithms to react promptly and respond rapidly the success of ai and ml algorithms requires large amounts of multimodal data for their efficiency and effectiveness which introduces a data challenge data extraction and curation from multi-source data needs different computational tools to accurately categorize and sort out various types of data which presents a tool challenge when we rush to deal with present threats we should be aware of potential side-effects unexpected consequences and biases of our solutions which suggests an ethic challenge in this section we will discuss these three challenges in detail though numerous covid-19 data sources are available online their datasets are available on various websites for different needs the major data challenge of isolated data sources is the awareness of their existence another related issue is that they are collected from different sources or under different crawl settings for example allen institute for ai ai2 released the scholarly articles dataset 2 collected from pmc medrxiv and biorxiv the frontiers 3 provided the latest research articles and litcovid chen allot and lu 2020 collected the scientific information from pubmed combining different data sources leads to higher quality of data and better coverage to address the data challenges we need to overcome some shortcomings disorganization -most of them merely list all the collected datasets on their websites without information summarizing the relationships among them specificity -data collected for a specific topic for example amazon provides the epidemic dataset on cloud 4 and covid-19 gis hub 5 only contain the academic findings and geospatial-related datasets respectively and inconvenience -most sites merely provide the reference links to the source datasets and do not provide data utility tools like covid19datahub guidotti and ardia 2020 for easy access there are existing resources that can assist users to identify malicious intent in websites googles safe browsing api for instance allows the user to enter a url and check it against googles constantly updated lists of unsafe web resources similar resources include isitphishingorg malwareurlcom and antivirus software among many others additionally users can check malicious domain lists through different sources such as phishtankcom or the aforementioned googles safe browsing lists as many malicious sites use url shorteners to disguise themselves to counteract potential attacks it would be safe to first use url expanders to figure out what they are before clicking them despite the easy access of those computational tools they are not available conveniently in a single place where different tools can be called up whenever needed the awareness of these existing tools and efficient use of them for quick response is vital for combating covid-19 an associate issue is the requirement for current and frequently updated black-lists sahoo liu and hoi 2017 as we know it is infeasible to manually maintain a dynamically changing list of malicious urls with new sites being generated everyday therefore it is necessary to develop aiml identifiers that can learn from the old malicious sites to estimate the threats of new ones the covid-19 pandemic is ushering in a new era of digital surveillance since governments are employing tools that track and monitor individuals south korea and israel for instance have demonstrated the effectiveness of harnessing different digital surveillance tools however such a new practice can breach data privacy in the meantime and may even remain in use after the pandemic in this section we discuss the potential privacy concerns trade-offs between stringent disease monitoring and patient privacy and ethical issues behind the disruption of civil liberties gauging the war-like severity of the coronavirus pandemic academics researchers companies and non-profits alike have come forward to contribute in any possible way however given the rapid nature of such responses and the subsequent lack of policy checks these otherwise novel endeavors may have ethical loopholes in an attempt to provide a transparent view of the degree of infection and prevent community spread of the virus many counties and states in the united states have decided to publicly release data corresponding to cases including the number of cases per zip-code mallory 2020  smartphone applications with geo-locating capabilities have come out for users to log their symptoms but the use of such applications has significant privacy concerns wetsman 2020 contact tracing has been identified as an effective way to control the spread of the virus in communities where the infection is not yet widespread or has slowed down significantly and companies including google and apple are currently developing applications to make this possible only when a sufficient number of people use the application and voluntarily report their cases can it be used as a reliable tool of tracking in this situation there is an obvious trade-off between user health privacy and data transparency and it is challenging to identify well-defined ethical boundaries when it comes to public health during a pandemic the success of such an app requires a majority of the population to download and use it in this section we present some current efforts that address the aforementioned challenges and show that the three challenges are solvable with collaborative research for the data challenge we collect the publicly available covid-19 datasets and cluster them into several groups 6  under each group researchers can reference complete datasets from different sources or settings for example in social media data we gather available tweet corpus on covid-19 banda et al 2020  chen lerman and ferrara 2020 with different query keywords and time spans the hierarchy cluster structure in figure 1 helps the researchers to quickly locate the dataset lastly our data repository includes areas in academics news social media and epidemic reports for multi-disciplinary research for example if a researcher wants to analyze the influence of the news or academic findings on social media like twitter she can use the data in academic or news topics and social media to help a researcher easily access the datasets in the repository we build a data-loader 7  it is a python package with a pandas dataframe pandas development team 2020 by calling data  dataloaderdownloadurl this widely used data format can help the downstream data analysis to tackle the too challenge we develop tellme a computational tool that provides an estimate if a piece of news or text is disinformation its input includes urls and text and its output is a score based on different functions of tellme as shown in figure 2  url checker fake news classifier website matcher credibility and trusty the trusty moturu yang and liu 2009 and credibility abbasi and liu 2013 scores are based on contents social engagements that malicious users share more similarity than general users the fake news score is returned from a state-of-the-art fake new detector  the website matcher compares the input url with websites that publish false information about the virus found by newsguard brille and crovitz 2019 now we use fake news as an example to illustrate our attempts to learn with weak social supervision to detect covid-19 disinformation more effectively and with explainability first for effective fake news detection we consider the relationships among publishers news pieces and consumers which is motivated by existing sociological studies on journalism on the correlation between the partisan bias of publishers the credibility of consumers and the veracity degree of news content and explore various auxiliary information from these relations to help detect fake news shu wang and liu 2019  second for explainable fake news detection we aim to derive explanation of prediction results to help decision makers and practitioners we attempt to explore user comments as a source and mine informative and relevant pieces to help explain why a piece of news is predicted as fake and pinpoint more fictional text in news text simultaneously shu et al 2019  to tackle the ethics challenge due to the increase in government surveillance and prevalence of smartphone apps to collect and gather userpatient data we need to take into account legitimate concerns regarding privacy and the degree to which such a regime of monitoring and enforcement will affect democracy after the pandemic ends it requires us to understand and acknowledge the fact that there is a clear difference between standard biomedical ethics versus privacy concerns and ethics during a public health crisis governments and public health officials may need to take certain measures aimed at minimizing the damage caused by the virus and for the common good during this trying time which under normal circumstances might have been inappropriate nevertheless measures could be taken to avoid potential misuse of data one possible way to have better guarantees on user privacy would be to make these contact tracing smartphone applications communicate in an encrypted peer to peer way rather than storing all the data in a central server these technologies should also be deployed in a way that is as transparent as possible so that the user is fully aware of what and how much personal information heshe permits the application to use furthermore there is significant ongoing discussion among experts researchers and policy-makers regarding a steady recovery into a normal functioning society for example the ethics research group at harvard university makes efforts at finding solutions without compromising user privacy to keep civil liberty and democracy at the forefront the significance of combating the covid-19 infodemic lies at protecting people from falling victims to the pandemic in this unexpected front and from disrupting otherwise already inconvenient daily routines so as to improve our resilience in our fight to contain the pandemic in this position paper we show a good number of problems posed by the covid-19 infodemic the vast amounts of data generated in the worlds effort to contain the pandemic and the need for concerted efforts at various levels to efficiently and effectively deal with current and future challenges in medical and information fronts it is evident that 1 we face both immediate and future challenges in this unprecedented fight 2 existing data will grow fast and existing computational tools are insufficient to contain and mitigate the covid-19 infodemic and 3 short-term solutions can have potential long-term impact therefore when we face hard choices we need to resist the temptation to trade-off so as to minimize long-term negative impact when we search for solutions we should consider those employing crowdsourcing and take long views for fairness and responsibility when we design methods we should rely on collective wisdom and diversity to aim for robustness and when we form teams we should give priority to multi-disciplinary collaboration and preemptively address hidden biases our future will always be uncertain but with the advancement in science and technology and with our preparedness trained and tested in our concerted efforts to contain the pandemic in all fronts our future will surely be brighter and healthier  genome detective coronavirus typing tool for rapid identification and characterization of novel coronavirus genomes sara cleemput wim dumon vagner fonseca wasim karim abdool marta giovanetti luiz alcantara carlos koen deforche tulio de oliveira   we are currently faced with a potential global epidemic of a new coronavirus that has infected thousands of people in china and is spreading rapidly around the world this week the who has declared it an global emergency who 2020 the wuhan novel coronavirus 2019-ncov has already caused more infections than the previous severe acute respiratory syndrome sars outbreak of 2002 and 2003 the virus is a sars related coronavirus sarsr-cov and it is genetically associated with sarsr-cov strains that infect bats in china zhu et al 2020 lu et al 2020 it causes severe respiratory illness has high fatality rate huang et al 2020 can be transmitted from person to person and has spread to over 15 countries in less than two months who 2020 this coronavirus outbreak has been unprecedented so too is the way that the scientific community has responded to it they have openly and rapidly shared genomic and clinical data as never seen before allowing research results to be released almost instantaneously this has helped the understanding of the transmission dynamics the development of rapid diagnostic and has informed public health response here we present a new contribution that can speed up this communal effort the genome detective coronavirus typing tool is a free-of-charge web-based bioinformatics pipeline that can accurately and quickly identify assemble and classify coronaviruses genomes the tool also identifies changes at nucleotides coding regions and proteins using a novel dynamic aligner to allow tracking new viral mutations figure 1 a reference dataset of previously published coronavirus whole genome sequences wgs was compiled from the virus pathogen resource vipr database wwwviprbrcorg this dataset consisted of 386 whole genome sequences wgs of nine important coronavirus species these included 132 sequences of severe acute respiratory syndrome related coronavirus sarsr-cov 121 sequences of beta coronavirus 97 sequences of middle east respiratory syndrome related coronavirus mersr-cov 19 sequences of human coronavirus hku1 9 sequences of murine hepatitis virus 4 of rousettus bat coronavirus hku9 3 of rat coronavirus and one wgs of tylonycteris bat coronavirus hku4 zariabatcoronavirus and longquan rl rat coronavirus to this reference dataset we added 47 whole genomes of the current coronavirus 2019 2019-ncov outbreak that originated in wuhan china in december 2019 the 2019-ncov sequences were downloaded from the gisaid database httpswwwgisaidorg together with annotation of its original location collection date and originating and submitting laboratory the 2019-ncov data generators are properly acknowledged in the acknowledgements section of this paper and detailed information is provided in supplementary table 1 the 431 reference wgs were aligned with muscle edgar 2004 the alignment was manually edited until a codon alignment was attained in all coding sequences cds a maximum likelihood phylogenetic tree 1000 bootstrap replicates was constructed in phyml guidon  gascuel 2003 lemoine et al 2018 and a bayesian tree using mrbayes ronquist  huelsenbeck 2003 were constructed the trees were visualized in figtree rambaut 2018 we selected 25 reference sequences that represent the diversity of each well-defined phylogenetic cluster with bootstrap support of 100 and posterior probability of 1 we identified five well supported phylogenetic clusters with more than two sequences of sarsr-cov and used them to set up our automated phylogenetic classification tool cluster 1 included sars strains from the 2002 and 2003 asian outbreaks in our tool we named this cluster sars-cov outbreak 2000s but may rename it as sars-a if a new proposed naming system for sarsr-cov is adopted in the near future rambaut 2020 cluster 2 provisionally named as sars related cov includes 7 sequences from bats which did not cause large human outbreaks cluster 3 named as bat sars-covhku3 includes three wgs sampled from rhinolophus sinicus ie chinese rufous horseshoe bats cluster 4 bat sars-cov zxc21zc45 includes two sarsr-cov sampled from rhinolophus sinicus bats in zhoushan china cluster 5 provisionally named as wuhan 2019-ncov which may be renamed as sars-b includes one public sequence from the outbreak in wuhan china we identified this cluster with many sequences from gisaid but kept only this one as this is the only genbank sequence accession number mn908947 which was kindly shared by prof yong-zhen zhang and colleagues in the virologicalorg website detailed information about the phylogenetic reference datasets are available in supplementary table 2 the phylogenetic reference dataset was used to create an automated coronavirus typing tool using the genome detective framework vilsker et al 2019 fonseca et al 2019 to determine the accuracy of this tool each of the 431 test wgs was considered for evaluation ie 384 reference sequences from vipr and 47 public 2019-ncov sequences the sensitivity specificity and accuracy of our method was calculated for both species assignment and phylogenetic clustering of sarsr-cov sensitivity was computed by the formula tptpfn specificity by tntnfp and accuracy by tptntpfpfntn where tp  true positives fp  false positives tn  true negatives and fn  false negatives classifying query sequences in an automated fashion involves two steps the first step enables virus species assignments and the second which is restricted to sarsr-cov includes phylogenetic analysis the first classification analysis subjects a query sequence to blast and aga analysi aga is a novel alignment method for nucleic acid sequences against annotated genomes from ncbi refseq virus database aga deforche 2017 expands the optimal alignment algorithms of smith-waterman smith  waterman 1981 and gotoh gotoh 1982 based on an induction state with additional parameters the result is a more accurate aligner as it takes into account both nucleotide and protein scores and identifies all of the polymorphisms at nucleotide and amino acid levels in the second step a query sequence is aligned against the phylogenetic reference dataset using -add alignment option in the mafft software katoh  standley 2013 in addition a neighbor joining phylogenetic tree is constructed using the hky distance metric with gamma among-site rate variation with 1000 bootstrap replicates using paup swofford the query sequence is assigned to a particular phylogenetic cluster if it clusters monophyletically with that clade or a subset of it with bootstrap support 70 if the bootstrap support is 70 the genotype is reported to be unassigned the result of the phylogenetic and mutational analysis performed by aga is available in a detailed report this report contains an interactive phylogenetic tree and genome mapper supplementary figure 1 it also presents the virus species and cluster assignments and a detailed table that provides information about open reading frames orfs cds and proteins this table can be expanded to show nucleotide and amino acid mutations that differentiate a query sequence from their species refseq or from a sequence in the phylogenetic reference dataset all results can be exported to a variety of file formats xml csv excel nexus or fasta the genome detective coronavirus typing tool correctly classified all of the 175 sarsr-cov sequences at species level ie specificity sensitivity and accuracy of 100 furthermore all of the 47 2019-ncov wgs that were isolated in china n36 usa n  5 france n2 thailand n2 japan n1 and taiwan n1 were correctly classified at phylogenetic cluster level as 2019-ncov which may be renamed as sars-b in addition we classified with very high specificity sensitivity and accuracy ie 100 all of the 112 sars outbreak wgs of 2002 and 2003 we also achieved perfect classification ie specificity sensitivity and accuracy of 100 for all of beta coronavirus humancoronavirushku1 mers-cov rousettusbatcoronavirushku9 and tylonycterisbatcoronavirushku4 at species level for a detailed overview of assignment performance please refer to the supplementary table 3 our tool also allows detailed analysis of coding regions and proteins for each of the coronavirus species for example the analysis of the first released 2019-ncov sequence the whhuman1china2019dec genbank mn908947 demonstrated at genome level the nucleotide nt identity was 790 to the reference strain of sarsr-cov accession nc0047183 and that the envelop small membrane protein protein e is the most similar protein in total 948 7377 of the amino acids were identical the four amino acid differences were located at positions 55 t55s 56 v56f 69 69deletion and 70 g70r the spike protein protein s which can be associated with virulence was 762 identical to the reference strain of sarsr-cov supplementary table 4a interestingly there were four amino acid insertions at position 237 a237f238inshrsy genome nt position 2220222203inscatagaagttat which is just upstream from a cleavage site the most diverse coding regions were the cds sars8a and sars8b in these two regions only 30 of the amino acids were identical sars8b protein was truncated early and its cds had four stop codons supplementary table 4sa our coronavirus typing tool also allows a query sequence to be analysed against a sequence in the phylogenetic reference dataset for example the whhuman1china2019dec genbank mn908947 the identity was 875 to the bat sequence batslcovzxc21 genbank mg772934 this was one of the bat-cov sequences that were most related to n2019-cov lu et al 2020 the envelop small membrane protein protein e was 100 identical supplementary table 4b when the 2019-ncov isolated from france betacovfranceidf03732020 was analysed with our tool and compared with the 2019-ncov whhuman1china2019dec strain accession mn908947 this sequence was 999 identical and had only two nt mutations supplementary table 4c these two differences were located on positions 22551gt  26016gt which caused three amino acid mutations e2 glycoprotein protein mutation v354f 22551gt sars3a protein mutations g250v 26016gt and sars3b protein mutations v110f 26016gt detailed in supplementary table 4c-ii the analysis of a wgs in fasta format takes approximately 60 seconds we developed and released the genome detective coronavirus typing tool as a free-of-charge resource in the third week of january 2020 in order to help the rapid characterization of ncov-2019 infections this tool allows the analysis of whole or partial viral genomes within minutes it accepts assembled genomes in fasta format or raw ngs data in fastq format from illumina ion torrent pacbio or oxford nanopore technologies ont can be submitted to the genome detective virus tool vilsker et al 2019 to automatically assemble the consensus genome prior to executing the coronavirus typing tool user effort is minimal and a user can submit multiple fasta sequences at once the tool uses a novel and dynamic aligner aga to allow submitted sequences to be queried against reference genomes using both nucleotide and amino acid similarity scores this allows accurate identification of other coronavirus species and the tracking of new viral mutations as the outbreak expands globally it also performs detailed analysis of the coding regions and proteins moreover it can easily be updated to add new phylogenetic clusters if new outbreaks arise or if the classification nomenclature changes the tool has been able to correctly classify all the recently released ncov-2019 genomes as well as all the 20022003 sars outbreak sequences in conclusion the genome detective coronavirus typing tool is a web-based and user-friendly software application that allows the identification and characterization of novel coronavirus genomes  a citizen science initiative for open data and visualization of covid-19 outbreak in kerala india collective for open data distribution-keralam codd-k  codd-k authors list jijo pulickiyil ulahannan  jijo ulahannan pulickiyil   nikhil narayanan nishad thalhath prem prabhakaran sreekanth chaliyeduth sooraj suresh p musfir mohammed sindhu joseph akhil balakrishnan jeevan uthaman manoj karingamadathil sunil thomas thonikkuzhiyil unnikrishnan sureshkumar shabeesh balan neetha vellichirammal nanoth  india-the second most populated country in the world-reported its first case in the state of kerala with a travel history from wuhan subsequently a surge of cases was observed in the state mainly through the individuals who traveled from europe and the middle east to kerala thus initiating an outbreak since public awareness through dissemination of reliable information plays a significant role in controlling the spread of the disease the department of health services government of kerala initially released daily updates through daily textual bulletins however this unstructured data requires refinement and enrichment for upstream applications such as visualization andor analysis here we reported a citizen science initiative that leveraged publicly available and crowd-verified data on covid-19 outbreak in kerala from the government bulletins supplemented with the information from media outlets to generate reusable datasets this data was further used to provide real-time analysis and daily updates of covid-19 cases in kerala through a user-friendly bilingual dashboard httpscovid19keralainfo for non-specialists we ensured longevity and reusability of the dataset by depositing it in a public repository aligning with open source principles for future analytical efforts finally to show the scope of the sourced data we also provided a snapshot of outbreak trends and demographic characteristics of the individuals affected with covid-19 in kerala during the first 99 days of the outbreak we acknowledge shane reustle for his help and support for forking the japan covid-19 coronavirus tracker repository and implementation of the dashboard we thank jiahui zhou for the original concept and design of the tracker we also thank sajjad anwar for generously providing the administrative boundary shapefiles and geojsons for kerala maps were generously provided by the mapbox community team  in december 2019 an outbreak of cases presenting with pneumonia of unknown etiology was reported in wuhan china the outbreak caused by a novel severe acute respiratory syndrome coronavirus-2 sars-cov-2 later evolved as a pandemic coronavirus disease 2019  claiming thousands of lives globally 1 2 3  initial studies revealed the clinical and prognostic features of covid-19 along with its transmission dynamics and stressed the need for implementing public health measures for containment of infection and transmission among the population at high-risk 2 4 5 6 7 8  in response to this several countries have implemented measures including travel restrictions and physical distancing by community-wide quarantine 2 5 9  these extensive measures were imposed taking into consideration the lack of adequate testing kits for detection a vaccine or proven antivirals for preventing or treating this disease along with reports of considerable strain on the health system leading to unprecedented loss of human life india-the second most populated country in the world-reported its first case in the state of kerala on january 30 2020 among individuals with travel history from wuhan the epicenter of the covid-19 outbreak 10  with the subsequent reports of an outbreak in the middle east and europe kerala has been on high-alert for a potential outbreak as an estimated 10 of the population work abroad and being an international tourist destination 11 12  the state has a high population density with a large proportion of the population falling in the adult and older age group 13  the population also shows a high incidence of covid-19-associated comorbidities such as hypertension diabetes and cardiovascular disease 14 15 16 17  as evidenced by reports of other countries these factors pose a significant threat for an outbreak and placing a tremendous burden on the public healthcare system 18 19 20  severe public health measures were implemented in the state of kerala and across india to prevent an outbreak international flights were banned by march 22 2020  and a nation-wide lockdown was initiated on march 25 2020 21  however before these measures were implemented several cases including travelers from europe and the middle east along with a few reports of secondary transmission were reported in kerala since the first case was reported the department of health services dhs government of kerala initiated diagnostic testing isolation contact tracing and social distancing through quarantine and the details of cases were initially released for the public through daily textual bulletins however when the outbreak evolves in an unprecedented manner the manual reporting process on outbreak trends and consolidated data in real-time for the public could be unsustainable for pandemics such as covid-19 public awareness via dissemination of reliable information plays a significant role in controlling the spread of the disease besides real-time monitoring for identifying the magnitude of spread helps in hotspot identification potential intervention measures resource allocation and crisis management 22  to this end collection of relevant information on infection and refining the dataset in a structured manner for the upstream purposes such as visualization andor epidemiological analysis is essential open or crowd-sourced data has immense potential during early stage of an outbreak considering the limitation of obtaining detailed clinical and epidemiological data in real-time during an outbreak 23 24 25  furthermore the structured datasets when deposited in open repositories and archived can ensure longevity for future analytical  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 18 2020 the multi-sourced data was refined to make a structured live dataset to provide realtime analysis and daily updates of covid-19 cases in kerala through a user-friendly dashboard httpscovid19keralainfo we aimed to disseminate the data of the outbreak trend hotspots maps and daily statistics in a comprehensible manner for nonspecialists with bilingual malayalam and english interpretation next we aimed for longevity and reusability of the datasets by depositing it in public repositories aligning with open source principles for future analytical efforts finally to show the scope of the sourced data we provided a snapshot of outbreak trends and demographic characteristics of the individuals affected with covid-19 in kerala during the first 99 days of the outbreak the collective defined the data of interest as minimal structured metadata of the covid-19 infections in kerala covering the possible facets of its spatial and temporal nature excluding the clinical records the resulting datasets should maintain homogeneity and consistency assuring the privacy and anonymity of the individuals the notion of this data definition is to make the resulting datasets reusable and interoperable with similar or related datasets a set of controlled vocabularies were  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 18 2020  formed as a core of this knowledge organization system to reduce anomalies prevent typographical errors and duplicate entries together with the controlled vocabularies identifiers of individual entries in each dataset make the datasets interlinked an essential set of authority control is used in populating spatial data to make it accurate in the naming and hierarchy a substantial set of secondary datasets were also produced and maintained along with the primary datasets including derived and combined information from the primary datasets and external resources we primarily sourced publicly available de-identified data released daily as textual bulletins from january 31 2020 by the dhs government of kerala india httpsdhskeralagovin of the individuals who were diagnostically confirmed positive for sars-cov-2 by reverse transcription-polymerase chain reaction rt-pcr at the government-approved test centers we also collected and curated reports from print and visual media for supplementing the data supplementary methods the quality of the data in terms of veracity and selection bias has been ensured as described supplementary methods utmost care was taken to remove any identifiable information to ensure the privacy of the subjects entries were verified independently by codd-k data validation team members and rectified for inconsistencies  figure 1  since the data collected were publicly available no individual consent and ethical approval were required for the study to demonstrate the utility of the collected dataset we provided the status of the first 99 days between january 30 2020 and may 7 2020 of the covid-19 outbreak in kerala and also described demographic characteristics of the individuals affected with covid-19  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020051320092510 doi medrxiv preprint which serves as an api for the frontend the api fetches data from the google sheet and generates json files periodically with github actions both the web application and the api were created with javascript as the programming language and maintained using nodejs these portals use static-file assets without any server-side technologies the website and the api are served through github pages a free static web hosting service provided by github  figure 2  the resulting open-data sets are published under open data commons attribution license v10 odc-by 10 a manually curated data archive is maintained as a github repository for the provenance 27  the datasets are provided with the schema definition and an actionable data-package declaration 28  periodic versioned snapshots were released as covid19keralainfo-data 29  codd-k manages the longevity and stewardship of the data sufficient documentation is provided to increase the adaptability of the datasets here we have collected cleaned and visualized publicly available data in a user- cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  since the sars-cov-2 infection outbreak occurs in clusters early identification and isolation of these clusters are essential to contain the outbreak accurate tracking of the new cases and real-time surveillance is essential for the effective mitigation of covid-19 however the daily public bulletins by dhs did not have any unique identification code for the covid-19 infected individuals and also for secondary contacts who have  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity management authority and were updated daily we manually curated the hotspot information from the dhs bulletins and plotted them on to the map in the web application for easy visualization to the best of our knowledge this feature is unique to our dashboard the back-end data was automated to fetch details of lsg names along with the district when the lsg-id was provided these locations were highlighted as red dots with details on the hotspot and when zoomed the lsg administration area will be displayed within each district on the map we also provided a toggle bar to visualize district-wise zones and areas declared as hotspots at lsg resolution  figure 3c  owing to the lack of data available in the public domain additional information such as the number of active cases in these hotspots could not be plotted we plan to integrate this feature in the future versions when the data becomes available  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020051320092510 doi medrxiv preprint to understand the outbreak trend and demographic characteristics of the covid-19 infections in kerala we analyzed the dataset for the first 99 days of the outbreak from january 30 2020 to may 07 2020 during this period kerala reported 502 cases of which 462 individuals recovered during the reported period along with three fatalities kerala was the first state in india to report the flattening of the curve for covid-19 infection  figure 3a  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg101101202005 1320092510 doi medrxiv preprint the fatality rate of kerala was 06 which was one the lowest among indian states and the individuals who succumbed were four months 61 and 68 years of age with a clinical history of comorbidities in this report we describe a citizen science initiative that leveraged publicly available unstructured covid-19 reports released daily by the government supplemented with news from media outlets and structured this into a knowledge bank for quick and easy interpretation through a user-friendly dashboard to the best of our knowledge we were the first to host a visualization dashboard for covid-19 specifically for kerala with a bilingual interface and unique features such as hotspot mapping which circumvents the paucity of data on the secondary transmission of infection we reason that accurate information about the pandemic has made the public vigilant and to adopt appropriate precautionary measures in preventing the outbreak our dashboard also has contributed to achieving this feat as evidenced by the usage statistics outbreak statistics have indicated wide popularity and impact of these community-led initiatives 23 24 25  however our approach differed from those as we sourced unstructured official data released by the government supplemented by the information from media  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  outlets this not only ensures authenticity but enriches the data available in the public domain into a structured dataset while this approach seems to be efficient an unexpected surge in cases can overturn the data collection thus limiting the feasibility kerala has effectively utilized this opencrowd-sourcing platform using citizen-led initiatives during the recent floods that devastated the state during 2018 and 2019 30 31 32  this initiative brought together residents and non-resident keralites through social media platforms to coordinate rescue missions during the crisis our collective codd-k evolved as a result of crowd-sourced volunteering and coordination during the floods in kerala from 2018 and continued its efforts during the covid-19 pandemic experts from various domains assembled and volunteered to source data build the dataset visualize distribute and interpret the data on infection outbreak through this collective our dataset compiled between january 30 2020 to may 07 2020 indicate that the infections reported in kerala were mainly among working-age men with travel history the absence of reported community spread in this period emphasizes the effectiveness of government implemented rapid testing and quarantine measures active tracking and isolation of cases with travel history lead to the flattening of the curve within 60 days of the first report along with minimal covid-19-associated death the majority of cases reported in kerala were within the age group of 20-40 years attributing to a better outcome furthermore all the patients were in constant inpatient care possibly contributing to the lesser mortality rate kerala implemented vigorous covid-19 testing and even though the test rate was relatively low 1098 tests per million of the population early testing combined with strict quarantine policies for individuals with travel history prevented community spread however the average  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity nevertheless this multi-sourced open data approach also has inherent limitations including issues with the veracity of data owing to the anonymity and depth of the data released including clinical symptoms since each infected case identified in kerala was not provided with a unique id it was impossible to20 track these cases for the assessment of vital epidemiological parameters like the reproduction number r0 based on our experience of collating and analyzing covid-19 data from the public  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020  domain in kerala we propose to frame specific guidelines for the public data release for covid-19 or other epidemics we recommend the release of official covid-19 data in a consistent structured and machine-readable format in addition to the bulletins which could be provided with a permanent url and also archived in a public repository for future retrospective analyses we also suggest releasing the assigned unique id for the individuals affected with covid-19 to avoid inconsistencies in reporting and to enable tracking the secondary transmission furthermore providing covid-19 associated symptomatic information without compromising the privacy of the infected individuals will also aid in the basic understanding of the disease through analytical approaches collectively we report a citizen science initiative on the covid-19 outbreak in kerala to collect data in a structured format utilized for visualizing the outbreak trend and describing demographic characteristics of affected individuals while the core aim of this initiative is to document covid-19 related information for the public researchers and policymakers an efficiently implemented data visualization tool also alleviates the citizens anxiety around the pandemic in kerala we hope that the dataset collected will form the basis for future studies supplemented with detailed information on clinical epidemiological parameters from individuals with covid-19 infection in kerala  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 18 2020 is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg1011012020051320092510 doi medrxiv preprint  cc-by-nc 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 18 2020  httpsdoiorg101101202005 1320092510 doi medrxiv preprint   spatiotemporal fluctuation scaling law and metapopulation modeling of the novel coronavirus covid-19 and sars outbreaks zhanshan  sam ma  we comparatively analyzed the spatiotemporal fluctuations of the 2019-novel coronavirus and sars outbreaks to understand their epidemiological characteristics methodologically we introduced tpl taylors power law to characterize their spatiotemporal heterogeneitystability and hubbells 2001 unified neutral theory of biodiversity untb specifically harris et al 2015 hdp-msn model hierarchical dirichlet process-multi-site neutral to approximate the metapopulation of coronavirus infections first tpl analysis suggested that the coronaviruses appear to have a specific heterogeneitystability scaling parameter tpl-b slightly exceeding 2 for cumulative infections or exceeding 1 for daily incremental infections suggesting their potentially chaotic unstable outbreaks another tpl parameter m 0  ie infection critical threshold depends on virus kinds covid-19sars time disease-stages space regions and public-health interventions eg quarantines and mobility control m 0 measures the infection level at which infections are random poisson distribution and below which infections follow uniform distribution and may die off if m 0 coincides or below the level of allee effects for example m 0 5758 covid-19 china or 196m 0 x34 in total vs m 0 2701 covid-19 world vs m 0 9475 sars world suggested that the potentially stabilized infection level of covid-19 in china is nearly  lower than that of sars worldwide but twice higher than that of covid-19 worldwide this may indicate that covid-19 outbreak seems nearly twice more risky than sars and the lower infection threshold may be due to its lower lethality than sars since lower fatality rates can facilitate the survival and spread of pathogen second metacommunity untb neutrality testing seems appropriate for approximating metapopulation of coronavirus infections specifically two parameters  and m borrowed from neutral theory may be used to assess the relative 2 significance of infection through local contagion vs infection through migration both of which may depend on time space virus kinds and particularly public-health interventions third comparing the levels of tpl-m 0 and  may leverage the complementary nature of tpl analysis and metapopulation modeling for example their difference -m 0 06 for the covid-19 in china confirmed the critical importance of controlling migration mobility in suppressing the outbreak although migration may have been a primary driving force for the initial outbreaks in several regions of china  the spatial andor temporal distributions of many biological populations including microbes and humans follows taylors power law tpl taylor 1961  taylor et al 1977  1983  1988  and recent studies have also revealed its applicability at the community scale ma 2012a  li  ma 2019  taylor 2019  ma  taylor 2020  tpl has been verified by hundreds if not thousands of field observations in nature taylor 2019 and it has also found cross-disciplinary applications beyond its original domains of biology and ecology in disciplines such as computer science natural disaster modeling and experimental physics eisler et al 2008  ma 2012b  tippett  cohen 2016  helmrich et al 2020  despite that tpl was proposed more than a half century ago taylor 1961 and there is not yet a consensus on the underlying mechanisms generating tpl eisler 2008  stumpf  porter 2012  there seems to be a recent resurgence of interests in this near universal model that captures the relationship between the population mean m and variance v in the form of a simple power function ie vam b  eg cohen  schuster 2012  cohen  xu 2015  giometto et al 2015  kalinin et al 2018  oh et al 2016  reuman et al 2017  among the numerous existing studies on tpl there have already been its applications to the analyses of spatial variation of human population cohen et al 2013  human mortality bohk et al 2015  and epidemiology rhodes  anderson 1996  given these previous applications to human demography and epidemiology we postulate that tpl should also be applicable to the outbreak analyses of coronavirus infection diseases such as the still ongoing 2019 novel coronavirus 2019-ncov or covid-19 coronavirus-infected pneumonia disease httpswwwwhointemergenciesdiseasesnovel-coronavirus-2019 li et al 2020  thompson et al 2020  kucharski et al 2020  zhang et al 2020 and 2003 sars severe acute respiratory syndrome httpswwwwhointcsrsars in the present report we test this hypothesis and further explore possible epidemiological processes mechanisms underlying the outbreak of covid-19 infections while tpl can be harnessed to investigate the spatiotemporal fluctuations of coronaviruses specifically the scaling changes law of coronaviruses infections over space and time we also aim to understand the spread of the virus infections from both local contagion endemic and external migration epidemic and pandemic perspectives nevertheless this can be rather challenging given the lack of controlled experimental data which is ethically infeasible to collect obviously in principle all of the infections existing globally constitute a metapopulation of people infected by the coronavirus but constructing standard epidemiological models eg wang et al 2018  rivers et al 2019 with existing data is rather difficult we realized that hubbells 2001 neutral theory of biodiversity which is one of the four major metacommunity models the other three include species sorting mass effect and patch dynamics rosindell et al 2011 rosindell et al  2012 vellend 2010 vellend  2016  might be adapted to approximate the meta-population dynamics this approximation allows us to obtain to the minimum an educated guess for the local contagion spread and global dispersal migration parameters of the coronavirus infections overall this study sets two primary objectives i to investigate the spatiotemporal fluctuation scaling law and ii to obtain an educated guess for the local contagion spread and global migration parameters of the covid-19 infections in addition we also perform comparative analyses with the sars to get more general insights on the epidemiology of coronavirus infections to the best of our knowledge this should be the first systematic application of tpl and untb in epidemiology and obtained scalingcontagionmigration parameters should also be of significant biomedical importance we collected the worldwide daily incremental and cumulative infections of 2019 novel coronavirus and sars respectively for the datasets collected in china the unit of data collections was set to chinese provinces in addition for the covid-19 infections we also collected the datasets of 17 cities of hubei province of china for the worldwide covid-19 infections the unit of data collections was set to country or region recognized by the who world health organization the date range for collecting the sars data was between march 17 and august 7 of 2003 136 days and that for covid-19 was between january 19 and feb 29 2020 40 days since the covid-19 infections are still continuing the analyses conducted in this report may be updated periodically taylor 1961 discovered that the relationship between mean abundance m and corresponding variance v of biological populations follows the following power function where b is termed population aggregation parameter and is thought to be species-specific and a is initially thought to be related to sampling schemes used to obtain the data the relationship is known as taylor  tpl was initially discovered in fitting the spatial or cross-sectional sampling data taylor 1961 and later found that it is equally applicable to temporal or time-series sampling data taylor 2019 in the context of time-series modeling b measures the population stability variability more recently it was found that tpl can be extended to community level from its original population level ma 2015  at the community level the four taylors power law extensions tple can be used to measure the community spatial temporal heterogeneity  on both the general principle of tpl explained above and system-or data-specific information such as the biology of covid or sars specifically regarding the first aspect or the fitting of tpl we adopt two fitting approaches one is the simple linear regression via log-transformation eqn 2 and another is the geometric mean regression gmr clark and perry 1995 warton et al 2007  the advantage of the first approach is its computational simplicity and the advantage of the second or gmr is that it is more robust for small sample size n15 according to clark and perry 1995  both approaches preserve the scale invariance of power law regarding the second aspect or the interpretation of the tpl there is controversy on the claim that tpl parameter b is species specific in particular when there are changes in sampling method life stage environment or spatial scale taylor et al 1988 clark and perry 1995  our opinion is that unlike parameter a parameter b is primarily shaped by evolutionary forces and less influenced by ecological or environmental factors however we do not take the invariance or constancy at ecological time scale or with environmental factors as granted instead we draw conclusions based on rigorous statistical tests of the differences in parameter b among treatments in the case of this study we perform the permutation randomization test to judge whether or not the tpl parameters are invariant for further information on the randomization test readers are referred to collingridge 2013  to further harness the tpl parameters ma 1991 ma   2012a ma   2015 derived a third parameter m 0  for tpl or its extensions at the community scale population aggregation critical density at the population scale or community critical heterogeneity at the community scale which is in the form of where a  b are tpl parameter m 0 is the level of mean population abundance the covid-19 or sars infection level in the case of this study at which the fluctuation dynamics of virus infection is random following poisson statistical distribution generally when mm 0  the population infection fluctuation dynamics is more irregular than random often following highly skewed distributions such as the negative binomial distribution or power-law statistical distribution in this case population is highly unstable and the infection may expand continuously when mm 0  the infection fluctuation is regular and may follow the uniform statistical distribution in this case the inflection level should be stabilized or might even die off when mm 0  the infection is random and should follow poisson distribution statistically in the context of this study we term pacd or m 0 as mean infection critical threshold or mean infection threshold which is similar to classic allee effects allee 1927  where     1  s  is the relative frequency of each species in the metacommunity at the metacommunity level a dirichlet process is also applicable and the metacommunity distribution can be modeled with a stick breaking process ie  stick  given that both local community and metacommunity follow dirichlet processes the problem can be formulated as a hierarchical dirichlet process hdp in the domain of machine learning teh et al 2006  harris et al 2015  furthermore dirichlet process dp can be formulated as the so-called chinese restaurant process from which antoniak equation antoniak 1974  can be derived the antoniak equation represents for the number of types or species s observed following n draws from a dirichlet process with concentration parameter  and is with the following form where sn s is the unsigned stirling number of the first kind and  is the gamma function by treating the coronavirus infections at different sites eg provinces of china or different countriesregions of the world in this study as a metacommunity consisting of n local communities eg each local community corresponding to a province the above-described metacommunity model can be built with the dataset of daily incremental infections nationally or internationally different from traditional metacommunity concept here the metacommunity is actually a metapopulation consisting of n local populations however if we treat the local infections at a particular time point eg day as a local sub-population then the virus subpopulations at different time points can be considered as a total population or species in the terminology of community ecology with this conceptual transformation the concept and models for metacommunity and hubbells untb can be readily applied to the metapopulation of coronavirus infections without a need to revise the models with this adaptive scheme the fundamental biodiversity number speciation rate  from the previously introduced hdp-msn model can be used to approximate the average local contagion infection rate similarly the fundamental dispersal number m can be used to approximate the average infection rate through migration the migration probability m which is a function of m has a similar interpretation as m but simply in the form of probability as a side note we expect that the coronavirus infections should follow hubbells 2001 untb theory which is not surprising to us due to our treatment of metapopulation as metacommunity this is because the species in our metacommunity are in fact populations of a single virus species and they should be equivalent in terms of the neutral theory therefore whether or not  from tables 1-2 which were summarized from tables s1-s8 in the osi online supplementary information we obtain the following findings which are also illustrated with figs 1-7 i the spatiotemporal fluctuation scaling changes of covid-19 infections follow tpl taylors power law at all scales schemes tested including world-wide country-wide cumulative and daily incremental infections as well as gradually shrunken partial datasets to test tpl robustness as evidenced by the p-value0001 from the tpl fittings the brief results were summarized in table 1 and the detailed results were displayed in tables s1-s2 and s4-s7 it was discovered that the infections of covid-19 like populations of other organisms follow seeming universal power law this implies that the infection of the novel coronavirus is highly contagious and its outbreak spread is chaotically unstable in general as indicated by the b-values exceeding 1 for daily incremental infection or exceeding 2 for cumulative infections ii the tpl aggregation stability parameter b for covid-19 seems rather stable or even invariant as evidenced by the randomization tests tables s3  s8  the randomization tests were performed by statistically comparing the model parameters fitted with the whole datasets and those fitted with the partial datasets the schemes of partial datasets were devised by removing in china is about twice higher than that of the worldwide but about  lower than that of worldwide sars infections in other words the covid-19 infections seem more dangerous than sars from a public health perspective note that m 0 is the mean infection level therefore it can be more meaningful to convert it into absolute value from a biomedical perspective for example m 0 5758 for covid-19 in china when converted to total national infection level would be 5758x34 provinces196 this threshold number may suggest that when the number of total infections nationally is at this level the infections are random when infections exceed this threshold level the infections can be nonrandom and highly unstable when infections are below this threshold level the infection should be rather stable following a uniform distribution statistically or may even die off if the threshold level coincides with the level of allee effects since the level of allee effects is still unknown whether or not the infections under m 0 will die off is still an open question iv as expected all datasets passed the neutrality tests of hdp-msn model as indicated by the p-value005 table 2  we use the ratio of qm as a measure of the relative importance of infection spread via migration vs infections spread via local contagion in spreading the infections with larger q indicating higher migration role and smaller q indicating higher local contagion the ratio of q133581632521 table 2 indicating that spread via migration is approximately 21 times more significant than spread via local contagion on average nationally in china however the worldwide q ratio is approximately q1 803710801 therefore the ratio q is dependent on time disease stages space regions and disease-kinds covid-19 or sars and perhaps most importantly public-health interventions such as quarantines andor mobility control compared with covid-19 sars appeared to exhibit a different pattern of the relative importance of migration vs local contagion in spreading the infections we postulate that this difference might signal the higher risk of pandemics of covid-19 compared with sars nevertheless we cannot exclude the possibility that the range of sars datasets were complete while covid-19 infections are not over yet the third parameter m or immigration probability suggests the level of infection via migration the m0052 in china vs m0003 worldwide for covid-19 suggested that the risk of infection via migration within china is approximately 10 times higher than that of worldwide migration this is most likely due to the disruption of international travels comparing the m for covid-19 and sars for worldwide data 0003 for covid-19 vs 0040 for sars may simply be due to stronger travel restrictions imposed for controlling covid-19 outbreaks similar to the above comparison this difference may be due to the difference in the data range v comparing the pacd m 0   table 1  and  in table 2 table 1 and 6325 of covid-19 in china table 2 is only approximately 06 we postulate that when the local speciation contagion  approximates the population aggregation critical density or infection critical threshold m 0  the infections could become random suggesting a potentially stabilized infection level that is without inputs from external migration local infections via local contagion measured by  could become random as indicated by m 0  if this postulation is true then it may suggest that the mobility control such as travel restrictions or quarantines can be critically effective in stabilizing outbreaks this finding indicates the complementary nature of the two approaches we adopted in this study nevertheless it is important to reiterate that this closeness between m 0 and  m 0  is likely to be an exception rather than the norm for the reasons explained below as to the lack of closeness between m 0 and  of covid-19 at worldwide scale m 0 2701 vs 10801 this may indicate that control mobility is not sufficient to stabilize infections world widely anymore at the current stage in the case of sars the worldwide m 0 9475 vs 27169 indicated that local contagion  alone already exceed the infection critical threshold m 0  therefore   m 0 pacd may be the norm we aimed to discover critical insights on the endemicepidemicpandemic characteristics of the while tpl-b appears rather stable or even invariant with time andor space the population aggregation critical density pacd ie m 0 or infection critical threshold which is the level at which infections are random and below which infections may be stabilized can depend on disease kinds covid-19 or sars time disease or outbreak stages and space regions we postulate that m 0 should also be influenced by public-health interventions such as quarantines and travel restrictions therefore m 0 can be an important epidemiological parameter for evaluating the characteristics of disease outbreaks for example covid-19 exhibited significantly lower threshold m 0  than sars suggesting a potentially lower infection tolerance threshold of covid-19 nevertheless it should be cautioned that the tolerance threshold only means the level of random infections which may signal the level of stabilized infections however whether or not the tolerance threshold is biometrically tolerable or safe depends on other biomedical characteristics among which allee effects can be a critical factor to determine whether or not the infections will die off or persist we postulate that if m 0 coincide or is below the level allee effects in action the infections may die off second all datasets we tested easily passed the neutrality test with hdp-msn and indicated that the ratio of qm may be used as a measure of the relative importance of infection spread via migration vs infections spread via local contagion in spreading the infections with larger q indicating higher migration role and smaller q indicating higher local contagion third both the tpl scaling law and metapopulation modeling may complement each other the difference closeness between m 0 the infection critical threshold from tpl and  local contagion or speciation rate from metapopulation modeling may signal the effectiveness of completely blocking the migration dispersal in spreading infections for example in the case of covid-19 infections in china both parameters m 0 and  are rather close to each other and their difference is only approximately 06 suggesting that without external inputs the infections from local contagion is only approximately 06 higher than the infection critical threshold m 0  this makes the mission of controlling local contagion for stabilizing infections much less challenging than the mission when   m 0 finally we suggest that the approaches demonstrated previously should be of general applicability for epidemiological research in particular we consider tpl-b can be a pathogen specific parameter primarily shaped by evolutionary forces another tpl parameter ma 1991 ma   2015  the covid-19 confirmed infection datasets during january 19 and february 29 were collected from httpsnewsqqcomzt2020pagefeiyanhtm and httpsnewsifengcomcspecial7tpldszdgvk the worldwide sars infections were from the who httpswwwwhointcsrsarscountryen tables 1-8   table s1a  fitting the tpl to the daily cumulative covid-19 infections in the whole china with two schemes fitting with the full datasets or fitting with the gradually shrinking datasets   -satellite an ai-driven system and benchmark datasets for hierarchical community-level risk assessment to help combat covid-19 yanfang ye shifu hou yujie fan yiyue qian yiming zhang shiyu sun qian peng kenneth laparo  the novel coronavirus and its deadly outbreak have posed grand challenges to human society as of march 26 2020 there have been 85377 confirmed cases and 1293 reported deaths in the united states and the world health organization who characterized coronavirus disease covid-19 -which has infected more than 531000 people with more than 24000 deaths in at least 171 countries -a global pandemic a growing number of areas reporting local sub-national community transmission would represent a significant turn for the worse in the battle against the novel coronavirus which points to an urgent need for expanded surveillance so we can better understand the spread of covid-19 and thus better respond with actionable strategies for community mitigation by advancing capabilities of artificial intelligence ai and leveraging the largescale and real-time data generated from heterogeneous sources eg disease related data from official public health organizations demographic data mobility data and user geneated data from social media in this work we propose and develop an ai-driven system named -satellite as an initial offering to provide hierarchical community-level risk assessment to assist with the development of strategies for combating the fast evolving covid-19 pandemic more specifically given a specific location either user input or automatic positioning the developed system will automatically provide risk indexes associated with it in a hierarchical manner eg state county city specific location to enable individuals to select appropriate actions for protection while minimizing disruptions to daily life to the extent possible the developed system and the generated benchmark datasets have been made publicly accessible through our website 1  the system description and disclaimer are also available in our website  coronavirus disease covid-19 34 is an infectious disease caused by a new virus that had not been previously identified in humans this respiratory illness with symptoms such as a cough fever and pneumonia was first identified during an investigation into an outbreak in wuhan china in december 2019 and is now rapidly spreading in the us and globally the novel coronavirus and its deadly outbreak have posed grand challenges to human society as of march 26 2020  there have been 85377 confirmed cases and 1293 reported deaths in the us figure 1  and the who characterized covid-19 -which has infected more than 531000 people with more than 24000 deaths in at least 171 countries -a global pandemic it is believed that the novel virus which causes covid-19 emerged from an animal source but it is now rapidly spreading from personto-person through various forms of contact according to the centers for disease control and prevention cdc 4  the coronavirus seems to be spreading easily and sustainably in the community -ie community transmission which means people have been infected with the virus in an area including some who are not sure how or where they became infected an example of community transmission that caused the outbreak of covid-19 in king county at washington state wa is shown in figure 2  the challenge with community transmission is that carriers are often asymptomatic and unaware that they are infected and through their movements within the community they spread the disease according to the cdc before a vaccine or drug becomes widely available ie this is the case for covid-19 by far community mitigation which is a set of actions that persons and communities can take to help slow the spread of respiratory virus infections is the most readily available interventions to help slow transmission of the virus in communities 5  a growing number of areas reporting local sub-national community transmission would represent a significant turn for the worse in the battle against the novel coronavirus which points to an urgent need for expanded surveillance so we can better understand the spread of covid-19 and thus better respond with actionable strategies for community mitigation unlike the 1918 influenza pandemic 2 where the global scope and devastating impacts were only determined well after the fact covid-19 history is being written daily if not hourly and if the right types of data can be acquired and analyzed there is the potential to improve self awareness of the risk to the population and develop proactive rather than reactive interventions that can halt the exponential growth in the disease that is currently being observed realizing the true potential of real-time surveillance with this opportunity comes the challenge the available data are uncertain and incomplete while we need to provide mitigation strategies objectively with caution and rigor ie enable people to select appropriate actions to protect themselves at increased risk of covid-19 while minimize disruptions to daily life to the extent possible to address the above challenge leveraging our long-term and successful experiences in combating and mitigating widespread malware attacks using ai-driven techniques 7 8 10 11 15 16 20 37 38 39 40 41 42 43 44 45  in this work we propose to design and develop an ai-driven system to provide hierarchical community-level risk assessment at the first attempt to help combat the fast evolving covid-19 pandemic by using the large-scale and real-time data generated from heterogeneous sources in our developed system named -satellite we first develop a set of tools to collect and preprocess the large-scale and real-time pandemic related data from multiple sources including disease related data from official public health organizations demographic data mobility data and user generated data from social media and then we devise advanced ai-driven techniques to provide hierarchical community-level risk assessment to enable actionable strategies for community mitigation more specifically given a specific location either user input or automatic positioning the developed system will automatically provide risk indexes associated with it in a hierarchical manner eg state county city specific location to enable people to select appropriate actions for protection while minimizing disruptions to daily life the framework of our proposed and developed system is shown in figure 3  in the system of -satellite 1 we first construct an attributed heterogeneous information network ahin to model the collected large-scale and real-time pandemic related data in a comprehensive way 2 based on the constructed ahin to address the challenge of limited data that might be available for learning eg social media data to learn public perceptions towards covid-19 in a given area might not be sufficient we then exploit the conditional generative adversarial nets cgans to gain the public perceptions towards covid-19 in each given area and finally 3 we utilize meta-path based schemes to model both vertical and horizontal information associated with a given area and devise a novel heterogeneous graph auto-encoder gae to aggregate information from its neighborhood areas to estimate the risk of the given area in a hierarchical manner the developed system -satellite and the generated benchmark datasets have been made publicly accessible through our website there have been several works on using ai and machine learning techniques to help combat covid-19 in the biomedical domain 6 24 28 32 35 use deep learning methods for covid-19 pneumonia diagnosis and genome study while 26 36 develop learning-based models to predict severity and survival for patients another research direction is to utilize public accessible data to help the estimation of infection cases or forecast the covid-19 outbreak 14 17 18 22 25 27 46  however most of these existing works mainly focus on wuhan china the studies of using computational models to combat covid-19 in the us are scarce and there has no work on community-level risk assessment to assist with community mitigation by far to meet this urgent need and to bridge the research gap in this work by advancing capabilities of ai and leveraging the large-scale and real-time data generated from heterogeneous sources we propose and develop an ai-driven system named -satellite to provide hierarchical community-level risk assessment at the first attempt to help combat the deadly and fast evolving covid-19 pandemic in this section we will introduce our proposed method integrated in the system of -satellite to automatically provide hierarchical community-level risk assessment related to covid-19 in detail realizing the true potential of real-time surveillance requires identifying the proper data sources based on which we can devise models to extract meaningful and actionable information for community mitigation since relying on a single data source for estimation and prediction often results in unsatisfactory performance we develop a set of crawling tools and preprocessing methods to collect and parse the large-scale and real-time pandemic related data from multiple sources which include the followings  disease related data we collect the up-to-date county-based coronavirus related data including the numbers of confirmed cases new cases deaths and the fatality rate from i official public health organizations such as who cdc and county government websites and ii digital media with real-time updates of figure 3  system architecture of -satellite ie an ai-driven system for hierarchical community-level risk assessment in -satellite a we first construct an ahin to model the collected large-scale and real-time pandemic related data in a comprehensive way b based on the constructed ahin we then exploit the cgans to gain the public perception towards covid-19 in an given area c we finally utilize meta-path based schemes to model both vertical and horizontal information associated with a given area and devise heterogeneous gae to aggregate information from its neighborhood areas to estimate the risk of the given area in a hierarchical manner covid-19 eg 1point3acres 2  the collected up-to-date countybased covid-19 related statistical data can be an important element for risk assessment of an associated area  demographic data the united states census bureau 3 provides the demographic data including basic population business and geography statistics for all states and counties and for cities and towns with more than 5000 people the demographic information will contribute to the risk assessment of an associated area for example as older adults may be at higher risk for more serious complications from covid-19 3 30  the age distribution of a given area can be considered as an important input in this work given a specific area we mainly consider the associated demographic data including the estimated population population density eg number of people per square mile age and gender distributions  mobility data given a specific area either user input or automatic positioning a mobility measure that estimates how busy the area is in terms of traffic density will be retained from location service providers ie google maps  user generated data from social media as users in social media are likely to discuss and share their experiences of covid-19 the data from social media may contribute complementary knowledge such as public perceptions towards covid-19 in the area they associate with in this work we initialize our efforts with the focus on reddit as it provides the platform for scientific discussion of dynamic policies announcements symptoms and events of covid-19 in particular we consider i three subreddits with general discussion ie rcoronavirus 4  rcovid19 5 and rcoronavirusus 6  ii four region-based subreddits which are rcoronavirusmidwest rcoronavirussouth rcoronavirussoutheast and rcoronaviruswest and iii 48 statebased subreddits ie washington dc and 47 states to analyze public perceptions towards covid-19 for a given area note that all users are anonymized for analysis using hash values of usernames we first exploit stanford named entity recognizer 12 to extract the location-based information eg county city and then utilize tools such as nltk 1 to conduct sentiment analysis ie positive neutral or negative more specifically positive denotes well aware of covid-19 while negative indicates less aware of covid-19 for example with the analysis of the post by a user with hash value of cf6 in subreddit of rcoronaviruspa on march 14 2020 i live in montgomery county pa and everyone here is acting like theres nothing going on the location-related information of montgomery county and pennsylvania state ie pa can be extracted and a users perception towards covid-19 in montgomery county at pa can be learned ie negative indicating less aware of covid-19 such automatically extracted knowledge will be incorporated into the risk assessment of the related area meanwhile it can also provide important information to help inform and educate about the science of coronavirus transmission and prevention to comprehensively describe a given area for its risk assessment related to covid-19 based on the data collected from multiple sources above we consider and extract higher-level semantics as well as social and behavioral information within the communities attributed features based on the collected data above we further extract the corresponding attributed features  a1 disease related feature for a given area its related covid-19 pandemic data will be extracted including the numbers of confirmed cases new cases deaths and the fatality rate which is represented by a numeric feature vector a 1  for example as of march 22 2020 the cuyahoga county at ohio state oh has had 125 confirmed cases 33 new cases 1 death and 08 fatality rate which can be represented as a 1  125 33 1 0008   a2 demographic feature given a specific area we obtain its associated citys or towns demographic data from the united states census bureau including the estimated population population density ie number of people per square mile age distribution ie percentage of people over 65 year-old and gender distribution ie percentage of females for example to assist with the risk assessment of the area of euclid ave in cleveland at oh the obtained demographic data associated with it are cleveland with population of 383793 population density of 5107 135 people over 65 year-old and 518 females which will be represented as a 2  383793 5107 0135 0518   a3 mobility feature given a specific area a mobility measure that estimates how busy the area is in terms of traffic density will be obtained from google maps which will represented by five degree levels ie 1 5  the larger number the busier  a4 representation of public perception after performing the automatic sentiment analysis based on the collected posts associated with a given area from reddit the public perceptions towards covid-19 in this area will be represented by a normalized value ie 01 indicated the awareness of covid-19 ie the larger value the more aware for the previous example of the reddit post of i live in montgomery county pa and everyone here is acting like theres nothing going on  a related perception towards covid-19 in montgomery county at pa will be formulated as a numeric vale of 0220 denoting people in this area were less aware of covid-19 on march 14 2020 after extracting the above features we concatenate them as a normalized attributed feature vector a attached to each given area for representation ie a  a 1 a 2 a 3 a 4  note that we zero-pad the ones in the elements when the data are not available relation-based features besides the above extracted attributed features we also consider the rich relations among different areas  r1 administrative affiliation according to the severity of covid-19 available resources and impacts to the residents different states may have different policies actionable strategies and orders with responses to covid-19 therefore given an area we accordingly extract its administrative affiliation in a hierarchical manner particularly we acquire the state-include-county and county-include-city relations from city-to-county finder 7   r2 geospatial relation we also consider the geospatial relations between a given area and its neighborhood areas more specifically given an area we retain its k-nearest neighbors at the same hierarchical level by calculating the euclidean distances based on their global positioning system gps coordinates obtained from google maps and wikipedia 8  with an entity type mapping  v  t and a relation type mapping   e  r where v  m i1 x i denotes the entity set and e is the relation set t denotes the entity type set and r is the relation type set a  m i1 a i  and t   r  2 network schema 21  the network schema of an ahin g is a meta-template for g denoted as a directed graph t g  t  r with nodes as entity types from t and edges as relation types from r in this work we have four types of entities ie nation state county and city t   4 two types of relations ie r1 and r2 r  2 and each entity is attached with an attributed feature vector as described above based on the definitions the network schema of ahin in our case is shown in figure 4   although the constructed ahin can model the complex and rich relations among different entities attached with attributed features there faces a challenge that there might be missing values of attributed features attached to the entities in the ahin because of limited data that might be available for learning more specifically given an area there may not be sufficient social media data ie reddit data in this work to learn the public perceptions towards covid-19 in this area for example for the state of montana as of march 22 2020 in its corresponding subreddit rcoronavirusmontana there only have been 12 posts by seven users discussing the virus to address this issue we propose to exploit cgans 23 for synthetic virtual social media user data generation for public perception learning to enrich the ahin different from traditional gans 13  a cgan is a conditional model extended from gans where both the generator and discriminator are conditioned on some extra information in our case we propose to exploit cgan to generate the synthetic posts for those areas where the data are not available in our designed cgan given an area where reddit data are not available the condition composes of three parts the disease related feature vector in this area a 1  its related demographic feature vector a 2 and its gps coordinate denoted as o as shown in figure 5  the generator in the devised cgan aims to incorporate the prior noise p z z with the conditions of a 1  a 2 and o as the inputs to generate the synthetic posts represented by latent vectors while in the discriminator real post representations obtained by using doc2vec 19 or generated synthetic post latent vectors along with a 1  a 2 and o are fed to a discriminative function both generator and discriminator could be a non-linear mapping function such as a multi-layer perceptron mlp the generator and discriminator play the adversarial minimax game formulated as the following minimax problem dgza 1  a 2  o  1 the generator and discriminator are trained simultaneously adjusting parameters for generator to minimize log1  d gza 1  a 2  o  while adjusting parameters for discriminator to maximize the probability of assigning the correct labels to both training examples and generated samples after applying cgan for synthetic post latent vector generation we further exploit deep neural network dnn to learn the public perceptions towards covid-19 in this area more specifically we first use doc2vec to obtain the representations of real posts collected from reddit and feed them to train the dnn model and then given a generated synthetic post latent vector we use the trained model to gain its related perception ie awareness of covid-19 meta-path expression to assist with the risk assessment of a given area related to the fast evolving covid-19 it might not be sufficient if only considering its vertical information eg its related city county or states responses strategies and policies the horizontal information ie information from its neighborhood areas will also be important inputs to comprehensively integrate both vertical and horizontal information we propose to exploit the concept of meta-path 29 to formulate the relatedness among different areas in the constructed ahin definition 2 meta-path a meta-path p is a path defined on the network schema t g  t  r and is denoted in the form of    r l between types t 1 and t l1  where  denotes relation composition operator and l is the length of p city denotes that to assess the risk of a specific city we not only consider the city itself but also the information from its related county and nearby cities heterogeneous graph auto-encoder given a node ie area in the constructed ahin guided by its corresponding meta-path scheme ie city level guided by p1 county level guided by p2 and state level guided by p3 to aggregate the information propagated from its neighborhood nodes we propose a heterogeneous graph auto-encoder gae model to achieve this goal the designed heterogeneous gae model consists of an encoder and a decoder the encoder aims at encoding meta-path based propagation to a latent representation and the decoder will reconstruct the topological information from the representation encoder we here exploit attentive mechanism 9 31 33 to devise the encoder it will first search the meta-path based neighbors n v for each node v and then each node will attentively aggregate information from its neighbors to learn the importance of the information from neighborhood nodes we first present each relation type r  r in the constructed ahin by r r  r d a d a  where d a denotes the dimension of the attributed feature vector and then the attentive weight  of node u the neighbor of v indicate the relevance of these two nodes measured in terms of the space r r  that is where a v and a u are the attributed feature vectors attached to node v and u we further normalize the weights across all the neighbors of v by applying softmax function then the neighbors representations can be formulated as the linear combination where the weight  r v u indicates the information propagated from u to v in terms of relation r  finally we aggregate vs representation a v and its neighbors representations a nv by decoder the decoder is used to reconstruct the network topological structure more specifically based on the latent representations generated from the encoder the decoder is trained to predict whether there is a link between two nodes in the constructed ahin to this end leveraging latent representations learned from the heterogeneous gae the risk index of a given area is calculated as where  i is the adjustable parameter that can be specified by human experts indicating the importance of i-th element in a v eg the number of confirmed cases population density age distribution mobility measure etc in the rapidly changing situation because of the critical need to act promptly and deliberately in this rapidly changing situation we have deployed our developed system -satellite ie an ai-driven system to automatically provide hierarchical community-level risk assessment related to covid-19 for public test given a specific location either user input or automatic positioning the developed system will automatically provide risk indexes associated with it in a hierarchical manner eg state county city specific location to enable people to select appropriate actions for protection while minimizing disruptions to daily life the link of the system is httpscovid-19yes-laborg which also include the brief description and disclaimer of the system as well as the following benchmark datasets data collection and preprocessing we have developed a set of crawling and preprocessing tools to collect and parse the largescale and real-time pandemic related data from multiple sources including disease related data from official public health organizations and digital media demographic data mobility data and user generated data from social media ie reddit we have made our collected and proprocessed data available for public use through the above link we describe each publicly accessible benchmark dataset ie db 1 -db 4  in detail below db 1  disease related dataset according to simplemaps 9  the us includes 50 states washington dc and puerto rico as well as 3203 counties and 28889 cities we have collected the up-to-date countybased coronavirus related data including the numbers of confirmed cases new cases deaths and the fatality rate from official public health organizations eg who cdc and county government websites and digital media with real-time updates of covid-19 eg 1point3acres by the date we have collected these data from 1531 counties and 52 states including washington dc and puerto rico on a daily basis from feb 28 2020 to date ie march 25 2020 db 2  demographic and mobility dataset we parse the demographic data collected from the the united states census bureau data updated on july 1 2019 in a hierarchical manner for each city county or state in the us the dataset includes its estimated population population density eg number of people per square mile age and gender distributions by the date we make the demographic and mobility dataset available for public use including the information of estimated population population density and gps coordinates for 28889 cities 3203 counties and 52 states including washington dc and puerto rico db 3  social media data from reddit in this work we initialize our efforts on social media data with the focus of public perception analysis on reddit as it provides the platform for scientific discussion of dynamic policies announcements symptoms and events of covid-19 in particular we have collected and analyzed 48 statebased subreddits ie washington dc and 47 states  in this section we evaluate the practical utility of the developed system -satellite for hierarchical community-level risk assessment related to covid-19 through a set of case studies case study 1 real-time risk index of a given area given a specific location either user input or automatic positioning by google map the developed system will automatically provide its related risk index ie ranging from 01 the larger number indicates higher risk and vice versa associated with the public perceptions ie awareness towards covid-19 in this area ie ranging from 01 the larger number denotes more aware and vice versa demographic density ie the number of people per square mile in its related county and traffic status ie ranging from 1 5  the larger number means more traffic and vice versa figure 7 a shows an example given the location of euclid ave cleveland oh 44106 the risk index provided by the system was 0662 with public perception of 0529 demographic density of 1389 and traffic status of 3 at 358pm edt on march 24 2020 at the same time the risk indexes and public perceptions of corresponding county ie cuyahoga county with risk index of 0665 and public perception of 0585 and state ie oh state with risk index of 0554 and public perception of 0557 will also be shown in a hierarchical manner to enable people to select appropriate actions for protection while minimizing disruptions to daily life case study 2 comparisons of risk indexes on different dates in this study given the same area we examine how the generated risk indexes change over time using the same location above figure  7 b shows the comparison results on different dates at the time of 358pm edt from which we have the following observations 1 in general its risk indexes increased over days from march 8 2020 in this study given the same time we examine how the generated risk indexes change over areas when a user inputs the areas heshe are interested in eg grocery stores near me in the search bar the system will display the nearby grocery stores using google maps application programming interface api and automatically provide the associated indexes for example using the same time in the first study ie 358pm edt on march 24 2020 figure 8 shows the grocery stores near me ie near the location of euclid ave cleveland oh 44106 and their related indexes from figure 8  we can observe that the indexes of nearby areas might vary due to the factors of different public perceptions towards covid-19 and different traffic statuses in specific areas as shown in the right part of figure 8  the system also provides related reddit posts to users case study 4 comparisons of different counties and states in this study we compare the indexes of different counties and different states given the same time using the time in the first study ie 358pm edt on march 24 2020 figure 9 a shows an example of comparisons more specifically at county-level using oh state as an example we choose the counties with top five largest numbers of confirmed cases on march 24 for comparisons cuyahoga 167 franklin 75 hamilton 38  summit 36 and lorain 30  figure 9b illustrates the risk indexes associated with multiple factors versus the numbers of confirmed cases in these counties for the comparisons of different states we also choose five states two most severe states new york ny with 26376 confirmed cases and 271 deaths california ca with 2628 confirmed cases and 54 deaths two medium severe states oh with 564 confirmed cases and 8 deaths virginia va with 304 confirmed cases and 9 deaths and one least severe state west virginia wv with 39 confirmed cases and 0 deaths figure 9c shows the risk indexes versus the numbers of confirmed cases in these states from which we can see that there is a positive correlation between the numbers of confirmed cases and the risk indexes to track the emerging dynamics of covid-19 pandemic in the us in this work we propose to collect and model heterogeneous data from a variety of different sources devise algorithms to use these data to train and update the models to estimate the spread of covid-19 and predict the risks at community levels and thus help provide actionable information to users for community mitigation in sum leveraging the large-scale and real-time data generated from heterogeneous sources we have developed the prototype of an aidriven system named -satellite to help combat the deadly covid-19 pandemic the developed system and generated benchmark datasets have made publicly accessible through our website in the future work we plan to continue our efforts to expand the data collection and enhance the system to help combat the fast evolving covid-19 pandemic we will continue to release our generated data and updates of the system to facilitate researchers and practitioners on the research to help combat covid-19 pandemic while assisting people to select appropriate actions to protect themselves at increased risk of covid-19 while minimize disruptions to daily life to the extent possible   coronavirus on social media analyzing misinformation in twitter conversations a preprint karishma sharma sungyong seo chuizheng meng sirisha rambhatla yan liu  the ongoing coronavirus disease covid-19 pandemic highlights the interconnected-ness of our present-day globalized world with social distancing policies in place virtual communication has become an important source of misinformation as increasing number of people rely on social media platforms for news identifying misinformation has emerged as a critical task in these unprecedented times in addition to being malicious the spread of such information poses a serious public health risk to this end we design a dashboard to track misinformation on popular social media news sharing platform -twitter the dashboard provides visibility into the social media discussions around coronavirus and the quality of information shared on the platform updated over time we collect streaming data using the twitter api from march 1 2020 to date and identify false misleading and clickbait contents from collected tweets we provide analysis of user accounts and misinformation spread across countries in addition we provide analysis of public sentiments on intervention policies such as socialdistancing and workfromhome and we track topics and emerging hashtags and sentiments over countries the dashboard maintains an evolving list of misinformation cascades sentiments and emerging trends over time accessible online at httpsusc-meladygithubiocovid-19-tweet-analysis  as social media becomes the primary source of information for people around the world it has become increasingly critical and challenging to curb misinformation according to pew research report social media outpaced print news paper in 2018 shearer 2018 accessed march 20 2020  mitchell 2018  as the share of americans who get their news online continues to increase geiger 2019 accessed march 20 2020  the misinformation surrounding covid-19 pandemic is especially damaging since any mis-steps can pose a serious public health risk by leading to exponential spread of the disease and accidental death due to self-medication vigdor 2020 accessed march 24 2020  the risk of misinformation surrounding the pandemic has motivated the world health organization who to launch a mythbuster page who 2020 accessed march 20 2020  however these counter measures face challenges with the fast-paced evolution and spread of news on social media as a result it is extremely important to identify and potentially curb the spread of misinformation as close as possible to its point of origin to this end we present a dashboard to provide insights about the nature of information that is currently shared through social media regarding the covid-19 pandemic the dashboard provides an analysis of topics sentiments and trends assessed from twitter posts along with identified false misleading and clickbait information spreading on social media related to covid-19 here we focus our analysis on twitter since it has the highest number of news focused users hughes and wojcik 2019 accessed march 20 2020 and provides access to public tweets data social media plays a pivotal role in information dissemination and consumption during the pandemic godfrey 2020 accessed april 20 2020  more so with increasing social distancing and growing reliance on online communication social media has both positive and negative social impacts during the crisis for instance safety tips such as wash your hands and stay home are shared widely to gain community support in fighting the covid-19 pandemic godfrey 2020 accessed april 20 2020  on the other hand misinformation and hate speech are growing problems that can adversely impact the safety of individuals and society the indirect and direct impacts such as instances of 5g towers being burned down due to conspiracy theories linking them to coronavirus parveen and waterson 2020 accessed april 17 2020 make it imperative to address the problem of misinformation in this work we attempt to analyze both the positive and negative aspects of social media during the pandemic and utilize computational techniques to process information shared on social media related to covid-19 we collect twitter conversations from march 1 2020 to date and provide analysis results on the dashboard the purpose of the dashboard is to provide transparency into twitter conversations surrounding the pandemic which is an important step towards curbing misinformation and increasing awareness about the overall nature and quality of information being consumed towards this end we use topic modeling to identify prominent topics from social media posts on covid-19 we also extract emerging hashtags in each country over time to understand the evolution of conversations as the pandemic situation changes in a given region or in association to important events or occasions that are impacted by the pandemic in addition we utilize social media posts to understand the sentiments over time in each countryregion we also leverage sentiment analysis to estimate the public perception on twitter towards policies related to social distancing and work from home towards the goal of curbing misinformation we extract information cascades on social media -that correspond to the sharing of news articles through retweetsreplies on twitter and identify misinformation cascades based on the source information in this work we define misinformation to include four types of information namely unreliable conspiracy clickbait and politicalbiased each of these types can have negative societal impact on public health intervention policies and elections in addition we analyze user accounts and activities to characterize the nature of spread on misinformation on social media the dashboard provides the list of misinformation cascades identified from march 1 2020 to date and misinformation spread across countries for prominent cascades the research on misinformation analysis related to healthcare and pandemics is important and understudied this work on covid-19 aims to provide new insights and analysis on social media and misinformation related to global crisis and pandemics in the following sections we will discuss the data collection analysis and results and future work we collect social media posts on twitter using the streaming api service starting from march 1 2020 to date we use keywords related to covid-19 to filter the twitter stream and obtain relevant tweets about the pandemic the dataset from march 1 2020 to march 30 2020 contains 308m tweets from 182 countries the subset of english tweets equals 205m the data collection is ongoing and will be used to update the analysis on the dashboard dataset statistics table 1 provides details about the tweets collected and the user accounts associated with the tweets the english tweets are utilized for further analysis and therefore the table reports the details about what fraction of english tweets contains geolocation information and count of unique user accounts associated with the tweets as well as the percentage of twitter verified accounts among those user accounts geolocation we obtain geolocation information at the country-level based on dredze et al 2013  the extracted geolocation based on dredze et al 2013 is not always available the information is available if the user has geo-enabled feature on twitter or if the user mentions a valid location in his public profile based on the extracted geolocation information we provide the distribution of english tweets among countries and states in the united states in table 2  misinformation forms an important aspect of our online world due to social distancing measures the reliance on information available online has become critical to address this challenge we identify misinformation and provide the list of identified posts propagating such information on the dashboard the task of distinguishing legitimate vs false misleading and clickbait content is challenging from both a human and machine perspective in fact this increases the importance of eliminating such information from social media platforms because the general public is easily manipulated into believing false information which in this case can be detrimental to public health and have dire consequences vigdor 2020 accessed march 24 2020  parveen and waterson 2020 accessed april 17 2020  misinformation detection poses new challenges in the domain of healthcare and pandemics there are numerous existing datasets on misinformation sharma et al 2019  labeled based on information collected from fact-checking websites the existing labeled datasets are either on general events reported during a period in time ma et al  2018  or domain specific events such as the syrian civil war or hurricane sandy salem et al 2019  gupta et al 2013  we focus on building a domain specific dataset on covid-19 to provide real-time identification of misinformation on the pandemic and in the future to improve research in misinformation analysis related to healthcare and pandemics information cascades in order to identify misinformation we first extract retweetreply links between tweets using the twitter api we found 19m edges ie retweetreply links between the 20m english tweets collected between march 1-30 2020 we cluster the retweet graph to identify source tweets propagating or being shared over the social network of the 19m tweets 217m are identified as source tweets and 172k have at least 10 retweets each source tweet has associated tweet text and user account features as well as the propagation related information available in the form of retweets and reply tweets the source tweet along with its propagation information is referred to as an information cascade yang and leskovec 2010  in this work we focus on identifying the list of misinformation cascades from the collected dataset detection misinformation is defined to incorporate false and misleading information of varying degrees of falsehood sharma et al 2019  disinformation more specifically refers to false information with a deliberate intent whereas misinformation is defined to include false information which might not have a deliberate malicious intent however varying degrees of falsehood and varying intents such as promotion of clickbait and political influence make it important to expose different kinds of malicious information therefore we construct four categories of information which can have negative societal impact and use the term misinformation to broadly encompass the four categories categorization we categorize information into four types -unreliable conspiracy clickbait and politicalbiased using news sources compiled from fact-checking websites namely media biasfact check 1  and list of news websites provided in zimdars 2016  we define the categories as follows  unreliable this category is defined to include false questionable rumorous and misleading news in addition we include satire based on the consideration that satire has the potential to perpetuate misinformation zimdars 2016 or be used as a cover for misinformation publication sharma et al 2019   conspiracy this category is defined to include conspiracy theories and scientifically dubious news  clickbait this category includes clickbait news ie exaggerated or misleading headlines andor body purposed to attract attention for reliable andor unreliable information  politicalbiased this category includes political and biased news written in support of a particular point of view or political orientation for reliable andor unreliable information such as propaganda note that a tweet can belong to more than one of the above mentioned categories as can multiple types be associated with a news source to label tweets we identify source tweets that contain external links to news articles using tweet metadata based on the news article links we extract the news source publisher information similar to  we construct labels for each source tweet containing external links based on the news source information misinformation during pandemics can impact public health intervention policies and future elections therefore we maintain separate categories of unreliable conspiracy clickbait politicalbiased and provide the list of source tweets identified under each category on the dashboard in the future we will update our analysis to include detection based on source and social context information extracted from the network the 625m user accounts engaging in twitter conversations from march 1-30 2020 in the collected dataset are studied to provide information about the spread of covid-19 related tweets we extract user account features using the twitter api the account creation date provides important information about the engagement of users on the social network discussion in figure 2  the distribution of user accounts with respect to the account creation date is visualized the user accounts engaging in covid-19 tweets include accounts created between 2006 to 2020 ie from when the twitter platform was introduced to date the trend shows a spikeincrease between december 2019 and march 2020 new accounts tweeting about covid-19 provides information about the changing social network structure during the pandemic in the future we will investigate the activity patterns of new user accounts to identify the role or function they play in either dissemination of useful information about the pandemic or towards misinformation propagation the misinformation spread across countries for sample tweets identified from the collected dataset is shown in figure 5  the figure shows the information cascade corresponding to each source tweet the points indicate the retweet or replies of the source tweet over the time scale tweets containing geolocation information are visualized based on the extracted latitude and longitude information the identified misinformation in the four categories -unreliable conspiracy clickbait and politicalbiased were found to contain both healthcare and political misinformation we provide and discuss examples of source tweets and their propagation patterns across countries discussion in fig 3a  a false claim circulated about nevada governors chief medical officer banning the use of hydroxychloroquine treatments was seen to circulate through social media in this case the observed geolocation of source tweet is in the united states the country with the highest twitter usage and it propagates to other countries within minutes in other cases source tweets are also observed to originate from other countries and travel to united states and other countries for example in fig 3b  for a false claim that the virus was found to transmit through toilet paper the source tweet geolocation was observed in australia with retweets traveling to several other countries in cases where the geolocation information of the source tweet is unobserved the geolocation of other tweets in the information cascade still provides estimates of the exposure and spread in different countries of the misinformation claims propagated through the source tweet for instance in fig 4a the spread of the conspiracy promoting that the virus is a bioweapon is observed over multiple countries whereas in fig 4b the spread of the claim that the pandemic is less deadly than the flu is observed within the united states misinformation of varying degree of falsehood and biasedclickbait news reporting can mislead and influence public perception especially with widespread propagation we observe that the largest cascade in the collected dataset has over 10 000 retweets spanning multiple countries shown in fig 5a it corresponds to a political clickbait news article published on the discussion surrounding affordability and price control on vaccines being researched for the virus we also find other cases of political misinformation with false claims regarding political figures maliciously attempting to worsen the crisis as shown in fig 5b  as seen these cases of misinformation have the potential to harm public health and effectiveness of health intervention policies country-wise sentiments we analyze the evolving country-wise sentiments related to the covid-19 pandemic the public perceptions constitute an important factor for gauging the reactions to policy decisions and preparedness efforts in addition they also reflect the nature of news coverage and potential misinformation we extract sentiments from social media posts at the country-level and over time to study the evolving public perceptions towards the pandemic using lexical sentiment extraction based on hutto and gilbert 2014 we obtain the valence positive or negative along with its intensity for each tweet based on its textual information the sentiment is aggregated over tweets to estimate the overall sentiment distribution the distribution of sentiments was found to vary over time and country social distancingwork from home sentiments in addition we analyze the public perception of emerging policies such as social distancing and remote work these disease mitigation strategies also provide unprecedented glimpse into the effect of remote work and isolation on mental health although the option to work remotely is limited to the white collar workforce nevertheless absence of child and dependent-care has emerged as an important challenge furthermore this forced remote work will impact workdays of white collar workers beyond the pandemic in order to understand public sentiment and opinion about different social issues we extract hashtag information from the collected tweets and filter based on keywords workfromhome wfm workfromhome workingfromhome wfhlife and socialdistance socialdistancing the filtered tweets are analyzed to obtain positive and negative sentiments and ranked and visualized based on valence and intensity the analysis is shown in fig 6 for sentiments on social distancing and on work from home policy interventions topic clusters we analyze twitter conversations to identify topics and trends in the twitter data on covid-19 we use topic modeling based on character embeddings joulin et al 2016 extracted from social media posts nguyen et al 2015 li et al 2016  we identified 20 different topics from the collected english tweets we found that the prominent topics of discussions during early march were centered around global outbreaks wuhan italy iran travel restrictions prevention measures such as hand washing and masks hoarding symptoms and infections immunization event cancellations testing kits and centers government response and emergency funding the topic clusters along with the most representative tweets in each cluster are provided on the dashboard the representative tweets of each cluster are obtained based on word similarity of the tweet to the tf-idf word distribution of the cluster the label to each cluster of tweets was assigned by manual inspection of the word distribution and representative tweets of the cluster emerging trends the emerging trend on twitter highlight changes in perception or importance of topics as the pandemic situation changes we extract hashtags from the tweet text for all tweets in the dataset for march2020 the hashtags with emerging popularity are estimated based on fitted linear regression curves on the usage counts of hashtags over the period on the dashboard fig 7 fig 8  we provide the top-30 emerging hashtags to show trendy interest in social media over the world as the hashtags also reflect spatial characteristics eg country-level policy or trend the top-10 emerging hashtags of each country for last 10 days are also visualized on the dashboard and regularly updated the countryregion-based emerging hashtags are particularly important to track peoples interest for instance the line chart in fig 7 shows which hashtags emerged in terms of a slope of usage counts in the united states from march22 to march31 while the use of some hashtags eg coronavirus and trump continuously dominates the conversations other hashtags eg coronalockdown coronavirustruth and nationaldoctorsday are temporally significant the end of march is when most of states announced lockdown on many business and a stayat-home order and it causes people to use lockdown-related hashtags coronalockdown moreover it shows that people get more and more interested in facts on coronavirus finally the slope-based extraction easily detects spike pattern of some hashtags nationaldoctorsday which are only used in a particular day in germany fig 7  we could detect that people are interested in wearing masks maskenpflicht mask required from the end of march in france fig 8 have counted the containment day confinementjour everyday and their patterns show time lags as expected finally the plot is also useful to see what trendy issues are coronajihad nizamuddin in india fig 8  geoinformation trends we also analyze the geographical distribution of daily counts of tweets and its trend using the extracted geolocation information the dashboard provides 1 the geographical distribution of the daily count of tweets over countriesregions 2 the daily increment of the count of tweets for each countryregion 3 the time for each countryregion when it encounters its peak of daily counts of tweets 1 shows a steady distribution of daily counts of tweets users in united states contribute more than half of the total daily counts of tweets around the world and users in europe india oceania and south america are also active 2 reveals that the daily counts of tweets of most countriesregions are steady during the time of our observation 3 illustrates the spatio-temporal pattern of which day each countryregion achieves it highest activity over the observation time period there are several critical directions of future work to address this large-scale infodemic surrounding covid-19 the proportion of twitter users in the united states is higher than in other countries like china with alternate social media platforms since the pandemic is at a global scale social media analysis for other platforms and languages is critical towards curbing misinformation the second important factor is that the annotation for misinformation is a challenging task and requires expert verification therefore research in unsupervised or distant supervision are important towards alternate forms of labeling to improve classification and handle the imbalance in the distribution of misinformation vs legitimate information we also plan to include social context information towards improving misinformation detection in addition we plan to analyze sentiments about other emerging topics and moreover to study the impact of public perception as well as misinformation on policy interventions on curbing the pandemic  title machine learning maps research needs in covid-19 literature anhvinh doanvo xiaolu qian divya ramjee msc helen piontkivska phd angel desai m mph d maimuna majumder  manually assessing the scope of the thousands of publications on the covid-19 coronavirus disease 2019 pandemic is an overwhelming task shortcuts through metadata analysis eg keywords assume that studies are properly tagged however machine learning approaches can rapidly survey the actual text of coronavirus abstracts to identify research overlap between covid-19 and other coronavirus diseases research hotspots and areas warranting exploration we propose a fast scalable and reusable framework to parse novel disease literature when applied to the covid-19 open research dataset cord-19 dimensionality reduction suggested that covid-19 studies to date are primarily clinical- modeling-or field-based in contrast to the vast quantity of laboratory-driven research for other non-covid-19 coronavirus diseases topic modeling also indicated that covid-19 publications have thus far focused primarily on public health outbreak reporting clinical care and testing for coronaviruses as opposed to the more limited number focused on basic microbiology including pathogenesis and transmission  on march 16 2020 the white house issued a call to action for the application of artificial intelligence ai methods to assist with research on coronavirus disease 2019 robbins 2020  which is caused by the severe acute respiratory syndrome coronavirus 2 sars-cov-2 designating machine learning ml as a potentially useful tool for gleaning critical insights from the existing coronavirus literature present attempts to examine covid-19related publications either mine texts to rapidly create study summaries for researchers joshi et al 2020 or focus on citations keyword co-occurrences and other metrics to identify influential literature chahrour et al 2020 golinelli et al 2020 hossain 2020  other large-scale efforts concentrate on cataloguing peer-reviewed covid-19 studies including litcovid a literature hub by the national center for biotechnology information chen et al 2020  and covid-19 data portal a literature search engine from the european bioinformatics institute embl-ebi 2020  although these efforts facilitate keyword-based searches to rapidly identify studies of interest and in litcovid specifically classify them into broad categories eg mechanism diagnosis etc they do not provide an overview of where research efforts are directed and whether these efforts have changed over time this presents an opportunity to leverage the application of ml methods to survey the ongoing influx of peer-reviewed and preprinted covid-19 studiescombined with prior publications on severe acute respiratory syndrome sars coronavirus sars-cov middle east respiratory syndrome mers coronavirus mers-cov and other coronavirusesand develop unique insights for covid-19 research needs a number of biomedical studies have already applied ml techniques in their work on surveillance trends and clinical predictors for the ongoing pandemic eg alimadadi et al 2020 carrillo-larco  castillo-cara 2020 ge et al 2020 kim et al 2020 kumar et al 2020 rao and vazquez 2020 yan et al 2020  our novel application of ml methods to available coronavirus abstracts including those about covid-19 offers insights into the themes of covid-19 research that overlap with studies about other coronaviruses we perform mlaided analysis of research abstracts in the covid-19 open research dataset cord-19 wang et al 2020 to automatically categorize ongoing research endeavors into dynamically-generated categories enabling us to identify topics that have received limited attention to date by understanding the knowledge overlap between recently released abstracts on covid-19 and abstracts related to other coronaviruses we are able to gain insight into potential areas of sars-cov-2 research warranting further exploration in addition we propose a reusable framework for parsing an existing knowledge base about other emerging pathogens like the highly pathogenic avian influenza h5n1 kilpatrick et al 2006 kissler et al 2019 before they escalate to the level of a major epidemic or pandemic threat without using any pre-existing knowledge about the abstracts topics we employed unsupervised ml to determine differences between covid-19 and non-covid-19 abstracts in our corpus of documents a dimensionality reduction approach was used to identify principal patterns of variation in the abstracts text followed by topic modeling to extract high-level topics discussed in the abstracts james et al 2013  our data pipeline is available on github 1  we obtained research abstracts from cord-19 on may 28 2020 generated by the allen institute for ai and in partnership with other research groups cord-19 is updated daily with coronavirus-related literature peer-reviewed studies from pubmedpubmed central as well as pre-prints from biorxiv and medrxiv are retrieved using specific coronavirus-related keywords covid-19 or coronavirus or corona virus or 2019-ncovor sars-cov or mers-cov or severe acute respiratory syndrome or middle east respiratory syndrome at time of writing cord-19 contained approximately 137000 articles including both full-text and metadata for all coronavirus research articles with 40 of the dataset classified as virology-related wang et al 2020  we focused our analysis on the abstracts of articles in as some of the cord-19 abstracts were neither relevant to sars-cov-2 nor other coronaviruses we filtered the cord-19 data to isolate coronavirus-specific abstracts by searching for abstracts that mentioned relevant terms these abstracts served as our documents associated with the document-term matrices dtms in our natural language processing nlp pipeline supplemental information 1 we also identified abstracts for only covid-19-related studies by filtering for covid-19-related keywords within this subset supplemental information 2 principal components analysis pca is a dimensionality reduction algorithm that summarizes data by determining linear correlations between variables hotelling 1933  pca identifies individual patterns of variance or principal components pcs in dtms that differentiate documents from one another highlighting key trends in the data supplemental information 3 for example in a simple corpus with two mutually exclusive topics like machine learning and health infrastructure the terms machine and learning would be correlated with one another pca would recognize these terms as an important source of variation providing a way to differentiate documents about either topic machine learning vs health infrastructure by the frequency of these terms when pca is applied to dtms pcs represent patterns differentiating different documents ordered by their prominence each detected pattern reflects both the contextual links between words and their level of importance within the texts words with component values of the greatest magnitude on each pc most strongly drive the pattern that each individual pc recognizes for example if machine and healthcare respectively have highly negative and highly positive values on a particular pc then that pc detects the pattern that when machine appears in a text healthcare appears less often another pc may detect a different pattern of variance such as when some documents mention deep learning more often than others the projection values of the text corpus onto the pcs suggest what concept each document discusses and to what extent relative to the average document within the corpus following the previous example strongly negative projection values on the first pc which would capture the datas most prominent patterns indicate that the document mentions machine more often than the average and thus is more likely to focus on machine learning in addition projection values on the second pc could distinguish between machine learning documents by focus or lack thereof on deep learning or other techniques this approach enables us to delineate between different groups of abstracts by visualizing differences in their projections on the top pcs so in short after applying pca to the dtms of our abstracts we identified which pcs successfully separated covid-19 and non-covid-19 abstracts we then used the component values with the largest magnitude on these pcs to interpret them after establishing high-level trends using pca we used latent dirichlet allocation lda a topic modeling method to add nuance to observed differences between covid-19 and non-covid-19 literature and examine potential topics of interest lda is an unsupervised probabilistic algorithm that extracts hidden topics from large volumes of text blei ng and jordan 2003  once trained to discover words that separate documents into a predetermined number of topics lda can estimate the mixture of topics associated with each document these mixtures suggest the dominant topic for a document that is then used to assign a document to an overarching topic category for example lda may separate documents into two topics one on machine learning and another on healthcare and if a particular documents mixture is 60 machine learning and 40 healthcare it would assign that document to a machine learning topic category the predetermined number of topics is the most important hyperparameter in an lda model as models with sub-optimal number of topics fail to summarize data in an efficient manner blei ng and jordan 2003 zhao et al 2015  the number of topics can be determined by 1 identifying a model that has a low perplexity score and high coherence value when applied to an unseen dataset or 2 conducting a principled manual assessment of the topics that arise perplexity is a statistical measure of how imperfectly the topic model fits a dataset and a low perplexity score is generally considered to provide better results zhao et al 2015  similarly topic models with high coherence values are considered to offer meaningful interpretable topics aletras et al 2013 newman bonilla and buntine 2011  thus a model with a low perplexity score and a high coherence value is more desirable when choosing the optimal number of topics our initial implementation of lda showed no optimal value for the number of topics even as it approached 100 potentially reflecting a relatively shallow yet broad pool of covid-19 publications we ultimately identified 30 topics via manual review of topics from topic models with different numbers of topics to identify which model satisfied two criteria 1 topics that were relatively specific focusing on a single subject matter and 2 topics that would typically be non-redundant with one another our initial corpus included 137326 entries in the cord-19 dataset as of may 28 2020 107557 entries had abstracts available and of those 35281 entries 26 of 47928 had abstracts mentioning search terms related to coronaviruses those that did not mention coronavirus search terms in their abstracts contained coronavirus-related terms somewhere else in the text such as in its citations of the latter subset 18412 publications 50 of the subset or 13 of the entire cord-19 dataset were covid-19-related publications while pca highlighted the abstracts most prominent patterns in the first pc these patterns were not effective at distinguishing between covid-19 and non-covid-19 literature figure 1a demonstrates no meaningful difference between the two distributions of projection values from covid-19 and non-covid-19 abstracts onto the first pc indicating a shared pattern of variance ie both groups appear to discuss similar questions approaches and techniques using similar vocabulary within this pattern the patterns that successfully differentiated between the two groups were beneath the first pc within the second pc where the projection value distributions presented distinguishing patterns figure 1b  our interpretation of this pc relied on identifying terms that had values with the greatest magnitude supplemental information 4 supplemental information 5 ultimately the figures below indicate that while variance among non-covid-19 abstracts blue stretched over much of the second pc projection values of covid-19 abstracts orange were concentrated in a smaller area reflecting the narrower scope of covid-19 abstracts considering that the virus and associated disease have only been studied since december 2019 indicating distinct vocabularies used in these abstracts when we split the studies into subsets for the three human coronaviruses that have potential for severe infection we found that the distributions of sars-cov and mers-cov abstracts in the pc projection space were unique to each virus  figure 2  sars-cov-2 abstracts appeared to share a space in common with both mers-cov and sars-cov likely reflecting some shared terminology and possible ongoing attempts to leverage existing knowledge of the other two viruses to learn about sars-cov-2 however sars-cov-2 abstracts are much more concentrated among lower projection values notably mers-cov and sars-cov abstracts were spread more evenly along the second pc reflecting greater breadth and variation along these pcs that can be attributed to a broader range of studies focused on these pathogens as compared to sars-cov-2 this may be in part due to the much longer time that has been spent studying these viruses to identify terms associated with differences between covid-19 and non-covid-19 abstracts on pc2 we examined patterns of lemmatized terms from the respective abstracts  figure 3  the projection values of covid-19 abstracts on pc2 were lower and associated with emergent covid-19 clinical- modeling-or field-based cmf researchsuch as observational clinical and epidemiological studies -exemplified by stem terms patient pandem estim and case words in the opposite direction on pc2 -such as protein cell bind and expresscan be associated with viral biology and basic disease processes studied in biomolecular laboratories covid-19 abstracts were thus mostly associated with research conducted outside of laboratories eg in hospitals likely reflecting the pandemic reality of data collection alongside and often secondary to clinical care the high-level abstraction reflected by pc2 informed our designation of the extent that covid-19 research included studies with any cmf designranging from epidemiological studies to retrospective reviews of clinical outcomes case studies and randomized clinical trials or laboratory-driven researchincluding observational microscopy experimentation with antiviral compounds derivation of protein structures and studies of animal or cell culture models overall covid-19 abstracts appeared more likely to have terms associated with cmf research rather than laboratory studies based on comparisons of distributions for key terms in the covid-19 and non-covid-19 abstracts figure 4  supplemental information 4 this partition along research design for non-covid-19 and covid-19 abstracts was also evident in the abstract texts 90 of the abstracts in the bottom 1 of projection values along the second pc were related to covid-19 conversely only 1 of the abstracts in the top 1 were related to covid-19  topic modeling helped characterize differences between research topics discussed in covid-19 and non-covid-19 abstracts results from the lda model suggested that similar to the pattern observed in figure 4  there was clear differentiation between covid-19 and non-covid-19 abstracts across 30 topics  figure 5  supplemental information 6 there were five topics in particular -1 topic 14 outbreaks impact on healthcare services 2 topic 15 testing for coronaviruses 3 topic 17 epidemic cases and modeling 4 topic 21 clinical care and therapeutics and 5 topic 25 lessons learned for epidemic preparednessthat accounted for 58 of all covid-19 abstracts and for just 17 of non-covid-19 abstracts covid-19 abstracts were thus disproportionately concentrated in these five topics relative to non-covid-19 abstracts across the 30 topics we grouped several topics into topic families based on internal commonalities supplemental information 6 including 1 updates on the spread of and events related to coronavirus outbreaks including two subfamilies general updates vs public health responses 2 testing for coronaviruses 3 clinical care therapeutics and the need for vaccinations and 4 basic microbiological research which included two subfamilies a general catch-all subfamily vs a subfamily specific to pathogenesis and transmission when divided by topic family the disparity between covid-19 and non-covid-19 research in the first and fourth topic families showed that covid-19 abstracts appeared to be heavily concentrated on topics that typically included field-based data the first topic family on outbreak reporting and excluded laboratory-based studies the fourth topic family on basic microbiology however one exception was that covid-19 was overrepresented in studies on testing especially diagnostics the second topic family which included both the laboratory development of the tests and their field application supplemental information 7 we also examined the rates of publication and preprint submission for covid-19 abstracts along pc2 figure 6a  and the previously mentioned topic families figure 6b  from the beginning of 2020 covid-19 abstracts tended to have lower projection values for the second pc reflecting the relatively higher number of cmf studies emerging during the early stages of the pandemic compared to laboratory-based studies likewise the growth of studies in the different topic families for covid-19 was unevenly distributed figure 6b  from january 2020 through the end of may 2020 publications related to covid-19 were dominated by studies involving 1 outbreak and responses and 2 patients and healthcare services similar to the observed faster pace of cmf research in the pca results publications regarding viral mechanisms and biomolecular processes related to sars-cov-2 grew at a slower pace our findings demonstrate the utility of our novel nlp-driven approach for determining potential areas of underrepresentation in current research efforts for covid-19 by applying unsupervised ml methods to cord-19 we identified overarching key research topics in existing coronavirus and covid-19-specific abstracts as well as the distribution of abstracts among topics and over time our results support a prior bibliometric study that also found more frequent appearances of epidemiological keywords in covid-19 research compared to research on other coronaviruses hossain 2020  however our study presents the unique finding that laboratory-based covid-19 studies including those on genetic and biomolecular topics are underrepresented relative to studies of epidemiological and clinical issues particularly when compared with the distribution of previous research on other coronaviruses furthermore we developed a framework that improves upon existing studies in two key ways 1 our method maps connections between abstracts or publications by relying directly on the abstracts text in comparison to other bibliometric analyses including those in other fields that rely on the analysis of metadata de oliveira et al 2019 campbell et al 2010  and 2 our method offers an unsupervised ml-driven approach to splice the data in multiple ways including adeptly measuring the scope of existing literature its topical changes over time and differences from literature on previous pandemics the distribution of covid-19 and non-covid-19 abstracts from our pca results suggest that at the time of writing cord-19 dataset release on may 28 2020 the breadth of published research for covid-19 is relatively narrow compared to that of published non-covid-19 studies figures 1 and 2  as shown in our results keywords associated with biomolecular processes eg viral structure pathogenesis and host cell interactions appeared more frequently in non-covid-19 abstracts than in covid-19 abstracts this finding reflects the emergent nature of sars-cov-2 and the research communitys struggle to understand it at the molecular level to the same extent as other coronaviruses nonetheless the availability of laboratory studies for other coronaviruses represents an opportunity for generating hypothesisdriven research questions grounded in empirical research it is worth noting that researchers may be working under the assumption that biological processes of sars-cov-2 including life cycle and interactions with the human host are comparable to those of sars-cov due to their genetic similarity and relatedness csg 2020 petrosillo et al 2020 zhang and holmes 2020  for example several prior sars-cov studies on host cell entry helped identify the angiotensin converting enzyme 2 ace2 protein as a mediator for sars-cov-2 infection hoffman et al 2020  likewise cd147 and grp78 proteins have been hypothesized to play a role in cell entry for sars-cov-2 based on earlier sars-cov and mers-cov findings although additional studies are needed wang et al 2020b chen et al 2005 ibrahim et al 2020 chu et al 2018  while building upon assumed similarities is an important first step as work progresses it becomes increasingly important to identify features that are unique to each virus however the scope of literature for biological processes unique to sars-cov-2 is currently quite limited and perhaps even more limited than what our pca results suggest if most sars-cov-2 literature relies heavily on other coronavirus research this underrepresentation of studies on biomolecular processes could also be attributed to the rapid worldwide spread of sars-cov-2 that occurred within mere months of its emergence necessitating an unprecedented response from healthcare and public health infrastructures globally our pca results reflect an overwhelming concern regarding the exponential spread of the virus and risks for transmission involved with more frequent appearances of stem terms such as pandem outbreak estim countri number and risk in covid-19 abstracts this was also supported by our topic modelling results which indicated that 58 of covid-19 abstracts fell into just five of 30 topics generally related to healthcare services the pandemics public health issues and testing for coronaviruses figure 6a 6b  the more rapid growth of cmf research relative to laboratory-driven research mirrors the current response to the pandemic in the united states where the initial focus on pressing epidemiological and clinical concerns is now followed by interest in experimental investigations including those of structural mechanisms for host cell entry and possible therapeutic targets overall our findings reflect a clear divide between covid-19 and non-covid-19 abstracts based upon research design unlike cmf research laboratory-driven sars-cov-2 research is either still underway or has only just been initiated this can be attributed in part to the fact that laboratory research is often a labor-intensive process within a federally-regulated infrastructure that depends on the availability of timely project-based funding as well as longerterm funding our findings also suggest that the pace of research on sars-cov-2 biomolecular processes is potentially insufficient given the global threat posed by the virus figure 6a 6b  this lag may adversely impact the development of antivirals and other therapeutic interventions adding strain to already overwhelmed healthcare systems furthermore these trends raise questions about the readiness of institutions supporting the research community in times of extraordinary stress previous experiences with global pandemics such as h1n1 have resulted in various policy recommendations french et al 2009 to maintain and enhance readiness in laboratory-based research and analysis on the effectiveness of recommendations arising from these experiences may be worthwhile while pca identified a prominent pattern that differentiated between covid-19 and non-covid-19 literature the topic families derived from lda refined our understanding of knowledge gaps and research needs in covid-19 literature by delineating specific research areas this included an underrepresentation of studies on basic microbiological examination of sars-cov-2 including its pathogenesis and transmission research on these issues is published at a slower pace than cmf studies eg those on clinical topics outbreak response and statistical reporting and research on testing figure 6b  even when compared with the distribution of non-covid-19 research covid-19 research was more heavily focused on topics within the cmf realm supplemental information 7 we recognize that the number of abstracts in each of these topics does not necessarily represent scientific progress made in these areas but they do reflect the pace of research and potential availability of public knowledge this indicates either a mismatch between the level of effort in these issues and the urgency of work or time lags inherent to these fields that constrain the responsiveness of the scientific community increased and consistent funding of emerging pathogens research including support of basic research even when there is no immediate threat of an outbreak would allow us to maintain a proactive posture in accumulating available knowledge rather than over-reliance on reactivity these conclusions must be caveated by several limitations that must be acknowledged first while cord-19 includes a vast quantity of coronavirus-related publications it potentially omits relevant literature from other databases such as the social science research network ssrn or arxiv a preprint server for studies in mathematics computer science and quantitative biology among other topics this may have constrained the representativeness of our analysis on covid-19 literature thus affecting the external validity of our findings second analyzing abstracts inherently excludes ongoing research efforts because not all relevant studies are publicly available or have released preprints third the number of publications does not directly represent progress in research areas fourth the high-level trends we observed through our unsupervised ml approaches may not completely align with how researchers identify and process specific research topics the counts of words in dtms informing the ml algorithms may not directly capture the ideas researchers are trying to convey and may therefore gloss over nuances in the literature yet these four limitations are somewhat mitigated by both the nature of the data sources and the needs of the research community for the first the excluded sources ssrn and arxiv heavily focus on research within the cmf arena indicating that if anything our conclusions on the rapid pace of cmf covid-19 research versus lab-based research are conservative for the second existing research pipelines have been accelerated in the pandemic especially with the proliferation of pre-print services this reduces the lag between the discovery of knowledge and the availability of an abstract to ingest in our data pipeline third the number of publications in each area may imply a relative difference in research productivity for different topics and thus may still serve as a proxy for indicating such progress or the attention given to specific issues and finally our ml-based method offers the chance to quickly review large quantities of text at scale and highlight underlying trends both this speed and this scale are crucial to informing time-sensitive decisions on policy and priorities to facilitate the most impactful research complex public health problems like the ongoing covid-19 pandemic require researchers to maintain robust knowledge on pathogenic threats including efforts dedicated to emerging or currently neglected pathogens our ml-based study offers insights into potential areas for future research opportunities and funding investments that build upon what is already known about coronaviruses we offer a conceptual framework that can be applied to other emergent or neglected pathogens that have the potential to become a pandemic threat such as highly pathogenic avian influenza h5n1 kilpatrick et al 2006 kissler et al 2019  enabling researchers to maintain a proactive preparedness posture the data of cord-19 is available to download from here all the code is free for download from github here  enhanced covid-19 data for improved prediction of survival wenhuan zeng anupam gautam daniel huson h  the current covid-19 pandemic caused by the rapid world-wide spread of the sars-cov-2 virus is having severe consequences for human health and the world economy the virus effects individuals quite differently with many infected patients showing only mild symptoms and others showing critical illness to lessen the impact of the pandemic one important question is which factors predict the death of a patient here we construct an enhanced covid-19 dataset by processing two existing databases from kaggle and who and using natural language processing methods to enhance the data by adding local weather conditions and research sentiment in this study we contribute an enhanced covid-19 dataset which contains 183 samples and 43 features application of extreme gradient boosting xgboost on the enhanced dataset achieves 95 accuracy in predicting patients survival with country-wise research sentiment and then age and local weather showing the most importance all data and source code are available at  the current covid-19 pandemic caused by the rapid world-wide spread of the 2 sars-cov-2 virus is affecting many aspects of society in particular human health but 3 also social issues 1 2  mental health and the economy 3  medical researchers and 4 researchers from different scientific fields including immunology genetics and 5 bioinformatics are studying the pandemic to find ways to slow its progression machine 6 learning approaches are being utilized to understand aspects of the problem 7 to date most machine learning research on covid-19 has used supervised learning 8 methods or deep learning 4 5 to investigate which might be the important features to 9 predict a predefined outcome running such approaches on the publicly available 10 datasets is associated with difficulties that are due to the fact that features are collected 11 depending on the needs of the data provider which can be a source of bias in 12 particular features that have high predictive value for the outcome for an infected 13 july 3 2020 17 patient might be missing generally speaking the presence or absence of features will 14 impact the accuracy of a model 15 the currently available covid-19 data is missing features and we explore the effect 16 of this by adding a number of features that might be important so as to determine how 17 this affects the accuracy of the model 18 we used data on patients that tested positive for the virus and added new features 19 based on 1 how different countries responded to the pandemic in terms of research 20 sentiment so as to calculate a weighted average polarity score for research abstracts per 21 country and 2 the local weather conditions when the patient was probably infected 22 we found that age is one of the most important factors when we have not incorporated 23 these additional features based on the initial data however after the addition of two new features country-specific research sentiment 25 followed by local weather and age came out to be the most important features recent 26 publications suggest that the weather as represented by the variables temperature and 27 humidity plays a role in covid-19 6 and sars 7  to summarize our main contributions are as follows  we demonstrate how to construct an enhanced set of covid-19 features using 30 additional available information  using this enhanced dataset we show that the extreme gradient 32 boosting xgboost method achieves 95 accuracy in predicting a patients 33 survival  we show that country-specific research sentiment followed by age and local 35 weather and are the most important features we first compiled an initial dataset by combining data from two sources processing was carried out on this dataset s1 file who covid-19 database 48 we downloaded a database of literature on covid-19 from the world health 49 organization who web site httpswwwwhointemergenciesdiseasesnovel-50 coronavirus-2019global-research-on-novel-coronavirus-2019-ncov on april 13 2020 of 51 the 5354 downloaded entries we kept only those whose journal name and doi 52 fields were not blank which resulted in 4683 publications in 590 journals we then 53 analyzed these publications to determine the authors institute and country s2 file in this paper we present an enhanced covid-19 dataset which is based on the above 56 described initial database the data is enhanced by adding features that reflect the 57 local weather and research sentiment in the country of the infected person as described 58 in the following s3 file addition feature construction 60 database construction was performed as outlined in fig 1 it has been demonstrated 61 that there is a link between environmental factors and the development of covid-19 8  62 indeed it seems reasonable to suspect that the weather conditions play a role for a given country we assume that the researchers attitude toward covid-19 will 70 reflect the response capacity of the country to some extent for journal publications 71 obtained from the who database we extracted the authors institution with the help 72 of the papers doi we then applied sentiment analysis on each abstract so as to obtain 73 a polarity score for every abstract and we then calculated an weighted average polarity 74 score for each country this feature was added to the enhanced dataset is filtered for patients for which the outcome has been recorded and then for these items the weather is determined using the httpswwwwundergroundcom website the who covid-19 literature database is filtered for items for which both a journal name and doi are provided and these are post-processed so as to obtain a country-wise research sentiment polarity score xgboost is then trained and run on both the initial and the enhanced data and the accuracy of survival prediction is shown to be 85 and 95 respectively variables sex age the time interval between the patients onset date confirmed infected 80 date and admission date symptoms description infection reason and outcome we will 81 refer to this as the initial dataset 82 we added local weather variables temperature humidity climate description and 83 the weighted polarity score of countrys research attitude we will refer to the result of 84 this as the enhanced dataset to prepare for analysis with xgboost as discussed below we tokenized all 86 multi-value text features such as symptom description or climate description into 87 july 3 2020 37 three-dimensional embedding vectors used label encoding on categorical variables such 88 as infected reason as shown in table 1  89 we assigned the constant -999 to all missing values after filtering for samples that 90 have a valid outcome value we obtain 183 samples processing on data obtained from various social media 10 11 12  along these lines we 102 performed sentiment analysis on the abstracts of research papers associated with 103 covid-19 using the python package textblob httpsgithubcomsloriatextblob 104 which operates by analyzing text content and assigns emotional values to word based on 105 matches to a built-in dictionary our aim is to predict whether the patient will survive the infection based on either the 108 initial dataset or the enhanced dataset 109 we use the extreme gradient boosting xgboost 13 method to address this xgboost is a powerful member of the gradient boosting family which is designed to 111 perform well on sparse features and is known to perform well on kaggle tasks this 112 approach avoids overfitting using its built-in l1 and l2 regularization on the target 113 function as an additive model xgboost consists of k base models and in most cases we 115 choose the tree model as its base model suppose for the k-th of t iterations that we 116 july 3 2020 47 train the tree model f k x then is the estimate result of the sample i after t times iteration during construct of each 118 tree xgboost minimizes the objective function with regularization term introduced in 119 eq 1 in the split phase of each node in each tree we calculate the gain of the feature 120 and choose the tree who has the biggest value as the leaf node to be split implementation 122 in this study we ran the xgboost algorithm on two different datasets namely the 123 initial dataset and the enhanced dataset the latter containing additional features 124 representing local weather and research sentiment as illustrated in fig 1 125 to obtain the model with the best capacity for prediction we used a grid search for 126 model tuning each subtree in our model is a simple tree whose maximum depth is 3 the learning rate was 001 during the training step we randomly sampled the columns 128 of each tree according to a ratio of 05 we evaluated the algorithms performance by calculating each models classification 131 accuracy the accuracy of the model created by using the initial dataset no added 132 features is 85 whereas using the enhanced dataset with added features the models 133 accuracy is 95 the method we chose to evaluate the importance score of feature is based on 135 counting the number of times that a feature occurred in a tree the feature importance 136 for both datasets is shown in fig 3 for the initial dataset age plays a more important 137 role than other features for the model based on the enhanced dataset weighted 138 average research sentiment polarity score is more important than age whereas the level 139 of importance of weather is similar to that of age the performance of machine learning methods depends on the amount and quality of 142 available features for our analysis we can say that the current publicly available data 143 is poor first the data is quite sparse and there are too few features here we see that 144 by enhancing the dataset the accuracy of survival prediction can be increased by 10 145 our study shows how one might enhance a dataset by adding informative features if 146 they are not available in the original dataset here we demonstrated this for 147 country-wise research sentiment and local weather local weather conditions has been 148 implicated as an important feature in the existing research our analysis confirms the observation that age is an important factor for survival of 150 covid-19 however in the data considered here the total number of deaths above age 151 july 3 2020 57 60 were 8 and 16 survived or were still alive while in the age group between 40-60 there 152 were 2 deaths and 36 alive or survived hence linking mortality to a particular age 153 group is not be appropriate based on the current result while this analysis suggests 154 that elderly have a higher risk of death which has already been observed 14 15  saying 155 mortality is associated with old age is probably generally true for any infectious disease 156 age is one of the confounding factors that could be responsible for enhanced covid-19 157 mortality rate so more emphasis should be be taken for the elderly care 16 17  for the model based on the enhanced dataset the weighted average of research 159 sentiment followed by weather and age appear as the most important features and 160 account for the increase in the accuracy of the model this confirms that environmental 161 conditions play a role also it suggests the research sentiment might reflect a countries 162 ability to tackle the disease finally this analysis suggests that enhancing a dataset rather than just analyzing 164 the originally given features might lead to a better prediction of the particular outcome 165 supporting information 166 s1 file initial covid-19 dataset 183 cases s2 file processed who publication data s3 file enhanced covid-19 dataset funding this work was supported by the bmbf-funded denbi cloud within the german network for bioinformatics infrastructure denbi 031a537b 031a533a 031a538a 031a533b 031a535a 031a537c 031a534a 031a532b also we acknowledge support by the open access publishing fund of university of tbingen  analyzing the epidemiological outbreak of covid19 a visual exploratory data analysis approach samrat dey k md rahman mahbubur umme siddiqi r arpita howlader   recent pneumonia outbreak in wuhan china has brought closely into our sight the 2019 novel coronavirus 2019ncov this new coronavirus named 2019ncov belonging to the orthocoronavirinae subfamily distinct from middle east respiratory syndromecoronavirus and severe acute respiratory syndrome coronavirus sarscov was described by zhu et al
1
 the first case of an unexplained new pneumonia origin was detected on 12 december 2019 and was later determined by the chinese center for disease control and prevention cdc as a nonsars ncov the coronaviridae family consists of a group of large single and plus stranded rna viruses isolated from multiple species and it is known to cause the common cold and diarrheal diseases in humans
2
 
3
 in 2003 the sars outbreak was associated with a new coronavirus that is sarscov
2
 
3
 however a number of cases of unknown cause of pneumonia occurred in wuhan hubei china in december 2019 with clinical presentations closely resembling viral pneumonia
4
 a total of 1975 cases of pneumonia have been confirmed in china so far according to the state council information office in beijing chinas capital 26 january 2020
5
 
6
 yet the virus has tended to spread out of china since one case in thailand one case in japan and two cases in korea had been sequentially reported since 15 january 2020
7
 surprisingly the transmission of animals to humans is considered the origin of epidemics as in november many patients reported to have visited a local fish and wild animal market in wuhan apart from this recently evidence has been gathered for the animal to the human and interhuman transmission of the virus
6
 
8
 the situation is getting serious day by day and for further prevention and control it is imperative that we have a better understanding of its pandemic nature on 30 january 2020 world health organization who declared that covid19 outbreak as the sixth public health emergency of international concern following h1n1 2009 polio 2014 ebola in west africa 2014 zika 2016 and ebola in the democratic republic of congo 2019
9
 meanwhile on 11 february 2020 the who announced a new name for the epidemic disease caused by 2019ncov coronavirus disease covid19 until 24 february 2020 2019ncov has affected more than 79 331 patients in 29 countriesregions and has become a major global health concern on the basis of situational report35 by who china has confirmed 77 262 cases with 2595 deaths reported until 24 february 2020 however the rest of the world has confirmed 2069 cases 300 new cases with 23 deaths reported so far till now china reported 415 new confirmed cases with 150 new deaths in comparison there were 300 new confirmed cases reported outside of china with the number of new deaths being 6 httpswwwwhointdocsdefault-sourcecoronavirusesituation-reports20200224-sitrep-35-covid-19pdfsfvrsn1ac4218d2 with regard to the virus itself the international committee on virus taxonomy has renamed 2019ncov as sarscov2
10
 as the outbreak of the novel sarscov2 is expanding rapidly in china and beyond threatening to become a global pandemic epidemiological data need to be analyzed in a way so that the exploratory data analysis eda methods and visualization model will increase the situational awareness among the mass community in upcoming days health workers governments and the public therefore need to cooperate globally to prevent its spread for this study various sources of the dataset have been used for our analysis and visualizations mainly we have used three different sources of dataset including 2019 coronavirus dataset januaryfebruary 2020 which tracks the spread of 2019ncov covid19 ncov19 coronavirus spread dataset which consists of number of confirmed death and recovered reported and 2019ncov dataset which handles the day level information on 2019ncov affected cases table 1 provides insight on each dataset and their respective data files with their column description moreover we also developed table 2 for providing the data analyst with a comprehensive knowledge of every column for the used dataset we have enlisted each distinguished column from all three different sources of the dataset and assemble it according to their appearances in the dataset we analyzed our datasets with different eda methods and visualize those data to provide a sufficient consciousness regarding the outbreak of covid19 all over the globe our exploit data performed with the 2019 coronavirus dataset januaryfebruary 2020 covid19 ncov19 coronavirus spread dataset and 2019ncov datasets here we present an effort to visualize and analyze data between 22 january 2020 and 16 february 2020 however a massive number of cases are reported in china compared to the rest of the world and interestingly the next few affected countries are the neighbors of china moreover even in china most of the cases reported are from a particular province hubei it is no surprise because hubeis capital is wuhan where the first cases are reported till now covid19 propagated almost 29 countries worldwide and 31 states or provinces in china outside china as expected there was not much death due to covid19 recorded only five deaths outside china are reported until 16 february 2020 there are however more cases of recovered than death and in comparison with 1770 deaths there were recovery cases of 10 865 patients we also provide a map representation of different countries with confirmed cases and death reported respectively till 16 february 2020 based on their longitude and latitude we have enlisted all 29 affected countries outside china and the number of confirmed cases of different chinese provinces for providing a vibrant depiction of this intense sarscov2 table 3 this section will discuss different timeseries data by using some visual exploratory data analysis veda methods we designed a worldwide map and provides a knowledge of how sarscov2 spread from 22 january 2020 to 16 february 2020 all around the globe each map segment represents a region by using visual data analytics it helps the individuals to understand the epidemiological nature of covid19 from the map representation it is apparent that china reported the highest confirmed cases with a high number of 70 446 likewise china reported the highest number of death and it was 1765 till 16 february 2020 we also examine the timeseries data using veda to provide a strong and understandable outcome of this extreme outbreak of covid19 it is obvious that analyzing these data in realtime is extremely useful in capturing an epidemic behavior of this severe disease we believe this method of analyzing data will certainly increase the understanding of the situation and inform interventions all the data analysis and visualization models that we have analyzed for this study including eda and veda are available at this url httpsamratdeymevisualizationhtml this study analyzed three different categories of data including confirmed death and recovered cases inside china for a time period of 22 january to 16 february 2020 this will also provide a comparative analysis of all the cases reported inside and outside of china however we present different cases worldwide to comprehend the specific numbers of cases reported for a specific time period after analyzing there were 58 182 confirmed cases of covid19 on 16 february 2020 however the highest number of deaths reported in china on 16 february 2020 was 1696 surprisingly there was a significant number of recovered cases reported till 16 february 2020 and it was around 6639 figure 1 another investigation on different cases of covid19 including confirmed death and recovered reported outside china also provided in this study this also contains three different data representations of confirmed death and recovered cases outside china for a time period of 22 january 2020 to 16 february 2020 after analyzing the data there were 399 confirmed cases of covid19 on 16 february 2020 globally however the highest number of deaths reported outside china on 16 february 2020 was 5 surprisingly there was also a significant number of recovered cases till 16 february 2020 and it is more than 100 according to the dataset we examined figure 2 table 3 represents all the confirmed cases reported in china provinces between 22 january 2020 to 16 february 2020 for a better understanding of the scenario we designed a tree map and visualize the data according to different criteria our designed tree map contains all the provinces that confirmed the presence of sarscov2 patients in china according to the tree map it is evident that hubei province reported the highest number of confirmed cases and the number was 58 182 the next province is guangdong with most 1316 confirmed cases reported alternately we also visualize the tree map for the number of deaths and recovered cases reported between 22 january 2020 to 16 february 2020 the highest number of deaths reported in hubei province in china the greatest number of deaths reported in hubei was 1696 till 16 february 2020 and the next highest number found in henan with 13 deaths reported till the mentioned date however 6639 successful recovered cases were reported in hubei and 465 were in guangdong and 464 were in hunan table 3 moreover a tree map representation for all the countries that have reported confirmed deaths and recovered cases globally except china also provided in this exploration singapore reported the highest number of confirmed cases of covid19 immediately after china with a number of 75 and japan with a confirmed case of 59 is in the next position after singapore in terms of deaths reported globally except china all the five countries france japan hong kong taiwan and the philippines reported one death case between 22 january 2020 to 16 february 2020 table 3 however singapore reported the highest number of recovered cases of 2019ncov with a number of 18 and thailand with a recovered case of 14 is in the next position after singapore in terms of recovery figure 3 enlists the data of comparative analysis confirmed  c recovered  r and deaths  d of hubei other provinces of china and the rest of the world till 16 february 2020 this representation demonstrates that hubei has endured the largest number of infected patients c  58 182 however hubei has also maintained a significant recovery rate of r  6639 patients along with the mortalities of d  1969 persons on the other hand rest of the provinces in china has confirmed c  12 264 patients infected by sarscov2 virus till 16 february 2020 like hubei other provinces in china also showed a dramatic recovery rate of r  4109 patients along with confirmed deaths of d  69 persons as of 16 february 2020 data from the different sources showed that there was a total of c  425 confirmed cases of covid19 worldwide among them only five deaths have been reported globally with a steady recovery rate of r  117 patients from the observation it is apparent that there has been a steady rise in the daily total number of covid19 cases globally both within and outside china till 16 february 2020 regarding new cases of covid19 till 24 february 2020 both within and outside china there has been a dramatic increase in the number of new cases it was reported that china has confirmed n  41577 262 confirmed new cases while the rest of the world has confirmed n  3002069 confirmed caused by the sarscov2 virus another example of the importance of animalhuman interface infections is the current outbreak of the covid19 disease and the issues resulting from the advent of a newly identified organism as it spreads through individuals and across national and international frontiers at the advent of an outbreak such as this readily available data and information are equally important to begin the evaluation needed to understand the risks and start containment outbreak activities such information includes initial reports of countries with confirmed death and recovered cases ratio also how the countries outside china are affecting how the province of china are struggling to handle the situation of covid19 and more importantly ratio analysis of these realworld data as well as information obtained from specific regions of globally from past outbreaks information and understanding of the consequences are needed to help us to refine the risk assessment as the outbreak continues and to ensure that patients are best managed much of this information emerges in realtime challenges our understanding and yet refines our responses the analysis presented here based on eda and veda with the help of the dataset provided by john hopkins university who cdc national health commission and dxy however we have preprocessed and cleaned the dataset information according to our needs for storing and analyzing the data we have used the pythonbased library of numpy httpsnumpyorg and pandas httpspandaspydataorg matplotlib httpsmatplotliborg plotly httpsplotly seaborn httpsseabornpydataorg and folium were also used to visualize the highlighted data in an interactive manner all the experiment with the dataset has been made by using the support of jupyter notebook httpsjupyterorg in a linux based local machine platform by using python language in the machine learning research lab at the dhaka international university we report here each and every single detail of different cases of covid19 between 22 january 2020 to 16 february 2020 currently there is an obvious urgency to understand the consequences of sarscov2 viruses not only in china but also worldwide to aware of ourselves for upcoming days therefore this is a minor initiative of analyzing realworld timeseries data and visualize them in such a manner so that people around the globe have better understandings of its severe nature we are still observing the undesirable prevalence of this sarscov2 virus and to date 24 february 2020 the number of death cases reported was 2618 and among them only 23 death cases reported outside china this is extremely alarming not only to china but to the rest of the world as well in this study we have enlisted the most reported cases in china outside china and in different provinces in china we also analyzed the number of affected countries with reported confirmed deaths and recovered cases apart from that we designed map view and tree maps view with an appropriate number to analyze the epidemiological outbreak of the covid19 in conclusion the dataset we have used for our experiment 2019 coronavirus dataset januaryfebruary 2020 covid19 ncov19 coronavirus spread dataset and 2019ncov dataset can be useful to monitor the emerging outbreaks such as 2019ncov such activities can help us to generate and disseminate detailed information to the scientific community especially in the early stages of an outbreak when there is a little else available allowing for independent assessments of key parameters that influence interventions we observe an interesting different case reported based on the different datasets of 2019ncov which helps us to understand that it needs more epidemiological and serological studies we also investigated early indications that the response is being strengthened in china and worldwide on the basis of a decrease in the case of detection time and rapid management of internationally identified travelrelated cases as a caveat this is an early data analysis and visualization approach of a situation that is rapidly evolving to the best of our knowledge this is the very first attempt on covid19 which focuses on the veda based on different data sources however knowledge about this novel sarscov2 virus remains limited among general people around the globe raw data released by various sources are not adequately capable to provide an informative understanding of covid19 caused of sarscov2 therefore a userfriendly data visualization model will be more effective to understand the epidemic outbreak of this severe disease visualization model like map view and tree map view provides an interactive interface and visualize each and every raw fact in a comprehensive manner hopefully in the coming weeks we will continue to monitor this outbreaks epidemiology data that we have used in this study and from other official sources the authors declare that there are no conflict of interests skd and mdmr had the idea for and designed the study and had full access to all the data in the study and take the responsibility for the data and accuracy of the data analysis with their visualization urs and ah contributed to the writing of the study mdmr contributed to critical revision of the report all the visualization and data presentation methods were developed by skd and mdmr all authors contributed to data acquisition data analysis and reviewed and approved the final version  analyzing covid-19 on online social media trends sentiments and emotions xiaoya li   mingxin zhou jiawei wu arianna yuan fei wu   jiwei li shannon ai  at the time of writing the ongoing pandemic of coronavirus disease has caused severe impacts on society economy and peoples daily lives people constantly express their opinions on various aspects of the pandemic on social media making user-generated content an important source for understanding public emotions and concerns in this paper we perform a comprehensive analysis on the affective trajectories of the american people and the chinese people based on twitter and weibo posts between january 20th 2020 and may 11th 2020 specifically by identifying peoples sentiments emotions ie anger disgust fear happiness sadness surprise and the emotional triggers eg what a user is angrysad about we are able to depict the dynamics of public affect in the time of covid-19 by contrasting two very different countries china and the unites states we reveal sharp differences in peoples views on covid-19 in different cultures our study provides a computational approach to unveiling public emotions and concerns on the pandemic in real-time which would potentially help policy-makers better understand peoples need and thus make optimal policy  the emergence of covid-19 in early 2020 and its subsequent outbreak have affected and changed the world dramatically according to the world health organization who by mid-may 2020 the number of confirmed covid-19 cases has reached 5 millions with death toll over 300000 world wide several mandatory rules have been introduced by the government to prevent the spread of the coronavirus such as social distancing bans on social gatherings store closures and school closures despite their positive effects on slowing the spread of the pandemaic they neverthless caused severe impacts on the society the economy and peoples everyday life there have been anti-lockdown and anti-social-distancing protests in many places around the world given these difficult situations it is crucial for policy-makers to understand peoples opinions toward the pandemic so that they can 1 balance the concerns of stoping the pandemic on the one hand and keeping people in good spirits on the other hand and 2 anticipate peoples reactions to certain events and policy so that the policymakers can prepare in advance more generally a close look at the public affect during the time of covid-19 could help us understand peoples reaction and thoughts in the face of extreme crisis which sheds light on humanity in moments of darkness people constantly post about the pandemic on social media such as twitter weibo and facebook they express their attitudes and feelings regarding various aspects of the pandemic such as the medical treatments public policy their worry etc therefore user-generated content on social media provides an important source for understanding public emotions and concerns in this paper we provide a comprehensive analysis on the affective trajectories of american people and chinese people based on twitter and weibo posts between january 20th 2020 and may 11th 2020 we identify fine-grained emotions including anger disgust fear happiness sadness surprise expressed on social media based on the user-generated content additionally we build nlp taggers to extract the triggers of different emotions eg why people are angry or surprised what they are worried etc we also contrast public emotions between china and the unites states revealing sharp differences in public reactions towards covid-19 related issues in different countries by tracking the change of public sentiment and emotion over time our work sheds light on the evolution of public attitude towards this global crisis this work contributes to the growing body of research on social media content in the time of covid-19 our study provides a way to extracting public emotion towards the pandemic in real-time and could potentially lead to better decision-making and the development of wiser interventions to fight this global crisis the rest of this paper is organized as follows we briefly go through some related work in section 2 we then present the analyses on topic trends in weibo and twitter section 3 the extracted emotion trajectories section 4 and triggers of those emotions section 5 we finally conclude this paper in section 6 at the time of writing analyses on peoples discussions and behaviors on social media in the context of covid-19 has attracted increasing attention 1 analyzed tweets concerning covid-19 on twitter by selecting important 1-grams based on rank-turbulence divergence and compare languages used in early 2020 with the ones used a year ago the authors observed the first peak of public attention to covid-19 around january 2020 with the first wave of infections in china and the second peak later when the outbreak hit many western countries 2 released the first covid-19 twitter dataset 3 provided a ground truth corpus by annotating 5000 texts 2500 short  2500 long texts in uk and showed peoples worries about their families and economic situations 4 viewed emotions and sentiments on social media as indicators of mental health issues which result from self-quarantining and social isolation 5 revealed increasing amount of hateful speech and conspiracy theories towards specific ethnic groups such as chinese on twitter and 4chans other researchers started looking at the spread of misinformation on social media 6  7  8 provide an in-depth analysis on the diffusion of misinformation concerning covid-19 on five different social platforms discrete emotion theory 9  10  11 think that all humans have an innate set of distinct basic emotions paul ekman and his colleagues 12 proposed that the six basic emotions of humans are anger disgust fear happiness sadness and surprise ekman explains that different emotions have particular characteristics expressed in varying degrees researchers have debated over the exact categories of discreate emotions for instance 13 proposed eight classes for emotions including love mirth sorrow anger energy terror disgust and astonishment automatically detecting sentiments and emotions in text is a crucial problem in nlp and there has been a large body of work on annotating texts based on sentiments and building machine tools to automatically identify emotions and sentiments 14  15  16  17  18 created the first annotated dataset for four classes of emotions anger fear joy and sadness in which each text is annotated with not only a label of emotion category but also the intensity of the emotion expressed based on the best-worst scaling bws technique 19  a follow-up work by 20 created a more comprehensively annotated dataset from tweets in english arabic and spanish the dataset covers five different sub-tasks including emotion classification emotion intensity regression emotion intensity ordinal classification valence regression and valence ordinal classification there has been a number of studies on extracting aggregated public mood and emotions from social media 21  22  23  24  facebook introduced gross national happiness gnh to estimate the aggregated mood of the public using the liwc dictionary results show a clear weekly cycle of public mood 25 and 26 specially investigate the influence of geographic places and weather on public mood from twitter data the mood indicators extracted from tweets are very predictive and robust 23  27  therefore they have been used to predict real-world outcomes such as economic trends 24  28  29  30  stock market 31  32  influenza outbreak 33  and political events 34  35  36  37  in this section we present the general trends for covid19related posts on twitter and weibo we first present the semisupervised models we used to detect covid-19 related tweets next we present the analysis on the topic trends on the two social media platforms for twitter we first obtained 1 of all tweets that are written in english and published within the time period between january 20th 2020 and may 11th 2020 the next step is to select tweets related to covid-19 the simplest way as in 2  7  is to use a handcrafted keyword list to obtain tweets containing words found in the list however this method leads to lower values in both precision and recall for precision usergenerated content that contains the mention of a keyword is not necessarily related to covid-19 for example the keyword list used in 2 include the word china and it is not suprising that a big proportion of the posts containing china is not related to covid-19 for recall keywords for covid-19 can change over time and might be missing in the keyword list to tackle this issue we adopt a bootstrapping approach the bootstrapping approach is related to previous work on semisupervised data harvesting methods 38  39  40  in which we build a model that recursively uses seed examples to extract patterns which are then used to harvest new examples those new examples are further used as new seeds to get new patterns to be specific we first obtained a starting seed keyword list by 1 ranking words based on tf-idf scores from eight covid-19 related wikipedia articles 2 manually examining the ranked word list removing those words that are apparently not covid-19 related and use the top 100 words in the remaining items then we retrieved tweets with the mention of those keywords next we randomly sampled 1000 tweets from the collection and manually labeled them as either covid-19 related or not the labeled dataset is split into the training development and test sets with ratio 811 a binary classification model is trained on the labeled dataset to classify whether a post with the mention of covid-related keywords is actually covid-related the model is trained using bert 41 and optimized using adam 42  hyperparameters such as the batch size learning rate are tuned on the development set next we obtain a new seed list by picking the most salient words that contribute to the positive category in the binary classification model based on the first-order derivative saliency scores 43  44  45  this marks the end of the first round of the bootstrapping next we used the new keyword list to re-harvest a new dataset with the mention of the keyword 1000 of which is selected and labeled to retrain the binary classification model we repeat this process for three times  we report the intensity scores for weibo and twitter in figure  1  we split all tweets by date where x t denotes all tweets published on day t the value of intensity is the number of posts classified as covid-related divided by the total number of retrieved posts ie x t  on weibo we observe a peak in late january and february then a drop followed by another rise in march and a gradual decline afterwards the trend on chinese social media largely reflects the progress of the pandemic in china the outbreak of covid-19 and the spread from wuhan to the rest of the country corresponds to the first peak the subsequent drop reflects the promise in containing the virus followed by a minor relapse in march for twitter we observe a small peak that is aligned with the news from china about the virus the subsequent drop reflects the decline of the attention to the outbreak in china the curve progressively went up since march corresponding to the outbreak in the us upon the writing of this paper we have not observed a sign of drop in the intensity score of covid19-related posts in this section we present the analyses on the evolution of public emotion in the time of covid-19 we first present the algorithms we used to identify the emotions expressed in a given post next we present the results of the analyses we adopted the well-established emotion theory by paul ekman 12  which groups human emotions into 6 major categories ie anger disgust worry happiness sadness and surprise given a post from a social network user we assign one or multiple emotion labels to it 46  47  this setup is quite common in text classification 48  49  50  51  52  for emotion classification of english tweets we take the advantage of labeled datasets from the semeval-2018 task 1e 20  in which a tweet was associated with either the neutral label or with one or multiple emotion labels by human evaluators the semeval-2018 task 1e contains eleven emotion categories in total ie anger anticipation disgust fear joy love optimism pessimism sadness surprise and trust and we only use the datasets for a six-way classification ie anger disgust fear happiness sadness and surprise given that the domain of the dataset used in 20 covers all kinds of tweets and our domain of research covers only covid-related tweets there is a gap between the two domains therefore we additionally labeled 15k covid-related tweets following the guidelines in 20  where each tweet can take either the neural label or onemultiple emotion labels since one tweet can take on multiple emotion labels the task is formalized as a a multi-label classification task in which six binary one vs the rest classifiers are trained we used the description-based bert model 53 as the backbone which achieves current sota performances on a wide variety of text classification tasks more formally let us consider a to-be-classified tweet x  x 1      x l  where l denotes the length of the text x each x will be tagged with one or more class labels y  y  1 n  where n  6 denotes the number of the predefined emotion classes the six emotion categories to compute the probability pyx each input text x is concatenated with the description q y to generate cls q y  sep x where cls and sep are special tokens the description q y is the wikipedia description for each of the emotions for example q y for the category anger is anger also known as wrath or rage is an intense emotional state involving a strong uncomfortable and hostile response to a perceived provocation hurt or threat next the concatenated sequence is fed to the bert model from which we obtain the contextual representations h cls  h cls is then transformed to a real value between 0 and 1 using the sigmoid function representing the probability of assigning the emotion label y to the input tweet x where w 1  w 2  b 1  b 2 are some parameters to optimize classification performances for different models are presented in table 3  for emotion y its intensity score st y for day t is the average probability denoted by p yx of assigning label y to all the texts in that day x t  for non covid-related texts p yx is automatically set to 0 we thus have for chinese emotion classification we used the labeled dataset in 54  which contains 15k labeled microblogs from weibo 1  in addition to the dataset provided by 54  we labeled covid-related 20k microblogs the combined dataset is then used to train a multi-label classification model based on the description-bert model 53  everyday emotion scores for weibo are computed in the same way as for twitter the time series of intensity scores of six different emotions ie sadness anger disgust worry happiness surprise for weibo and twitter are shown in figures 2 and 3  respectively for weibo as can be seen the trend of worry is largely in line with the trend of the general intensity of the covidrelated posts it reached a peak in late january and then gradually went down followed by a small relapse in mid-march for anger the intensity first went up steeply at the initial stage of the outbreak staying high for two weeks and then had another sharp increase around february 8th the peak on february 8th was due to the death of wenliang li a chinese ophthalmologist who issued the warnings about the virus the intensity for anger then gradually decreased with no relapse afterwards the intensity for disgust remained relatively low across time for sadness the intensity reached the peak at the early stage of the outbreak then gradually died out with no relapse for surprise it went up first mostly because of the fact that the public was surprised by the new virus and the unexpected outbreak but then gradually went down the intensity for happiness remained relatively low across time with a small peak in late april mostly because the countrywide lockdown was over for twitter the intensity for worry went up shortly in late january followed by a drop the intensity then went up steeply in mid-march in response to the pandemic breakout in the states reaching a peak around march 20th then decreased a little bit and remained steady afterwards the intensity for anger kept going up after the outbreak in mid-march with no drop observed the trend for sadness is mostly similar to that of the overall intensity for surprise the curve went up first after the breakout in early march reaching a peak around mar 20th then dropped and remained steady afterwards for happiness the intensity remained low over time twitter data in order to extract the emotional triggers from twitters noisy text we first annotate a corpus of tweets for the ease of annotation each emotion is associated with only a single trigger the personentityevent that a user has a specific emotion towardswithabout a few examples are shown as follows with target triggers surrounded by brackets  angry protesters are traveling 100s of miles to join organized rallies over covid- 19 in order to build an emotional trigger tagger we annotated 2000 tweets in total and split them into training development and test sets with ratio 811 we treat the problem as a sequence labeling task using conditional random fields for learning and inference with bert-mrc features 55  comparing with vanilla bert tagger 41  the bert-mrc tagger has the strength of encoding the description of the tobe-extracted entities eg what they are worried about as this description provides the prior knowledge about the entities it has been shown to outperform vanilla bert even when less training data is used in addition to the representation features from bert-mrc we also considered the twittertuned pos features 56  the dependency features from a twitter-tuned dependency parsing model 57 and the twitter event features 58  the precision and recall for segmenting emotional triggers on english tweets are reported in table v  the precision and recall for segmenting triggering event phrases are reported in table 3  we observe a significant performance boost with linguistic features such as pos and dependency features this is mainly due to the small size of the labeled dataset the best model achieves an f1 score of 066 since different extracted tokens may refer to the same concept or topic we would like to cluster the extracted trigger mentions the method of supervised classification is unsuitable for this purpose since 1 it is hard to predefine a list of potential triggers to peoples anger or worry 2 it is extremely labor-intensive to annotate tweets with worry types or anger types and 3 these types may change over time for these reasons we decided to use semi-supervised approaches that will automatically induce worryanger types that match the data we adopt an approach based on lda 59  it was inspired by work on unsupervised information extraction 60  58  61  we use the emotion anger to illustrate how trigger mentions are clustered each extracted trigger mentions for anger is modeled as a mixture of anger types here we use subcategory type and topic interchangeablely all referring to the cluster of similar mentions each topic is characterized by a distribution over triggers in addition to a distribution over dates on which a user talks about the topic taking dates into account encourages triggers that are mentioned on the same date to be assigned to the same topic we used collapsed gibbs sampling 62 for inference for each emotion we ran gibbs sampling with 20 topics for 1000 iterations obtaining the hidden variable assignments in the last iteration then we manually inspected the top mentions for different topics and abandoned the incoherent ones the daily intensity score for a given subcategory k belonging to emotion y is given as follows where pkx is computed based on the parameters of the latent variable model we report the top triggers for different emotions in table ii  we adopt a simple strategy of reporting the most frequent triggers for different emotions for sadness the most frequent triggering events and topics are being test positive and the death of families and friends for anger the top triggers are shutdown quarantine and other mandatory rules people also express their anger towards public figures such as president donald trump mike pence along with china and chinese for worry the top triggers include jobs getting the virus payments and families for happiness the top triggers are recovering from the disease city reopening and returning to work for surprise the public are mostly surprised by the virus itself its spread and the mass deaths it caused next we report the results of the mention clustering for anger and worry in tables 4 and 5  respectively the unsupervised clustering reveals clearer patterns in the triggering events top subcategories for anger include china with racist words such as chink and chingchong lockdown and social distancing public figures like president donal trump and mike pence treatments in hospitals and the increasing cases and deaths table 4 displays the change of intensity scores for the subcategories of anger we observe a sharp increase in public anger toward china and chinese around march 20th in coincidence with president donald trump calling coronavirus chinese virus in his tweets public anger towards the lockdown sharply escalated in mid-march but decreased a bit after late april when some places started to reopen top subcategories for worry include syndromes for covid-19 finance and economy families jobs and food and increasing cases and deaths table 5 displays the change of intensity scores for subcategories of worry people increasingly worried about families over time it is interesting to see that the worry about finance and economy started going up in mid-february earlier than other subcategories in this paper we perform analyses on topic trends sentiments and emotions of the public in the time of covid-19 on social media by tracking the change of public emotions over time our work reveals how the general public reacts to different events and government policy our study provides a computational approach to understanding public affect towards the pandemic in real-time and could help create better solutions and interventions for fighting this global crisis  fully automatic deep convolutional approaches for the analysis of covid-19 using chest x-ray images a preprint joaquim de moura jorge novo marcos ortega  covid-19 is a new infectious disease caused by severe acute respiratory syndrome coronavirus 2 sars-cov-2 given the seriousness of the situation the world health organization declared a global pandemic as the covid-19 rapidly around the world among its applications chest x-ray images are frequently used for an early diagnosticscreening of covid-19 disease given the frequent pulmonary impact in the patients critical issue to prevent further complications caused by this highly infectious disease in this work we propose complementary fully automatic approaches for the classification of chest x-ray images under the analysis of 3 different categories covid-19 pneumonia and healthy cases given the similarity between the pathological impact in the lungs between covid-19 and pneumonia mainly during the initial stages of both lung diseases we performed an exhaustive study of differentiation considering different pathological scenarios to face these classification tasks we exploited and adapted to this topic a densely convolutional network architecture which connects each layer to every other layer in a feed-forward fashion to validate the designed approaches several representative experiments were performed using images retrieved from different public chest x-ray images datasets overall satisfactory results were obtained from the designed experiments facilitating the doctors work and allowing better an early diagnosticscreening and treatment of this relevant pandemic pathology  recognized risk factors such as heart disease lung disease hypertension and diabetes among others 6  furthermore the covid-19 virus is transmitted quite efficiently since an infected person is capable of transmitting the virus to 2 or 3 other people an exponential rate of increase 7  over the years x-ray examination of the chest plays an important clinical role in detecting or monitoring the progression of different pulmonary diseases such as emphysema chronic bronchitis pulmonary fibrosis lung cancer or pneumonia among others 8 9 10  today given the severity of the coronavirus pandemic radiologists are asked to prioritize chest x-rays of patients with suspected covid-19 infection over any other imaging studies allowing for more appropriate use of medical resources during the initial screening process and excluding other potential respiratory diseases a process which is extremely tedious and time-consuming in this context the visibility of the covid-19 infection in the chest x-ray is complicated and requires experience of the clinical expert to analyze and understand the information and differentiate the cases from other diseases of respiratory origin with similar characteristics such as pneumonia as illustrated in figure 1  for that reason a fully automatic system for the classification of chest x-ray images between healthy pneumonia or specific covid-19 cases is significantly helpful as it drastically reduces the workload of the clinical staff complementary it may provides a more precise identification of this highly infectious disease reducing the subjectivity of clinicians in the early screening process and thus also reducing healthcare costs figure 1  representative examples of chest x-ray images 1 st row chest x-ray images from healthy patients 2 nd row chest x-ray images from patients with pneumonia 3 rd row chest x-ray images from patients with covid-19 2  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg101101202005 0120087254 doi medrxiv preprint given the relevance of this topic several approaches were recently proposed using chest x-ray images for the classification of covid-19 as reference sun et al 11 proposed an approach based on deep transfer learning using chest x-ray images for the detection of patients infected with coronavirus pneumonia in the work of hassanien et al 12  the authors proposed a methodology for the automatic x-ray covid-19 lung classification using a multi-level threshold based on otsu algorithm and support vector machine for the prediction task apostolopoulos et al 13 proposed a study on the possible extraction of representative biomarkers of covid-19 from x-ray images using deep learning strategies wang et al 14 proposed a deep convolutional neural network called covid-net design tailored for the detection of covid-19 cases from chest radiography images in the work of hammoudi et al 15  the authors proposed a deep learning strategy to automatically detect if a chest x-ray image is healthy pneumonia bacterial or viral assuming that a patient infected by the covid-19 tested during an epidemic period has a high probability of being a true positive when the result of the classification is a virus despite the satisfactory results obtained by these works most of them only partially address this recent and relevant problem of global interest limiting their practical utility for usage and interpretation for support in clinical decision scenarios such as emergency triage for example therefore in order to offer a more comprehensive methodology we propose in this work complementary fully automatic approaches for the classification of covid-19 pneumonia and healthy chest x-ray radiographs to achieve this we adapted to this issue a densely convolutional network architecture which generally connects each layer to every other layer in a feed-forward fashion in this way the proposed approaches allow to make predictions using complete chest x-ray images of arbitrary sizes which is very relevant considering the great variability of x-ray devices currently available in health centers to validate our proposal exhaustive representative experiments were performed using images compiled from different public image datasets the manuscript is organized as follows section 2 describes the materials and methods that were used in this research work section 3 presents the results and validation of the proposed approaches section 4 includes the discussion of the experimental results finally section 5 presents the conclusions about the proposed systems as well as possible future lines of work in this enormeous topic of interest 3  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 6 2020  a schematic representation of the proposed paradigm of the different approaches can be seen in figure 2  the proposed systems receive as input a chest x-ray radiography during the acquisition procedure the patient is exposed to a small dose of ionizing radiation to produce images of the interior of the chest the technician will usually be behind a protected wall or in an adjacent room to activate the x-ray machine the proposed system then uses advanced artificial intelligence techniques to classify chest x-ray images into 3 different clinical categories healthy pneumonia or covid-19 as a result the system provides useful clinical information for the initial screening process and for subsequent clinical analyses given the similarity between the pathological impact in the lungs between covid-19 and common types of pneumonia mainly during the initial stages of both lung diseases we performed an exhaustive analysis of differentiation considering different pathological scenarios in this line we proposed 4 different and independent computational approaches for the classification of covid-19 pneumonia and healthy chest x-ray radiographs each of these approaches is explained in more detail below using as reference a consolidated public image dataset for the identification of pneumonia subjects with a considerable amount of image samples 16  we firstly used as baseline a trained model for the differentiation of healthy and pneumonia chest x-ray images taking advantage of this large amount of available information subsequently we tested the potential of this approach to classify chest x-ray radiographs of patients diagnosed with covid-19 and measure their similarity with both situations in this way we can analyze the percentage of chest x-ray images of patients with covid-19 that may be classified as pneumonia giver their relation in the pathological pulmonary impact given the pathological similarity between pneumonia and covid-19 subjects subsequently we designed an screening process that analyzes the degree of separability between healthy and pathological chest x-ray radiographs considering these both pathological scenarios in this sense we include the chest x-ray images of patients diagnosed with pneumonia or covid-19 under the same class in a training of the model to predict 2 different categories pathological and healthy cases additionally we adapted and trained a model to specifically identify covid-19 subjects measuring the capability of differentiation not only from healthy subjects but also from those pathological cases with a significant similarity as patients suffering from pneumonia with this in mind our system was designed to identify two different classes including healthy and patients with pneumonia in the same category finally we designed another approach to simultaneously determine the degree of separability between the 3 categories of chest x-ray images considered in this work to this end we trained a model with a set of chest x-ray radiographs of the 3 different classes healthy subjects patients diagnosed with pneumonia patients diagnosed with covid-19 the use of deep learning architectures has been rapidly increasing in the field of medical imaging including computeraided diagnosis system and medical image analysis for more accurate screening diagnosis prognosis and treatment of many relevant diseases 17 18  these architectures have proven to be superior in both accuracy and predictive efficiency compared to classical machine learning techniques 19  in this work we use a densely convolutional network architecture inspired by densenet proposed by huang et al 20  given its simplicity and potential providing adequate results for many similar classification tasks in pulmonary diseases 21 22 23  in our case this dense network architecture was initialized with weights from a model pretrained on imagenet connecting each layer to every other layer in a feed-forward fashion within each dense block on the one hand we benefit from the pre-trained model weights making the learning process much more efficient in other words the model converges fast because his weights are already stabilized initially on the other hand it significantly reduces the amount of labeled data required 4  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg1011012020050120087254 doi medrxiv preprint for model training for each layer feature maps from all preceding layers are processed as separate inputs while their own feature maps are transferred as inputs to all subsequent layers in this study we employed a modification of the original structure of the densenet-161 architecture as illustrated in figure 3  in particular we have adapted the classification layer of the architecture used to support the output according to the specific requirements of each proposed approach which is to categorize chest x-ray images into 2 or 3 different clinical classes considering healthy pneumonia and covid-19 figure 3  an illustration of the densenet architecture that was adapted for the different and independent computational approaches of this work regarding the training stage of the different approaches considering the limited amount of covid-19 subjects we decided that the employed chest x-ray radiographs dataset was randomly divided into 3 smaller datasets specifically with 60 of the cases for training 20 for validation and the remaining 20 for testing additionally the classification step was performed with 5 repetitions being calculated the mean cross-entropy loss 24 and the mean accuracy to illustrate the general performance and stability of the proposed approaches the densenet-161 architecture was trained using stochastic gradient descent sgd with a constant learning rate of 001 a mini-batch size of 4 and a first-order momentum of 09 in particular sgd is a simple but highly efficient approach for the discriminative learning of classifiers under convex loss functions 25  data augmentation is a widely used strategy that enables practitioners to significantly increase the diversity of data available for training models reducing the overfitting and making the models more robust 26 27  this is especially significant in our case given the limited amount of positive covid-19 cases that were used in the different approaches to this end we applied different configurations of affine transformations to increase the training data and improve the performance of the neural network architecture that was used to classify chest x-rays images in particular we automatically generate additional training samples through a combination of scaling with horizontal flipping operations considering the common variety of possible resolutions as well as the simmetry of the human body the training and evaluation of the deep convolutional models were performed with chest x-ray radiographs that were taken from 3 different chest x-ray public datasets of reference the chest x-ray pneumonia dataset 16  the covid-19 image data collection dataset 28 and the covid-19 sirm dataset 29  the chest x-ray pneumonia dataset of the radiological society of north america rsna 16 is composed by of 5863 chest x-ray radiographs this public dataset was labeled into 2 main categories healthy patients and patients with different types of pneumonia viral and bacterial presenting therefore a high level of heterogeneity currently public chest x-ray datasets of patients diagnosed with covid-19 are very limited despite this important restriction we have built a dataset composed of 207 radiographs 155 were taken from covid-19 image data collection dataset 28 and 52 were taken from the covid-19 sirm dataset of the italian society of medical radiology 29  in order to test its suitability the designed paradigm of the different approaches was validated using different statistical metrics commonly used in the literature to measure the performance of computational proposals in similar medical imaging tasks accordingly precision recall f1-score and accuracy were calculated for the quantitative validation of the classification results in particular the first three metrics are calculated for each one of the considered classes in the different experiments are they are more meaningful in that way  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg1011012020050120087254 doi medrxiv preprint fully automatic deep convolutional approaches for the analysis of covid-19 using chest x-ray images mathematically these statistical metrics are formulated as indicated in equations 1 2 3 and 4 respectively these performance measures use as reference the true negatives tn false negatives fn true positives tp and false positives fp to evaluate the suitability of the different proposed approaches in the pathological classification related to covid-19 in chest x-ray images we conducted different complementary experiments taking as reference the available datasets in particular for each experiment we performed 5 independent repetitions each time with a different random selection of the samples splits specifically with 60 of the cases for training 20 for validation and the remaining 20 for testing additionally the training stage was stopped after 200 epochs given the lack of significant further improvement in both accuracy and cross-entropy loss results given the availability of a public image dataset of reference with a significant number of healthy and pneumonia chest x-ray images 16  we trained a model to obtain a consolidated approach to distinguish between healthy patients and different pathological cases of pneumonia in this line we designed an experiment with a total of 5856 x-ray images being 1583 from healthy patients and 4273 from patients with pneumonia figure 4 shows the performance that was obtained using the deep learning architecture after as previously indicated 5 independent repetitions in the training and validation stages the method achieved satisfactory results reaching a best average accuracy of 09971  00026 for training in epoch 194 and the best average accuracy for validation of 09624  00067 in epoch 68 as we can see in figure 4 a furthermore in figure 4 b we can observe that the model converged quite fast in the training and validation steps in terms of the loss cross-entropy function in table 1  we can see the precision recall and f1-score results obtained at the test stage providing an accuracy of 097 all the results obtained show the robustness of the proposed system in the classification of the different pathological cases of pneumonia and healthy patients in addition to measure the capability of the adapted architecture to this pathological analysis another relevant goal of this experimentation was to conduct a comprehensive analysis about the potential similarity of the covid-19 subjects with pneumonia scenarios that is the percentage of chest x-ray images of patients with covid-19 that are classified as pneumonia with the previously trained network thus we extended the experiments of this case using an additional blind test dataset in particular this dataset consisted of the used 207 x-ray images of patients diagnosed with covid-19 as we can see in table 2  the proposed system achieves satisfactory results in terms of precision recall and f1-score considering covid-19 cases correctly 6  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020 classified as pneumonia in opposition to healthy cases in this scenario the system achieves an accuracy of 083 demonstrating that the system is capable of correctly screening x-ray images of covid-19 patients in the pathological pneumonia category under the results of the analysis of the first approach we designed another scenario with a screening context including covid-19 cases by separating the pathological pneumonia and covid-19 subjects with respect to healthy images with this in mind in this case the designed experiment included a total of 1242 x-ray images being 828 from healthy patients whereas 207207 from covid-19 and pneumonia patients respectively in this case we randomly selected the 828 healthy images and the 207 pneumonia images from the total amount of the used image dataset 16  having the limiting factor of 207 covid-19 images we balanced the amount of the other cases to obtain a proportion of 2 3 and 1 3 between the negative and positive classes figure 5 shows the performance that was obtained from the training and validation stages using the proposed dataset through 5 independent repetitions as we can see in figure 5 a the best mean accuracy that was produced is 09989  00011 for training in epoch 121 and the best average accuracy for validation of 09887  00044 in epoch 167 in addition the proposed approach achieved its stability in the loss cross-entropy function both for training and for validation after epoch 100 as we can see in figure 5 b table 3 show the performance measurements obtained in the test stage in terms of precision recall and f1-score for each class as we can see satisfactory results were provided as a global accuracy of 098 for both categories thus the results obtained show that this screening approach is capable of successfully separating the considered pathological cases from the healthy ones 7  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020 the third scenario was designed to evaluate the performance of the proposed approach to specifically distinguish between cases of patients with covid-19 from other similar cases such as pneumonia or also from healthy patients thus we measure the potential separability between those potentially similar pathological cases of covid-19 from pneumonia to do so we used a total of 621 x-ray images being 207 from covid-19 207 from patients with pneumonia and 207 from healthy patients once again those subjects from pneumonia and healthy patients were randomly selected to obtain a proportion of 2 3 and 1 3 between negative and positive classes respectively figure 6 shows the performance that was obtained from the densenet-161 architecture after 5 independent repetitions in both the training and validation steps in particular as we can observe in figure 6 table 4 we present the quantitative performance of the proposed system in the test dataset in terms of precision recall and f1-score our method shows a good performance for both categories providing a global accuracy of 098 demonstrating the robustness of the proposed system to distinguish between cases of patients with covid-19 from other cases such as pneumonia or health this result is specially significant considering the separation of similar pathological scenarios as pneumonia and covid-19 in this last scenario considering the satisfactory results of the other considered approaches we also trained another model to directly analyze the performance of the proposed system to separate the chest x-ray radiographs into 3 different categories healthy pneumonia and covid-19 to do so we designed a complete experiment using a total of 621 x-ray radiographs being 207 from covid-19 207 from patients with pneumonia and 207 from healthy patients  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020 in this case again we randomly selected the healthy and pneumonia images to balance the amount of available images for the 3 considered classes figure 7 shows the performance of the proposed system using training and validation sets after 5 independent repetitions as we can see in figure 7 a the best mean accuracy that was produced is 09967  00029 for the training stage in epoch 166 and the best average accuracy for validation of 09725  00185 in epoch 189 in the same line the proposed system achieved its stability in the loss cross-entropy function both for training and for validation after epoch 100 as we can see in figure 7 b in table 4  we can see the precision recall and f1-score results obtained at the test stage providing an accuracy of 099 as we can see the trained model is capable to predict with accuracy of 099 all the mentioned categories generally the obtained results in all the cases are satisfactory demonstrating the robustness of the proposed system in the classification of the 3 categories of chest x-ray images considered in this work  in this work we analyzed different and complementary fully automatic approaches for the classification of healthy pneumonia and covid-19 chest x-ray radiographs all the experimental results demonstrate that the proposed system is capable of successfully distinguishing healthy patients from different pathological cases of pneumonia and covid-9  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 6 2020 19 this pathological differentiation is understandable given the abnormality of the pathological scenarios with respect to normal patients and also the pathological impact in the lungs of both pneumonia and covid-19 diseases but also it is significant the accurate capability of differentiation also of covid-19 patients from other with pneumonia which were correctly separated in the proposed third and forth approaches also corroborated by the experiments with the first approach in addition the proposed system allows to make accurate predictions using chest x-ray images of arbitrary sizes which is very relevant considering the great variability of x-ray devices currently available in the healthcare centers despite the attained satisfactory performance the proposed system obtain an acceptably small number of misclassified cases in particular some misclassification is caused by the poor contrast of the x-ray images used in this work other times in some cases there is a great similarity between covid-19 and pneumonia mainly during the initial stages of both diseases in figure 8  we can see representative examples illustrating the significant variability of the possible scenarios that are represented in this research work as no exhaustive classification method for x-ray covid-19 images has been published to date we cannot make any comparison with other state-of-the-art approaches instead we use different public datasets for the evaluation of our proposed system validating their accuracy in comparison with manual annotations from different clinical experts coronaviruses are a large family of viruses that can cause disease in both animals and humans in particular the new coronavirus sars-cov-2 also know covid-19 was firstly detected in december 2019 in wuhan city hubei province china given its drastic spread the who declared a global pandemic as the covid-19 rapidly spreads across the world in this context chest x-ray images are widely used for early screening by clinical experts allowing for more appropriate use of other medical resources during initial screening in this work we proposed complementary fully automatic approaches for the classification of covid-19 pneumonia and healthy chest x-ray radiographs to achieve this we adapted a densely convolutional network architecture which connects each layer to every other layer in a feed-forward fashion we evaluated the robustness and accuracy of the different classification approaches obtaining satisfactory results for all the experiments that were proposed using different public image datasets of reference despite the complex and challenging scenario the proposed approaches has proven to be robust and reliable facilitating a more complete and precise analysis of the pathological lung regions and consequently the production of more adjusted treatments of this highly infectious disease  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg1011012020050120087254 doi medrxiv preprint fully automatic deep convolutional approaches for the analysis of covid-19 using chest x-ray images as future work we plan to expand the proposed methodology with the incorporation of other relevant lung diseases such as chronic bronchitis emphysema or lung cancer additionally further analysis with larger and maybe more comprehensive chest x-ray datasets should be done in order to reinforce the conclusions of this work  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg1011012020050120087254 doi medrxiv preprint  cc-by-nc-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 6 2020  httpsdoiorg1011012020050120087254 doi medrxiv preprint  genome detective coronavirus typing tool for rapid identification and characterization of novel coronavirus genomes sara cleemput wim dumon vagner fonseca wasim abdool karim marta giovanetti luiz alcantara carlos koen deforche tulio de oliveira pier luigi martelli   we are currently faced with a potential global epidemic of a new coronavirus that has infected thousands of people in china and is spreading rapidly around the world in the end of january 2020 the who has declared it a global emergency who 2020 the novel coronavirus sars-cov-2 first isolated in wuhan china has already caused more infections than the previous severe acute respiratory syndrome sars outbreak of 2002 and 2003 the virus is a sars-related coronavirus sarsr-cov and it is genetically associated with sarsr-cov strains that infect bats in china lu et al 2020 zhu et al 2020 it causes severe respiratory illness which the who recently named covid-19 disease it has high fatality rate huang et al 2020 can be transmitted from person to person has infected over 70 000 individuals and has spread to over 30 countries in less than 2 months who 2020 this coronavirus outbreak has been unprecedented so too is the way that the scientific community has responded to it they have openly and rapidly shared genomic and clinical data as never seen before allowing research results to be released almost instantaneously this has helped the understanding of the transmission dynamics the development of rapid diagnostic and has informed public health response here we present a new contribution that can speed up this communal effort the genome detective coronavirus typing tool is a free-of-charge web-based bioinformatics pipeline that can accurately and quickly identify assemble and classify coronaviruses genomes the tool also identifies changes at nucleotides coding regions and proteins using a novel dynamic aligner to allow tracking new viral mutations fig 1 a reference dataset of previously published coronavirus whole-genome sequences wgs was compiled from the virus pathogen resource vipr database wwwviprbrcorg this dataset consisted of 386 wgs of nine important coronavirus species these included 132 sequences of severe acute respiratory syndrome related coronavirus sarsr-cov 121 sequences of beta coronavirus 97 sequences of middle east respiratory syndrome related coronavirus mersr-cov 19 sequences of human coronavirus hku1 9 sequences of murine hepatitis virus 4 of rousettus bat coronavirus hku9 3 of rat coronavirus and 1 wgs of tylonycteris bat coronavirus hku4 zariabatcoronavirus and longquan rl rat coronavirus to this reference dataset we added 47 whole genomes of the current coronavirus 2019 sars-cov-2 outbreak that originated in wuhan china in december 2019 the sars-cov-2 sequences were downloaded from the gisaid database httpswwwgisaidorg together with annotation of its original location collection date and originating and submitting laboratory the sars-cov-2 data generators are properly acknowledged in the acknowledgements section of this article and detailed information is provided in supplementary table s1 the 431 reference wgs were aligned with muscle edgar 2004 the alignment was manually edited until a codon alignment was attained in all coding sequences cds a maximum likelihood phylogenetic tree 1000 bootstrap replicates were constructed in phyml guindon and gascuel 2003 lemoine et al 2018 and a bayesian tree using mrbayes ronquist and huelsenbeck 2003 were constructed the trees were visualized in figtree rambaut 2018 we selected 25 reference sequences that represent the diversity of each well-defined phylogenetic cluster with bootstrap support of 100 and posterior probability of 1 we identified five well-supported phylogenetic clusters with more than two sequences of sarsr-cov and used them to set up our automated phylogenetic classification tool cluster 1 included sars strains from the 2002 and 2003 asian outbreaks in our tool we named this cluster sars-cov outbreak 2000s but may rename it as sars-cov-1 if a new proposed naming system for sarsr-cov is adopted in the near future cluster 2 provisionally named as sars related cov includes seven sequences from bats which did not cause large human outbreaks cluster 3 named as bat sars-cov hku3 includes three wgs sampled from rhinolophus sinicus ie chinese rufous horseshoe bats cluster 4 bat sars-cov zxc21zc45 includes two sarsr-cov sampled from rhinolophus sinicus bats in zhoushan china cluster 5 virus named sars-cov-2 by the ictv committee and disease named covid-19 by the who includes three public sequences from the outbreak we identified this cluster with many sequences from gisaid but kept only three ones as these were the first genbank sequences the first whole genome of sars-cov-2 was kindly shared by prof yong-zhen zhang and colleagues in the virologicalorg website detailed information about the phylogenetic reference datasets is available in supplementary table s2 the phylogenetic reference dataset was used to create an automated coronavirus typing tool using the genome detective framework fonseca et al 2019 vilsker et al 2019 to determine the accuracy of this tool each of the 431 test wgs was considered for evaluation ie 384 reference sequences from vipr and 47 public sars-cov-2 sequences the sensitivity specificity and accuracy of our method were calculated for both species assignment and phylogenetic clustering of sarsr-cov sensitivity was computed by the formula tptpfn specificity by tntnfp and accuracy by tptntpfpfntn where tp  true positives fp  false positives tn  true negatives and fn  false negatives classifying query sequences in an automated fashion involves two steps the first step enables virus species assignments and the second which is restricted to sarsr-cov includes phylogenetic analysis the first classification analysis subjects a query sequence to blast and aga analysis aga is a novel alignment method for nucleic acid sequences against annotated genomes from ncbi refseq virus database aga deforche 2017 expands the optimal alignment algorithms of smith and waterman 1981 and gotoh 1982 based on an induction state with additional parameters the result is a more accurate aligner as it takes into account both nucleotide and protein scores and identifies all of the polymorphisms at nucleotide and amino acid levels in the second step a query sequence is aligned against the phylogenetic reference dataset using -add alignment option in the mafft software katoh and standley 2013 in addition a neighbor-joining phylogenetic tree is constructed using the hky distance metric with gamma among-site rate variation with 1000 bootstrap replicates using paup swofford 2003 the query sequence is assigned to a particular phylogenetic cluster if it clusters monophyletically with that clade or a subset of it with bootstrap support 70 if the bootstrap support is 70 the genotype is reported to be unassigned the result of the phylogenetic and mutational analysis performed by aga is available in a detailed report this report contains an interactive phylogenetic tree and genome mapper supplementary fig s1 it also presents the virus species and cluster assignments and a detailed table that provides information about open reading frames orfs cds and proteins this table can be expanded to show nucleotide and amino acid mutations that differentiate a query sequence from their species refseq or from a sequence in the phylogenetic reference dataset all results can be exported to a variety of file formats xml csv excel nexus or fasta the genome detective coronavirus typing tool correctly classified all of the 175 sarsr-cov sequences at species level ie specificity sensitivity and accuracy of 100 furthermore all of the 47 sars-cov-2 wgs that were isolated in china n  36 usa n  5 france n  2 thailand n  2 japan n  1 and taiwan n  1 were correctly classified at phylogenetic cluster level as sars-cov-2 which may be renamed as sars-b in addition we classified with very high specificity sensitivity and accuracy ie 100 all of the 112 sars outbreak wgs of 2002 and 2003 we also achieved perfect classification ie specificity sensitivity and accuracy of 100 for all of beta coronavirus humancoronavirushku1 mers-cov rousettus bat coronavirus hku9 and tylonycterisbatcoronavirushku4 at species level for a detailed overview of assignment performance please refer to the supplementary table s3 our tool also allows detailed analysis of coding regions and proteins for each of the coronavirus species for example the analysis of the first released sars-cov-2 sequence the whhuman1china2019dec genbank mn908947 demonstrated at genome level the nucleotide nt identity was 790 to the reference strain of sarsr-cov accession nc0047183 and that the envelop small membrane protein protein e is the most similar protein in total 948 7377 of the amino acids were identical the four amino acid differences were located at positions 55 t55s 56 v56f 69 69deletion and 70 g70r the spike protein protein s which can be associated with virulence was 762 identical to the reference strain of sarsr-cov supplementary table s4a interestingly there were four amino acid insertions at position 237 a237f238inshrsy genome nt position 2220222203inscatagaagttat which is just upstream from a cleavage site there is also a four amino acid insertion prra at the spike protein at positions 681 to 684 this is at the junction of s1 and s2 and creates a new polybase cleavage site our tool also allows us to compare mutations with other-related sequences such as the pangolin bat ratg13 the bat sars-cov and sars sin940 figure 2 and supplementary table s2 the most diverse coding regions were the cds sars8a and sars8b in these two regions only 30 of the amino acids were identical sars8b protein was truncated early and its cds had four stop codons supplementary table s4a our coronavirus typing tool also allows a query sequence to be analyzed against a sequence in the phylogenetic reference dataset for example the whhuman1china2019dec genbank mn908947 the identity was 875 to the bat sequence batslcovzxc21 genbank mg772934 this was one of the bat-cov sequences that were most related to n2019-cov lu et al 2020 the envelop small membrane protein protein e was 100 identical supplementary table s4b when the sars-cov-2 isolated from france betacovfranceidf03732020 was analyzed with our tool and compared with the sars-cov-2 whhuman1china2019dec strain accession mn908947 this sequence was 999 identical and had only two nt mutations supplementary table s4c these two differences were located on positions 22551gt and 26016gt which caused three amino acid mutations e2 glycoprotein protein mutation v354f 22551gt sars3a protein mutations g250v 26016gt and sars3b protein mutations v110f 26016gt detailed in supplementary table s4c-ii the analysis of a wgs in fasta format takes approximately 60 s we developed and released the genome detective coronavirus typing tool as a free-of-charge resource in the third week of january 2020 in order to help the rapid characterization of covid-19 infections this tool allows the analysis of whole or partial viral genomes within minutes it accepts assembled genomes in fasta format or raw next-generation sequencing data in fastq format from illumina ion torrent pacbio or oxford nanopore technologies ont can be submitted to the genome detective virus tool vilsker et al 2019 to automatically assemble the consensus genome prior to executing the coronavirus typing tool user effort is minimal and a user can submit multiple fasta sequences at once the tool uses a novel and dynamic aligner aga to allow submitted sequences to be queried against reference genomes using both nucleotide and amino acid similarity scores this allows accurate identification of other coronavirus species and the tracking of new viral mutations as the outbreak expands globally it also performs detailed analysis of the coding regions and proteins moreover it can easily be updated to add new phylogenetic clusters if new outbreaks arise or if the classification nomenclature changes the tool has been able to correctly classify all the recently released sars-cov-2 genomes as well as all the 20022003 sars outbreak sequences in conclusion the genome detective coronavirus typing tool is a web-based and user-friendly software application that allows the identification and characterization of novel coronavirus genomes  mega-cov a billion-scale dataset of 65 languages for covid-19 muhammad abdul-mageed abdelrahim elmadany dinesh pabbi kunal verma rannie lin  we describe mega-cov a billion-scale dataset from twitter for studying covid-19 the dataset is diverse covers 234 countries longitudinal goes as back as 2007 multilingual comes in 65 languages and has a significant number of location-tagged tweets  32m tweets we release tweet ids from the dataset hoping it will be useful for studying various phenomena related to the ongoing pandemic and accelerating viable solutions to associated problems  the seeds of the coronavirus disease 2019 pandemic are reported to have started as a local outbreak in wuhan hubei china in december 2019 but soon spread around the world who 2020 as of april 25 2020 the number of confirmed cases around the world is estimated at 2877487 1 in response to this ongoing public health emergency researchers are mobilizing to track the pandemic and study its impact not only on human life but possibly on all sorts of life in our planet the different ways the pandemic has its footprint on our lives is a question that will probably be studied for years to come importantly enabling such a scholarship by providing relevant data is an important endeavor toward this end we focus our efforts on collecting mega-cov a billion-scale multilingual twitter dataset with geolocation information as several countries and regions around the world went into lockdown the public health emergency has restricted physical aspects of human communication considerably as hundreds of millions of people spend more time at home communication over social media becomes more important than what it has ever been in particular the content of social media communication promises to capture useful aspects of the lives of the millions of people involved mega-cov is intended as a repository of such a content in this version of our work the largest part of the dataset is focused on north america our next release will however bring a significant update with the size of the dataset doubling based on additional content from outside north america while other early efforts to collect twitter data are ongoing our goal is to complement these existing resources in significant ways more specifically we designed our methods to harvest a dataset that is unique in the following means  longitudinal coverage we collect multiple data points up to 3200 from the same users with a goal to allow for comparisons between the present and the past across the same users communities and geographical regions section 4  topic diversity we do not restrict our collection to tweets carrying certain hashtags this makes the data general enough to comprise content and topics directly related to covid-19 regardless of existence of accompanying hashtags as well as themes that may not be directly linked to the pandemic but where the pandemic may have some bearings which should be taken into account when investigating such themes section 6 and section 7 provide a general overview of issues discussed in the dataset  language diversity since our method of collection targets users rather than hashtagbased content mega-cov is linguistically diverse in theory the dataset should comprise any language posted to twitter by a user whose data we collect based on twitterassigned language codes we identify a total of 65 languages section 5  no distribution shift related to two previous points but from a machine learning perspective collecting the data without conditioning on existence of specific or any hashtags avoids introducing distribution bias in other words the data can be used to study various phenomena in-the-wild this warrants more generalizable findings and models even though our current releaase of mega-cov has more focus on north america the set of users in the data have posted widely from outside this specific region in fact based on our location assignment criteria we identify a substantial set of users to belong to other regions see section 3 as stated earlier mega-cov is continuously growing and our next release will bring  half a billion tweets primarily from users from outside north america the next version of the current paper will describe our next release we will refer to our current release as mega-cov v01 which we now describe to collect a sufficiently large dataset we put crawlers using the twitter streaming api 2 on all world continents ie asia africa north america south america antarctica europe and australia starting in early january 2020 our goal was to initially acquire a diverse set of tweets from which we can extract user ids we then iteratively crawl the user timelines up to 3200 tweets using all collected ids this gives us data from april 10 th  2020 backwards depending on how prolific of a poster a user is 3  for our current release mega-cov v01 we describe a total of 482 214 users who contribute 566 402 269 tweets our next release will add data from at least  600k more users whose data we have already collected but have not yet analyzed and hence do not include here 2 streaming api link httpsgithubcom tweepytweepy 3 see table 3 for a breakdown once the timeline tweets are collected we put them through a pipeline that involves merging all user files into a single file for most of our analyses in this paper we remove re-tweets and replies 4 table 1 offers a breakdown of the distribution of tweets re-tweets and replies in mega-cov v01 tweet ids of the dataset are available at our github 5 and can be downloaded for research the dataset repository will be updated semi-regularly tweet location can be associated with a specific point location or a twitter place with a bounding box that describes a larger area such as city town or country we refer to tweets in this category as geo-located tweets additionally a smaller fraction of tweets are geo-tagged with longitude and latitude as table 2 shows mega-cov v01 has  32m geo-located tweets from  208k users and  71m geo-tagged tweets from  767k users table 2 also shows the distribution of tweets and users over canada the us and other locations for the year 2020 mega-cov v01 has  105m geo-located tweets from  150k users and  980k geo-tagged tweets from  289k users 6 figure 1 shows where the tweets were posted cities on the left and actual point co-ordinates on the right mega-cov v01 has data posted from a total of 56 139 cities from 243 countries figure 2 shows the distribution in terms of numbers of cities from which the tweets were posted ie geo-located tweets over the top countries in a the whole dataset as well as b data posted during 2020 as figure 2 shows mega-cov v01 comprises data posted from several european countries eg france the uk germany and italy latin america eg brazil mexico and asia eg indonesia india  mega-cov v01 aims at making it possible to compare user social content over time since we crawl user timelines the dataset comprises content going back as early as 2007 figure 3 shows the distribution of data over the period 2007-2020 simple frequency of user posting shows a surge in twitter use in the period of jan-april 10 th 2020 7 7 we started crawling the april data in april 7 th for some users and in april 15 th for others hence april 10 th is an approximation across these users still for about 150k users we have collected data before march 30 th but will update the compared to the same period in 2019 as shown in figure 4  indeed we identify 4053 more posting during the first 3 months of 2020 compared to the same period in 2019 with the same trend seeming to continue in april this is expected both due to physical distancing and a wide range of human activity moving online figure 3 also shows a breakdown of tweets retweets and replies a striking discovery is that for 2020 users are engaged in conversations with one another short as these typically are in twitter more than tweeting directly to the platform this is the first time this happens compared to any previous dataset with more recent tweets from these users years based on our dataset in addition for 2020 we also see users re-tweeting more than tweeting this is also happening for the first time  we perform the language analysis based only on tweets n 218m  excluding re-tweets and replies 8 based on twitter-assigned language ids mega-cov v01 comprises 65 languages however we suspect the dataset has other languages represented as well but those cannot be tagged using twitters current language identification technology for languages it cannot detect twitter also assigns an und for undefined tag mega-cov v01 has  15m  69 tweets tagged as und 9 as figure 3 shows english french and spanish are unsurprisingly the top 3 languages in terms of the number of users who have posted in these languages in the dataset these 3 languages are also the most frequent in terms of the number of actual tweets in the data as shown in table 5  overall non-english comprises 1643 of the tweets n21m  hashtags usually correlate with the topics users post about we provide the top 30 hashtags in the data in table 4  as the table shows users tweet heavily about the pandemic using hashtags such as covid19 coronavirus coronavirus covid19 covid19 covid19 and stayathome simple word clouds of hashtags from the various languages figure 6 shows clouds from the top 10 languages 8 this is an arbitrary decision otherwise re-tweets and replies could counted as relevant for some tasks 9 we also plan to run a language id tool on the data and provide a comparison to twitter-provided language tags we also note frequent occurrence of political hashtags in languages such arabic farsi indian and urdu this is in contrast to discussions in european languages where politics are not as visible for example in urdu discussions involving the army and border issues show up in indiana languages such as tamil and hindi posts focused on movies such as valimai tv shows such as big boss doctors and even fake news are observed along with the pandemic-related hashtags an interesting observation from the chinese language word cloud is the use of hashtags such as chinapneumonia and wuhanpneumonia to refer to the pandemic we did not observe these same hashtags in any of the other languages additionally for some reason apple seems to be trending during the first 4 months of 2020 in china owing to hashtags such as appledaily and appledailytw some of the languages such as romanian and vietnamese have shown bitcoin and cryptocurrency to be a hot topic of discussion this was also seen in the chinese language word cloud but not as prominently another surprising observation is seen from the finnish language where users post about the corona virus and gaming but also about kirtan gurbani which are religious terms related to sikh religion domains in urls shared by users also provide a window on what is share-worthy we perfrom an analysis on domains shared in tweets 10 a comparison between the ranks of the top 40 domains in 2020 and their ranks in 2019 yields a number of observations as follows news we observe urls with news organization domains are higher in rank in 2020 this is true for canada where canadian domains such as cbcca ctvnewsca theglobeandmailcom thestarcom and radio-canadaca are higher but also international news such as theguardiancom and nytimescom have jumped at least 10 positions and cnncom and applenews a whopping 26 and 252 positions respectively the us twittersphere shows a similar trend with cnncom nytimescom washingtonpostcom foxnewscom showing in the top 40 domains jumping 25 15 19 and 48 positions 10 we note that the same analysis could also be performed on re-tweets and replies which we intend to carry out respectively it is striking that foxnewscom has moved from a rank of 81 in 2019 to 33 in 2020 with the 48 positions jump we note a somewhat similar international trend with sites such as lebnewsonlinecom and theguardiancom rising much higher in rank other domains other noteworthy domain activities including those related to gaming video and music and social media tools where ranks of these domains have not necessarily shifted higher but remain prominent this shows these themes still being relevant in 2020 in spite of the economic impact of the pandemic shopping domains such as etsyme and poshmarkcom have markedly risen in rank as people moved to shoppoing online in more significant ways with mega-cov v01 geo-location information in mega-cov v01 can be used to characterize and track human mobility in various ways we investigate some of these next  mega-cov v01 can be exploited to generate responsive maps where end users can check mobility patterns between different regions over time in particular geo-location information can show mobility patterns between regions as an illustration of this use case we provide figure  we also use information in mega-cov v01 to map each user to a single home region ie city stateprovince and country we follow geolocation literature in setting a condition that a user must have posted at least 10 tweets from a given region however we also condition that at least 60 of all user tweets must have been posted from the same region 11 for all the analyses in the sections to follow we exclusively use data from users we successfully located using our method described above henceforth located users we exploit mega-cov v01 to show interstateprovince mobility during a given window of time here due to increased posting in 2020 we normalize the number of visits between states by 11 we will provide a table with the distribution of users over global locations we could map them to in the next release the total number of all tweets posted during 2020 figure 8 shows user mobility between different canadian provinces over each of the jan-april months during 2020 as a general pattern as the various provinces went in lockdown starting from earlymid-march user mobility drops noticeably leading to a much quieter april activity figure 9 shows mobility between different us states the figure shows a clear change from higher mobility in jan and feb to much less activity in march and especially april clear differences can be can be seen in key states where the pandemic has hit hard such as new york ny and california ca and to some extent washington state wa we can also visualize user mobility as a distance from an average mobility score on a weekly basis namely we calculate an average weekly mobility score for the year 2019 using geo-tag information longitude and latitude and use it as a baseline against which we plot user mobility for each week of 2019 and 2020 up until april in general we observe a drop in user mobility in canada starting from mid-march for us users we notice a very high mobility surge starting around end of feb and early march only waning down the last week of march and continuing in april for both the us and canada we hypothesize the surge in early march much more noticeable in the us is a result of people moving back to their hometowns returning from travels moving for basic need stocking etc we can exploit the data to plot user mobility between two or more points based on geo-tagged tweets within the same region thus painting a more detailed picture as an illustration figure 11 shows user monthly mobility within new york state during 2020 the figure shows the surge in activity in march 2020 we discuss in the previous section 9 related works 91 twitter in emergency and crisis social media can play a useful role in disaster and emergency since they provide a mechanism for wide information dissemination and their content can be mined for prompt action simon et al 2015  for example in the typhoon haiyan in the philippines twitter was used for dissemination of second-hand information aiding relief efforts and condolence to the victims takahashi et al 2015  prior to an emergency twitter can also be useful for preparedness and early warning carley et al 2016 studied the potential value of twitter for the warning and response to tsunami in padang indonesia showing it could be used to support predisaster management as it contained information about mobility population linguistic needs and local opinion leaders in different regions which could all contribute to the construction of an early response system verma et al 2019 also studies the effectiveness of social media in disaster response and recovery in context to the nepal 2015 earthquakes making a comparison with the conventional newspapers and concluding that social media such as twitter and the news article share complementary perspectives that form a holistic view marx et al 2020 studied the different strate-gies media organisations followed during a disaster such as harvey hurricane they identified three sense-giving strategies retweeting of local inhouse outlets bound amplification of messages of individual to the organisation associated journalists and open message amplification a number of works have focused on developing systems for emergency response for example mccreadie et al 2019 produce a series of curated feeds of social media posts where a particular type of information request is mapped to feeds they also make use of a criticality score which represents how important it is that a user be shown a given post they use twitter feeds to present 6 categories of event wildfire earthquake flood typhoonhurricane bombing and shooting to tackle irrelevant or off-topic content  several works have focused on creating datasets for enabing covid-19 research to the best of our knowledge all these works depend on a list of hashtags related to covid-19 and focus on a given period of time for example started collecting tweets on jan 22 nd and continued updating by actively tracking a list of 22 popular keywords such as coronavirus corona and wuhancoronavirus they also crawled data from 8 related accounts such as pneumoniawuhan coronavirusinfo and v2019n as of apr 23 rd  the authors have released a total of 67m million english tweets 101m non-english tweets singh et al 2020 collect a dataset covering jan 16 th 2020-march 15 th 2020 using a list of hashtags such as 2019ncov chinapneumonia and chinesepneumonia for a total of 28m tweets  18m retweets and  457k direct conversations using location information on the data authors report that tweets strongly correlated with newly identified cases in these locations more precisely they state that for the located conversations the pattern of volume changes led the covid-19 cases by 2-5 days in the united states italy and china they suggest that this pattern would be helpful to predict the outbreak of cases similarly alqurashi et al 2020 use a list of keywords and hashtags related to covid-19 with twitters streaming api to collect a dataset of arabic tweets the dataset covers the period of march 1 st 2020-march 30 th 2020 and is at 4m tweets the authors goal is to help researchers and policy makers study the various societal issues prevailing due to the pandemic authors note that the number of re-tweets increased significantly in late march in the same vein lopez et al 2020 also collect a dataset of  65m in multiple languages with english accounting for  634 of the data the dataset covers jan 22 nd 2020-march 2020 analyzing the data authors observe the level of re-tweets to rise abruptly as the crisis ramped up in europe in late february and early march misinformation can spread fast during disaster and especially during health outbreaks social data have been used to study rumors and various types of fake information related to the zika ghenai and mejova 2017 and ebola kalyanam et al 2015 viruses in the context of covid-19 a number of works have focused on investigating the effect of misinformation on mental health rosenberg et al 2020  the types sources claims and responses of a number of pieces of misinformation about covid-19 brennen et al 2020  the propagation pattern of rumours about covid-19 on twitter and weibo do et al 2019  the check-worthiness ie whether or not a piece of textual information is critical enough to be checked for veracity wright and augenstein 2020  modeling the spread of misinformation and related networks about the pandemic cinelli et al 2020  osho et al 2020  pierri et al 2020  koubaa 2020  estimating the rate of misinformation in covid-19 associated tweets kouzy et al 2020  the use of bots ferrara 2020  and predicting whether a user is covid19 positive or negative karisani and karisani 2020  singh et al 2020 examine the quality of shared links in tweets by identifying a set of reputable and questionable domains which comes from top medical journals hospitals and official recommendations and a set of questionable domains which are created by newsguard they found that the number of useful shared links is about the same as the misleading links they identify a list of top 5 common myths origin of covid-19 flu comparison home remedies heat kills disease and vaccine development from the search phrase coronavirus common myths by matching the phrases and words in tweets with the broad descriptions of myths from google search they discover over 16000 tweets containing these myths which was a small fraction of twitter content authors also identify the top 10 most frequent words in their dataset including words such as china people cases wuhan and coronavirus they also identify the top 8 most prevalent themes in their data as healthcareillness global nature information providers government response individual concernsstrategies emotion and social through grouping frequent words in twitter conversations sharma et al 2020 collect a dataset of 308m tweets from 182 countries out of which the majority is english speaking with english language in the data making up 205m tweets the data cover march 1 st 2020-march 30 th 2020 analyzing their data authors observe a 558 spike new users during the period from november 2019march 2020 authors also perform some initial analyses on their dataset including to identify fake stories topical distribution and sentiment analysis aiming at understanding perception of the public towards the pandemic just as coronavirus spread fast in the world hate speech towards certain communities is also spreading fast devakumar et al 2020 raises the concern that discrimination towards ethnic minority groups like colored people and immigrants could lead to a higher risk of infection for these groups due to their limited access to medical resources and the lack of social protection the rise of fake news has also worsened the problem of discrimination a number of works have focused on related phenomena for example schild et al 2020 identify an increase in sinophobic behaviour on the web and that its spread is a cross-platform phenomenon similarly shimizu 2020 find that xenophobia towards chinese people spread in japan due to a piece of misinformation stating that chinese passengers from wuhan with fever slipped through the quarantine at kansai international airport and the hashtag chinesedontcometojapan trending in twitter despite who officially naming coronavirus as covid-19 use of controversial terms such as chinese virus wuham virus lyu et al 2020 reports work to predict twitter users who are more likely to use controversial terms related to the covid-19 crisis to investigate emotional response to covid-19 kleinberg et al 2020 collected and analyzed the real world worry dataset a dataset comprising 2 500 participants indications of worry level and emotion type and their written long and short texts about their emotional states unlike other works this dataset is not from twitter but rather is collected via the crowd-sourcing platform pro-lific participants express their level of worry and emotion anger anxiety desire disgust fear happiness relaxation and sadness in written form short and long that is matched with a 9-point scale using use a lexicon lwic2015 authors find significantly high correlations of worrying thoughts in long texts with the categories family and friends topic models revealed the prevalent topics for short texts are related to government slogans and suggesting social distancing for others whereas common topics for long texts are lockdown and worries about employment and the economy authors also point out that participants tended to use short texts tweet-sized to call for solidarity and long texts to show their actual worries about family and friends we collect mega-cov from the public domain twitter in compliance with twitter policy we do not publish hydrated tweet content rather we only publish publicly available tweet ids all twitter policies including respect and protection of user privacy apply we encourage all researchers who decide to use mega-cov to review twitter policy at httpsdevelopertwittercomen developer-termspolicy before they start working with the data we presented mega-cov a billion-scale dataset of 65 languages for studying global response to the ongoing covid-19 pandemic in addition to being large and highly multilingual our dataset comprises data long pre-dating the pandemic this allows for comparisons over time we have provided initial analyses of the data with a focus on potential use of investigating human mobility we hope our dataset will be useful for accelerating research on the topic  covid-19 datasets a survey and future challenges junaid shuja eisa alanazi waleed alasmary abdulaziz alashaikh   the covid-19 virus has been declared a pandemic by the world health organization who with more than three million cases and 224172 deaths across the world as per who statistics of 1 may 2020 1  the cure to covid-19 can take twelve months due to its clinical trials on humans of varying ages and ethnicity before approval the cure to most of the articles included in this survey have not been rigorously peer-reviewed and published as pre-prints however their inclusion is necessary as the current pandemic situation requires rapid publishing process moreover the inclusion of non-peer-reviewed studies in this article is supported by their transparent open-source methods which can be independently verified for the search of the relevant literature review we searched the online databases of google scholar biorxiv and medrxiv the keywords employed were covid-19 and data-set we separately searched two online open-source communities ie kaggle and github for data-sets that are not yet part of any publication we focused on articles with applications of computer science and mathematics in general we hope that our efforts will be fruitful in limiting the spread of covid-19 through elaboration of scientific fact-finding efforts we divide the data sets into two main categories ie a medical images and b textual data medical images based data sets are mostly brought into service for screening and diagnosis of covid-19 medical images are either chest ct scan or x-rays medical image data sets should consider patients consent and preserve patient privacy medical image based diagnosis lowers burden on conventional pcr based screening textual data sets serve three main purposes ie a forecasting the transmission and spread of covid-19 based on reported cases b analyzing public sentimentopinion by trackingmining covid-19 related keywords on popular social media platforms and c collecting scholarly articles on covid-19 for a centralized view on related research and application of information extractiontext mining a consolidated view of the taxonomy of covid-19 open source data sets is illustrated in figure 1  the rest of the article is organized as follows section 2 presents the comprehensive list of medical covid-19 data sets divided into categories of ct scans and x-rays section 3 details a list of textual data sets classified into covid-19 case report social media and scholarly article collections in section 4 a comparison of listed data-sets is provided in terms of openness application and data-type section 5 discusses the dimensions that need attention from scholars and future perspectives on covid-19 research section 6 provides the concluding remarks for the article medical images in the form of chest ct scans and x-rays are essential for automated covid-19 diagnosis some of the leading hospitals across the world are utilizing aiml algorithms to diagnose covid-19 cases from ct scansx-ray images after preliminary trails of the technology 2  a generic work-flow of ml based covid-19 diagnosis is illustrated in figure 2  we discuss ct scan and x-ray image data-sets separately in the following subsections cohen et al 11 describe the public covid-19 image collection consisting of x-ray and ct-scans with ongoing updates the data set consists of more than 125 images extracted from various online publications and websites the 2 httpswwwbbccomnewsbusiness-52483082 3  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg101101202005 19 20107532 doi medrxiv preprint figure 2  a generic work-flow of aiml based covid-19 diagnosis data set specifically includes images of covid-19 cases along with mers sars and ards based images the authors enlist the application of deep and transfer learning on their extracted data set for identification of covid-19 while utilizing motivation from earlier studies that learned the type of pneumonia from similar images 24  each image is accompanied by a set of 16 attributes such as patient id age date and location the extraction of ct scan images from published articles rather than actual sources may lessen the image quality and affect the performance of the machine learning model some of the data sets available and listed below are obtained from secondary sources the public dataset published with this study is one of the pioneer efforts in covid-19 detection and most of the listed studies utilize this data set researcher 25 published a data set consisting of 275 ct scans of covid-19 positive patients the data set is extracted from 760 medrxiv and biorxiv preprints about covid-19 the authors also employed a deep convolutional network for training on the data set to learn covid-19 cases for new data with an accuracy of around 85 the model is trained on 183 covid-19 positive ct scans and 146 negative cases the model is tested on 35 covid-19 positive cts and 34 non-covid cts and achieves an f1 score of 085 due to the small data set size deep learning models tend to overfit therefore the authors utilized transfer learning on the chest x-ray data set released by nih to fine-tune their deep learning model the online repository is being regularly updated and currently consists of 349 ct images containing clinical findings of 216 patients wang et al 26 investigated a deep learning strategy for covid-19 screening from ct scans a total of 453 covid-19 pathogen-confirmed ct scans were utilized along with typical viral pneumonia cases the covid-19 ct scans were obtained from various chinese hospitals with an online repository maintained at 3  transfer learning in the form of pre-trained cnn model m-inception was utilized for feature extraction a combination of decision tree and adaboost were employed for classification with 839 accuracy segmentation helps health service providers to quickly and objectively evaluate the radiological images segmentation is a pre-processing step that outlines the region of interest eg infected regions or lesions for further evaluation shan et al 27 obtained ct scan images from covid-19 cases based mostly in shanghai for deep learning-based lung infection quantification and segmentation however their data set is not public the deep learning-based segmentation utilizes vb-net a modified 3-d convolutional neural network to segment covid-19 infection regions in ct scans the proposed system performs auto-contouring of infection regions accurately estimates their shapes volumes and percentage of infection poi the system is trained using 249 covid-19 patients and validated using 300 new covid-19 patients radiologists contributed as a human in the loop to iteratively add segmented images to the training data set two radiologists contoured the infectious regions to quantitatively evaluate the accuracy of the segmentation technique the proposed system and manual segmentation resulted in 90 dice similarity coefficients other than the published articles few online efforts have been made for image segmentation of covid-19 cases a covid-19 ct lung and infection segmentation dataset is listed as open source 28  the data set consists of 20 covid-19 ct scans labeled into left right and infectious regions by two experienced radiologists and verified by another radiologist 4  three segmentation benchmark tasks have also been created based on the data set 5  3 httpsainscc-tjcnthaideploypublicpneumoniact 4 httpszenodoorgrecord3757476 5 httpsgiteecomjunma11covid-19-ct-seg-benchmark 4  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg101101202005 1920107532 doi medrxiv preprint another such online initiative is the covid-19 ct segmentation dataset 6  the segmented data set is hosted by two radiologists based in oslo they obtained images from a repository hosted by the italian society of medical and interventional radiology sirm 7  the obtained images were segmented by the radiologist using 3 labels ie ground-glass consolidation and pleural effusion as a result a data set that contains 100 axial ct slices from 60 patients with manual segmentations in the form of jpg images is formed moreover the radiologists also trained a 2d multilabel u-net model for automated semantic segmentation of images in the following paragraph we list covid-19 data set initiatives that are public but are not associated with any publication the coronacases initiative shares 3d ct scans of confirmed cases of covid-19 8  currently the web repository contains 3d ct images of 10 confirmed covid-19 cases shared for scientific purposes the british society of thoracic imaging bsti in collaboration with cimar uks imaging cloud technology deployed a free to use encrypted and anonymized online portal to upload and download medical images related to covid-19 positive and suspected patients 9  the uploaded images are sent to a group of bsti experts for diagnosis each reported case includes data regarding the age sex pcr status and indications of the patient the aim of the online repository is to provide covid-19 medical images for reference and teaching the sirm is hosting radiographical images of covid-19 cases 10  their data set has been utilized by some of the cited works in this article another open-source repository for covid-19 radiographic images is radiopaedia 11  multiple studies 29 30 employed this dataset for their research researchers 31 present covid-net a deep convolutional network for covid-19 diagnosis based on chest x-ray images motivated by earlier efforts on radiography based diagnosis of covid-19 the authors make their data set and code accessible for further extension the data set consists of 13800 chest radiography images from 13725 patient cases from three open access data repositories the covid-net architecture consists of two stages in the first stage residual architecture design principles are employed to construct a prototype neural network architecture to predict either of a normal b non-covid infection and c covid-19 infections in the second stage the initial network design prototype data and human-specific design requirements act as a guide to a design exploration strategy to learn the parameters of deep neural network architecture the authors also audit the covid-net with the aim of transparency via examination of critical factors leveraged by covid-net in making detection decisions the audit is executed with gsinquire which is a commonly used aiml explainability method 32  author in 33 utilized the data set of cohen et al 11 and proposed covidx-net a deep learning framework for automatic covid-19 detection from x-ray images severn different deep convolutional neural network architecture namely vgg19 densenet201 inceptionv3 resnetv2 inceptionresnetv2 xception and mobilenetv2 were utilized for performance evaluation the vgg19 and densenet201 model outperform other deep neural classifiers in terms of accuracy however these classifiers also demonstrate higher training times apostolopoulos et al 34 merged the data set of cohen et al 11  a data set from kaggle 12  and a data set of common bacterial-pneumonia x-ray scans 35 to train convolutional neural networks cnn to distinguish covid-19 from common pneumonia five cnns namely vgg19 mobilenet v2 inception xception and inception resnet v2 with common hyper-parameters results demonstrate that vgg19 and mobilenet v2 perform better than other cnns in terms of accuracy sensitivity and specificity the researchers extended their work in 30 to extract biomarkers from x-ray images using a deep learning approach the authors employ mobilenetv2 a cnn is trained for the classification task for six most common pulmonary diseases mobilenetv2 extracts features from x-ray images in three different settings ie from scratch with the help of transfer learning pre-trained and hybrid feature extraction via fine-tuning a layer of global average pooling was added over mobilenetv2 to reduce overfitting the extracted features are input to a 2500 node neural network for classification the data set include recent covid-19 cases and x-rays corresponding to common pulmonary diseases the covid-19 images 455 are obtained from cohen et al 11  sirm rsna and radiopaedia the data set of common pulmonary diseases is extracted from a recent study 35 among other sources the training from scratch strategy outperforms 6 httpmedicalsegmentationcomcovid19 7 httpswwwsirmorgencategoryarticlescovid-19-database 8 httpscoronacasesorg 9 httpswwwbstiorguktraining-and-educationcovid-19-bsti-imaging-database 10 httpswwwsirmorgencategoryarticlescovid-19-database 11 httpsradiopaediaorgarticlescovid-19-3 12 httpswwwkagglecomandrewmvdconvid19-x-rays 5  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint transfer learning with higher accuracy and sensitivity the aim of the research is to limit exposure of medical experts with infected patients with automated covid-19 diagnosis researchers 36 merged the data set of cohen et al 11 50 imagesand a data set from kaggle 13 50 images for application of three pre-trained cnns namely resnet50 inceptionv3 and inceptionresnetv2 to detect covid-19 cases from x-ray radiographs the dataset was equally divided into 50 normal and 50 covid-19 positive cases due to the limited data set deep transfer learning is applied that requires smaller data set to learn and classify features the resnet50 provided the highest accuracy for classifying covid-19 cases among the evaluated models authors in 37 propose support vector machine based classification of x-ray images instead of predominately employed deep learning models the authors argue that deep learning models require large data sets for training that are not available currently for covid-19 cases the data set brought to service in this article is an amalgam of cohen et al 24  a data set of kaggle 14  and data set of kermany et 35  author of 34 also utilized data from same sources the dataset consists of 127 covid-19 cases 127 pneumonia cases and 127 healthy cases the methodology classifies the x-ray images into covid-19 pneumonia and normal cases pre-trained networks such as alexnet vgg16 vgg19 googlenet resnet18 resnet50 resnet101 inceptionv3 inceptionresnetv2 densenet201 xceptionnet mobilenetv2 and shufflenet are employed on this dataset for deep feature extraction the deep features obtained from these networks are fed to the svm classifier the accuracy and sensitivity of resnet50 plus svm is found to be highest among cnn models similar to sethy et al 37  afshar et al 38 also negated the applicability of dnns on small covid-19 data sets the authors proposed a capsule network model covid-caps for the identification of covid-19 based on x-ray images each layer of a capsule network consists of several capsules each of which represents a specific image instance at a specific location with the help of several neurons the length of a capsule determines the existence probability of the associated instance covid-caps uses four convolutional and three capsule layers and was pre-trained with transfer learning on the public nih dataset of x-rays images for common thorax diseases covid-caps provides a binary output of either positive or negative covid-19 case the covid-caps achieved an accuracy of 957 a sensitivity of 90 and specificity of 958 authors 39 contributed towards a single covid-19 x-ray image database for ai applications based on four sources the aim of the research was to explore the possibility of ai application for covid-19 diagnosis the source databases were cohen et al 11  italian society of medical and interventional radiology dataset images from recently published articles and a data set hosted at kaggle 15  the cumulative data set contains 190 covid-19 images 1345 viral pneumonia images and 1341 normal chest x-ray images the authors further created 2500 augmented images from each category for the training and validation of four cnns the four tested cnns are alexnet resnet18 densenet201 and squeezenet for classification of xray images into normal covid-19 and viral pneumonia cases the squeezenet outperformed other cnns with 983 accuracy and 967 sensitivity the collective database can be found at 16  authors 40 utilized data augmentation techniques to increase the number of data points for cnn based classification of covid-19 x-ray images the proposed methodology adds data augmentation to basic steps of feature extraction and classification the authors utilize the data set of cohen et al 11  the authors design five deep learning model for feature extraction and classification namely custom-made cnns trained from scratch transfer learning-based fine-tuned cnns proposed novel covid-renet dynamic feature extraction through cnn and classification using svm and concatenation of dynamic feature spaces covid-renet and vgg-16 features and classification using svm svm classification is brought to serve to further increase the accuracy of the task the results showed that the proposed covid-renet and custom vgg-16 models accompanied by the svm classifier show better performance with approximately 983 accuracy in identifying covid-19 cases a comprehensive list of ai-based covid-19 research can be found at 41  a list open-source data sets on the kaggle can be found at 17  a simple search list of covid-19 based works on github can be found at 18  covid-19 case reports global and county-level dashboards epidemiological data demographic data mobility data social media posts and scholarly article collections are detailed in the following subsections 13 httpswwwkagglecompaultimothymooneychest-xray-pneumonia 14 kaggle https wwwkaggle ecomandre wmvdconvid19-x-rays 15 httpswwwkagglecompaultimothymooneychest-xray-pneumonia 16 httpswwwkagglecomtawsifurrahmancovid19-radiography-database 17 httpswwwkagglecomcovid-19-contributions 18 httpsgithubcomsearchqcovid-19 6  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint the earliest and most noteworthy data set depicting the covid-19 pandemic at a global scale was contributed by john hopkins university 42  the authors developed an online real-time interactive dashboard first made public in january 2020 19  the dashboard lists the number of cases deaths and recoveries divided into countryprovincial regions a data is more detailed to the city level for the usa canada and australia a corresponding github repository of the data is also available 20  the data collection is semi-automated with main sources are dxy a medical community 21  and who the dxy community collects data from multiple sources and updated every 15 minutes the data is regularly validated from multiple online sources and health departments the aim of the dashboard was to provide the public health authorities and researchers with a user-friendly tool to track analyze and model the spread of covid-19 kucharski et al 4 modeled covid-19 cases based on datasets from and outside wuhan the purpose of the study was to estimate human-to-human transmissions and virus outbreaks if the virus was introduced in a new region the four time-series datasets used were the daily number of new internationally exported cases the daily number of new cases in wuhan with no market exposure the daily number of new cases in china and the proportion of infected passengers on evacuation flights between december 2019 and february 2020 the study while employing stochastic modeling found that the r0 declined from 235 to 105 after travel restrictions were imposed in wuhan the study also found that if four cases are reported in a new area there is a 50 chance that the virus will establish within the community author in 43 presented a framework for serial interval estimation of covid-19 as the virus is easily transmitted in a community from an infected person it is important to know the onset of illness in primary to secondary transmissions the date of illness onset is defined as the date on which a symptom relevant to covid-19 infection appears the serial interval refers to the time between successive cases in a chain of disease transmission the authors obtain 28 cases of pairs of infector-infectee cases published in research articles and investigation reports and rank them for credibility a subset of 18 high credible cases are selected to analyze that the estimated median serial interval lies at 40 days the median serial interval of covid-19 is found to be smaller than sars moreover it is implied that contact tracing methods may not be effective due to the rapid serial interval of infector-infectee transmissions tindale et al 44 study the covid-19 outbreak to estimate the incubation period and serial interval distribution based on data obtained in singapore 93 cases and tianjin 135 the incubation period is the period between exposure to an infection and the appearance of the first symptoms the data was made available to the respective health departments the serial interval can be used to estimate the reproduction number r0 of the virus moreover both serial interval and incubation period can help identify the extent of pre-symptomatic transmissions with more than a months data of covid-19 cases from both cities the mean serial interval was found to be 456 days for singapore and 422 days for tianjin the mean incubation period was found to be 71 days for singapore and 9 days for tianjin researchers 45 described an econometric model to forecast the spread and prevalence of covid-19 the analysis is aimed to aid public health authorities to make provisions ahead of time-based on the forecast a time-series database was built based on statistics from johns hopkins university dashborad 22 and made public auto-regressive integrated moving average arima model prediction on the data to predict the epidemiological trend of the prevalence and incidence of covid-2019 the arima model consists of an autoregressive model moving average model and seasonal autoregressive integrated moving average model the arima model parameters were by autocorrelation function arima 104 model was selected for the prevalence of covid-2019 while arima 103 was selected as the best arima model for determining the incidence of covid-19 the research predicted that if the virus does not develop any new mutations the curve will flatten in the near future researchers 46 investigated the serial interval of covid-19 based on publicly reported cases a total of 468 covid-19 transmission events reported in china outside of hubei province between january 21 2020 and february 8 2020 formulated the data set the data is compiled from reports of provincial disease control centers the data indicated that in 59 of the cases the infectee developed symptoms earlier than the infector indicated pre-symptomatic transmission the mean serial interval is estimated to be 396 with a standard deviation of 475 the mean serial interval of covid-19 is found to be lower than similar viruses of mers and sars the production rate ro of the data set is found to be 132 dey et al 47 analyzed the epidemiological outbreak of covid-19 using a visual exploratory data analysis approach the authors utilized publicly available data sets from who the chinese center for disease control and prevention and johns hopkins university for cases between 22 january 2020 to 16 february 2020 all around the globe the data set consisted of time series information regarding the number of cases origin country recovered cases etc the main 19 httpswwwarcgiscomappsopsdashboardindexhtml 20 httpsgithubcomcssegisanddatacovid-19 21 httpsncovdxycnncovh5viewpneumonia 22 httpsgisanddatamapsarcgiscomappsopsdashboardindexhtml 7  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint objective of the study is to provide time-series visual data analysis for the understandable outcome of the covid-19 outbreak researcher 48 investigated the transmission control measures of covid-19 in china the authors compiled and analyzed a unique data set consisting of case reports mobility patterns and public health intervention the covid-19 case data were collected from official reports of the health commission the mobility data were collected from locationbased services employed by social media applications such as wechat the travel pattern from wuhan during the spring festival was constructed from baidu migration index 23  the study found that the number of cases in other provinces after the shutdown of wuhan can be strongly related to travelers from wuhan in cities with a lesser population the wuhan travel ban resulted in a delayed arrival 291 days of the virus cities that implemented the highest level emergency before the arrival of any case reported 333 lesser number of cases the low level of peak incidences per capita in provinces other than wuhan also indicates the effectiveness of early travel bans and other emergency measures the study also estimated that without the wuhan travel band and emergency measures the number of covid-19 cases outside wuhan would have been around 740000 on the 50th day of the pandemic in summary the study found a strong association between the emergency measures introduced during spring holidays and the delay in epidemic growth of the virus a global mobility data collected from google location services can be found at google 24 and kaggle 25  liu et al 49 formulated a spatio-temporal data set of covid-19 cases in china on the daily and city levels as the published health reports are in the chinese language the authors aim to facilitate researchers around the globe with data set translated to english the data set also divides the cases to citycounty level for analysis of city-wide pandemic spread contrary to other countriesprovince categorizations 26  the data set consists of essential stats for academic research such as daily new infections accumulated infections daily new recoveries accumulated recoveries daily new deaths etc each of these statistics is compiled into a separate csv file and made available on github the first two authors did cross-validation of their data extraction tasks to reduce the error rate researcher 50 utilize reported death rates in south korea in combination with population demographics for correction of under-reported covid-19 cases in other countries south korea is selected as a benchmark due to its high testing capacity and well-documented cases the author correlates the under-reported cases with limited sampling capabilities and bias in systematic death rate estimation the author brings to service two datasets one of the datasets is who statistics of daily country-wise covid-19 reports the second dataset is demographic database maintained by the un this dataset is limited from 2007 onwards and hosted on kaggle for country wise analysis 27  the adjustment in number of covid-19 cases is achieved while comparing two countries and computing their vulnerability factor which is based on population ages and corresponding death rates as a result the vulnerability factor of countries with higher age population is greater than one leading to higher death rate estimations a complete workflow of the analysis is also hosted on kaggle 28  kraemer et al 51 analyzed the effect of human mobility and travel restrictions on spread on covid-19 in china real-time and historical mobility data from wuhan and epidemiological data from each province were employed for the study source baidu inc the authors also maintain a list of cases in hubei and a list of cases outside hubei the data and code can be found at 29  the study found that before the implementation of travel restrictions the spatial distribution of covid-19 can be highly correlated to mobility however the correlation is lower after the imposition of travel restrictions moreover the study also estimated that the late imposition of travel restrictions after the onset of the virus in most of the provinces would have lead to higher local transmissions the study also estimated the mean incubation period to identify a time frame for evaluating early shifts in covid-19 transmissions the incubation period was estimated to be 51 days the aforementioned authors also list and maintain the epidemiological data of covid-19 cases in china as a part of a separate study 8 52  the data set contains individual-level information of laboratory-confirmed cases obtained from city and provincial disease control centers the information includes a key dates including the date of onset of disease date of hospital admission date of confirmation of infection and dates of travel b demographic information about the age and sex of cases c geographic information at the highest resolution available down to the district level d symptoms and e any additional information such as exposure to the huanan seafood market the data set is updated regularly the aim of the open access line list data is to guide the public health decision-making process in the context of the covid-19 pandemic 23 killeen et al 53 accounted for the county-level dataset of covid-19 in the us the machine-readable dataset contains more than 300 socioeconomic parameters that summarize population estimates demographics ethnicity education employment and income among other healthcare system-related metrics the data is obtained from the government news and academic sources the authors obtain time-series data from 42 and augment it with activity data obtained from safegraph safegraph is a digital footprint platform that aggregates location-based data from multiple applications the journalistic data is used to infer the implementation of lock-down measures at the county level the dataset is envisioned to serve the scientific community in general and ml applications specifically for epidemiological modeling researchers 54 provide another study for evaluating the effects of travel restrictions on covid-19 transmissions the authors quantify the impact of travel restrictions in early 2020 with respect to covid-19 cases reported outside china using statistical analysis the authors obtained an epidemiological dataset of confirmed covid-19 cases from government sources and websites all confirmed cases were screened using rt-pcr the quantification of covid-19 transmission with respect to travel restrictions was carried out for the number of exported cases the probability of a major epidemic and the time delay to a major epidemic lai et al 55 quantitatively studied the effect of non-pharmaceutical interventions ie travel bans contact reductions and social distancing on the covid-19 outbreak in china the authors modeled the travel network as susceptibleexposed-infectious-removed seir model to simulate the outbreak across cities in china in a proposed model named basic epidemic activity and response covid-19 model the authors used epidemiological data in the early stage of the epidemic before the implementation of travel restrictions this data was used to determine the effect of nonpharmaceutical interventions on onset delay in other regions with first case reports as an indication the authors also obtained large scale mobility data from baidu location-based services which report 7 billion positioning requests per day another historical dataset from baidu was obtained for daily travel patterns during the chinese new year celebrations which coincided with the covid-19 outbreak the study estimated that there were approximately 01 million covid-19 cases in china as of 29 february 2020 without the implementation of non-pharmaceutical interventions the cases were estimated to increase 67 fold the impact of various restrictions was varied with early detection and isolation preventing more cases than the travel restrictions in the case of a three-week early implementation of non-pharmaceutical interventions the cases would have been 95 less on the contrary if the non-pharmaceutical interventions were implemented after a further delay of 3 weeks the covid-19 cases would have increased 18 times a study on a similar objective of investigating the impact of non-pharmaceutical interventions in european countries was carried out in 56  at the start of pandemic spread in european countries non-pharmaceutical interventions were implemented in the form of social distancing banning mass gathering and closure of educational institutes the authors utilized a semi-mechanistic bayesian hierarchical model to evaluate the impact of these measures in 11 european countries the model assumes that any change in the reproductive number is the effect of non-pharmaceutical interventions the model also assumed that the reproduction number behaved similarly across all countries to leverage more data across the continent the study estimates that the non-pharmaceutical interventions have averted 59000 deaths up till 31 march 2020 in the 11 countries the proportion of the population infected by covid-19 is found to be highest in spain followed by italy the study also estimated that due to mild and asymptomatic infections many fold low cases have been reported and around 15 of spain population was infected in actual with a mean infection rate of 49 the mean reproduction number was estimated to be 387 real-time data was collected from ecdc european centre of disease control for the study researchers 57 contributed towards a publicly available ground truth textual data set to analyze human emotions and worries regarding covid-19 the initial findings were termed as real world worry dataset rwwd in the current global crisis and lock-downs it is very essential to understand emotional responses on a large scale the authors requested multiple participants from uk on 6th and 7th april lock-down pm in icu to report their emotions and formed a dataset of 5000 texts 2500 short and 2500 long texts the number of participants was 2500 each participant was required to provide a short tweet-sized text max 240 characters and a long open-ended text min 500 characters the participants were also asked to report their feelings about covid-19 situations using 9-point scales 1  not at all 5  moderately 9  very much each participant rated how worried they were about the covid-19 situation and how much anger anxiety desire disgust fear happiness relaxation and sadness they felt one of the emotions that best represented their emotions was also selected the study found that anxiety and worry were the dominant emotions stm package from r was reported for topic modeling the most prevalent topic in long texts related to the rules of lock-down and the second most prevalent topic related to employment and economy in short texts the most prominent topic was government slogans for lock-down chen et al 58 describe a multilingual coronavirus data set with the aim of studying online conversation dynamics the social distancing measure has resulted in abrupt changes in the society with the public accessing social platforms 9  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint for information and updates such data sets can help identify rumors misinformation and panic among the public along with other sentiments from social media platforms using twitters streaming api and tweepy the authors began collecting tweets from january 28 2020 while adding keywords and trending accounts incrementally in the search process at the time of publishing the data set consisted of over 50 million tweets and 450gb of raw data authors in 59 collected a twitter dataset of arabic language tweets on covid-19 the aim of the data set collection is to study the pandemic from a social perspective analyze human behavior and information spread with special consideration to arabic speaking countries the data set collection was started in march 2020 using twitter api and consists of more than 2433660 arabic language tweets with regular additions arabic keywords were used to search for relevant tweets hydrator and twarc tools are employed for retrieving the full object of the tweet the data set stores multiple attributes of a tweet object including the id of the tweet username hashtags and geolocation of the tweet researcher 5 analyzes a data set of tweets about covid-19 to explore the policies and perceptions about the pandemic the main objective of the study is to identify public response to the pandemic and how the response varies time countries and policies the secondary objective is to analyze the information and misinformation about the pandemic is presented and transmitted the dataset is collected using twitter api and covers 22 january to 13 march 2020 the corpus contains 6468526 tweets based on different keywords related to the virus in multiple languages the data set is being continuously updated the authors propose the application of natural language processing text mining and network analysis on the data set as their future work similar data sets of twitter posts regarding covid-19 can be found at github 30 and kaggle 31  zarei et al 60 gather social media content from instagram using hashtags related to covid-19 coronavirus covid19 and corona etc the authors found that 58 of the social media posts concerning covid-19 were in english language the authors proposed the application of fake new identification and social behavior analysis on their data set sarker et al 61 mined twitter to analyze symptoms of covid-19 from self-reported users the authors identified 203 covid-19 patients while searching twitter streaming api with expressions related to self-report of covid-19 the patients reported 932 different symptoms with 598 unique lexicons the most frequently reported covid-19 symptoms were fever 65 and cough 56 the reported symptoms were compared with clinical findings on covid-19 it was found that anosmia 26 and ageusia 24 reported on twitter were not found in the clinical studies the allen institute for ai with other collaborators started an initiative for collecting articles on covid-19 research named cord-19 6  the data set was initiated with 28k articles now contains more than 52k articles and 41k full texts multiple repositories such as pmc biorxiv medrxiv and who were searched with queries related to covid-19 covid-19 coronavirus corona virus 2019-ncov etc along with the articles data set a metadata file is also provided which includes each article doi and publisher among other information the dataset is also divided into commercial and non-commercial subsets the duplicate articles were clustered based on publication iddoi and filtered to remove duplication design challenges such as machine-readable text copyright restrictions and clean canonical metadata were considered while collecting data the aim of the data set collection is to facilitate information retrieval extraction knowledge-based discovery and text mining efforts focused on covid-19 management policies and effective treatment the dataset has been popular among the research community with more than 15 million views and more than 75k downloads a competition at kaggle based on information retrieval from the proposed data set is also active on the other hand several publishers have created separate sections for covid-19 research and listed on their website researchers 62 provided a scoping review of 65 research articles published before 31 january 2020 indicating early studies on covid-19 the review followed a five-step methodological framework for the scoping review as proposed in 63  the authors searched multiple online databases including biorxiv medrxiv google scholar pubmed cnki and wanfang data the searched terms included ncov 2019 novel coronavirus and 2019-ncov among others the study found that approximately 90 of the published articles were in the english language the largest proportion 385 of articles studied the causes of covid-19 chinese authors contributed to most of the work 677 the study also found evidence of virus origin from the wuhan seafood market however specific animal association of the covid-19 was not found the most commonly reported symptoms were fever cough fatigue pneumonia and headache form the studies conduction clinical trails of covid-19 the surveyed studies have reported masks hygiene practices social distancing contact tracing and quarantines as possible methods to reduce virus transmission the article sources are available as supplementary resources with the article 30 httpsgithubcombayesfordayscoronada 31 httpswwwkagglecomsmid80coronavirus-covid19-tweets 10  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint researchers 12 detailed a systematic review and critical appraisal of prediction models for covid-19 diagnosis and prognosis multiple publication repositories such as pubmed were searcher for articles that developed and validated covid-19 prediction models out of the 2696 available titles and 27 studies describing 31 prediction models were included for the review three of these studies predicted hospital admission from pneumonia cases eighteen studies listed covid-19 diagnostic models out of which 13 were ml-based ten studies detailed prognostic models that predicted mortality rates among other parameters the critical analysis utilized probast a tool for risk and bias assessment in prediction models 64  the analysis found that the studies were at high risk of bias due to poorly reported models the study recommended that covid-19 diagnosis and prognosis models should adhere to transparent and open-source reporting methods to reduce bias and encourage realtime application researcher from berkeley lab have developed a web search portal for dataset of scholarly articles on covid-19 32  the data set is composed of several scholarly data sets including wang et al 6  litcovid and elsevier novel coronavirus information center the continuously expanding dataset contains approximately 60k articles with 16k specifically related to covid-19 the search portal employs nlp to look for related articles on covid-19 and also provides valuable insights regarding the semantic of the articles in this section we provide a tabular and descriptive comparison of the surveyed open-source data sets table 1 presents the comparison of medical image data sets in terms of application data type and ml method in tabular form we can categorize and compare all of the listed works on their openness some of the works do not have data and code publicly available and it is difficult to validate their work 65  others have code or data publicly available 66  such studies are more relevant in the current pandemic for global actions concerning scientific research against covid-19 on the other hand some studies merge multiple data sets and mention the source of data but do not host it as a separate repository 67  the highly relevant studies have made public both data and code 25 53  an equal number of reported works have utilized ct scans and x-ray images however segmentation techniques to identify infected areas have been only applied to ct scans 27  similarly augmentation techniques to increase the size of the data set have been applied in one of the listed studies 40  all of the works provided 2d ct scans except for one resource from the coronacases initiative 33  most of the covid-19 diagnosis works employed cnns for classification some of the works utilized transfer learning to further increase the accuracy of classification 26 30  moreover few works augmented cnns with svm for feature extraction and classification tasks 40 37  higher accuracies were reported from works augmenting transfer learning and svm with cnns cnns and deep learning models are reported to overfit models due to the limited size of the dataset 25  therefore authors also researched alternative approaches in the form of capsule network 38 and svm 37 for better classification on limited data sets of covid-19 cases most of the covid-19 diagnosis works distinguished between two outcomes of covid-19 positive or negative cases 36  however some of the works utilized three outcomes ie covid-19 positive viral pneumonia and normal cases for applicability in real-world scenarios researchers 30 expanded the classification to six common types of pneumonia such methodologies require the extraction and compilation of data sets with other categories of pneumonia radiographs the ml-based covid-19 diagnosis is difficult to fully automate as a human in the loop hitl is required to label radiographic data 27  resnet mobilenet and vgg have been commonly employed as pre-trained cnns for classification 33 34  aiml explainability methods have been seldom used to delineate the performance of cnns 32  most of the works report accuracies greater than 90 for covid-19 diagnosis 40 37  the data sets of cohen et al 11 is considered pioneering effort and is mostly utilized for the covid-19 cases and kermany et al 35 is employed for common pneumonia cases table 2 presents the comparison of textual data-sets in terms of application data type and statistical method in tabular form the textual data sets are applied for multiple purposes such as a reporting and estimated new covid-19 cases 42 4  b estimating community transmission 43  c correlating the effect of mobility on virus transmissions 51  d estimating effect of non-pharmaceutical interventions on covid-19 cases 55 56  e learning emotional and socioeconomic issue from social media 57 58  and f analyzing scholarly publications for semantics 62  most of the articles apply statistical techniques stochastic bayesian and regression for estimation and correlation 56 45  there is scope for the application of aiml technique as proposed in some studies 5 53  however only statistical techniques have been applied to textual data sets in most of the listed works most of the studies that estimate covid-19 32 httpscovidscholarorg 33 httpscoronacasesorg 11  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint 11 transmissions utilize covid-19 case data collected from various governmental journalistic and academic sources 53  the studies that have human emotions have utilized twitter api to collect data 59 5  studies estimating effect of non-pharmaceutical interventions on covid-19 bring to service locationmobility epidemiological and demographic data 56  the collection of scholarly articles have proposed potential of nlp techniques 6 while a demonstration of the same is available at covidscholar however the details of the semantic analysis algorithms applied by the covidscholar are not available github is the first choice of researchers to share open access data while kaggle is seldom put to use 57 36  most of the research on covid-19 is currently not peer-reviewed and in the form of pre-prints the covid-19 pandemic is a matter of global concern and necessitates that any scientific work published should go through a rigorous review process at the same time the efficient diffusion of scientific research is also demanded therefore this survey had to include pre-prints that are currently bypassing the review process to compile a comprehensive list of articles the pre-prints contribute approximately 50 of the cited research in this survey the credibility of surveyed work is supported by the open-source data sets and code accompanying the pre-prints the are multiple challenges to the open-source data sets and research in fight against covid-19 the main challenges related to the theme of our article are but not limited to a forecasting covid-19 cases and fatalities on city and 12  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020   cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint county levels b predicting transmission factors and incubation period c estimated effect of existing preventive measures on covid-19 infections and transmissions d using natural language processing nlp to analyze public sentiments from social media e applying nlp on scholarly articles to automatically infer scientific findings f identifying key health obesity air pollution etc and social risk factors g identifying demographics at more risk of infection from existing cases and h ethical and social consideration of analyzing patients data multiple challenges are being hosted on kaggle 34  35  36 and elsewhere on the internet 37  we detail the challenges in the following subsection most of the researchers studying image-based diagnoses of covid-19 have emphasized that further accuracy is required for application of their methods in clinical practices contact-less work-flows need to be developed for ai-assisted covid-19 screening and detection to keep medics safe from the infected patients 18 68  moreover researchers have also emphasized that the primary source of covid-19 diagnosis remains the rt-pcr test and medical imaging services aim to aid the current shortage of test kits as a secondary diagnosis method 26 69  furthermore a patient with rt-pcr test positive can have a normal chest ct scan at the time of admission and changes may occur only after more than two days of onset of symptoms 69  therefore further analysis is required to establish a correlation between radiographics and pcr tests 70  data sets are available for most of the research directions in biomedical imaging however these data sets are limited in size for the application of deep learning techniques researchers have emphasized that larger data sets are required for deep learning algorithms to provide better insights and accuracy in diagnosis 40  detectingscreening covid-19 from cough using ml techniques has indicated promising results 7 71  the accuracy of the study is hindered due to the small data set of covid-19 cough samples several researchers are gathering cough based data and have made appeals for contribution from public 38 39  three reported studies collected scholarly articles related to covid-19 62 6 12  however the application of nlp is proposed in these works the inference of scientific facts from published scholarly articles remains a challenge yet to be addressed in reference to covid-19 the only resource available in this direction is the covidscholar developed by berkeley lab for semantic analysis of covid-19 research similarly data sets have been curated from social media platforms 59 5  the human emotions and psychology in the pandemic sentiments regarding lock-downs and other non-pharmaceutical interventions are yet to be investigated thoroughly another research direction is related to social distancing in the pandemic given the preferred social distance across multiple countries and the open-source data what can be its effect on covid-19 72  moreover data set curation is also needed to provide an update on preferred social distances during the covid-19 lock-down initiatives researcher 73 collected evidence of reduced air pollution in urban cities after implementation of lock-downs while comparing february 2019 and february 2020 air quality index aqi the study compared aqi of six cities and observed that the cities under lock-down in the evaluation time showed decreased air pollutants the data was collected from world air quality index project and illustrated in the article however the study was conducted early when the covid-19 was not declared as a pandemic and the actual effect of lock-downs on aqi could not be measured privacy issues are a concern regarding user mobility medical and social data privacy concerns are further escalated due to open-source nature of data google baidu and safegraph have been identified as sources for mobility data in this survey users have concerns about large-scale governmental surveillance in case such data from applications is shared with a third party the automated contact tracing application initiated by several governments in the wake of covid-19 transmissions also demands consideration of user privacy issues 74  automated contact tracing application monitor the user-user interactions with the help of bluetooth communications the population at risk can be identified if one user is diagnosed as covid-19 positive from his user-user interactions automatically saved by the contact tracing application 75  the contact tracing applications can be utilized for large-scale surveillance as user data is updated in a central repository frequently it is yet to be debated the compliance of contact tracing applications with country-level health and privacy laws similarly patient privacy concerns need to be addressed on the country level based on health and privacy laws and social norms 76  public hatred and discrimination have also been reported against covid-19 patients and health workers 77  the situation demands complete anonymity of medical and mobility data to avoid any discrimination generating from data shared for academic purposes all of the open-source data either medical or textual regarding covid-19 patients should be anonymized before sharing although the digital technologies have significantly aided the combat against covid-19 it has also provided ground for vulnerabilities that can be exploited in terms of social behaviors 78  fake newsmisinformation sharing on social media platforms 20  racist hatred 21  propaganda against 5g technologies and governments 22  and online financial scams 23 are few forms of digital platform exploitation in covid-19 pandemic fake news and rumors have been spread about lock-downs policies over-crowded places and death cases on social media platforms fake-news identification is already a popularly debated topic among the social and computer science community 79  existing nlp techniques for fake news identification need to be applied on covid-19 social media data-sets for evaluation of proposed works in the current pandemic 80 21  the social media platforms also need to be analyzed for human perceptions and sentiments regarding specific ethnicity for example sinophobia and lock-down policies 81  the propaganda that 5g networks are responsible for covid-19 spread has also received widespread attention on social media with the society heavily relying on online shopping and banking transactions in the pandemic increased number of online scamming and hacking activities have been reported 23  this article provided a comprehensive review of covid-19 open-source data sets the review was organized on the type of data and application textual and medical image data formed the main data types the application of open-source data set were covid-19 diagnosis infection estimation mobility and demographic correlations socio-economic analysis and sentiment analysis we found that although scientific research works on covid-19 are growing exponentially there is room for open-source data curation and extraction in multiple directions such as expanding of existing ct scan data sets for application of deep learning and compilation of cough samples we compared the listed works on their openness application and mlstatistical methods moreover we provided a discussion of future research directions and challenges concerning covid-19 open-source data sets further work is required on a the curation of data set for cough based covid-19 diagnosis b expanding ct scan and x-ray data sets for higher accuracy of deep learning techniques c establishment of privacy-preserving mechanisms for patient data user mobility and contact tracing d contact-less diagnosis based on biomedical images to protect front-line health workers from infection e sentiment analysis and fake new identification from social media for policy making and f semantic analysis for automated fact-finding from scholarly articles to list a few we advocate that the works listed in this survey based on open-source data and code are the way forward to extendable transparent and verifiable scientific research on covid-19  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint  cc-by-nd 40 international license it is made available under a is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity which was not certified by peer review the copyright holder for this preprint this version posted may 26 2020  httpsdoiorg1011012020051920107532 doi medrxiv preprint  mapping the landscape of artificial intelligence applications against covid-19 joseph bullock alexandra luccioni   katherine pham hoffmann cynthia sin nga lam miguel luengo-oroz  covid-19 the disease caused by the sars-cov-2 virus has been declared a pandemic by the world health organization with over 25 million confirmed cases as of april 23 2020 1  in this review we present an overview of recent studies using machine learning and more broadly artificial intelligence to tackle many aspects of the covid-19 crisis at different scales including molecular clinical and societal applications we also review datasets tools and resources needed to facilitate ai research finally we discuss strategic considerations related to the operational implementation of projects multidisciplinary partnerships and open science we highlight the need for international cooperation to maximize the potential of ai in this and future pandemics   there is a broad range of potential applications of ai covering medical and societal challenges created by the covid-19 pandemic however few of them are currently mature enough to show operational impact  from a molecular perspective ai can be used to estimate the structure of sars-cov-2-related proteins identify existing drugs that may be repurposed to treat the virus propose new compounds that may be promising for drug development identify potential vaccine targets improve diagnosis and better understand virus infectivity and severity  from a clinical perspective ai can support covid-19 diagnosis from medical imaging provide alternative ways to track disease evolution using non-invasive devices and generate predictions on patient outcomes based on multiple data inputs including electronic health records  from a societal perspective ai has been applied in several areas of epidemiological research modeling empirical data including forecasting the number cases given different public policy choices other works use ai to identify similarities and differences in the evolution of the pandemic between regions ai can also help investigate the scale and spread of the infodemic to address the propagation of misinformation and disinformation including the emergence of hate speech  sharing and hosting data and models whether they be clinical molecular or scientific is critical to accelerate the development and operationalization of ai to support the response to the covid-19 pandemic  applications targeting critical applications -such as clinical ones -should take into account existing regulatory and quality frameworks to ensure the validity of use and safety as well as minimize potential risks and harms with the continued growth of the covid-19 pandemic researchers worldwide are working to better understand mitigate and suppress its spread key areas of research include studying covid-19 transmission facilitating its detection developing possible vaccines and treatments and understanding the socio-economic impacts of the pandemic in this article we discuss how artificial intelligence can contribute to these goals by enhancing ongoing research efforts improving the efficiency and speed of existing approaches and proposing original lines of research we have conducted an extensive review of the rapidly emerging literature and identified specific applications of ai at three different scales the molecular scale including drug discovery-related research the clinical scale including individual patient diagnosis and treatment and the societal scale including epidemiological and infodemic research we also review open-source datasets and resources that are available to facilitate the development of ai solutions the mobilization of the scientific community to address the pandemic is unprecedented in its scale an automated search for papers posted on the arxiv and in the covid-19 open research dataset cord-19 identified over 4500 coronavirus-related papers posted between january 1 and april 5 2020 of these papers over 100 included the phrases machine learning artificial intelligence deep learning or neural network in the title or abstract as shown in figure 1  the number of papers has grown dramatically since mid-march 2020 the publication of scientific articles and preprints related to covid-19 between january 2 and april 5 2020 article counts were derived from the cord-19 research dataset and the arxiv api we omitted 2020 articles with no specific publication date and articles published on january 1 2020 since this appeared to be a default date for many articles we also dropped cord-19 articles missing both a title and summary note that the y axis scales differ between plots the purpose of this review is not to evaluate the impact of the described techniques nor to recommend their use but to show the reader the extent of existing applications and to provide an initial picture and road map of how artificial intelligence could help the global response to the covid-19 pandemic the scope of this review is restricted to applications of machine learning ml and artificial intelligence ai and we have therefore made judgment calls regarding whether certain methodologies fall into this category for example we have included applications where authors have explicitly described the use of models such as neural networks and decision trees while excluding applications based on simple linear regression models furthermore we note that many of the articles cited are still preprints at the time of writing this review given the fast-moving nature of the crisis we strove to be comprehensive in our coverage but their full scientific rigor should still be assessed by the scientific community through peer-reviewed evaluation and other quality control mechanisms for specificity we signify all preprints with   finally since this article assumes a certain background knowledge of both machine learning and the nature of the sars-cov-2 virus we invite our readers to consult 2  for further explanation regarding the potential of ml for scientific research and 3  for additional information about the virology clinical features and epidemiology of covid-19 accessible overviews of sars-cov-2 proteins the infection process and molecular modeling can be found in 4 5 6  at the most granular scale of the scientific response to covid-19 biochemistry applications of ai have been used to better understand the proteins involved in sars-cov-2 infection and to inform the search for potential treatments with respect to the virus itself four types of structural proteins are of interest nucleocapsid proteins n envelope proteins e membrane proteins m and spike proteins s 7 8   also of interest are a number of non-structural proteins nsps which are crucial for viral pathogenesis including the 3-chymotrypsin-like 3c-like protease also known as 3clpro the main proteasempro or nsp5 and the papain-like protease plpro part of nsp3 on the human side research has focused on the angiotensin-converting enzyme 2 ace2 protein a receptor that facilitates the virus entry into host cells 9  potential applications of ai on this scale include predicting the structure of these associated proteins identifying existing drugs which may be effective in targeting these proteins and proposing new chemical compounds for further testing as potential treatments 10  residues as characterized by five different angles this approach may allow for better performance by jointly learning features that are relevant to predicting both distance and orientation 18  heo and feig refine trrosetta and alphafolds predicted structures using molecular dynamics simulations and compare the results with structure predictions from a third approach -c-i-tasser 19 -which incorporates nine different methods for contact prediction while the authors find that the predicted structures generally have much variability between them there is some consensus for predicted structures of the papain-like protease part of nsp4 and the m protein in addition to better understanding the structure of key proteins involved in sars-cov-2 infection a number of research efforts have focused on identifying known compounds which might be effective in mitigating infection -including potentially already-approved drugs we have identified four distinct approaches to this problem which are facilitated by ai the construction of biomedical knowledge graphs the prediction of protein-ligand binding affinities molecular docking simulations and the analysis of gene expression signatures biomedical knowledge graphs are networks capturing the relationships between different entities -such as proteins and drugs -in order to facilitate higher-level exploration of how they connect richardson et al 20 use this technique to identify baricitinib a drug which is commonly used to treat arthritis via inhibition of jak12 kinases as a promising therapy for covid-19 because it inhibits the ap2-associated protein kinase 1 aak1 enzyme and may therefore make it harder for the virus to infect host cells related work has described two approaches which potentially inform the graph construction first segler et al 21 describe an approach to mining a structured database of chemical reactions reaxys using a three-part neural network pipeline combined with a monte carlo tree search approach 3n-mcts in order to understand how various compounds are formed hierarchically from reactions between simpler component compounds second fauquer et al 22 describe a strategy for mining an unstructured scientific article database pubmed to identify stylized relationships between genedisease pairs expressed in individual sentences eg gene promotes disease ge et al 23  describe a similar approach to constructing a knowledge graph connecting human proteins viral proteins and drugs using databases that capture the relationships between these entities the graph is used to predict potentially effective candidate drugs this list is further refined using a natural language processing nlp model ie a biomedical entity relation extraction bere approach 24  applied to the pubmed database filtered for mentions of the candidate drug compounds coronaviruses or their associated proteins the authors identify a poly adp-ribose polymerase 1 parp1 inhibitor cvl218 as a promising candidate and it is currently undergoing clinical testing trigger a signal which can be activation or inhibition hu et al 25  use a multitask neural network to predict these affinities identifying a list of 8 sars-cov-2 related proteins which they attempt to target using a database of 4895 drugs they suggest 10 promising drugs along with their target proteins and binding affinity scores which indicate the likelihood that the drug will act as an inhibitor in an attempt to increase model interpretability they also estimate the precise regions of each target protein where binding is likely to occur in a similar vein zhang et al 26  use their dense fully connected neural network architecture trained to predict binding affinities on the pdbbind database in order to identify potential inhibitors of the 3c-like protease they develop a homology template model of the target protein using its sars variant and explore databases of existing compounds eg chemdiv and targetmol as well as tripeptides to find treatments which may be effective at targeting this protein nguyen et al 27  also build a sars-based homology model of the 3c-like protease and apply their mathematical deep learning mathdl approach to identify potential inhibitors for this protease in particular their model uses mathematical representations of molecules as inputs and relies on two main datasets information on 84 sars coronavirus protease inhibitors from the chembl database and a more general set of 15843 protein-ligand binding affinities from the pdbbind database they fit two different convolutional neural networks cnns 28 on this dataset -a pooled 3dall cnn which is trained on both datasets together and a multitask 3dmt cnn which is trained on each dataset separately 29  using a consensus between these cnn models the authors identify a list of 15 promising drug candidates from the drugbank dataset finally beck et al 30  use their own molecule transformer-drug target interaction mt-dti model of binding affinities to identify us food and drug administration fda approved antivirals which may be effective in targeting six coronavirus-related proteins the 3c-like protease rna-dependent rna polymerase helicase 3-to-5 exonuclease endornase and 2-o-ribose methyltransferase the mt-dti model ingests string data in the form of simplified molecular-input line-entry system smiles data and amino acid sequences and applies a text-modeling approach that leverages ideas from the bert algorithm 31  the model identifies drugs that are expected to be effective in targeting each protein studied hofmarcher et al 32  likewise apply a text-based approach to smiles data chemai which in turn relies on a long short-term memory lstm 33 model called smileslstm to screen almost 900 million compounds from the zinc database for effectiveness in inhibiting the sars coronavirus 3c-like protease and the papain-like protease they rank compounds according to predicted inhibitory effects toxicity and proximity to known compounds and produce a list of 30000 candidate compounds for screening another approach to drug repurposing and discovery involves molecular docking simulations in a docking simulation a wide range of candidate ligands interact with a protein in different orientations and conformations generating a variety of poses also known as the binding modes -ie the resulting interactions between the ligand and the protein as they bind the poses are subsequently scored and used to predict the ligands binding affinity since docking simulations are computationally expensive some research has studied how to make the search more efficient by narrowing the pool of candidates which must be docked for example ton et al 34 develop a deep docking dd platform which trains a neural network to predict the outcomes of docking simulations which they use to identify a set of 3 million candidate 3c-like protease inhibitors from a set of over 1 billion compounds extracted from the zinc database the authors then run a full docking simulation on the resulting compounds presenting the top 1000 results on the other hand batra et al 35  train a random forest algorithm on smiles data to predict binding affinities which would result from docking simulations and use this approach to select 187 promising molecules to target the coronavirus s-protein and the ace2 receptor for the final docking simulation they also identify 19000 additional candidate compounds in the bindingdb dataset a fourth approach to drug repurposing involves discovering therapies which have similar effects to other known effective treatments to this end donner et al 36 use the lincs dataset of gene expressions from cells targeted by various perturbagens chemical or genetic reagents to treat cells and alter intracellular processes they learn an embedding with a deep neural network classifier that predicts the perturbagen associated with each signature ie the gene expression that is specifically correlated with a biological state of interest such as therapeutic response in order to correctly classify signatures associated with the same perturbagen the learned embedding should abstract away from the noise in the input data and identify core features that are associated with a perturbagens effect following this logic their approach can utilize similarity in the learned embedding space to predict pharmacological similarities in structurally different compounds and hence expand the horizon of drug repurposing avchaciov et al 37  adapt this approach to find drugs that produce gene expression signatures that are similar to the cobp2 gene knockout which might limit the replication of sars-cov-2 based on the genes role in the replication of the related sars coronavirus they list twenty of the most promising drugs many of which have already been identified as antivirals since these drugs have been authorized for clinical trials or already approved the authors argue that their approach could facilitate the rapid discovery of potentially effective therapies some research attempts to discover entirely new compounds for use in targeting sars-cov-2 zhavoronkov et al 38  use a proprietary pipeline to find inhibitors for the 3c-like protease their models use three types of input the crystal structure of the protein the co-crystalized ligands and the homology model of the protein for each input type the authors fit 28 different models including generative autoencoders 39 and generative adversarial networks 40  the authors explore potential candidates using a reinforcement learning approach with a reward function that incorporates factors such as measures of drug-likeness novelty and diversity moreover they confirm that the identified candidate molecules are dissimilar to existing compounds suggesting that they have indeed found novel candidate drugs tang et al 41  also apply a reinforcement learning approach to the discovery of compounds that inhibit the 3c-like protease specifically the authors create a list of 284 molecules known to act as inhibitors in the context of sars they break these proteins into a series of 316 fragments which can then be combined using an advanced deep q-learning network with fragment-based drug design adqn-fbdd that rewards three aspects of discovered molecules a drug-likeness score the inclusion of pre-determined favorable fragments and the presence of known pharmacophores which are abstract design patterns believed to be correlated with a compounds effectiveness 42  the 4922 results are heuristically filtered and the 47 top compounds are assessed with molecular docking simulations from which the researchers then select the top most promising compound and manually tailor it to produce suggested variants for testing in a third approach bung et al 43  build a generative model for smiles input strings treating the strings as a time series of characters the model is a classifier that predicts the next character in the string the model is first trained on 16 million molecules from the chembl database and then adapted to a smaller dataset of protease inhibitors using transfer learning reinforcement learning was then used to train the model to generate compounds with desirable properties after filtering the resulting molecules and docking them the authors propose 31 candidate inhibitors finally gao et al 29  apply a generative network complex gnc for drug discovery their pipeline involves gated recurrent unit gru based encoders and decoders which ingest smiles strings and propose new variants with the help of a dnn between the encoder and decoder that optimizes candidate variants they also use a pretrained 2d fingerprint-based dnn 2dfp-dnn as well as mathdl cnns described briefly above to further predict the properties of the resulting drugs the authors identify 15 novel candidate drugs and also test two proposed hiv drugs to estimate their efficacy against sars-cov-2 a similar approach to drug discovery is described in chenthamarakshan et al 44   the authors controlled generation of molecules cogmol framework uses a variational autoencoder vae trained on smiles strings to learn molecule embeddings on these embeddings the authors train a model to predict drug properties and protein binding affinities and use this to constrain the search for novel strings using conditional latent attribute space sampling class the authors also use a multitask dnn to predict toxicity in order to avoid proposing candidate drugs with a low probability of success later on in the testing pipeline the authors focus their search on drugs which target nsp9 the 3c-like protease and the receptor-binding domain rbd associated with the s protein proposing 3000 of the top candidates for further study in the process of mounting an immune response b-cells in the body produce antibodies which attack the part of the pathogen the virus known as an antigen magar et al 45  thus take a different approach to discovering new therapies in which they search for antigen-neutralizing antibodies they first construct a training dataset virusnet consisting of 1933 known antigen-antibody sequences from related illnesses such as hiv sars ebola and influenza the authors then train classification models such as xgboost 46 on graph embeddings of these antigens and antibodies to predict whether an antibody will have a neutralizing effect on an antigen finally the authors mutate the sars coronavirus antibody sequence to generate 2589 candidate antibody sequences given the subset of these antibodies which are predicted to be effective by the algorithm they filter these mutations for valid and stable variants which they identify through the use of molecular dynamics simulations proposing 8 antibodies as potentially effective treatments another area of interest is vaccine discovery in addition to producing virus-neutralizing antibodies via b-cells as described above humoral immunity the body also uses t-cells to attack the virus directly cellular immunity there is a subset of t-cells called memory cells which recognize the antigen of a formerly eliminated pathogen and can quickly activate more effector t-cells upon re-exposure these processes inform targets for vaccine design as part of the response helper proteins called major histocompatibility complex proteins mhc i and mhc ii proteins present binding regions of antigens called epitopes for antibodies b-cells or t-cells to bind and attack these mhc i and mhc ii proteins are encoded by human leukocyte antigen hla gene complexes which vary from person to person therefore vaccine design involves two key objectives 1 identifying suitable epitopes for targeting and 2 ensuring that these epitopes can be presented by mhc proteins which are produced by different hla alleles variants of a gene that occur in the population for example fast et al 47 search for b-and t-cell epitopes they identify 405 potential t-cell epitopes that can be presented by mhc i and mhc ii proteins as well as two b-cell epitopes on the s-protein the search for the t-cell epitopes relies on two previously-developed neural networks to predict mhc presentation netmhcpan4 and maria upon identifying potential epitopes the authors examine 68 different genetic variants of sars-cov-2 to study how the virus mutates and identify parts of the virus that are more or less prone to evolution they conclude that the s-protein epitopes may be a good target for vaccines because they contained no nearby mutations in their sample in an alternative approach ong et al 48  use their vaxign-ml framework which leverages supervised classification models such as xgboost 46  in an effort to predict which viral proteins may serve as effective vaccine targets while the authors find that the s-protein is the best candidate they also identify five possible nsps -most promisingly nsp3 and nsp8 -as good candidates for the vaccine target researchers are also applying machine learning in an attempt to improve the current virus nucleic acid detection test metsky et al 49  combine ml with crispr a tool which uses an enzyme to edit genomes by cleaving specific strands of genetic code to develop assay designs for detecting 67 respiratory viruses including sars-cov-2 the authors note that this technology can speed up the processing of test samples in order to assist overburdened diagnostic facilities and help address the challenge of false positives which occur as a result of sequence similarity between sars-cov-2 and other coronaviruses ml models have been built to rapidly design assays which are predicted to be sensitive and specific and cover a diverse range of genomes the authors state that they are aiming to build a cas13-based point-of-care assay for sars-cov-2 in the future lopez-rincon et al 50 take another approach in which they employ a convolutional neural network cnn model to nucleic acid sequences to classify whether they are associated with sars-cov-2 and therefore potentially improve the specificity of diagnosis they contrast sars-cov-2 with other human coronaviruses from the 2019ncovr repository as well as other genome sequences with the orf1ab protein from genbank the authors use a 21-base pair convolution over the whole genome as opposed to previous approaches which only examine sequences of fixed length and visualize the networks convolution and max-pooling layers to understand which particular sequences help to identify sars-cov-2 using the 21-base-pair sequences retained after the max-pooling layer they subsequently fit a classification model eg logistic regression for the same task using feature selection to identify the most predictive sequences they also apply this classification approach to distinguish between hospitalized and asymptomatic cases using just 12 21-base-pair sequences the authors are able to identify sars-cov-2 with over 99 accuracy and classify asymptomatic cases with 85 accuracy although the data of the study was limited this work highlights that opportunities lie in bioinformatic processes for researchers to improve existing diagnostic tools additional efforts have used machine learning to better understand sars-cov-2 infection severity and infectivity how likely it is that a pathogen can infect a host using protein sequences for example gussow et al 51  use support vector machines svm 52 on genomes from different coronaviruses to identify which parts of coronavirus protein sequences distinguish high case fatality rate high-cfr from low-cfr variants bartoszewicz et al 53  use reverse-complement neural networks built from cnn and lstm architectures to detect whether a virus has the potential to infect a human host using its viral genome sequence and apply machine learning interpretability techniques to identify the parts of the sequence that are most associated with infectivity finally randhawa et al 54 use a machine learning with digital signal processing ml-dsp approach which uses supervised learning approaches such as svm 52 and k-nearest neighbors knn as well as decision trees to predict the taxonomic classifications of viruses based on their genomic sequences their findings support the classification of sars-cov-2 as a sarbecovirus of the betacoronavirus class and the hypothesis that it came from bats to date most clinical applications of ai to the covid-19 response have focused on diagnosis based on medical imaging with an increasing number of studies exploring non-invasive monitoring techniques in recent literature we have found several works that use ai to support diagnosis from computational tomography ct and x-ray scans in addition to others that use patient medical data to predict the evolution of the disease and original non-invasive measurements for monitoring purposes reverse transcription polymerase chain reaction rt-pcr tests are the key approach used for diagnosing covid-19 however they have limitations in terms of resources specimen collection time required for the analysis and performance 55  as such there is growing interest in other diagnostic methodologies which use medical imaging for the screening and diagnosis of covid-19 cases 56  this is notably due to the fact that covid-19 exhibits particular radiological signatures and image patterns which can be observed in medical imagery 55 57  but the identification of these patterns remains time-consuming even for expert radiologists this makes image analysis from lung ct and x-ray scans of covid-19 patients a prime candidate for ml-based approaches which could help accelerate the analysis of these scans although the extent to which imaging can be used for diagnosis is still under discussion 58 59  nonetheless there are several approaches that aim to leverage machine learning for diagnosing covid-19 from ct scans via binary ie healthy vs covid-19 positive 60-62  or multi-class healthy patients vs covid-19 vs other types of pneumonia classification tasks using neural networks trained from scratch 63 64  65  these approaches use different architectures such as inception 66  unet 67 and resnet 13  which can be trained directly either on raw ct scans or scans labeled with regions of interest identified by radiologists some studies also adopt a hybrid approach combining off-the-shelf software with bespoke ml approaches in order to achieve higher accuracy for example in gozes et al 68   a commercial medical imaging program is used for initial image processing and then combined with an ml pipeline the two-step ml approach consists of a u-net architecture 69 trained on medical images of lung abnormalities in order to pinpoint lung regions of interest and a resnet-50 13 trained on imagetnet 70 and fine-tuned on covid-19 cases in order to classify the images as covid-positive or healthy the resulting architectures are able to extract relevant features from the images and identify covid-19 pneumonia even in cases where there are several competing potential diagnoses and can be deployed both at hospitals to help radiologists accelerate the analysis of new cases and shared on the internet to enable rapid review of new images x-ray images and specifically chest radiographs also can be used for covid-19 detection given the accessibility and potential portability of the imaging equipment needed they can be an alternative in settings where access to advanced medical equipment such as ct scanners is limited as shown in 71 72 73   there is potential in the use of deep learning approaches on x-ray imagery using architectures similar to the ones used for ct scans eg resnet 13 and convolutional neural networks cnns 28  further work aims to make predictions interpretable 74 75  and ensure that the models can be deployed in mobile and low-resource settings 76   studies which report operational deployment such as 77   have opted for human-inthe-loop approaches to reduce the analysis time required while utilizing ml architectures the authors use small manually-labeled batches of data for training an initial model based on the v-net architecture 78  this model then proposes segmentation of new ct scans which can then be corrected by radiologists and fed back into the model in an iterative process this approach has enabled the development of a deep learning-based system for both automatic segmentation and the counting of infection regions as well as assessing the severity of covid-19 ie the percentage of infection in the whole lung the authors show not only that the model improved its own performance incrementally but also that the human time required for analysis of new images dropped from over 30 minutes initially to under 5 minutes after 200 annotated examples were used to train the model reducing the effort required by radiologists to review a new scan this is a promising direction which harnesses the power of ml alongside human annotation and expertise which can be complementary and mutually beneficial while encouraging results have been achieved by many medical imagery-based ai diagnostics methods in order for these methods to be used as clinical decision support systems they should undergo clinical investigations and comply with regulatory and quality control requirements in particular their performance should be validated on a relevant and diverse set of training validation and test datasets and they should demonstrate effectiveness in the clinical workflow 79  we note that most of the papers we reviewed lacked provisions for these measures relying on small and poorly-balanced datasets with flawed evaluation procedures and no plan for inclusion in clinical workflows there are also a number of original approaches that do not require specialized medical imaging equipment for diagnosing and tracking covid-19 for example one study used a gru neural network 80 trained on footage from kinect depth cameras to identify patient respiratory patterns 81   based on recent findings suggesting that covid-19 has respiratory patterns which are distinct from those of the flu and common cold notably that they exhibit tachypnea rapid respiration 82  while these abnormal respiratory patterns are not necessarily correlated with a real-world diagnosis of covid-19 prediction of tachypnea could be a relevant first-order diagnostic feature that may contribute to large-scale screening of potential patients furthermore new studies are being carried out which aim to understand how wearable device data can help covid-19 surveillance based on clinical research that demonstrates the value of aggregated signals from resting heart rates acquired from smart watches for influenza surveillance 83  finally a growing number of efforts aim to utilize mobile phones for covid-19 detection using eg embedded sensors to identify covid-19 symptoms 84   phonebased surveys to filter high-risk patients based on responses to key questions regarding travel and symptoms 85  or the analysis of cough sounds for preliminary covid-19 diagnosis 86   while these are important efforts given the ubiquity and accessibility of mobile phone technology these studies are not sufficiently advanced to evaluate their performance so more extensive testing and clinical investigations are needed for deployment forecasting potential patient outcomes is critical for preparation planning and optimization in overstretched health systems during the covid-19 pandemic it is important to know which factors can put patients at risk for hospitalization developing acute respiratory distress syndrome ards and death from respiratory failure in this vein there have been several recent papers that propose triage approaches based on features contained in patients medical data and blood tests in order to help clinicians identify high-risk patients and those at risk of later development of ards 87 88   89  using approaches such as the xgboost algorithm 46 and support vector machines 52  these approaches aim to identify key measurable features to predict mortality risk which can later be tested for in hospitals upon patient admission and during the hospital stay clinical indicators that were identified using these ml-driven approaches include lactic dehydrogenase ldh lymphocyte and high-sensitivity c-reactive protein crp 88   alanine aminotransferase alt myalgias and hemoglobin 89  and interleukin-6 systolic blood pressure and monocyte ratio 87   although more research is needed to define specific thresholds and ranges of these indicators furthermore several complementary studies aim to leverage medical imagery for patient outcome prediction these include carrying out severity assessment 90  predicting the need for long-term hospitalization based on ct imaging data 91   and patient risk stratification based on x-ray images 92   such approaches could help identify patients that might require intensive and long-term care enabling hospitals to plan and manage their resources more effectively as well as to monitor the state of patients and recognize when their condition worsens a hybrid approach has also been proposed for this purpose utilizing both ct findings as well as clinical features to predict the severity of covid-19 93  the clinical features that were identified in this study ie ldh and crp are similar to those identified in the purely clinical studies mentioned above this overlap is promising for eventual clinical monitoring of these indicators while these studies are limited both in scope and in data they constitute important avenues of research that can be complemented and extended with clinical data from incoming cases around the world thereby hopefully improving the prognosis of all patients and reducing the mortality of those that are critically ill the spread of the sars-cov-2 virus across the globe has received much policy attention with advice at the national and local level changing daily in many locations as new information and model forecasts become available understanding how the virus is transmitted and its likely effect on different demographics and geographic locations is therefore crucial for public policy health care interventions the field of epidemiological research is incredibly vast and the relevance and scale of the pandemic in addition to the new data becoming available has resulted in multiple modeling efforts while most of these endeavors build on well-established classical models such as susceptible-infected-recovered sir models and fine-tune them to the covid-19 situation we focus here on cases specifically employing machine learning techniques for epidemiological modeling tasks most ai applications developed for epidemiological modeling have focused on forecasting national and local statistics such as the total number of confirmed cases mortality and recovery rates many authors have attempted to identify optimal approaches or model architectures for understanding and forecasting data eg on a daily basis 94 95  96  these works employ modeling techniques such as an lstm-gru architecture 33 80 for time series analysis and prediction 94   and cnn 28 based approaches in which numerical data has been combined and reshaped into images 95   in addition new forecasting models for predicting the total number of confirmed cases have been developed 96  in this study the authors combine an adaptive neuro-fuzzy inference system anfis 97 with an enhanced flower pollination algorithm fpa 98 and salp swarm algorithm ssa 99 for optimizing the parameters of the model in addition they assess the robustness of their approach by training and testing on weekly confirmed influenza cases as collected by the us centers for disease control and the who over two different four-year periods while these studies show how a range of different architectural choices can be made when building forecasting models they demonstrate the complexities involved in choosing between such models and the non-trivial interplay between architectures hyperparameters and datasets moreover since much of the data collected for covid-19 modeling tasks is extremely limited the choice of models and datasets can have significant effects on overall performance in an attempt to address this a simple framework entitled group of optimized and multi-source selection grooms has been developed for exploring models and datasets during testing by ensuring that models of different categories as defined by the authors are tested in parallel for comparison 100  using the framework the authors propose and test a polynomial neural network with corrective feedback pnncf 101 against other model architecture this model is found to achieve optimal performance in predicting daily statistics on small datasets taken from chinese health authorities social media other online data sources also provide a rich source of information for understanding public opinion perception and behaviour such information can be april 24 2020 1332 incorporated into modeling efforts to augment existing data with the aim of providing more contextual understanding for example liu et al 102  combine related internet search and news media activity with data from the chinese center for disease control and daily forecasts from gleam 103  an agent-based mechanistic model in order to produce 2-day forecasts for a range of daily statistics the authors first cluster provinces based on geo-spatial similarities in covid-19 activity and then train a separate model on each cluster the authors adapt an existing autoregressive model 104 105  for forecasting similarly data pertaining to google search queries and news media volumes have been used as inputs to forecasting models for predicting daily trends 106   the authors assess the frequency of searches for different symptoms derived from a uk national health service survey of covid-19 patients in which symptoms were recorded once the frequencies of each symptom search were calculated they were weighted according to a probability distribution derived from these questionnaires using this data along with prior daily statistics the authors train an elasticnet 107 model for forecasting future trends moreover the authors investigate the transferability of their models between countries this type of approach could be useful for probing the viability of training a model on data-rich countries and applying it to a data-poor ones although the results of such a transferred model will have to be tailored for local contexts given that there may be different demographic characteristics and cultural norms during the course of the outbreak different countries experience different outbreak timing and growth depending on a range of factors including international travel demographics socioeconomic factors health care system characteristics and policy interventions by assessing commonalities in virus propagation trends as well as other country and regional data it may be possible to cluster countries and regions and thereby use data from other similar areas to predict the outbreak in others while useful at a high level a significant limitation of many articles in this category is the heterogeneous data collection and reporting in different countries due to multiple factors including testing case tracking and reporting quality and standard a simple approach to clustering countries using an unsupervised k-means algorithm is given by carollo-larco et al 108  the authors cluster 155 countries using data relating to disease prevalence average health status air quality gross domestic product gdp and universal health coverage and find that their model was able to stratify countries according to the number of confirmed cases although could not stratify them in terms of the number of deaths or the case fatality rate science and engineering csse at johns hopkins university the authors then study the latent variables of the ta to create a 2-dimensional clustering of countries 111  in attempting to manage the pandemic many national and local governments have introduced public policy interventions such as social distancing and the quarantining of individuals showing symptoms of covid-19 the impacts of such measures can be modeled using agent-based models or by introducing regularizers in differential equations governing interaction models such as sir for instance hu et al 112  use data from who reports to train a modified autoencoder mae to predict the number of cases and deaths on a daily basis the authors encoded different intervention mechanisms according to their perceived strength and used this variable as an input to the model a different approach used data from wuhan china to build on the classical sir model by adding a time-dependent regularizer to model the number of infected people who are in quarantine 113   instead of specifying the form of this function and fitting parameters the authors use a neural network to learn the quarantine strength qt based on the data which in turn helps to determine the number of people who are able to infect others due to quarantine while such work is heavily dependent on the available data and does not differentiate between symptomatic and asymptomatic individuals the use of neural networks to augment well-understood techniques could serve as a powerful modeling tool the models discussed in the previous sections mainly focused on predicting daily aggregate statistics for different regions or countries other work has specifically attempted to forecast risks of outbreaks in such regions often by reducing aggregate statistical trends into a single risk score which facilitates interpretation distills information for rapid analysis and acts as a precursor to further investigation however it is important to note that such a distillation may not be robust to important changes in the underlying data or its coverage and so should be interpreted with caution by policy makers pal et al 114  train an lstm 33 on variables including daily statistics and weather data to predict the long-term duration of the disease in assessing which variables should be included in the model the authors use an ordinary least-squares regression model to assess the p-value of all candidate features the output of the lstm is then used alongside explicit fuzzy rules based on rates of death confirmed cases and recovery to determine a risk category for the country or region a similar study looked at the inherent risk of contagion irc which is defined and calculated by the authors for similar geographic regions based on the acceleration of disease spread 115   the authors use k-means clustering to identify similar regions based on a non-linear combination of demographic and social characteristics and trained a fully connected network fcn on data from lombardy italy to forecast the irc of the remaining provinces and municipalities of the country a more detailed approach was taken by ye et al 116   who develop a hierarchical community-level risk assessment given a location the proposed satellite framework provides risk indices associated with different geographic levels eg state county city to achieve and test this framework the authors use data from the who and united states centers for disease control as well as county governments and other media for new cases death rates and confirmed cases demographic data mobility data and social media data which is to be used as an indication of public perception for regions in which social media data is sparse the authors use a cgan 117  trained on similar areas to generate social media content the authors then attempt to incorporate how information at each of the different regional levels impacts the others as well as how different attributes at each level influence the overall spread of the disease after building a graph defining relationships between different geographic levels the authors extract the latent variables from an autoencoder which is designed to aggregate information propagated between different nodes on the graph the autoencoder here plays the role of a dimensionality reduction algorithm to better understand the interplay between different geographic areas and their attributes although they are sometimes considered to be statistical rather than machine learning approaches bayesian analysis techniques can provide useful insights with respect to uncertainty and the handling of small datasets in one study roy et al 118  develop a time-varying bayesian autoregressive model for counts tvbarc with a linear link function for the estimation of time-dependent coefficients which could allow for better temporal modeling of the virus spread a more case-specific application of such methods is employed by 119  who seek to understand the rate of asymptomatic cases using data on 634 confirmed cases collected during the covid-19 outbreak on-board the diamond princess cruise ship the authors use a bayesian time-series model with hamiltonian monte carlo hmc algorithm and a no-u-turn-sampler 120 for model parameter estimation to estimate the probability that a given patient is asymptomatic conditional on infection along with the duration for which an individual is infected the authors conclude that 179 of patients are asymptomatic although it is unclear if this result applies to the broader population contained environments such as this one can be useful for tracking infection because they allow for comprehensive case data collection the who defines an infodemic as an over-abundance of information -some accurate and some not -that makes it hard for people to find trustworthy sources and reliable guidance when they need it and deems it a second disease which needs fighting 121  in this section we highlight efforts to quantify the spread of information surrounding the pandemic and to understand its dynamics dealing with this vast amount of information requires the development and adoption of new tools particularly surrounding the dissemination of misinformation and disinformation while this is already an area in which much ai and more specifically ml research has been carried out there is still a need for greater understanding of the underlying social dynamics during the pandemic social media and online platforms have become key distribution channels for information surrounding the virus although national and international organizations have used these platforms to constructively communicate with the public we are also seeing that populations can become overwhelmed with information and that the propagation of misinformation and disinformation is increasingly prevalent furthermore as highlighted in figure 1  we have seen a significant increase in the number of published scientific articles related to the sars-cov-2 virus given that the virus is still relatively new and our understanding is quickly developing many of these articles are disseminated via preprint archives making it difficult to assess their quality this does not mean that information contained within these articles cannot be valuable but rather that effort should be made to distill this vast body of literature understanding more about the dissemination of information is crucial to intervening proactively or reactively while we acknowledge a wealth of literature on information propagation network analysis and social media interaction in this section we discuss specifically those works applying such methods to the current infodemic at a high-level work such as that by singha et al 122  looks at global trends on twitter by country this work analyzes tweet volume according to specific themes discovered in coronavirus-related queries the authors also analyze posts pertaining to specific myths surrounding the virus examining the number of tweets containing certain terms they deem related to the myths as well as the websites linked from them categorized as either high-quality or low-quality sources in an effort to find early warning signals of a country or region experiencing an infodemic galotti et al 123  analyze social media posts on twitter across 64 languages the authors develop an infodemic risk index iri to quantify the rate at which a given generic user from a country or region is exposed to such unreliable posts from different classes of users ie verified humans unverified humans verified bots and unverified bots the iri considers the number of followers of the users which fall into each class the number of messages those users post and their reliability as measured by fact-checkers applied to samples of the user posts this study highlights potentially actionable insights observing that the escalation of the epidemics leads people to progressively pay attention to more reliable sources thus potentially limiting the impact of the infodemics but the actual speed of adjustment may make a major difference in determining the social outcome and in particular between a sic controlled epidemics and a sic global pandemics in a broad-ranging study 124   interaction and engagement with covid-19-related social media content is analyzed from a collection of eight million comments and posts selected using covid-19 related keywords from twitter instagram youtube reddit and gab the authors estimated engagement and interest in covid-19 and comparatively assess the evolution of discourse on each platform interaction and engagement were measured using the cumulative number of posts and the number of reactions eg comments likes etc to these posts across the 45-day period the authors then employed phenomenological 125 and classical sir models to characterize the reproduction numbers specifically they examined the average number of secondary cases users that start posting about covid-19 created by an infectious individual already posting on each of the social media platforms as in epidemiological models the authors simulated the likelihood of an infodemic in which discussion of covid-19 will grow exponentially at least in its initial stages moreover the authors examined the spread of misinformation which they identify using external fact-checking organizations they find that information from both reliable and unreliable sources propagate in similar patterns but that user engagement with posts from less-reliable sources is lower than engagement with content from reliable sources on major social media streams similarly mejova et al 126  have examined the use of facebook advertisements april 24 2020 1732 with content related to the virus the authors used the facebook ad library to search for all advertisements using the keywords coronavirus and covid-19 and collected results across 34 countries with most in the us 39 and the eu italy made up 25 of the advertising market while the majority of advertisements were paid for by non-profits to disseminate information and solicit donations the authors found that around 5 of advertisements contained possible errors or misinformation along with the propagation of misinformation and disinformation the rise in hate speech in recent months has been of significant concern as reported by the united nations there is an alarming rise of verbal abuses which might turn into physical violence against vulnerable and discriminated groups 127  velasquez et al 128  take a high-level approach to understanding the spread of hateful and malicious covid-19 information and content within a variety of different social media channels and attempt to characterize the methods by which such content moves between different channels concerningly the authors find that hateful content is rapidly evolving and becoming increasingly coherent as time continues as in 124   this study makes a comparison to the epidemiological r0 reproduction number in an attempt to determine the tipping point at which information will spread rapidly between information channels others have looked at the emergence of sinophobic behavior on social media specifically twitter and 4chan 129   this study uses data from october 2019 to march 2020 and uses word embeddings to assess context and word similarity over the entire five month period as well as on a weekly basis the authors also compared their findings to models trained on content taken from historical pre-covid-19 content the authors observed a distinct increase in sinophobic content across the social media channels analyzed and concluded that the web is being exploited for disseminating disturbing and harmful information including conspiracy theories and hate speech targeting chinese people understanding and fighting the spread of hate speech is of vital importance for the protection of human rights in particular those of the most vulnerable and marginalized by better comprehending the dynamics and the landscape of hateful speech effective intervention mechanisms can be designed to disrupt and change the narrative in the process of studying the features and dynamics of the infodemic many of the works mentioned above suggest possible intervention options in this section we explore several examples of such positive actions that are being considered andor deployed to counter the infodemic the world health organization has taken steps to proactively confront the infodemic and bring together actors to assess aspects which still need to be addressed indeed the who has been combating this infodemic through the use of its information network for epidemics epi-win platform for sharing information with key stakeholders 130  and is also working with social media and internet search companies to track the spread of specific rumors and to ensure that who content is displayed at the top of searches for terms related to the virus indeed in april 2020 the who conducted a wide-ranging consultation on understanding and managing the infodemic resources from which will be publicly available 121  efforts are also underway to curate specific news content related to the virus and perform both manual and automated fact-checking and relevance analysis for instance pandey et al 131  have developed a pipeline for assessing the similarity between daily news headlines and who recommendations the pipeline uses word embedding and similarity metrics such as cosine similarity to assess the relevance level of who recommendations to news article titles and content if the similarity is above a certain threshold then the new article displays on the users timeline with the associated relevant who recommendation the setting of the similarity threshold is determined by human reviewers prior to release and then can be updated through user feedback in the face of conflicting information such methods could help identify accurate and trustworthy news articles which highlight important guidelines and promote official recommendations another possible intervention strategy under consideration is the use of chatbots which can be used to disseminate information while relieving pressure on other communication channels such as question-and-answer hotlines for example the who who has developed an interactive chatbot in multiple languages that allows users to explore pre-coded topics 132  finally digital personal assistants could also be used to interactively disseminate official information although governments and international actors would need ways to update recommendations as understanding changes the success of the global effort to use ai techniques to address the covid-19 pandemic hinges upon sufficient access to data machine learning and deep learning in particular requires notoriously large amounts of data and computing power in order to develop and train new algorithms and neural network architectures in this section we describe some of the datasets and data collection efforts that exist at the present time the current number and location of cases is essential for tracking the progress of the covid-19 pandemic calculating the growth rate of new infections and observing the impact of preventive measures several datasets from organizations such as the who and national centers for disease control cdcs exist for this purpose they have been aggregated into public repositories hosted by institutions such as the johns hopkins csse or platforms like github 133  in order provide day-level information on covid-19 affected cases gathered from a variety of reliable sources there are also other complementary data sources -including regional data on school closures bank interest rates and even community perceptions of the virus -which are continuously being added to a data portal hosted by the humanitarian data exchange a multitude of ai algorithms can be applied on this kind of data including time series forecasting approaches such as lstm networks 33 to predict the evolution of cases on a global and regional scale there is also an increasing quantity of tools and resources developed specifically for medical professionals and institutions using data to help them prepare for managing the pandemic for instance chime is an open-source covid-19 hospital impact model for epidemics based on sir modeling which uses the number of susceptible infectious and recovered individuals to compute the theoretical number of people infected over time and to predict outcomes in specific circumstances and plan for the quantity of hospital beds that may be needed while the chime project does not currently use ml techniques it could benefit from these techniques in order to incorporate more features and data points such as hospital capacity information to be used in applications such as dynamic ventilator allocation and surge capacity planning finally there are also efforts underway to use de-identified large-scale data to assess mobility changes and their impact on the local evolution of the epidemic for instance in italy and in north america as mentioned in previous sections machine learning approaches can be used to analyze and parse the vast quantity of written information on covid-19 and other coronaviruses in order to make it easier for researchers and clinicians to use this information key questions of interest include 134 which uses a graph-based model to search through abstracts and to find relevant information and 135   which extracts key terms and compares their usage with pre-covid-19 articles there are also several ongoing kaggle challenges involving this data with dozens of questions submitted daily and many teams involved other scientific research datasets that can be exploited include litcovid and the dimensions ai covid-19 dataset which can contain important supplementary information such as clinical trial data when available using any of the sources mentioned above nlp techniques can be applied to develop text mining tools and resources that can help the medical community find answers to key scientific questions regarding the nature and progress of covid-19 such as covid-19 tweetids covid19 tweets the covid-19 twitter dataset 136   which are maintained with general coronavirus-related tweets can be useful for tracking the propagation of misinformation and unverified rumors on twitter 137  as well as for monitoring the reactions of different populations to the virus a potentially complementary source of information for this task is the covid-19 real world worry dataset 138   which includes labeled texts of individuals emotional responses to covid-19 and therefore can contain important data regarding public sentiment and the impacts of the pandemic on mental well-being in different regions of the world there is also important information available from official sources and news outlets since the global media coverage of the pandemic is substantial and ongoing for instance the institutional news and media tweet dataset 139  brings together tweets based on a list of manually verified sources and can be used to track official and institutional messaging around the pandemic in different countries repositories such as the covid-19 coronavirus news article database and the covid-19 television coverage dataset can also be used to explore the question of how both print and television media outlets are covering the outbreak these are rich sources of data for researchers interested in analyzing how media coverage evolves as the virus spreads globally or tracking misleading reports and disinformation in the media at this time there are not many open-source datasets and models that can be used for diagnostic purposes some of the ct scan detection approaches described in the diagnosis section are available online and accessible to the public for instance those of wang et al and song et al however the data used to train the various models described is not systematically shared although such sharing would be of great value to the ml research community several initiatives exist to crowdsource and open-source relevant data for instance the covid chest x-ray dataset 140 for medical imagery and the covid-19 risk calculator for symptoms but these are challenging to assemble and maintain manually also while data collection and ml model training can be carried out by computer scientists data labeling vetting and annotation often need to be done by medical professionals such as radiologists or clinicians to address this lack of accessible data there is an increasing number of initiatives and repositories that aim to share data and models for instance the data4covid living data repository and the covid-19 dataset clearinghouse have links to dozens of opensource data repositories from different geographical areas and levels of granularity also initiatives such as united against covid-19 are particularly important since they have become online platforms where data scientists and ml researchers can apply their skills to address requests for help from the research community for instance by performing cleaning of clinical data extracting actionable information regarding covid-specific research questions and collaborating to develop tools for deployment on the ground in hospitals such initiatives are promising given their potential to bridge the gap between those with medical and biological knowledge or experience and those with computational and data skills in terms of genomic sequencing and drug discovery there are several datasets available from pre-existing initiatives or which have been created from scratch for covid-19 specifically on the one hand tracking the genome sequence of sars-cov-2 is crucial for designing and evaluating diagnostic tests tracing the pandemic and identifying the most promising intervention options notably the gisaid initiative founded over a decade ago for the specific purpose of promoting the international sharing of influenza virus sequences and related clinical and epidemiological data is tracking the genomic epidemiology of sars-cov-2 other projects such as nextstrain are looking at the genetic diversity of coronaviruses in order to characterize the geographic spread of covid-19 by inferring the lineage tree of hundreds of publicly shared genomes of sars-cov-2 on the other hand in terms of drug discovery there are well-established initiatives such as the rcsb protein data bank and the global health drug discovery institute that have created centralized portals with data and resources for better understanding covid-19 and for carrying out structure-guided drug discovery in addition cas a division of the american chemical society has recently released the open-source covid-19 antiviral candidate compounds dataset containing information regarding antiviral compounds and molecules that have similar chemical structures to existing antivirals to help the discovery of both new and repurposed treatments against the disease finally another potentially interesting crowdsourced resource is the citizen science game foldit which leverages collective intelligence against covid-19 by proposing to design an antiviral protein this research mapping exercise suggests that ml and ai can support the response against covid-19 in a broad set of domains in particular we have highlighted emerging applications in drug discovery and development diagnosis and clinical outcome prediction epidemiology and infodemiology however we note that very few of the reviewed systems have operational maturity at this stage in order to operationalize this research it is crucial to define a research road map and a funnel for ai applications to understand how this technology can immediately assist with the response how it might help later on in the evolution of the current pandemic and how it can be used to combat future pandemics in the face of overstretched health care networks we must strengthen our health systems to sustain services beyond the control and management of covid-19 in order to truly protect the vulnerable such as people living with noncommunicable diseases ncds as members of a global community of researchers and data scientists we identify three key calls for action first we believe that scalable approaches to data and model sharing using open repositories will drastically accelerate the development of new models and unlock data for the public interest global repositories with anonymized clinical data including medical imaging and patient histories can be of particular interest in order to generate and transfer knowledge between clinical institutions to facilitate the sharing of such data clinical protocols and data sharing architectures will need to be designed and data governance frameworks will need to be put in place it is important to reinforce that research with medical data is subject to strong regulatory requirements and privacyprotecting mechanisms in particular ai for clinical applications should demonstrate not only their performance on tests datasets  but also their effectiveness and safety when integrated into real clinical workflows overall any ai application developed should undergo an assessment to ensure that it complies with ethical principles and above all respects human rights second the multidisciplinary nature of the research required to deploy ai systems in this context calls for the creation of extremely diverse complementary teams and longterm partnerships beyond the examples shown in this review other promising domains in which ai could be used to fight against covid-19 include robotics eg cleaning or disinfecting robots and logistics eg the allocation and distribution of personal protective equipment funding opportunities which encourage such collaborations and define key research directions may help accelerate the success of such partnerships third we believe that open science and international cooperation can play an important role in this pandemic that knows no borders proven solutions can be shared globally and adapted to other contexts and situations prioritizing those that target local unmet needs in particular given that many international organizations private sector companies and ai partnerships operate across international borders they may be in the position to facilitate the knowledge dissemination and capacity building of national health systems regions with less capacity can benefit from global cooperation and concentrate their efforts on the most important local challenges ai systems methods and models can act as a compact form of knowledge sharing which can be used and adapted to other contexts if they are designed to be widely deployable requiring low energy and compute resources we acknowledge the difficulty of adding value through ai in the current situation nevertheless we hope that this review is a first step towards helping the ai community understand where it can be of value which are the promising domains for collaboration and how research agendas can be best directed towards action against this or the next pandemic  towards efficient covid-19 ct annotation a benchmark for lung and infection segmentation jun ma yixin wang xingle an cheng ge ziqi yu jianan chen qiongjie zhu guoqiang dong jian he zhiqiang he ziwei nie xiaoping yang  accurate segmentation of lung and infection in covid-19 ct scans plays an important role in the quantitative management of patients most of the existing studies are based on large and private annotated datasets that are impractical to obtain from a single institution especially when radiologists are busy fighting the coronavirus disease furthermore it is hard to compare current covid-19 ct segmentation methods as they are developed on different datasets trained in different settings and evaluated with different metrics in this paper we created a covid-19 3d ct dataset with 20 cases that contains 1800 annotated slices and made it publicly available to promote the development of annotation-efficient deep learning methods we built three benchmarks for lung and infection segmentation that contain current main research interests eg few-shot learning domain generalization and knowledge transfer for a fair comparison among different segmentation methods we also provide unified training validation and testing dataset splits and evaluation metrics and corresponding code in addition we provided more than 40 pre-trained baseline models for the benchmarks which not only serve as out-ofthe-box segmentation tools but also save computational time for researchers who are interested in covid-19 lung and infection segmentation to the best of our knowledge this work presents the largest public annotated covid-19 ct volume dataset the first segmentation benchmark and the most pre-trained models up to now we hope these resources httpsgiteecomjunma11 covid-19-ct-seg-benchmark could advance the development of deep learning methods for covid-19 ct segmentation with limited data  abstract-accurate segmentation of lung and infection in covid-19 ct scans plays an important role in the quantitative management of patients most of the existing studies are based on large and private annotated datasets that are impractical to obtain from a single institution especially when radiologists are busy fighting the coronavirus disease furthermore it is hard to compare current covid-19 ct segmentation methods as they are developed on different datasets trained in different settings and evaluated with different metrics in this paper we created a covid-19 3d ct dataset with 20 cases that contains 1800 annotated slices and made it publicly available to promote the development of annotation-efficient deep learning methods we built three benchmarks for lung and infection segmentation that contain current main research interests eg few-shot learning domain generalization and knowledge transfer for a fair comparison among different segmentation methods we also provide unified training validation and testing dataset splits and evaluation metrics and corresponding code in addition we provided more than 40 pre-trained baseline models for the benchmarks which not only serve as out-ofthe-box segmentation tools but also save computational time for researchers who are interested in covid-19 lung and infection segmentation to the best of our knowledge this work presents the largest public annotated covid-19 ct volume dataset the first segmentation benchmark and the most pre-trained models up to now we hope these resources httpsgiteecomjunma11 covid-19-ct-seg-benchmark could advance the development of deep learning methods for covid-19 ct segmentation with limited data 4  5  6  ct is shown to be more sensitive in the early diagnosis of coivd-19 infection compared to reverse transcription-polymerase chain reaction rt-pcr tests 7  wang et al trained a deep learning model on 325 covid-19 ct images and 740 typical pneumonia images their model identified 46 covid-19 cases that were previously missed by the rt-pcr test further quantitative information from ct images such as lung burden percentage of high opacity and lung severity score can be used to monitor disease progression and help us understand the course of covid-19 8  9  artificial intelligence ai methods especially deep learning-based methods have been widely applied in medical image analysis to combat covid-19 10  for example ai can be used for building a contactless imaging workflow to prevent transmission from patients to health care providers 11  in addition most screening and segmentation algorithms for covid-19 are developed with deep learning models 7  12  13  and most automatic diagnosis and infection quantification systems in covid-19 studies rely on segmentation generated by deep neural networks 12  14  15  16  although tremendous studies show that deep learning methods have potential for providing accurate and quantitative assessment of covid-19 infection in ct images 10  the solutions mainly rely on large and private datasets due to the patient privacy and intellectual property issues the datasets and solutions may not be publicly available however researchers may hope that the datasets entire source code and trained models could be provided by authors 17  existing studies demonstrate that the classical u-net or v-net can achieve promising segmentation performance if hundreds of well-labelled training cases are available 15    831 to 916 of segmentation performance in dice score coefficient was reported in various u-net-based approaches shan et al developed a neural network based on v-net and the bottle-neck structure to segment and quantify infection regions   18  in their human-in-the-loop strategy they achieved 851 910 and 916 dice with 36 114 and 249 labelled ct scans respectively huang et al 15 employed u-net 19 for lung and infection using an annotated dataset of 774 cases and demonstrated that trained model could be used for quantifying the disease burden and monitoring disease progression or treatment response in general we observed a trend that training deep learning models with more annotations will decrease the time needed for contouring the infection and increase segmentation accuracy which is consistent with the shan et als findings   however annotations for 3d ct volume data are expensive to acquire because it not only relies on professional diagnosis knowledge of radiologists but also takes much time and labor especially in current situation thus a critical question would be how can we annotate covid-19 ct scans efficiently towards this question three basic but important problems remain unsolved  there is no publicly well-labelled covid-19 ct 3d dataset data collection is the first and essential step to develop deep learning methods for covid-19 segmentation most of the existing studies rely on private and large dataset with hundreds of annotated ct scans  there are no public benchmarks to evaluate different deep learning-based solutions and different studies use various evaluation metrics similarly datasets for training validating and testing are split diversely which also makes it hard for readers to compare those methods for example although medseg httpmedicalsegmentationcomcovid19 dataset with 100 annotated slices were used in 20  21  22  23 to develop covid-19 segmentation methods it was split in different ways and the developed methods were evaluated by different metrics  there are no publicly available baseline models trained for covid-19 u-net a well-known segmentation architecture is commonly used as a baseline network in many studies however due to different implementations the reported performance variesin different studies 21  24  even though the experiments are conducted on the same dataset in this paper we focus on annotation-efficient deep learning solutions and aim to alleviate the above problems by providing a well-labelled covid-19 ct dataset and a benchmark in particular we first annotated left lung right lung and infections of 20 covid-19 3d ct scans and then established three tasks to benchmark different deep learning strategies with limited training cases finally we built baselines for each task based on u-net our tasks target on three popular research fields in medical imaging community  few-shot learning building high performance models from very few samples although most existing approaches focus on natural images it has received growing attention in medical imaging because generating labels for medical images is much more difficult 25  26   domain generalization learning from known domains and applying to an unknown target domain that has different data distribution 27  adversarial training is usually adopted to achieve it in recent studies 28  29   knowledge transfer reusing existing annotations to boost the trainingfine-tuning on a new dataset transferring knowledge from large public datasets is a well-established practice in medical imaging analysis 30  31  using generative adversarial network to transfer knowledge from publicly available annotated datasets to new datasets for segmentation also receives significant attention 32  33  our contributions can be summarized as following  20 well-labelled covid-19 ct volume data are publicly released to the community to the best of our knowledge this is the largest public covid-19 3d ct segmentation datasets  3 benchmark tasks are set up to promote studies on annotation-efficient deep learning segmentation for covid-19 ct scans specifically we focus on few-shot learning domain generalization and knowledge transfer  40 trained models are publicly available that can serve as baselines and out-of-the-box tools for covid-19 ct lung and infection segmentation annotations of covid-19 ct scans are scarce but several lung ct annotations with other diseases are publicly available thus one of the main goals for our benchmark is to explore these existing annotations to assist covid-19 ct segmentation this section introduces the public datasets used in our segmentation benchmarks figure 1 presents some examples from each dataset a existing lung ct segmentation datasets 1 structseg lung organ segmentation 50 lung cancer patient ct scans are accessible and all the cases are from one medical center this dataset served as a segmentation challenge 1 during miccai 2019 six organs are annotated including left lung right lung spinal cord esophagus heart and trachea in this paper we only use the left lung and right lung annotations 2 nsclc left and right lung segmentation this dataset consists of left and right thoracic volume segmentations delineated on 402 ct scans from the cancer imaging archive nsclc radiomics 34  35  36  b existing lung lesion ct segmentation datasets 1 msd lung tumor segmentation this dataset is comprised of patients with non-small cell lung cancer from stanford university palo alto ca usa publicly available through tcia the dataset served as a segmentation challenge 2 during miccai 2018 the tumor is annotated by an expert thoracic radiologist and 63 labelled ct scans are available 2 structseg gross target volume segmentation of lung cancer the same 50 lung cancer patient ct scans as the above structseg lung organ segmentation dataset are provided and gross target volume of tumors are annotated in each case 3 nsclc pleural effusion pe segmentation the ct scans in this dataset are the same as those in nsclc left and right lung segmentation dataset while pleural effusion is delineated for 78 cases 34  35  36   we collected 20 public covid-19 ct scans from the coronacases initiative and radiopaedia which can be freely downloaded 3 with cc by-nc-sa license the left lung right lung and infection were firstly delineated by junior annotators then refined by two radiologists with 5 years experience and finally all the annotations were verified and refined by a senior radiologist with more than 10 years experience in chest radiology on average it takes about 400  45 minutes to delineate one ct scan with 250 slices we have made all the annotations publicly available 37 at httpszenodoorgrecord3757476 with cc by-nc-sa license as mentioned in section 1 there is a need for innovative strategies that enable annotation-efficient methods for covid-ct segmentation thus we set up three tasks to evaluate potential annotation-efficient strategies in particular we focus on learning to segment left lung right lung and infection in covid-19 ct scans using table i  are held-out for validation all labelled covid-ct-seg ct scans are kept for testing table ii  task 3 is designed to address the problem of knowledge transfer with heterogeneous annotations where out-of-domain data are used to boost the annotation-efficient training on limited in-domain data specifically in both subtasks lung segmentation and lung infection segmentation 80 and 20 of out-of-domain data are used for training and validation while 20 and 80 of in-domain data are used for training and testing respectively table iii  motivated by the evaluation methods of the well-known medical image segmentation decathlon 4  we also employ two complementary metrics to evaluate the segmentation performance dice similarity coefficient a region-based measure is used to evaluate the region overlap normalized surface dice 38  a boundary-based measure is used to evaluate how close the segmentation and ground truth surfaces are to each other at a specified tolerance   for both two metrics higher scores admit better segmentation performance and 100 means perfect segmentation let g s denote ground truth and segmentation respectively we formulate the definitions of the two measures as follows 1 region-based measure 2 boundary-based measure s  r 3 denote the border region of ground truth and segmentation surface at tolerance   which are defined as b   we set tolerance  as 1mm and 3mm for lung segmentation and infection segmentation respectively the tolerance is computed by measuring the inter-rater segmentation variation between two different radiologists which is also in accordance with another independent study 38  the main benefit of introducing surface dice is that it ignores small boundary deviations because small inter-observer errors are also unavoidable and often not clinically relevant when segmenting the objects by radiologists u-net  19  39  has been proposed for 5 years and many variants have been proposed to improve it however recent study 40 demonstrates that a basic u-net is still difficult to surpass if the corresponding pipeline is designed adequately 4 httpmedicaldecathloncomfilesmsd-ranking-schemepdf in particular nnu-net no-new-u-net 40 was proposed to automatically adapt preprocessing strategies and network architectures ie the number of pooling convolutional kernel size and stride size to a given 3d medical dataset without manual tuning nnu-net achieved better performance than most specialised deep learning pipelines in 19 public international segmentation competitions and set a new state-of-theart in the majority of 49 tasks the source code is publicly available at httpsgithubcommic-dkfznnunet u-net is often used as a baseline model in existing covid-19 ct segmentation studies however reported results vary a lot even in the same dataset which make it hard to compare different studies to standardize the u-net performance we build our baselines on nnu-net which is the most powerful u-net implementation to the best of our knowledge to make it comparable between different tasks we manually adjust the patch size and network architectures in task 2 and task 3 to be the same as task 1 figure 2 shows details of the u-net architecture a results of task 1 learning with limited annotations table iv presents average dsc and nsd results of lung and infection of each subtask in task 1 it can be found that  the average dsc and nsd values among different folds vary greatly it demonstrates that reporting 5-fold cross validation results is necessary to obtain an object evaluation because 1-fold results may be biased  promising results for left and right lung segmentation in covid-19 ct scans can be achieved with as few as four training cases models trained for segmenting lung obtained significantly better results compared with those trained for segmenting lung and infection simultaneously  there is still large room for improving infection segmentation with limited annotations statistical tests are needed to justify whether there is an improvement from training the model for segmenting lung and infection at the same time this task is quite challenging as the model does not see any cases from target domain during training in other words the trained models are expected to generalize to the unseen domain covid-19 ct table v shows left lung and right lung segmentation results in terms of average and standard deviation values of dsc and nsd it can be found that  3d u-net achieves excellent performance in terms of dsc on the validation set that is from the same source domain average nsd values are lower than dsc values implying that most of the errors come from the boundary  the performance on the testing set drops significantly on both subtasks the performance of the model trained on nsclc lung dataset is worse than the model trained on stuctseg lung we suppose it is because the difference in the distribution lung appearance is smaller between structseg and covid-19-ct  on the validation set the performance of lesion segmentation is not as good as the performance of lung segmentation table v  which means that tumor segmentation remains a challenging problem this observation is in line with recent results in miccai tumor segmentation challenge ie liver tumor segmentation 41 and kidney tumor segmentation 42   the models almost fail to predict covid-19 infections on testing set which highlights that the lesion appearances differ significantly among lung cancer pleural effusion and covid-19 infections in ct scans one possible limitation would be the number of cases in our dataset is relatively small on one hand we focus on how to learn from a few training casestask 1 and how to efficiently use existing annotations thus the number of training cases is acceptable for our benchmark tasks on the other hand we have 16 or 20 testing cases in each task the number of testing cases is comparable with the setting of recent miccai 2020 segmentation challenges for example myops 2020 myocardial pathology segmentation combining multi-sequence cmr has 15 testing cases 43  structseg automatic structure segmentation for radiotherapy planning challenge 2020 has 10 testing cases 44 and asoca automated segmentation of coronary arteries has 20 testing cases 45  another possible limitation is that only quantitative results of 3d u-net are reported we will also provide 2d u-net baselines for all tasks in the near future in this paper we created a covid-19 ct dataset established three segmentation benchmark tasks towards annotation-efficient methods and provided 40 baselines models based on state-of-the-art segmentation architectures all the related results are publicly available at httpsgiteecom junma11covid-19-ct-seg-benchmark we hope this work can help community to accelerate designing and developing ai-based covid-19 ct analysis tools with limited data acknowledgment this project is supported by the national natural science foundation of china no 91630311 no 11971229 we would like to thank all the organizers of miccai 2018 medical segmentation decathlon miccai 2019 automatic structure segmentation for radiotherapy planning challenge the coronacases initiative and radiopaedia for the publicly available lung ct dataset we also thank joseph paul cohen for providing convenient download link of 20 covid-19 ct scans we also thank all the contributor of nsclc and covid-19-seg-ct dataset for providing the annotations of lung pleural effusion and covid-19 infection we also thank the organizers of tmi special issue on annotation-efficient deep learning for medical imaging because we get lots of insights from the call for papers when designing these segmentation tasks we also thank the contributors of the two great covid-19 related resources covid19 imaging ai paper list 5 and medseg 6 for their timely update about covid-19 publications and datasets last but not least we thank chen chen xin yang and yao zhang for their important and valuable feedback on this benchmark  subject areas pharmacodynamics sinead oaposdonovan m hunter eby nicholas henkel d justin creeden ali imami sophie asah xiaolu zhang xiaojun wu rawan alnafisah r taylor travis james reigle alexander thorman jarek meller robert mccullumsmith e robert educorresponding author mccullumsmithutoledo  the covid-19 pandemic caused by the novel sars-cov-2 is more contagious than other coronaviruses and has higher rates of mortality than influenza as no vaccine or drugs are currently approved to specifically treat covid-19 identification of effective therapeutics is crucial to treat the afflicted and limit disease spread we deployed a bioinformatics workflow to identify candidate drugs for the treatment of covid-19 using an omics repository the library of integrated network-based cellular signatures lincs we simultaneously probed transcriptomic signatures of putative covid-19 drugs and signatures of coronavirus-infected cell lines to identify therapeutics with concordant signatures and discordant signatures respectively our findings include three fda approved drugs that have established antiviral activity including protein kinase inhibitors providing a promising new category of candidates for covid-19 interventions  severe acute respiratory syndrome coronavirus 2 sars-cov-2 is responsible for the first global pandemic in a decade coronavirus disease 2019 covid-191 initial reports of a novel sars-like acute respiratory syndrome emerged in late 2019 from wuhan china2 since then covid-19 has spread to over 150 countries and all continents except antarctica34 at the time of writing over one million people have been infected over 60000 deaths have been attributed to this outbreak4 and millions of additional infections are projected to occur globally in upcoming months34 covid-19 is less infectious than sars-cov1 but more lethal than the common flu1 with an estimated mortality rate of 342 the incubation period on average is 52 days in severe cases the median time course from disease onset to death is 14 days5 while fever cough fatigue and myalgias6-10 are common mild presentations of covid-19 the disease can fatally evolve into a severe pneumonia complicated by acute respiratory distress syndrome hypoxemic respiratory failure and cytokine storm secondary to prolonged infection8 in addition to the significant medical burden imposed by this outbreak it is estimated that the global economic cost of covid-19 will be over 1 trillion in 202011 the emotional toll on individuals will be incalculable with prolonged quarantine policies restricting personal freedom and social contacts current treatment is supportive and is focused on managing disease complications and secondary symptoms12-14 drugs indicated for other infectious diseases such as antiviral and antiparasitic therapies have been used for covid-19 patients but there is a paucity of evidence supporting their efficacy15 there is now an immediate need to identify therapeutic compounds that can be rapidly repurposed for covid-19 treatment recent efforts to address this pressing public health concern include a comprehensive network-based approach to identify 16 candidate drugs and drug combinations that may be repurposed for the treatment of this disease16 to further expand this area of novel research in the present study we employ a bioinformatics approach with the goal of datamining an extensive drug signature resource to distill a list of drug therapies that may prove fruitful in the search for covid-19 therapies to accomplish this we apply a signature-based connectivity analysis17-19 utilizing the extensive chemical perturbagen omics datasets deposited in the library of integrated network-based signatures lincs database172021 lincs is a repository for systematically generated gene signatures based on the l1000 assay22 these gene signatures reflect cellular perturbations in response to pharmacological treatments lincs contains datasets for over 40000 small molecules drugs in various cell lines different small molecules that produce signatures composed of highly similar patterns of gene expression changes or concordant signatures reflect shared connections between small molecules here we apply a two-pronged approach to identify novel compounds for the treatment of covid-19 first we identify pharmacologic therapies that are effective in the treatment of pathogens in the coronavirus family like sars and middle east respiratory syndrome mers as well as other viral illnesses23-27 we then generate gene signatures for these targets in ilincs httpilincscom and highlight connected small molecule signatures to identify which of these candidate drugs are highly concordant with current therapies simultaneously we generate gene signatures from coronavirus infected human cell line transcriptomic datasets we analyze these data in ilincs to directly match disease signatures with discordant small molecule signatures thereby identifying drugs that reverse the disease signature finally we compile a list of drugs from these two approaches to identify high-yield candidate drugs that may have therapeutic utility in the treatment of covid-19 applying the workflow outlined in figure 1  we identified nine drugs with known efficacy in treating coronavirus family pathogens for which there are gene signatures in ilincs table 1 and extended   information in table s1  these drugs were clustered into five groupings according to their mechanism of action anatomical therapeutic chemical atc classification andor structural similarity table 1  simultaneously we extracted differential gene expression data on the 978 genes that comprise the l1000 from a publically available sars gse56192 transcriptomic dataset gene signatures composed of genes changed lfc  05 and  -05 were generated for the disease signature table   s2  in ilincs we conducted connectivity analysis to identify chemical perturbagens that are highly concordant to the drug target groupings  0321 or highly discordant to the disease signatures  -0321 established minimum ilincs concordance score cutoffs 2228 this resulted in identification of 112 chemical perturbagens common to two cell lines mcf7 and ha1e  figure 2  fourteen chemical perturbagens were identified at concordance scores  08 in both cell lines and were considered candidate drugs for the treatment of covid-19  table 2  the tanimoto scores for the candidate drugs and the original 9 drug targets were generated showing structural similarity between drugs currently in use for the treatment of coronavirus family pathogens and our newly identified candidate drugs  figure s1  unsupervised clustering of l1000 disease gene signatures demonstrates significant differences in patterns of gene expression induced by sars mers and influenza  figure s2  influenza is utilized as a control dataset as it represents a non-coronavirus pathogen that also causes respiratory illness as promising genistein has yet to be explored as an antiviral therapy in humans at9283 is a broad protein kinase inhibitor5556 canonically it acts as a receptor and nonreceptor tyrosine kinase inhibitor but also effectively inhibits serinethreonine kinases aurora ab kinases janus kinases jak 23 and abl kinases57 in a clinical setting at9283 has been predominantly studied as an antineoplastic for hematologic malignances and several trials are underway58-60 at9283 has not been explored directly as an antiviral although the abl kinase inhibitor imatinib is efficacious in preventing coronavirus sars-cov and mers-cov viral fusion with endosomes effectively halting viral activity61 given its role as a broad-spectrum kinase inhibitor researching the antiviral properties of at9283 may prove fruitful serine threonine kinase inhibitors alvocidib also known as flavopiridol is a pan-specific cyclindependent kinases cdk inhibitor that inhibits cdk1 cdk2 cdk4 cdk5 cdk6 cdk7 and cdk962 alvocidib is under clinical investigation as an antineoplastic for both solid tumors and hematologic malignancies63 like other cdk kinase inhibitors alvocidib has been implicated as a broad antiviral against several dna virus families64 alvocidib has been studied as an inhibitor of transcriptional activation and elongation in the infectious lifecycle of dna viruses hsv-1 hsv-2 and retroviruses hiv29-32 and also suppresses replication of influenza a33 this suggests that alvocidib is a strong candidate drug for repurposing the antiviral activity of the following serinethreonine kinase candidate drugs have yet to be studied directly and further research is required to determine their potential as repurposable antiviral therapies however these candidates are members of drug families with demonstrated antiviral or antimicrobial properties that could be exploited for the treatment of covid-19 gsk-1059615 is a reversible atp-competitive thiazolidinedione inhibitor of phosphoinositide 3-kinase pi3k and has been studied as an antineoplastic for solid tumors65 though the antiviral activity of gsk-1059615 has yet to be determined the thiazolidinedione drug family has a broad range of antibacterial and antiparasitic activity66-68 idelalisib is a phosphoinositol 3-kinase  idebenone is a synthetic derivative of ubiquinone also known as coenzyme q108283 this drug acts to increase the production of atp by enhancing oxidative phosphorylation as a general antioxidant idebenone may prevent lipid peroxidation reduce membrane oxidative stress and scavenge free radicals84 idebenone has been used for a number of human neurodegenerative disorders and its safety has been validated8586 antioxidants such as idebenone have been hypothesized to mitigate the deleterious effects of a reactive-oxygen species burst45 from viruses with a pulmonary and respiratory predilections influenza and sars44 however further work is required to determine the utility of antioxidants like idebenone as adjunct treatments for covid-19 penicillin v is a beta-lactam antibiotic and is indicated primarily for treating gram-  the covid-19 outbreak is an escalating public health concern that requires a swift and baricitinib may reduce the ability of the virus to enter cells and is currently in clinical use as a treatment for rheumatoid arthritis to advance therapeutic discovery and the identification of candidate drugs for covid-19 we employ an alternative signature-based bioinformatic approach in this study we data mine the extensive lincs database which acts as a repository of l1000 gene signatures generated by treating various cell lines with over 20000 small molecules the l1000 genes are a reduced representation of the transcriptome a method by which a select group of genes account for 82 of the information content of the transcriptome95 the approach involved feature selectionreduction techniques applied to 12063 gene expression samples profiled on microarrays from geo96 benchmarking of the l1000 assay versus rnaseq yielded a cross-platform correlation of 08495 suggesting the l1000 assay represents an efficient alternative to rna-seq utilizing this resource our two-pronged connectivity analysis approach identified candidate drugs that are 1 highly concordant to current drugs employed to treat coronavirus family pathogens and 2 highly discordant to sars disease gene signatures interestingly the antibiotic quinacrine was also identified in a recent network-based covid-19 drug screen16 and another -lactam antibody ceftriaxone is in clinical trial for adjunct treatment of covid-19 nct02735707 the immunosuppressant sirolimus was identified in our study albeit at a less stringent threshold as well as in another in silico drug screen16 as a candidate repurposable drug for treating covid19 immunosuppressants may address the symptoms resulting from overactivation of the immune system cytokine storm in response to covid-19 infection114 and this class of drug is also in clinical trial as adjunct treatments eg thalidomide nct04273529 the antimicrobial drugs that comprise our drug target groupings are limited to those that have gene signatures in ilincs we analyze gene signatures generated in two cell lines mcf7 and ha1e as data was available for all of our drug targets in these cell lines only  the workflow for this study is outlined in figure 1  we conducted a pubmed search using search terms coronavirus or covid-19 and antiviral or drug or therapy and generated a list of compounds utilized to treat coronavirus family pathogens or identified as putative covid-19 therapeutics we identified seventeen drugs for potential analysis table s1  l1000 gene signature datasets were available for nine of the seventeen drugs table 1 next we grouped the nine drug targets based on canonical mechanism of action the anatomical therapeutic chemical atc classification and structural similarity drugs were grouped together if they were categorized by at least two of the three methods the database drugbank httpswwwdrugbankca was used to group the drugs by their canonical mechanisms of actions drug identification was only referenced from drug bank id if no drug bank id was available this is indicated in table 1 and table s1  if there was no listed moa from drug bank then the moa was appropriately cited referenced from ilincs or was referenced from gene ontology go molecular function 2018 accessed via enrichr httpamppharmmssmeduenrichrenrich next drugs were classified based on the atc classification system httpswwwwhoccnoatcdddindex if a particular drug did not have an atc classification it was marked as unclassified from drugbank we also collected the clinical indications gene targets and trade names in addition we probed the atc index httpswwwwhoccnoatcdddindex to identify the first-and second-level of drug classifications the first-level classification was used to confirm drug grouping finally to group drugs based on structural similarity the structural data files sdf for the nine drugs under investigation were downloaded from drugbank the package chemminer was used to generate 1024-bit binary fingerprints for each compound the tanimoto coefficient between all pairs of fingerprints were then computed also using the r package chemminer the tanimoto coefficient also known as the jaccard similarity represents the most popular measure for chemical similarity115 and is the ratio of the intersection of the two fingerprints divided by the union of the two fingerprints the data were visualized using the corrplot package with a final list of drug clusters the individual drug signatures within each grouping were collected and averaged across the l1000 using the ilincs portal we acquired the lincs chemical perturbagen signatures 978 genes that comprise the l1000 for each drug candidate genes with a log fold change lfc value of  085 or  -085 indicating differential gene expression induced by the drug target compared to a corresponding control cell line were exported to microsoft excel gene lists were pooled and averaged such that a master list of differentially expressed genes was generated for each drug candidate family for example genes with a lfc  085 or  -085 that appeared in both the hydroxychloroquine gene signature and the chloroquine gene signature were averaged to calculate mean values for each differentially expressed gene in drug target grouping 1 next the upregulated genes lfc  085 were clustered and the downregulated genes  -085 were clustered these clusters were uploaded as user generated signatures into ilincs next we identified connected chemical perturbagens utilizing a concordance threshold score of  0321 an established minimum concordance score cutoff 2228 to identify chemical perturbagen signatures that are considered highly correlated with our drug target grouping signatures selected for these datasets is 24h we conducted differential gene expression analysis of these datasets comparing virus infected samples to corresponding control samples rnaseq raw count data were analyzed using edger r package v3281 for differential gene expression118 as a quality control step we require that a gene have a count of at least 10 in at least some libraries before it is considered to be expressed normalization was performed using the default method trimmed mean of m-values tmm this step is performed by using the calcnormfactor function which returns the dgelist argument with a set of calculated normalization factors one for each sample to eliminate composition biases between libraries orghsegdb r package v3100 was used to complement the gene annotation following analysis of the disease transcriptomic datasets the subset of genes that comprise the lincs l1000 were extracted from the differentially expressed gene list for each dataset when the l1000 genes were extracted from the sars and mers rnaseq datasets 944 and 947 overlapping genes respectively were extracted we used the existing gene raw counts from geo and did not remap the genes thus if some of the l1000 genes were not mapped we were unable to access their expression in addition we applied a gene raw count cutoff of 10 as a quality control step which also reduced the number of l1000 genes found in the final differential expression gene lists the extracted l1000 genes were uploaded into ilincs genes with lfc in expression within three thresholds 026 lfc 05 lfc and all l1000 genes were identified with a custom r script for further processing as described above upregulated and down regulated disease gene signatures were generated for each disease dataset within each threshold and uploaded into ilincs to identify connected perturbagens for disease gene signatures chemical perturbagen signatures that are highly discordant discordance score  -0321 indicating these perturbagens may reverse the disease signature were identified genes at lfc  05 and  -05 threshold were then carried forward for further analysis utilizing this gene threshold generated optimal sars disease signatures to identify a large number of discordant chemical perturbagens in addition we identified a microarray dataset of human airway epithelium cells infected with influenza a of the h1n1 serotype gse47963 36h time point the microarray data were extracted using geoquery r package v2541 119 specifically soft format files from geo containing all of the information in the geo records were parsed the normalized gene expression values associated with soft files for selected samples filewere then analyzed using limma r package v3422 linear model for microarray120 as described above the l1000 genes were extracted since this dataset was performed on the microarray platform which limits the gene detection to the probes available in the arrays we extracted 968 l1000 genes in addition to the mers dataset which represents a pathogenic coronavirus like sars the influenza dataset was used as a comparison dataset in unsupervised clustering and biological pathway analyses described below influenza a represents a non-corona family virus that also causes respiratory disease121 a total of 906 overlapping l1000 genes were used in clustering analysis and heat map generation candidate drugs were identified from the chemical perturbagen connectivity analysis using a custom script in r the script downloaded the data from the ilincs api and used the following criteria chemical perturbagens had a concordance score  0321 compared to drug target grouping signatures or a discordance score  -0321 compared to disease signatures in cell lines mcf7 in total 112 chemical perturbagens were common to both cell lines chemical perturbagens with concordance scores  08 in the mcf7 and ha1e cell lines are considered candidate drugs resulting in a final list of 14 candidate drugs heat maps of the l1000 gene data for each of the drug target groupings candidate drugs and disease sars and mers datasets were generated as described above in addition the l1000 gene data for furosemide fur a drug which is not used to treat viral pathogens or related illnesses and influenza a non-coronavirus family pathogen associated with respiratory illness were included in the heat maps these datasets acts as controls suggesting that the drug target groupings and candidate drugs but not unrelated drugs are discordant with coronavirus disease signatures specifically the sdf for the 9 drugs that comprise the drug target groupings were downloaded from drugbank the sdf files for the 14 candidate drugs were downloaded from pubchem and chembl chemminer was used to convert the sdf files to binary chemical fingerprints the tanimoto coefficients of the chemical fingerprints between the 9 drugs that comprise the drug target groupings and the 14 candidate drugs for all pairs were computed using the chemminer library the tanimoto coefficient also known as the jaccard similarity represents the established measure for chemical similarity the data were then visualized as a heat map using the gplots package r scripts utilized in data processing can be accessed at httpsgithubcomalisajidcovid19 unsupervised clustering analysis was performed on log fold change values of disease and drug target signatures and heat map was generated using pheatmap package122 in r programming language to generate biological pathways associated with the drug targets and disease datasets the gene list  this is a list of supplementary files associated with this preprint click to download supplementalmaterialpdf  geocov19 a dataset of hundreds of millions of multilingual covid-19 tweets with location information umair qazi muhammad imran ferda ofli  the past several years have witnessed a huge surge in the use of social media platforms during mass convergence events such as health emergencies natural or human-induced disasters these non-traditional data sources are becoming vital for disease forecasts and surveillance when preparing for epidemic and pandemic outbreaks in this paper we present geocov19 a large-scale twitter dataset containing more than 524 million multilingual tweets posted over a period of 90 days since february 1 2020 moreover we employ a gazetteer-based approach to infer the geolocation of tweets we postulate that this largescale multilingual geolocated social media data can empower the research communities to evaluate how societies are collectively coping with this unprecedented global crisis as well as to develop computational methods to address challenges such as identifying fake news understanding communities knowledge gaps building disease forecast and surveillance models among others  social media platforms such as twitter receive an overwhelming amount of messages during emergency events including disease outbreaks natural and human-induced disasters in particular the information shared on twitter during disease outbreaks is a goldmine for the field of epidemiology research studies show that twitter provides timely access to health-related data about chronic disease outbreaks and epidemics 4  in this paper we present geocov19 a large-scale twitter dataset about the covid-19 pandemic coronavirus disease 2019 or covid-19 is an infectious disease that was first identified in december 2019 and has spread globally since then the world health organization who declared the covid-19 outbreak a pandemic on march 11 2020 as of may 6 2020 more than 263k fatalities have been recorded and more than 38 million people have been infected globally twitter has seen a massive surge in the daily traffic amid covid-19 people try to connect with their family and friends look for the latest information about the pandemic report their symptoms and ask questions moreover many conspiracies rumors and misinformation began to surface on social media eg drinking bleach can cure it bill gates is behind it etc furthermore both benefits and unanticipated consequences of lockdowns and closure of businesses around the globe and social distancing are among the top topics on social media in this dataset collection we aimed at covering several different perspectives related to the covid-19 pandemic ranging from social distancing to food scarcity and symptoms to treatments and shortage of supplies and masks the dataset contains more than 524 million multilingual tweets collected over a period of 90 days starting from february 1 2020 till may 1 2020 using hundreds of multilingual hashtags and keywords various public health and disease surveillance applications that use social media rely on broad coverage of location information however only a limited number of tweets contain geolocation information ie 1-3 to increase the geographical coverage of the dataset we employ a gazetteer-based approach that leverages various metadata information from twitter messages to infer their geolocation the dataset contains around 378k geotagged tweets with gps coordinates and 54 million tweets with place information however with the help of the proposed geolocation inference approach we extracted additional geolocation information for 297 million tweets using the location information from the user profile and for 452 million tweets using toponyms in tweet content consequently our dataset contains around 491 million tweets with at least one type of geolocation information which constitutes 94 of the entire dataset overall there are 43 million unique users in the dataset which includes around 209k users who have verified twitter accounts in terms of its multilingualism the dataset covers 62 international languages among top languages there are approximately 348 million english 65 spanish 93 million italian and 55 million arabic tweets the rest of the paper is organized as follows in the next section we describe covid-19 related datasets specifically from twitter in section 3 we provide the details of data collection section 4 explains the gazetteer approach followed for geolocation inference section 5 presents various statistics about the dataset followed by a section on research implications finally we conclude the paper in section 7 the number of preliminary studies on analyzing social media data related to the covid-19 pandemic has been increasing almost on a daily basis some researchers analyze human behavior and reactions to the spread of covid-19 in the online world 13 1 14 16  or investigate conspiracy theories and social activism 17 6  many others collect and share large-scale datasets publicly to enable further research on the topic some datasets are restricted to a single language such as english 11 12 or arabic 2 8 while some others contain multiple languages 5 3 10  the data collection periods also vary between these datasets among all 3 stands out as one of the long-running collections with the largest amount of tweets ie 250 million also thanks to its multilingual nature however these datasets mostly contain only the raw content obtained from twitter except that 11 12 associate a sentiment score with each tweet while 8 provides tweet propagation networks on the other hand our dataset enriches the raw tweet content with additional geolocation information which is otherwise almost non-existent furthermore with more than 524 million tweets our dataset is more than twice as large as the largest dataset available to date  twitter offers different apis for data collection we use the twitter streaming api through which one can collect data using hashtags keywords or geographical bounding boxes we used the aidr system 9 1 for the data collection through hundreds of different multilingual hashtags and keywords related to the covid-19 pandemic the data collection started on february 1 2020 using trending hashtags such as covid19 coronavirus covid 19 and new hashtags were added as they emerged in the later days as of may 1 the total number of hashtagskeywords is 803 although twitter offers filtered streams for specific languages we do not filter by any language thus our data is multilingual as of may 1 2020 we have collected 524353432 tweets in total our data collection still continues however all the statistics and analyses presented in this paper are based on tweets collected till may 1 2020 figure 1 depicts the daily ingested volume of tweets during the 90 days of data collection february 1 to may 1 2020 the data does not show any gaps however the daily volume in the first two weeks is lower than the later weeks on average 1 million tweets were ingested daily during the first two weeks whereas the average daily volume substantially increased during the later weeks ie around 8 million tweets per day the maximum number of tweets recorded in one day is 105 million on april 8 we make the dataset publicly available for research and non-profit uses adhering to twitter data redistribution policies we cannot share full tweets content therefore we only share tweet ids and user ids along with high-level geolocation information for each tweet moreover we develop and provide a tool to rehydrate full tweet content using the shared tweet ids weekly data files and all the hashtags and keywords used for the collection are available at httpscrisisnlpqcriorgcovid19 the problem of geolocation inference on twitter data can be divided into two sub-problems i user geolocation ie determining the location of a user and ii tweet geolocation ie determining the location of a tweet the user geolocation inference problem can be further categorized as a user home location inference and b user current location inference similarly the tweet geolocation inference problem can be further divided as a determining the origin of a tweet ie the location where the tweet is posted from and b extracting and disambiguating the mentioned locations in a tweet despite these subtle distinctions many research studies map all of these problems into one which is the tweet geolocation inference although the dataset described in this paper can be used to address each of these sub-problems hereinafter we refer all of them as the tweet geolocation inference problem although twitter provides an option to enable the exact geolocation service on mobile devices only 1 of the tweets contain gps coordinates ie latitude and longitude an alternative option to infer geolocation of a tweet is to use the place field-an optional field that the user fills by selecting one of the twitter-suggested locations this place tag associates a tweet with a location referring to a city an area or a famous point-of-interest such as a building or a restaurant twitter provides the place field data in a json dictionary that contains several types of information including city or country name and a bounding box representing the specified location when present the place field indicates that the tweet is associated with a place but may not be originating necessarily from that place another widely used meta-information for geolocation inference is user location which is provided by the user in their profile this is an optional free-form text field where values such as united states usa us nyc we use four types of information from a tweet i geo-coordinates latitude longitude ii place field json dictionary iii user location field text and iv tweet content text to infer locations from textual content we tried several publicly available named-entity recognition systems including nltk 2  a recent version of stanford ner 7  and a system by 15  however due to either unsatisfactory results eg most of them need proper capitalization to detect location names or not enough computational speed to deal with our large dataset we were unable to employ these tools in our study instead we have adopted a gazetteer-based approach and used nominatim which is a search engine for openstreetmap osm data 3 specifically we used the nominatim tool to perform two operations i geocoding-the process of obtaining geo-coordinates from a location name and ii reverse geocoding-the process of obtaining a location name from geo-coordinates the official online nominatim service does not permit more than 60 callsminute and hence is not suitable for our purpose to make millions of calls in a reasonable time period therefore we set up six local nominatim servers on our infrastructure and tuned each server to handle 4000 callssecond next we describe in detail how we determine the geolocation of a tweet  geo-coordinates although geo-coordinates can be directly used for mapping we perform reverse geocoding to obtain additional location details eg street city or country the coordinates belong to for this purpose we use nominatims reverse api endpoint to extract city county state and country information from the nominatim response if available  place the place field data contains several location attributes we use the full name attribute which represents the name of a location in textual form and query nominatims search api endpoint to extract city county state and country names from the nominatim response if available  user location the user location field is a free-form text entered by the user and hence is often noisy as the users can type anything eg california usa earth mars and home we follow the procedure explained below for toponym extraction from text using this field the extracted toponyms are then used to obtain geolocation information including country state county and city from nominatim  tweet content tweet content represents the actual tweet in text format to extract location mentions inside tweet text we follow the toponym extraction from text procedure described below the extracted toponyms are then used to obtain geolocation information at various levels such as country state county city and gps-coordinates if available toponym extraction from text given a piece of text either from the user location field or the actual tweet content we follow the below steps to extract geolocation information from it 1 preprocessing replace all urls rt user mentions starting with  sign numbers and other special characters with a noise token all remaining words are lowercased 2 candidate generation all remaining single words ie uni-grams and their bi-grams are considered toponym candidates after dropping bi-grams that contain the noise token the bi-grams ensure that locations with more than one term such as new york are not missed 3 non-toponyms pruning all candidates uni-grams and bi-grams are checked i whether they are stopwords and ii whether they appear in an index consisting of 3174209 location names taken from kaggle 4 this o1 operation removes all stop-words as well as those that are not found in the location index removing stop-words after generating bi-grams allows us to account for toponyms such as new york the remaining candidates are used to query nominatim specifically we make a nominatim search call for each candidate for valid toponym queries nominatim returns a non-empty response in case there are multiple toponyms with non-empty nominatim response we resolve them to one location using majority voting based on country information here we evaluate the accuracy of the toponym extraction approach described above the two fields from which we extract toponyms include user location and tweet content as described earlier the extracted toponyms are used to obtain geolocation information at different granularity levels such as country state county and city for this purpose we take a random sample of 5000 tweets with exact gps-coordinates and for which the geolocation information ie country state county and city is derived from the user location and tweet content fields the evaluation aims to determine if the derived geolocation information at different granularity levels ie country state county and city matches with the geolocation information of the tweets with gps-coordinates table 1 shows evaluation results in terms of accuracy for four geolocation granularity levels we see that the lower the granularity ie higher administrative level the better the accuracy scores at the country level for both user location and tweet content fields the accuracy scores are plausible at 086 for user location and 075 for tweet content at the state level the user location field seems to yield better results than the tweet content however for both county and city levels the accuracy scores are low this section describes various details of the dataset such as geographical coverage users and multilingualism geolocated tweets and their geographical coverage table 2 shows the details of tweets with different location fields and their quantities in the dataset the dataset contains 378772 geotagged tweets with latitude longitude for which the geolocation information is received from users devices gps and it is the most trusted and accurate geolocation information around 54 million tweets are tagged with place information which is the second most reliable geolocation information figure 2a shows the combined geographical distribution of these geotagged tweets on a world map the toponym extraction approach described in section 4 was employed to identify geolocation information from the user location and tweet content fields using this approach we were able to find geolocation information for around 297 million tweets using the user location field and around 452 million tweets using the tweet content field figure 2b shows the geographical distribution of the users for which the geolocation information is derived from the user location field figure 2c shows the geographical distribution of the most mentioned locations extracted from the tweet content the dataset covers 218 countries and 47328 unique cities worldwide and several types of amenities such as hospitals parks and schools the countries also include islands such as the british virgin islands and the solomon islands table 3 shows countries with tweet volume breakdown there are 9 countries with more than 10 million tweets 40 with more than 1 million tweets and 28 with more than 500k tweets table 4 shows a similar breakdown for cities figure 3  the four countries which are severely hit by the covid-19 pandemic include the united states italy france and spain to determine the country-level geographic coverage of tweets in figure 4  we show the geographical distribution of tweets with geo-coordinates tweets for which the location information is extracted from the user location and most-mentioned locations which are extracted from the tweet content table 5 shows various details of the users in the dataset the dataset contains 43463225 unique users of all 136448 users tweeted at least one geotagged tweet and 1355032 users tagged at least one tweet with the place information around 22792120 users have valid user location value table 6 shows the details of verified users in the dataset in total the dataset contains 209372 verified users and among them 1853 users posted at least one geotagged tweet and 21216 users with place information and 165495 with valid user location value language-related statistics in total the dataset covers 62 international languages figure 5 a shows all the languages and the corresponding number of tweets on a log scale the english language dominates with 348 million tweets and the second and third largest languages are spanish and french respectively there are around 12 million tweets for which the language is undetermined figure 5 b and 5c show the daily distribution of top-40 languages datasets are fundamental building blocks for both core and applied research the recent widespread adoption of social media platforms especially during emergency events provides timely access to citizen-generated data which can be useful for several tasks including event forecasting surveillance and response the geocov19 dataset presented in this paper is the largest covid-19 related twitter dataset available to date with broad coverage of geographical regions as well as languages which we believe can foster multidimensional research in many application areas including the ones listed below  understanding knowledge gaps emergency events such as covid-19 pandemic bring uncertainty which raises questions that the general public asks through social media identification of such questions can help authorities to understand knowledge gaps among masses and address them as quickly as possible one can look at country-specific tweets in geocov19 dataset to identify knowledge gaps  disease forecast and surveillance early detection of disease outbreaks can prevent further spread and loss of lives automatic identification of messages where people report signs and symptoms of a disease can be used to identify hot spots where authorities can prioritize besides the textual content images and videos associated with the tweets in our dataset can also be used to assess the publics adherence to policies such as social distancing and mask usage  identifying urgent needs of individuals at the onset of emergency situations like covid-19 when governments have no choice other than closing businesses and imposing lockdowns low-income communities especially in developing countries suffer the lack of financial resources could even affect their daily meal intake leaving them without sufficient nutrition such communities may have several other types of urgent needs which can be identified through social media data this dataset can be explored and further analyzed to develop automatic approaches to identify such unanticipated needs of the general public such automatic methods can then be employed during a future event  tracking misinformation and fake news during critical events rapid identification of misinformation fake news and false rumors is of utmost importance for authorities to turn them down as quickly as possible identification of such content on social media is even more important due to the fact that such information spreads quickly on social media platforms compared to other new media channels there were several times when fake information related to covid-19 treatment associated with who appeared on social media the dataset covers many such fake news stories which can be helpful in developing misinformation and fake news detection systems in addition to the textual content analyzing the visual content associated with tweets ie images videos animated gifs can provide further insights about the flow of misinformation eg memes and help assess publics reaction to such cases  understanding public reactions and sentiment social media platforms are a great source to learn about issues and difficulties that the general public is facing and their reactions to government response during emergency events covid-19 poses several unanticipated challenges for the general public businesses as well as authorities which can be learned from this dataset especially joint analysis of textual and visual content in our dataset can afford the ability to develop models for monitoring mental health consequences of the pandemic on the public  covid-19 image data collection prospective predictions are the future joseph cohen paul paul morrison lan dao karsten roth vector mila  tim duong q marzyeh ghassemi  across the worlds coronavirus disease 2019 hot spots the need to streamline patient diagnosis and management has become more pressing than ever as one of the main imaging tools chest x-rays cxrs are common fast noninvasive relatively cheap and potentially bedside to monitor the progression of the disease this paper describes the first public covid-19 image data collection as well as a preliminary exploration of possible use cases for the data this dataset currently contains hundreds of frontal view x-rays and is the largest public resource for covid-19 image and prognostic data making it a necessary resource to develop and evaluate tools to aid in the treatment of covid-19 it was manually aggregated from publication figures as well as various web based repositories into a machine learning ml friendly format with accompanying dataloader code we collected frontal and lateral view imagery and metadata such as the time since first symptoms intensive care unit icu status survival status intubation status or hospital location we present multiple possible use cases for the data such as predicting the need for the icu predicting patient survival and understanding a patients trajectory during treatment data can be accessed here https githubcomieee8023covid-chestxray-dataset preprint under review  in the times of the rapidly emerging coronavirus disease 2019  hot spots and growing concerns about a second wave are making it crucial to streamline patient diagnosis and management many experts in the medical community believe that artificial intelligence ai systems could lessen the burden on hospitals dealing with outbreaks by processing imaging data kim 2020 rubin et al 2020  hospitals have deployed ai-driven computed tomography ct scan interpreters in china simonite 2020 and italy lyman 2020  as well as ai initiatives to improve triaging of covid-19 patients ie discharge general admission or icu care and allocation of hospital resources strickland 2020 hao 2020  data is the first step to developing any diagnostic or management tool while there exist large public datasets of more typical chest x-rays cxr wang et al 2017 bustos et al 2019 irvin et al 2019 johnson et al 2019 demner-fushman et al 2016  there was no public collection of covid-19 cxr or ct scans designed to be used for computational analysis at the time of creating our dataset we first made data public in mid february 2020 and the dataset has rapidly grown cohen et al 2020c  more recently in june the bimcv covid-19 dataset de la iglesia vay et al 2020 was released while it has more samples than the dataset we present we complement their work with a focus on prospective metadata from multiple medical centers and countries many physicians remain reluctant to share their patients anonymized imaging data in open datasets even after obtaining consent due to ethical concerns over privacy and confidentiality and a hospital culture that in our experience does not reward sharing keen et al 2013 lee  yoon 2017 kostkova et al 2016 floca 2014  in order to access data in one hospital researchers must submit a protocol to the hospitals institutional review board irb for approval and build their own data management system while this is important for patient safety such routines must be repeated for every hospital resulting in a lengthy bureaucratic process that hurts reproducibility and external validation the ultimate goal of this project is to aggregate all publicly available radiographs including papers and other remixable datasets for research articles images are extracted by hand while for websites such as radiopaedia and eurorad data collection is partially automated using scrapers that extract a subset of the metadata while we hand review case notes to determine clinical events this work provides three primary contributions  we create the first public covid-19 cxr image data collection totalling 542 frontal chest x-ray images from 262 people from 26 countries the dataset contains clinical attributes about survival icu stay intubation events blood tests location as well as freeform clinical notes for each imagecase in contrast to other works we focus on prospective metadata for the development of prognostic and management tools from cxr images collected have already been made public and are presented in an ml-ready dataset with toolchains that are easily used in many testable settings  we establish a larger more robust set of potential tasks for evaluation such as predicting pneumonia severity survival outcome and need for the intensive care unit icu with benchmark results  we also discuss how to use the location information in this dataset for a leave-one-countrycontinent-out loco evaluation to simulate domain shift and provide a more robust evaluation currently all images and data are released under the following github url httpsgithub comieee8023covid-chestxray-dataset we hope that this dataset and tasks will allow for quick uptake of covid-related prediction problems in the machine learning community this project is approved by the university of montreals ethics committee cerses-20-058-d 2 background and related work in recent years ml applications on cxr data have seen rising interest such as lung segmentation islam  zhang 2018  tuberculosis and cancer analysis stirenko et al 2018 lakhani  sundaram 2017  abnormality detection islam et al 2017  explanation singla et al 2020  and multi-modality predictions rubin et al 2018  hashir et al 2020  with the availability of large-scale public cxr datasets created with ml in mind eg chexpert irvin et al 2019  chest-xray8 wang et al 2017  padchest bustos et al 2019 or mimic-cxr johnson et al 2019  neural networks have even been able to achieve performance near radiologist levels rajpurkar et al 2018 rajpurkar et al  2017 irvin et al 2019 putha et al 2018a majkowska et al 2019 putha et al 2018b  ever since the dawn of the outbreak imaging has stood out as a promising avenue of research and care zu et al 2020 poggiali et al 2020  particularly in the beginning of the outbreak computed tomography ct scans captured the attention of both the medical ng et al 2020 kanne et al 2020 and the ml mccall 2020 communities from march to may 2020 the fleischner society rubin et al 2020  american college of radiology acr acr 2020  canadian association of radiologists car dennie et al 2020a  canadian society of thoracic radiology cstr dennie et al 2020b  and british society of thoracic imaging bsti nair et al 2020 released the following recommendations however cxrs remain the first choice in terms of the initial imaging test when caring for patients with suspected covid-19 cxrs are the preferred initial imaging modality when pneumonia is suspected acr 2018 and the radiation dose of cxr 002 msv for a pa film is lower than the radiation dose of chest ct scans 7 msv putting the patients less at risk of radiation-related diseases such as cancer fda 2017  in addition cxr are cheaper than ct scans making them more viable financially for healthcare systems and patients beek 2015 ball et al 2017  finally portable cxr units can be wheeled into icu as well as emergency rooms er and are easily cleaned afterwards reducing impact on patient flow and risks of infection dennie et al 2020a acr 2020  while other works have attempted to predict covid-19 through medical imaging the results have been on small or private data for example while promising results were achieved by raj 2020 90 auc these were reported over a large private dataset and are not reproducible additionally much research is presented without appropriate evaluations potentially leading to overfitting and performance overestimation maguolo  nanni 2020 tartaglione et al 2020  up until now many prediction models are not viable for use in clinical practice as they are inadequately reported particularly with regard to their performance and at strong risk of bias wynants et al 2020  the current statistics as of june 6th 2020 are shown in table 1  which presents the distribution of frontal cxr by diagnosis types of pneumonia and responsible micro-organisms when applicable for each image attributes shown in appendix  as mentioned earlier data was largely compiled from public databases on websites such as radiopaediaorg the italian society of medical and interventional radiology 1  figure1com 2  and the hannover medical school winther et al 2020b  both manually and using scrapers a full list of publications is included in appendix d1 images were extracted from online publications websites or directly from the pdf using the tool pdfimages 3  throughout data collection we aimed to maintain the quality of the images many articles were found using the list of literature provided by peng et al 2020  a hidden challenge in extracting metadata is the alignment with images to belong in the same row as an image a clinical measurement must have been taken on the same day it can be difficult to automatically determine when the measurement was taken if the metadata appears outside of image captions for details on scraper design see appendix c using this dataset as a benchmark is very challenging because by the nature of its construction it is very biased and unbalanced this can lead to many negative outcomes if treated as a typical benchmark dataset maguolo  nanni 2020 cohen et al 2020b seyyed-kalantari et al 2020 kelly et al 2019  unless otherwise specified only ap and pa views are used to avoid confounding image artifacts of the ap supine view in order to deal with issues of bias we perform a leave one countrycontinent out loco evaluation this approach is motivated from training bias common in small unbalanced datasets for our evaluation the test set will be composed of data from a single continent ideally we would separate by countries but this is not possible given the current distribution of data we also note that not every sample is labelled so models may be trained and evaluated on data from the same continent but we ensured that samples in training and evaluation did not originate from the same research groupimage source this approach should give us a distributional shift that will allow us to correctly evaluate the model for each task a subset of the samples will have enough representation to be included continents which do not have at least one representative for each class are filtered out and not used in place of images features are extracted using a pre-trained densenet model from the torchxrayvison library cohen et al 2020d which is trained on 7 large cxr datasets features will be used in the following constructions  intermediate features -the result of the convolutional layers and global averaging 1024 dim vector  18 outputs -each image was represented by the 18 outputs pre-sigmoid here atelectasis consolidation infiltration pneumothorax edema emphysema fibrosis effusion pneumonia pleural thickening cardiomegaly nodule mass hernia lung lesion fracture lung opacity enlarged cardiomediastinum  4 outputs -a hand picked subset of the above mentioned outputs pre-sigmoid were used containing radiological findings more frequent in pneumonia lung opacity pneumonia infiltration and consolidation  lung opacity output -the single output pre-sigmoid for lung opacity was used because it was very related to this task this feature was different from the opacity score that we would like to predict  image pixels -the image itself as a vector of pixels 22422450176 in order to avoid overfitting these features are used in a linear or logistic regression with default parameters from sci-kit learn pedregosa et al 2011 or in the case of image pixels an mlp with 100 hidden units is used with default parameters as well we present multiple clinical use cases and potential tools which could be built using this dataset and present a baseline task which is evaluated we describe the scenarios in detail to both convey what our group has learned while interacting with clinicians as well as solidify what the value is of such a model to avoid misguided efforts solving a problem that doesnt exist for the results presented in tables 2 and 3 a full listing over all data splits is presented in appendix d2 motivation while reverse transcriptase polymerase chain reaction rtpcr assay remains the gold standard for diagnosis cxr play a major role as the top initial imaging test for patients with suspected covid-19 pneumonia dennie et al 2020b  because of their relative lack of sensitivity 69 wong et al 2019 and the fact that they are often normal early in the disease dennie et al 2020b wong et al 2019  a negative cxr should not be used to rule out covid-19 infection dennie et al 2020b  instead features of covid-19 pneumonia in cxr although nonspecific raise pretest probability of infection for example distinguishing between viral and bacterial pneumonia could influence management in addition to other clinical clues heneghan et al 2020  according to the cstr car and cxr are most useful when an alternative diagnosis is found that completely explains the patients presenting symptoms such as but not limited to pneumothorax pulmonary edema large pleural effusions lung mass or lung collapse dennie et al 2020b  task specification the hierarchy of labels table 1  allows us to perform multiple different classification tasks a first task is to classify covid-19 from other causal agents of pneumonia such as bacteria or other viruses a second task is to distinguish viral from bacterial pneumonia results when classifying between covidnon-covid most recent works using this dataset have taken other datasets and treated them as non-covid-19 wang  wong 2020 apostolopoulos  mpesiana 2020 while results with balanced datasets from tartaglione et al 2020 report much lower performance our loco evaluation aims to avoid these issues and in table 3 we find that performance is higher than random we specifically note that the 4 outputs which are commonly associated with pneumonia are not predictive possibly implying something about the disease that should be explored more we find that due to imbalanced classes performance when predicting between bacterial and viral has an almost random auroc and a high auprc motivation the icu is reserved for patients who require life support such as mechanical ventilation in this invasive intervention reserved for patients unable to breathe on their own an endotracheal tube is inserted into the windpipe intubation and the lungs are mechanically inflated and deflated tobin  manthous 2017  predicting the need for mechanical ventilation in advance could help plan management or prepare the patient another challenge is knowing when to remove mechanical ventilation extubation which falls in a specific window of time thille et al 2013  assessing the severity of a patients condition is a key aspect of patient management the brixia score borghesi  maroldi 2020b signoroni et al 2020 and the modified rale score wong et al 2019 cohen et al 2020a were developed specifically with in context of assessing covid-19 severity from cxr 94 severity score labels were contributed to images in this dataset by cohen et al 2020a and 192 images were given a brixia score by signoroni et al 2020  non-ml work by allenbach et al 2020 combines information from ct scans and cxr with other clinical information to create a score-based predictive model for transfer to the icu a model which predicts the severity of covid-19 pneumonia and pneumonia in general based on cxr could be used as an assistive tool when managing patient care for escalation or de-escalation of care especially in the icu predicting these events could be confounded by the presence of an endotracheal tube in the cxr of a mechanically ventilated patient when available this is annotated in our dataset due to the reduced mobility of icu patients cxr are often obtained with the patient lying down in a view referred to as ap supine khan et al 2009  which includes but is not limited to patients who are intubated or soon to be because this position drastically modifies the appearance of the cxr a naive approach could confound such changes with the need to be intubated task specification the severity scores created for this dataset by cohen et al 2020a provide two scores for 94 pa images geographic extent 0-8 how much opacity covers the lungs and opacity 0-6 how opaque the lungs are have been created for 94 pa images in this dataset by cohen et al 2020a  also an icu stay and the patient being intubation are both predicted given patients between 0 and 8 days from symptomspresentation images marked as intubation present are excluded from the evaluation as this would be a visible confounder in the image for icu stay predictions images marked as already in the icu are excluded results table 2 shows pre-trained models work well and performance is similar to that reported by cohen et al 2020a  the evaluation used in that work is not loco table 3 shows icu stay and intubation are predicted reasonably well at 72 auroc and 66 auprc respectively using either all 18 outputs or the single lung opacity output this may imply that icu stay is predictive by features not related to pneumonia motivation not too dissimilar to severity prediction determining at what point survival can be predicted could be useful for patient management given a series of patient chest x-rays over time it could be possible to determine the probability of survival non ml models are able to predict in-hospital mortality for patients with covid-19 using an original severity score for cxr brixia score combined with two predictive factors which are patient age and immunosuppressive conditions borghesi et al 2020a  ml models are able to predict survival based on clinical features lactate deshydrogenase lymphocyte count and high-sensitivity c-reactive protein with high accuracy yan et al 2020  task specification in order to make predictions which are relevant to the clinical context it is important to control for the time period when the patient is observed our evaluation is on data between 0 and 8 days since symptoms or admission in order to simulate predicting at the beginning of management predictions will be made on non-intubated patients to avoid this confounder of severity results in table 3  reasonable performance of 72 auroc is obtained using the 4 hand picked features of pneumonia in general high performance is obtained using any model outputs which warrants further analysis motivation the ability to gauge severity of covid-19 lung infections could be used to complement other severity tools for escalation or de-escalation of care especially in the icu following diagnosis patients cxr could be scored periodically to objectively and quantitatively track disease progression and treatment response eventually physicians could track patients response to various drugs and treatments using cxr and uploading the images to the dataset allowing researchers to create predictive tools to measure recuperation such a model could also be used as an objective tool to compare response to different management algorithms and inspire better management strategies if the representation is expressive enough patients can be plotted as shown in figure 3a  a conceptual figure and our current realization are shown using a pre-trained cxr model showing the available trajectories and patient outcomes this approach could serve as a way to iterate quickly with a medical team to simply explore the learned representation instead of building complete tools to make sense of the complexities of these models and patients task specification using this dataset we could visualize a models representation of cxrs and plot the trajectory of patients according to a color scheme representing their stateimage good or bad the representation is based off of the 18 pre-sigmoid outputs of the densenet discussed a good state is defined as non-intubated not in the icu or the last state before discharge a bad state is defined as intubated in the icu or the last state before death a kernel density estimation is taken of the 2d embeddings of all bad states to illustrate severity here pa ap and ap supine images are used to maximize the data visualized as we did not observe any shift in the representation results figure 3b shows clustering of images which represent patients in a good or bad state demonstrating the potential insight already contained in these pre-trained models the three patients display trajectories as we would imagine patient 178 4 spirals in an area which appears bad patient 205 5 progresses into bad states but manages to pull themselves out patient 332 sivakorn et al 2020 seems to recover quickly to a region which is only populated by good states  there are many potential tasks that we were not able to explore in this work primarily the use of saliency maps and the utility of out-of-distribution models evaluating saliency maps can be challenging but it is a useful evaluation for methods which aim to explain model predictions taghanaki et al 2019 singla et al 2020  our dataset contains lung bounding boxes which were contributed by andrew gough at general blockchain inc inc for 167 images annotated for the left and right lung also 209 generated lung segmentations were added to the dataset by selvan et al 2020  the team trained a model on an external dataset and applied it to our dataset as there are no ground truth segmentations these can be used to examine if saliency maps are located reasonably within lung regions to detect overfitting this can be seen by checking if the predictive regions of the image lie outside of the region of interest ross et al 2017 badgeley et al 2019 viviano et al 2019  general anomaly detection could be a useful model when trying to identify what is new about a illness such as covid-19 out-of-distribution ood tools shafaei et al 2018 or unpaired distribution matching models zhu  park 2017 kim et al 2017 could capture the shift in distributions and present them as changes to images cohen et al 2018  identifying what about the covid-19 distribution is different from other viral or bacterial pneumonias could aid in studying both the disease as well as the models representations for overfitting singla et al 2020  transfer learning methods actively under development in the ml community such as fewzero-shot wang et al 2019 tian et al 2020b larochelle et al 2008 ren et al 2019  meta-learning andrychowicz et al 2016 snell et al 2017  deep metric learning roth et al 2020  and domain adaptation motiian et al 2017 will likely be useful in this setting this paper presents a dataset of covid-19 images together with clinical metadata which can be used for a variety of tasks this dataset puts existing ml algorithms to the test given the number of existing large cxr datasets novel tasks related to covid-19 present a relevant challenge to overcome we note that a major limitation of this work is the selection bias when gathering publicly available images which are likely made public for educational reasons because they are clear examples or interesting cases therefore they do not represent the real world distribution of cases furthermore another selection bias is that the information given on public platforms such as figure  1 or radiopaedia might not be complete for all patients andor omit normal values eg presence or absence of transfer to the icu lymphocyte count lastly we do not yet have variables such as ethnic background pre-existing conditions and immunosuppression status any clinical claims made from models must therefore be backed by rigorous evaluation and take into account these limitations nevertheless we believe that this dataset and the discussion of clinical context will contribute towards the machine learning community developing solutions with potential use in healthcare this project aims to make a dataset of patients with a novel life-threatening disease accessible to researchers so that tools can be created to aid in the care of future patients the manner in which we collect existing public data ensures that patients are not put at risk data impact image data linked with clinically relevant attributes in a public dataset that is designed for ml will enable parallel development of diagnosis and management tools and rapid local validation of models furthermore this data can be used for a variety of different tasks tool impact tools developed using this data and with the ideas presented can give physicians an edge and allow them to act with more confidence while they wait for the analysis of a radiologist by having a digital second opinion confirm their assessment of a patients condition in addition these tools can provide quantitative scores which can enable large scale analysis of cxr without the need for costlytime consuming manual annotations tom and de freitas nando learning to learn by gradient descent by gradient descent neural information just considering pa ap and ap supine views there are 367 jpeg files and 171 pngs all images are 8-bit except for 1 which is 16-bit  each scraper identifies relevant radiographs using the search feature of the site being scraped and saves the images together with a csv file of the corresponding metadata internally the scraper uses selenium to visit each page of the search results saving metadata and images from any new cases images are downloaded at the highest resolution available data from each case is converted to a single interoperable json format finally the needed metadata is exported as a csv file currently the scrapers only retrieve metadata from structured fields that can be easily detected on webpages such as age sex imaging notes and clinical notes other clinically important information such as whether the patient went to the icu or was intubated is extracted by human annotators care has been taken to use these websites resources conservatively and avoid creating a burden on their servers crawling is done in a lazy manner so that no pages are requested until they can be used also within each scraping session each web page is kept open in an individual browser instance as long as it is needed to reduce re-requested resources    understanding the covid19 outbreak a comparative data analytics and study anis koubaa  the coronavirus also known as the covid-19  the coronavirus covid-19 outbreak nowadays represents the most critical event worldwide it has been declared by the world health organization who as a global public health emergency by the end of january 2020 and then as a global pandemic in march 2020 the impressive fast spread of the virus is unprecedented and has exceeded all expectations the containment of the virus is increasingly challenging as almost all countries in the world become infected the virus has begun on from wuhan district in china where the first confirmed case was reported to have happened on november 17 2020 2  initially the confirmed cases in china were continually increasing on january 31 the total infections reached a bit less than 10000 confirmed cases with 214 recovered and 213 reported deaths 2 death rate and similar for recovery although the chinese authorities have taken incremental and prompt preventive measures to avoid the exponential outbreak the virus continued to spread not only within chinese borders but also worldwide the virus was transmitted through travelers around the world one of the most dangerous aspects of the coronavirus is that it has an incubation period of 2-14 days during which the patient transmits the virus without having any symptoms all these circumstances have favored the exponential growth of the infection leading to a world health emergency crisis as a consequence after only two months from the official declaration of covid-19 as global public health emergency and despite the numerous exceptional preventive measures that every country has taken to avoid the outbreak the virus has contaminated almost all the world countries figure 1 illustrates the evolution of the number of countries that were affected by the coronavirus outbreak from january 22 until march 26 2020 based on the daily report data provided by johns hopkins university repository 3 it is observed from the figure that the outbreak pick growth started towards the end of february which is almost four weeks since the disease was declared as a global public health emergency besides asia the first continent to be severely affected the outbreak has been more generalizing to other continents during march 2020 putting first europe into a crisis followed by the americas and finally the african countries at the time of writing this paper a total of 173 countries are reported to have confirmed cases with different gravity while only 60 countries had confirmed cases at the end of february 2020 and only 25 countries at the end of january 2020 this means that the increase rate was between 24 to 29 each month almost all countries worldwide are currently infected but the impact of the covid-19 virus has widely varied between the continents regions and countries this represents the motivation of this data analytics study our objective is to unveil the secrets of the covid-19 virus and understand its evolution in the world we aim to know the distribution of confirmed and death cases across the continents regions and countries and the correlation between them furthermore we compare the impact of the the rest of the paper is organized as follows since its spread there have been several initiatives to investigate the impact of covid-19 from the scientific community in 4  the authors have proposed to analyze the use of social media to exchange information about the coronavirus they proposed to identify situational information to investigate the propagation of covid-19 related information in social media they used natural language processing techniques to classify covid-19 information into several types of situational information in 5  the authors develop a predictive model to forecast the propagation of covid-19 in wuhan and its impact on public health by considering the social preventive measures some other researchers like in 6  the authors have analyzed the covid-19 outbreak during its early phases in italy they provided estimates of the reproduction number and serial intervals in 7  the authors investigated the impact of preventive measures such as social distancing lockdown in the containment of the virus outbreak they developed prediction models that forecast how these measures can reduce the mortality impact of aged people the authors of 8 addressed the question about how the virus has spread from the epicenter of wuhan city to the whole world they have also analyzed the impact of preventive measures such as quarantine and city closure in mitigating the adverse impact of the spread the authors have demonstrated visual graphs and developed a mathematical model of the disease transmission pattern in 9  the author has analyzed the virus outbreak in italy based on early data collected to predict the outcome of the process he argued that there is a strong correlation between the situation in italy and that of hubei province some researchers have attempted to use deep learning and artificial intelligence in the context of covid-19 in 10  the authors have proposed covid-net which is a deep convolutional neural network for the detection of covid-19 infection from chest radiography images open-source dataset the dataset contains 5941 chest radiography images of 2839 patient cases in 11  the authors have developed an image processing technique for the detection quantification and tracking of the covid-19 virus they utilized deep neural network models for the classification of suspected covid-19 thoracic ct features using data from 157 patients from the usa and china the classification area under the curve auc of the study was found to be 0996 in 12  the authors investigated drop-weights based bayesian convolutional neural networks bcnn and its effect on improving the performance of the diagnostic of covid-19 chest xray they showed that the uncertainty in prediction is highly correlated with the accuracy of prediction in this paper we propose a detailed data analytics study about the covid-19 virus to understand its impact besides we compare its severity against ebola 2014 mers 2012 and sars 2003 to achieve this objective we have collected data from authentic sources and widely accepted by the scientific community in what follows we present the datasets used in this study we searched for datasets that provide credible data about the covid-19 outbreak the 2019 novel coronavirus covid-19 data repository provided by johns hopkins university 3 is the most comprehensive up-to-date and complete dataset that gives daily reports of the covid-19 outbreak in terms of confirmed cases death cases and recovered cases besides johns hopkins university maintains an active dashboard that reports daily updates of the coronavirus 13  also the same dataset is being extensively used by the data science community of kaggle to develop several analytics notebooks and dashboard about covid-19 14  each row in the covid-19 dataset contains the following relevant data  observation date it represents the date when the corresponding data row was reported  country the country from where the data emerged  confirmed cases the number of covid-19 confirmed cases  death cases the number of covid-19 death cases  recovered cases the number of covid-19 recovered cases in addition to this data we have processed the dataset to add additional information related to  continent the continent of the country related to the collected data we considered five continents including africa the americas north and south asia europe and oceania australia and new zealand  region the region is a level between country and continent we considered the following regions in our study northernsoutherneasternwesternmiddle africa northernsoutherneasternwestern europe northersouthernsouth-easterneasternwestern asia northernsouthcentral america arabic gulf caribbean australia and new zealand melanesia and micronesia the mapping between countries and their corresponding regions and continents was performed using the following csv file 1  we have also collected datasets for the other epidemic diseases namely 16  ebola 2014-2016 outbreak complete dataset 17 all the dataset provides time-series information about confirmed and death cases per country per day during the observation period except the mers dataset that provides only the final statistics of the disease for the confirmed cases no death cases reports we could not find any credible data source for the time series evolution of mers neither the death cases 1 httpswwwkagglecomstatchaityacountry-to-continent we have processed these datasets to clean the data and also add the mapping of the countries to their region and continent to develop region-level and continent-level statistics also we have created an all-in-one dataset with all data combined for comparative purposes in this work we have used tableau professional software to analyze the collected data and develop visualization dashboards about the coronavirus disease our methodology consists in creating descriptive models of the coronavirus outbreak using statistical charts to understand the nature of the spread and its impact we develop our analysis at three levels namely at the country-level at region-level and continent-level each level provides different granularities towards understanding the distribution of the disease around the world the descriptive model provides different types of statistical charts including bar charts geographic maps heat maps box plot and packed bubbles to represent different features of the covid-19 outbreak we also develop some predictive models using linear and polynomial regressions to predict the evolution of the outbreak given the historical data in this study we also compare covid-19 with the other three most critical world epidemic outbreaks namely ebola 2014 mers 2012 and sars 2003 we visualize the difference in terms of the impact of these diseases in terms of confirmed and death cases analyze the characteristic of each disease the lessons learned in this data analytics study serves as a ground for data science for further investigation of the covid-19 epidemic outbreak in this section we will present the results of this data analytics study the dashboards of this study are also available online 1 a how does covid-19 evolve figure 2 depicts the evolution of the covid-19 outbreak in the logarithmic scale during the period from january 22 2020 to march 27 2020 ie two months period let us consider january as the reference month we performed a linear regression analysis on the different curves shown in figure 2  and we determined the confirmedrecovereddeathactive rates during the observation periods these rates are shown in table i  the rates are the slope of the regression lines they are shown as the first parameter between parenthesis in the table below by observing the trend lines of linear regression models on the different curve at each month we conclude the following observation this is also confirmed by the ratio between the recovered rate and the death rate comparing the trend death and the trend of recovered cases it can be observed that the ratio of recovered to death rates was 086 1922 in january 2020 meaning that the death rate was a bit higher in january than the recovered rate however in february 2020 the ratio of recovered to death rates increases to 1333 1373103 and reaches 366 2906792 in march 2020 thus the general trend is that the disease is being more controlled in terms of fatality rates due to increasing emergency procedures that the different countries have implemented the results presented above a coarse grain in the sense that they related a global assessment of the evolution however the evolution of the covid-19 infection depends much on the countries the region and the continent it is therefore important to assess the evolution at these levels to get a better understanding of it figure 4 presents the cumulative confirmedrecovereddeathactive cases reported as of march 27 2020 for the top-10 countries then their regions and continents we observe that in the top-10 countries there are six countries are from europe ie italy spain germany france united kingdom and switzerland three from asia china iran and south korea and the united states of america which recently become top-1 in terms of the number of infections nonetheless the highest death rate is in italy with more than 9000 death cases reported because italy has been severely affected by the virus well before the usa since the end of february however the usa is currently having a death rate of 39 deaths per day whereas it has 0 deaths in february 2020 based on the data collected there is a strongly believed that the covid-19 takes almost one month to transit from one continent to another in the direction from the east to the west since the pick was in china at the end of january then it was in italy south of europe at the end of february and it reached the usa at the end of march where the pick of infection are in new york located at the eastern side of the usa it can be observed in figure 3 that the eastern side of the usa and mainly new york are the most affected considering that it is closer to europe if the trend is the same it will be expected that the west side of the usa will reach its pick by the end of april 2020 in figure 4  it can be observed that europe has the most significant confirmed cases currently and the most severe fatality rates where the maximum reached are the south of europe with more than 14000 death cases among which more than 9000 are located in italy italy is currently having the third of fatalities in the whole world it is also observed that oceania and africa are less affected by the virus as compared to other continents finally the distribution of the different cases is illustrated in the heatmap presented in figure 5  dark colors mean a high concentration of cases and lighter colors mean smaller concentrations of cases in the previous section we have presented a comprehensive analysis of the covid-19 virus and we got a better understanding of how it was evolved and its impact on the country region and continent level in this section we address the question how does covid-19 compare to other epidemics several other epidemics have emerged in the last 20 years in particular severe acute respiratory syndrome sars2003 in 2003 in hong kong the middle east respiratory syndrome mers2012 in saudi arabia and the middle east and ebola 2014 in western african coast namely guinea liberia and sierra leone these three epidemics in addition to covid-19 are the most remarkable world diseases in the last 20 years which we proposed to compare and analyze 1 comparative evolution over time figure 6 presents a dashboard that compares the four epidemic outbreaks on the top we observe the geographic heat map for the four diseases it is visually apparent that covid-19 is the largest outbreak to a considerable extent followed by sar it can be observed that covid-19 is considered as a more acute specie of sars 2003 as they both share some common features including 1 both have started from china 2 they belong to the same family of coronavirus syndrome affecting the respiratory system 3 they have the highest contamination rate as compared to other epidemics based on these observations it seems that the covid-19 containment will take a more extended period for its complete containment as compared to sars 2003 the third row of figure 6 shows the daily confirmed cases for covid-19 ebola 2014 and sars 2003 the trends of covid-19 are exponential whereas the trends of ebola 2014 and sars 2003 are high at the start of disease then start to decrease after two months of the first confirmed cases this shows that the behavior of covid-19 is more aggressive as compared to the other epidemics 2 comparative impact we address the question how do the impacts of the epidemics compare to each other in terms of confirmed cases and death cases figure 7 shows the comparative impact with respect to the confirmed cases and figure 10 shows the comparative impact with respect to the death cases at continent-level region-level and country-level the blue color refers to the covid-19 the red color refers to the ebola 2014 and the yellow color refers to sars 2003 looking at the two figures we can conclude that the covid-19 is exceptionally more aggressive in terms of confirmed cases with more than 90 of the share of the heatmap where it is at around 80 concerning the fatality impact at the country-level the usa has the most significant share of confirmed cases as of march 27 2020 with 1643 followed by italy 1398 then china 1315 we can also observe that the number of confirmed cases of ebola 2014 in sierra leon is similar to the covid-19 spread in south korea and countries in the west of europe namely netherlands belgium and austria regarding the death cases impact it is different from the confirmed cases at continent-level the highest death impact is in europe with 5626 then asia with 1856 with covid-19 which is of the same magnitude as the fatality of ebola 2014 in africa looking at table ii  we can observe a strong correlation between the median age at a continentregion and the fatality rate europe is the oldest of all continents with a median age of 42 has the highest fatality rates mainly in southern and western europe at region-level we observed that the deadly impact of ebola 2014 on western africa is the second most severe after the deadly impact of death in the south of europe at the country-level the impact of covid-19 is the highest in italy followed by the impact of ebola 2014 in in what concerns sars 2003 its fatality rate is much lower than eolba 2014 and covid-19 diseases figure 9 and table 8 present the average confirmeddeath cases per continent for each of the epidemics per continent the results confirm the heatmap and packed bubbles presented above and provide the average distribution of death in each continent the highest average of confirmed cases    andre esteva anuprit kale romain paulus kazuma hashimoto wenpeng yin dragomir radev richard socher  the covid-19 global pandemic has resulted in international efforts to understand track and mitigate the disease yielding a significant corpus of covid-19 and sars-cov-2-related publications across scientific disciplines as of may 2020 128000 coronavirus-related publications have been collected through the covid-19 open research dataset challenge 23  here we present co-search a retriever-ranker semantic search engine designed to handle complex queries over the covid-19 literature potentially aiding overburdened health workers in finding scientific answers during a time of crisis the retriever is built from a siamese-bert18 encoder that is linearly composed with a tf-idf vectorizer 19  and reciprocal-rank fused 5 with a bm25 vectorizer the ranker is composed of a multi-hop question-answering module 1  that together with a multi-paragraph abstractive summarizer adjust retriever scores to account for the domain-specific and relatively limited dataset we generate a bipartite graph of document paragraphs and citations creating 13 million citation title paragraph tuples for training the encoder we evaluate our system on the data of the trec-covid22 information retrieval challenge co-search obtains top performance on the datasets of the first and second rounds across several key metrics normalized discounted cumulative gain precision mean average precision and binary preference preprint under review  the global response to covid-19 has yielded a growing corpus of scientific publications -increasing at a rate of thousands per week -about covid-19 sars-cov-2 other coronaviruses and related topics the individuals on the front lines of the fight -healthcare practitioners policy makers medical researchers etc -will require specialized tools to keep up with the literature co-search is a retriever-ranker semantic search engine that takes search queries including questions in natural language and retrieves scientific articles over the coronavirus literature co-search displays content from over 128000 coronavirus-related scientific papers made available through the covid-19 open research dataset challenge cord-19 23 -an initiative put forth by the us white house and other prominent institutions in early 2020 retrieval is done using a semantic model and two keyword models for the semantic model we create a bipartite graph from paragraphs and their cited articles to generate over 22 million paragraph title tuples that we use to train a siamese-bert sbert model 18 on the binary task of classifying a title as being cited by a paragraph sbert 6 is used to embed queries and documents into the same latent space enabling nearest-neighbor semantic retrieval further we combine these embeddings with tfidf and bm25 specifically we linearly combine sbert paragraph-level retrieval scores with tf-idf document-level retrieval scores to generate a document list then use reciprocal ranked fusion to combine this list with that obtained from bm25 retrieval using anserini 25  ranking takes the retrieved scores and modulates them with a question answering module and an abstractive summarizer we train a multi-hop question answering model with wikipedia following 1  that treats the query as a question and generates answers from the retrieved documents we train an abstractive summarizer composed of a bert encoder and modified gpt-2 decoder 15  in the same way to generate a summary our ranker then takes the scores of the retrieved documents and modulates them based on the degree to which the documents contain the generated answers and summaries to return a ranked document set we evaluate co-search on data from the first two rounds of the trec-covid challenge 22 -a five-round ir competition for covid-19 search engines -using several standard metrics normalized discounted cumulative gain ndcg precision with n documents pn mean average precision map and binary preference bpref trec-covid considers ir system submissions that are either manual -in which queries and retrieved documents may be manually adjusted by a human operatoror automatic -in which they may not a third category is accepted in rounds 2-5 of type feedback in which systems are trained with supervision from the annotations of prior rounds submissions compete on a pre-defined set of topics and are judged using various metrics including those listed above expert human annotators provide relevance judgements on a small set of topic-document pairs which are included together with non-annotated pairs in the evaluation co-search is an automated system on round 1 and round 2 data evaluated against other automated systems using strictly judged topic-document pairs it achieves a rank of 1 on ndcg10 p5 p10 map and bpref full details in section 5 taken one step further by evaluating against all systems and all topic-documents judged  non-judged co-search ranks in the top 21 on round 1 and top 3 on round 2 we do not propose a novel algorithm but rather we present a functioning system and open-source code rigorously evaluated to support the global effort and encourage others to build on our work the cord-19 challenges 23 coronavirus-related corpus primarily from pubmed mostly published in 2020 has quickly generated a number of data science and computing works 4  these cover topics from information retrieval to natural language processing including applications in question answering 21  text summarization and document search 22  information retrieval over the last three months more than 20 organizations have launched publicly accessible search engines using the cord-19 corpus for instance neural covidex 27 was constructed from various open source ir building blocks including pyserini blacklight and apache solr for retrieval as well as a t5 transformer 16  finetuned on the ms marco dataset 2  to predict query-document relevance for ranking sledge 14 extends this by using scibert 3  also fine-tuned on ms marco to re-rank articles retrieved with bm25 question answering and text summarization one of the first question answering systems built on top of the cord-19 corpus is covidqa 1 21  which includes a small number of questions from narrative seeking information about protein-protein interactions for any of the sars-cov-2 structural proteins that represent a promising therapeutic target and the drug molecules that may inhibit the virus and the host cell receptors at entry step the cord-19 tasks a multi-document summarization system caire is described in 20  it generates abstractive summaries using a combination of unilm 7 and bart 11  fine tuned on a biomedical review data set to evaluate our system we combine the cord-19 corpus with the the trec-covid competitions evaluation dataset topics and relevance judgements which annotate query-document pairs into either irrelevant partially relevant or relevant the us white house along with the us national institutes of health the allen institute for ai the chan-zuckerberg initiative microsoft research and georgetown university recently prepared the cord-19 challenge in response to the global crisis as of may 2020 this resource consists of over 128000 scientific publications up from 29000 at the challenge inception in february 2020 about covid-19 sars-cov-2 and earlier coronaviruses 23  this challenge represents a call-to-action to the ai and ir communities to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions it is currently the most extensive coronavirus literature corpus publicly available the tasks put forth by the cord-19 challenge are broad open-ended and qualitative with submissions scored by human experts based on accuracy documentation and presentation the unstructured nature of the challenge has resulted in submissions ranging from ir systems to interactive visualizations as such we do not use these tasks to evaluate our system in response to cord-19 the text retrieval conference trec recently partnered with the national institute of standards and technology nist to define a structured and quantitative evaluation system for coronavirus ir systems the trec-covid challenge 22 is composed of 5 successive rounds of evaluation on 30-50 topics the first round includes 30 topics each subsequent round takes the prior rounds topics and adds five new ones each topic is represented as a tuple consisting of a query a question and a narrative with an increasing amount of detail in each see table 1  ir systems must retrieve up to 1000 ranked documents per topic from the cord-19 publications and are evaluated on a set of metrics including ndcg10 pn map and bpref co-search consists of a retriever and a ranker with an offline pre-processing step to create a document index the index is created by embedding documents in three ways a pre-trained sbert model embeds paragraphs and both a tf-idf vectorizer and bm25 vectorizer using the anserini framework 25  see fig 1 the system computes a linear combination of tf-idf and sbert retrieval scores then combines them via reciprocal ranked fusion with the retrieval scores of bm25 the retrieved documents and query are parsed through a question answering model and an abstractive summarizer prior to being ranked based on answer match summarization match and retrieval scores scores of the sbert model with query-document scores from tf-idf then fusing them via reciprocal rank fusion 5 with query-document scores from the bm25 model this defines a retrieval score for each document given a query ranking takes this set of documents runs them through a question answering module qa and an abstractive summarizer then ranks the documents by a weighted combination of their retrieval scores the qa output and the summarizer output full details below we use a semantic method bert embeddings to embed paragraphs and image captions and two keyword methods tfidf bm25 to embed entire documents see fig 1a  in this use-case semantic embeddings face the challenge of working with a relatively small number of long documents to address this we split the documents into paragraphs extract the titles of the citations of each paragraph and form a bipartite graph of paragraphs and citations with edges implying that a citation c came from a paragraph p we use the graph to form tuples -p c st c  p -for training an sbert model 18 to classify if a title was cited by a paragraph additionally we generate an equivalent number of negative training samples of incorrect tuples -p c st c   p we train the model with cross-entropy loss adam optimization 10 with a learning rate of 2e-5 a linear learning rate warmup over 10 of the training data and a default pooling strategy of mean see fig 2a  the structure of the learned latent space embeds queries near documents that share semantic meaning visualizing this reveals a human-understandable clustering of documents and topics fig 2b shows a two-dimensional t-sne 13 plot of the embedded space with different colors representing topics of trec-covid and points representing documents qualitatively we observe that semantically similar documents cluster by topic at runtime the retrieval step takes an input query embeds it using sbert computes approximate nearest neighbors over the sbert paragraph embeddings and returns a set of paragraphs together with each paragraphs cosine similarity to the query tf-idf and bm25 operate at the document level each returning vectors t  r m and b  r m such that t i  tfidfquery document i b i  bm25query document i and m is the size of the document corpus the sbert and tf-idf scores are combined linearly for document d containing paragraphs p and query q with subscript es denoting an sbert embedding their combination c is given by this induces a ranking r q c on the documents which is then combined with the bm25-induced ranking r q b using reciprocal ranked fusion 5  to obtain a final retrieved ordering in practice we find that the constants   07 and k  60 yield good results future work could consider using a learned layer to attend over semantic and keyword embeddings given the query ranking combines the rrf scores of the retrieved documents with the outputs of the qa engine and the summarizer we define q to measure the degree to which a document answers a query where 1x is the indicator function 1x  1 if x is true 0 otherwise the set aq contains the text span outputs of the qa model we define s to measure the degree to which a document summarizes the set of documents retrieved for a query where m q e is the embedded abstractive summary of q summarized across all retrieved documents then the final ranking score rd q of a document for a particular query is given by with higher scores indicating better matches in essence rank score r is determined by letting s and q modulate the retrieval score of a query-document pair whereas standard question answering systems generate answers our model extracts multiple answer candidates text spans from the retrieved paragraphs which are then used downstream for ranking the qa model takes the query the retrieved paragraphs and uses a sequential paragraph selector model following 1  to filter for paragraphs that could answer the query specifically the model uses multi-hop reasoning to model relationships between paragraphs and select sequentially ordered sets of them as opposed to simply providing relevance scores for each paragraph independently it is pre-trained using hotpotqa 26 -a wikipedia-derived dataset of 113k question-answer pairs and sentence-level supporting facts and further fine-tuned on the pubmedqa dataset 9 for biomedical specificity following 1  we use paragraphs with high tf-idf scores for the given query as negative examples for the sequential paragraph selector the original beam search is modified to include paragraph diversity and avoid extracting the same answers from different paths once filtered the paragraph sets are fed into an extractive reading comprehension model -trained on the squad 17 dataset given its similarity to the trec topics -to extract answer candidates our summarizer takes the retrieved documents and generates a single abstractive summary it is an encoder-decoder model with bert 6 as the encoder and a modified gpt-2 model 15 as the decoder we extend the original gpt-2 model by adding a cross-attention function alongside every existing self-attention function we constrain the cross-attention function to attend strictly to the final layer outputs of the encoder we use the base models of 24  with 12 layers 768-dimensional activations in the hidden layers and 12 attention heads the model is pre-trained using self-supervision with a gap-sentence generation objective 28  followed by single-document supervised training using cord-19 full documents as input and the paper abstract as target output to increase the probability that a generated summary matches and thus helps re-rank the contents of the retrieved paragraphs we architect the model to generate short summaries following 8  abstracts are split into five groups based on the number of tokens 65 65-124 125-194 195-294 295 during training a special token is provided to specify the summary length in these 5 categories at inference time the model is initialized to output summaries of token lengths 65 to adjust the model to operate on multiple retrieved paragraphs we concatenate the first four sentences of the retrieved paragraphs until they reach an input length of 512 tokens then feed this into the summarization model we evaluate our system quantitatively using the cord-19 document dataset and the topics and relevance judgements provided by the trec-covid competition see section 3 for full details in each of its five rounds the competition provides a set of topics -as query question narrative tuples -to evaluate submissions along with relevance judgements -scores of 0 for irrelevance 1 for partial relevance and 2 for relevance -on a small subset of all possible topic-document pairs teams submit up to 1000 ranked documents per query and the organizers pool from amongst the most common topic-document pairs for judging depth-7 pooling in which the top 7 documents from each response provided by the set of contributing systems are judged for relevance by human assessors 12  this yields extremely sparse and biased labeling as a result it is critical to evaluate systems both on the full dataset and on the annotated subsets given the inherent difficulty of comparing ir systems trec has historically used a number of metrics to judge them key amongst them are high-precision metrics such as ndcg10 which the leaderboards use precision5 precision10 and map the critical limitation with these is that their effectiveness relies on complete relevance judgements across all topic-document pairs which is intractable to account for this the competition considers measures that work with incomplete relevance judgements such as bpref below we define key metrics in evaluation throughout this work we adopt the standard convention that mn refers to an evaluation using metric m and the top n retrieved documents  normalized discounted cumulative gain ndcg for position i  0 1     n  the normalized discounted cumulative gain of a retrieved s et of documents over q queries is given by where rel q i denotes the relevance of entry i ranked according to query q idcg denotes the ideal and highest possible dcg in the limit of perfect annotations ndcg performs reliably in measuring search engine performance since it treats non-annotated documents as incorrect rel i evaluates to zero it is less reliable for datasets with incomplete annotations the average precision ap of a retrieved document set is defined as the integral over the normalized precision-recall curve of the sets query map is defined as the mean ap over all queries where r is recall p q is precision as a function of recall for a particular query note that as in the case of ndcg map penalizes search engines that yield accurate but unique ie non-annotated results since non-annotated documents are treated as irrelevant by p  binary preference bpref bpref strictly uses information from judged documents it is a function of how frequently relevant documents are retrieved before non-relevant documents in situations with incomplete relevance judgements most ir datasets it is more stable than other metrics and it is designed to be robust to missing relevance judgements it gives roughly the same results with incomplete judgements as map would give with complete judgements 12  it is defined as 1  n ranked higher than r r where r is the number of judged relevant documents r is a relevant retrieved document n is one of the first r irrelevant retrieved documents and non-judged documents are ignored  as of the submission of this work two rounds of the trec-covid competition have elapsed and relevance judgements have been generated for each our results on this data are shown in table 2  we present our system in two contexts the first is within the general set of submissions this includes metric evaluations on all documents -annotated and non-annotated -and this includes ranking against manual automatic and feedback systems manual submissions use human operators to adjust the query or the retrieved documents to improve ranking feedback systems use supervision from the relevance judgements of prior rounds automated search engines may not do either in the second context we evaluate our system and all others strictly on relevance judgements and we compare our automated system strictly against other automated systems to determine rankings we account for multiple submissions with the same score and assign to each the highest one ie if the top two scoring submissions for a metric have the same score each would be ranked 1 in the first context our system ranks in the top 21 round 1 144 systems and in the top 3 round 2 136 systems see table 2  the column judgedn shows the average percentage across queries of topic-documents that have been annotated as expected as the percentage rises from round 1 to round 2 so do the scores and rankings of the system in the second context our system ranks 1 across metrics in both rounds against 102 systems in round 1 and 73 systems in round 2 round 1 used 30 topics and 51000 documents yielding 153 million possible topic-document pairs there were 8691 relevance judgements representing 057 annotation coverage round 2 used 35 topics 30 from round 1 with 63000 documents yielding 22 million possible topic-document pairs there were 12037 relevance judgements representing 054 annotation coverage here we present co-search a covid-19 scientific search engine over the growing corpus of coronavirus literature we train the system using the scientific papers of the covid-19 open research dataset challenge and evaluate it against other search engines using the data of the trec-covid competition our systems ranks best amongst automated systems and near the best amongst all systems as judged using various metrics including ndcg10 p5 p10 map and bpref the system uses a combination semantic and keyword retriever that combines sbert-derived embeddings with tfidf  bm25 vectors to retrieve documents it leverages a wikipedia  pubmed pre-trained multi-hop question answering system together with an abstractive summarizer to modulate retrieval scores this structure allows co-search to disambiguate between subtle word orderings that in biological contexts result in critically different meanings eg what regulates expression of the ace2 protein vs what does the ace2 protein regulate maximizing its utility to the medical and scientific communities in a time of crisis this work is intended as a tool to support the fight against covid-19 in this time of crisis tens of thousands of documents are being published only some of which are scientific rigorous and peer-reviewed this may lead to the inclusion of misinformation and the potential rapid spread of scientifically disprovable or otherwise false research and data people on the front lines -medical practitioners policy makers etc -are time-constrained in their ability to parse this corpus which could impede their ability to approach the returned search results with the appropriate levels of skepticism and inquiry available in less exigent circumstances coronavirus-specialized search capabilities are key to making this wealth of knowledge both useful and actionable the risks are not trivial as decisions made based on returned incorrect or demonstrably false results might jeopardize trust or public health and safety the authors acknowledge these risks but believe that the overall benefits to researchers and to the broader covid-19 research agenda outweigh the risks  spring 2020 discussing privacy and surveillance on twitter a case study of covid-19 jayati dev   technology is uniquely positioned to help us analyze large amounts of information to provide valuable insight during widespread public health concerns like the ongoing covid-19 pandemic in fact information technology companies like apple and google have recently launched tools for contact tracing -the ability to process location data to determine the people who have been in contact with a possible patient in order to contain the spread of the virus 1  while china and singapore have successfully led the effort 2  more and more countries are now implementing such surveillance systems raising potential privacy concerns about this long term surveillance for example it is not clear what happens to the information post-pandemic because people are more likely to share their information during a global crisis without governments having to elaborate their data policies 3  digital ethnography on twitter which has over 330 million users worldwide with a majority in the united states where the pandemic has the worst effects 4 provides a unique opportunity to learn about real-time opinions of the general public about current affairs in a rather naturalistic setting consequently it might be useful to highlight privacy concerns of users 6  should they exist through analysis of twitter data and information sharing policies during unprecedented public health outbreaks this will allow governments to protect its citizens both during and after health emergencies the specific research questions in this study would be to see how the discussion around privacy and surveillance has evolved over the duration of the covid-19 pandemic they are as follows 1 what are the various discussion topics involving covid-19 surveillance and how frequently do they occur 2 what are users sentiments about surveillance during the covid-19 outbreak using python libraries for topic modelling using latent dirichlet analysis lda and sentiment analysis using natural language toolkit nltk i report the discussions around privacy and peoples sentiments towards surveillance at large i also observe the discussion over time and user engagement on twitter in these topics which reveal that users engage in privacy discussions around covid-19 possibly propelled by popular media articles with a rising negative sentiment for government surveillance and other privacy concerns the findings indicate a need for better planning in data collection and analysis by governments and companies that are privacy-preserving while providing an important information source for governments in containing public health emergencies like covid-19 the current covid-19 pandemic has raised important questions about the way we deal with privacy and security concerns of personal information in the wake of a public health emergency a number of countries have implemented widespread surveillance of its citizens by using location information for contact-tracing this helps them understand if people with covid-19 symptoms have been in contact with other people who in turn might get infected while articles claim that 3 surveillance has not been particularly effective in controlling the outbreak many countries argue otherwise 11  this has raised concerns among privacy think-tanks about what would be done with user information which users readily provide in efforts to contain the pandemic after the outbreak 3  with google and apple combining efforts for contact-based tracing using granular location information 1  people who are vulnerable during the outbreak must not be affected by the consequences of big data collection after the same furthermore the extent of remote work and school during the novel coronavirus outbreak has also enabled people to connect with their workplace and academic work from home via the internet while maintaining social distancing norms video conferencing software such as zoom have been found to collect user information and have multiple security bugs that lead to zoom-bombing 12  with several efforts being made by zoom to fix such bugs this also creates a need to study the privacy concerns and sentiment around technological problems or misuse of products that are instrumental for enabling people to be connected during extended periods of social isolation and government lockdowns though twitter is often studied for user sentiment in socio-political contexts like the spread of false information 9  it is rarely used for evaluating privacy concerns among people privacy concerns are usually studied through traditional mixed method research tools like surveys and interviews to the best of my knowledge there is limited research on privacy concerns during such a large scale emergency like never before however despite its limitations twitter provides a naturalistic setting to understand popular conversation about privacy and security that emerge as an indirect effect of the novel coronavirus outbreak the variety of discussion on twitter provides a starting point for a more nuanced analysis of privacy concerns and has been the focus of this study while twitter users tend to be younger more educated and more liberal than the general population 5  it nevertheless provides an opportunity to provide insight into privacy concerns of individuals the data was collected using get old tweets api that maintains an archive of old 1 1 httpsgithubcomjefferson-henriquegetoldtweets-python tweets newer tweets the last 100 tweets in the dataset have been collected using the tweepy twitter api  i conducted a time-series analysis from march 1 2020 first week of government 2 lockdown 7  to april 20 2020 current date of data collection to measure the number of tweets by users over time this was followed by a measure of occurrences of retweets and favorites for the specific tweets collected to study how users engaged with privacy-specific content regarding covid-19 on twitter i used a predetermined set of keywords that include coronavirus and privacy the first research question required an in-depth analysis of tweets since this is a novel phenomenon with limited previous research theories i used latent dirichlet analysis for topic modelling in order to highlight the different privacy themes and opinions that emerge this was done using the lda model available through the gensim package in python and the natural language toolkit nltk nltk was used to tokenize words from tweets remove existing stop words like prepositions and reduce the words to its stem form the number of topics in lda was set to 10 for the dataset tweets from march and those from april respectively the resulting topics were then tagged with a description to address the emerging topic in order to answer the second research question i performed a sentiment analysis of tweets using pythons using a naive bayes classification approach previously known as automatic indexing 8  i used pandas for string handling and converting the dataset from csv into a pandas dataframe for easy manipulation i also used sklearn to access the bayes classification algorithm and nltk packages for data pre-processing sklearn was also used to create datasets for both training and testing the sentiment analysis model i used the same dataset for both the results from the analysis are described below please refer to the appendix for code and datasets ethics i chose not to collect twitter user information in order to make the dataset de-identified and protect user privacy the collection of publicly available tweets does not require an institutional review board irb application the resulting dataset from mining twitter for march-april 2020 generated a total of 22208 tweets  april -10567 march -11641 i used time-series analysis for tweets retweets and favorites to see the number of discussions over time and the impact per discussion tweet during the pandemic this is followed by topic modelling for the 10 topics that people talk about spring 2020 regarding privacy specific to surveillance during covid-19 and a sentiment analysis of such tweets positive negative or neutral the findings are explained in detail below figure 1 shows the number of tweets over time since march 1 2020 that express surveillance concerns the graph peaks on march 23 2020 which is the date of publication of the first electronic frontier foundation article on government surveillance during covid-19 and the resulting privacy concerns the graph first sees a spike during the first week of lockdown which started on march 13 2020 in the united states a polynomial trendline r 2 0583 shows that tweets expressing privacy concerns peaked during the first week of april and then subsequently started to decrease in frequency  analysis of the retweets and favorites for each tweet presented an interesting result as can be seen in figure 2 and figure 3  there is a disproportionate amount of engagement with specific tweets while most tweets usually have a number of retweets and favorites under the numeric 10 baseline table 1 shows the mean median maximum value and minimum value of retweets and favorites respectively for all the 22208 tweets as mentioned in the table i had to remove an outlier tweet with retweets  7743 and favorites  16943 as of april 20 2020 to adjust the scatter plots in figure 2 and 3  the average number of retweets was 314 and favorites was 1020 with median for both being zero this shows that most tweets had very less engagement with users and certain tweets received a lot of attention given by the maximum value of the outlier tweet in table 1   users seem to discuss both positive and negative aspects of surveillance and loss of privacy during covid-19 while some users are concerned about the government and private companies tracking individuals others note the possible benefits of contact-tracing through smartphones in containing the spread the use of devices like smartphones to collect health information and long-term surveillance in the face of european privacy laws was discussed similarly users also discussed the need for surveillance to protect people a third category of topics just inform about the situation without taking any sides like cross-country efforts in surveillance another common theme that emerged through topic modelling was privacy concerns about technology in use topic 5 6 and 8 there seemed to be concerns regarding data security about information gathered on mobile applications there was also discussion around the impact of data collection by companies like google whose products are being spring 2020 increasingly used for long-distance connectivity for both personal and professional reasons table 2 shows the ten topics modelled using lda for both march and april along with their description however the majority of the tweets were neutral in emotion 34 in march to 38 in april this indicates that tweets were mostly used to report facts and information rather than expression of an opinion on privacy concerns this is also supported by the fact that the tweet with the maximum engagement retweets favorites is a short description of an article about widespread surveillance and not an expression of sentiment figure 4 and figure 5 show the results of sentiment analysis for the month of march and april respectively expressed as percentages this study provides insight into privacy expectations of users on twitter during pandemic situations when technology is used to control outbreaks lower privacy concerns expressed in the initial lockdown period are replaced with a more negative sentiment towards governmental and organizational efforts to maintain privacy in health information disclosure over time more positive views towards surveillance in controlling the spread of covid-19 with country-specific discussion topics on public and private sector efforts for technological intervention in monitoring spread of the virus is replaced by post-pandemic privacy concerns topic modelling indicates that the discussions about privacy is usually geared more towards surveillance probably driven by the discussion around surveillance started by the electronic frontier foundation with a covid-19 surveillance article 3  followed by a new york times opinion article 10  this is supported by the hike in the number of tweets around march 23 2020 which is the date of the published article 3  further research on privacy concerns of contact-tracing in order to address a public health emergency like the novel coronavirus outbreak would help form policy around long-term continuous location tracking post the pandemic in order to delete such information after use while being instrumental in protecting public safety during the outbreak  a comprehensive investigation of the mrna and protein level of ace2 the putative receptor of sars-cov-2 in human tissues and blood cells yiliang wang yun wang weisheng luo lianzhou huang ji xiao feng li shurong qin xiaowei song yanting wu qiongzhen zeng fujun jin yifei wang   the 2019 novel coronavirus disease covid-19 is highly infectious 1 2 outbreak of which has been announced as a global pandemic by the world health organization on 11 march 2020 the pathogen contributed to covid-19 is severe acute respiratory syndrome coronavirus 2sars-cov-2 a sister of sars-cov 3 4 as showed by the center for systems science and engineering at johns hopkins university latest updated at 05202020 the global cumulative number of confirmed cases has reached 5019609 with 1983479 cured cases and 325855 deaths 5 however there were currently no effective drugs and vaccines against sars-cov-2 which was partly limited by the lack of recognition of the mechanism of sars-cov-2 infection previous studies had reported that sars-cov-2 used angiotensin-converting enzyme 2ace2 as the host receptor but not other coronavirus receptors such as aminopeptidase n and dipeptidyl peptidase 4 3 6 the rna binding domain of sars-cov-2 spike protein binds to ace2 with a 20-30-fold higher affinity than sars-cov 6-9 which may contribute to the rapid transmission of covid-19 therefore the distribution of ace2 in human tissues may indicate the susceptibility of different human organs to sars-cov-2 infection 10 some studies explored the heterogeneity of ace2 expression in specific tissue at the single-cell level 10-13 also there were several studies using transcriptome data to analyze the distribution of ace2 mrna in human tissues 14-17 however all these researches only analyzed the mrna level of ace2 while failed to determine the distribution of ace2 protein indeed given the complex composition of a tissue the result of single-cell rna-sequencing also cannot reflect the average abundance of ace2 in the whole tissues further ace2 is a membrane and secreted protein while the abundance of ace2 in blood and common blood cells remains uncertain to obtain the accurate distribution of ace2 in human tissues we comprehensively investigated the level of ace2 mrna and protein across human tissues and blood using public transcriptome datasets mass spectrum-based tissues proteomic and secretome and antibody-based immunochemistry ihc our study would have implications for understanding the transmission routes of sars-cov-2 as well as the pathogenesis and future treatment for sars-cov-2 to comprehensively investigate the mrna expression of ace2 in human tissues we collected the transcriptome datasets in three public databases including the tissue atlas of human protein atlas hpa genotype-tissue expression gtex and functional annotation of mammalian genomes 5 fantom5 cap analysis of gene expression cage 18-22 these data were separately analyzed according to the description in materials and methods given the samples of each tissue maybe come from the different individuals all rna-seq tissue data were present as mean protein-coding transcripts per million ptpm corresponding to mean values of the different individual samples from each tissue as indicated by the result of hpa ace2 mrna can be virtually detected in many tissues figure 1a the top 10 tissues with the highest abundance of ace2 mrna are small intestine ptpm 3663 duodenum 2645 gallbladder 1346 testis 1200 kidney 1072 heart muscle 311 colon 141 rectum 90 seminal vesicle 70 and thyroid gland 58 figure 1a most belonged to gastrointestinal tract-associated tissues or organs including the small intestine duodenum colon and rectum several were male reproductive system-associated organs including testis and seminal vesicle in accordance with the findings of previous publication 16 as showed by the result of gtex rna-seq data the top 10 tissue with the highest abundance of ace2 mrna were small intestine ptpm552 testis 367 adipose tissue 88 thyroid gland 70 kidney 68 heart muscle 65 colon 56 breast 44 ovary 24 salivary gland 18 and esophagus 18 figure 1b consistent with the result of hpa a relatively high abundance of ace2 mrna was observed in gastrointestinal tract-associated organs ie small intestine and colon the breast and female reproduction system-associated organs including breast and ovary also rank in the top 10 which is distinct from hpa figure 1b the vagina also expressed a low level of ace2 mrna with a ptpm of 14 which was close to the salivary gland and esophagus figure 1b the mrna expression of ace2 in tissue obtained from the fantom5 project was reported as scaled tags per million the top 10 tissue with the highest abundance of ace2 mrna in fantom5 project included small intestine scaled tags per million 4209 colon 1973 testis 923 gallbladder 526 heart muscle 316 kidney 315 thyroid gland 167 epididymis 102 ductus deferens 65 and adipose tissue 56 figure 1c of note despite small intestine and colon rank in top2 tissues with the highest abundance of ace2 mrna the male reproduction system-associated organs also occupied three of ten ie testis epididymis and ductus deferens figure 1c given the minor difference between these three data sets we analyzed the overlapped tissues among them using a venn diagram as demonstrated by the venn diagram there were six overlapped tissues with the top10 highest abundance of ace mrna including testis kidney heart muscle colon and thyroid gland figure 1d given the level of ace2 mrna cannot represent the abundance of a functional protein we investigated the distribution of ace2 protein in human tissues through analyzing the immunohistochemistry data from normal tissue in hpa and the public mass spectrometry ms-based proteomics data in human integrated protein expression database hiped the antibody-based protein profiles were analyzed by hpa which based on basic annotation and knowledge-based annotation to describe the rough relative abundance of ace2 proteins in various tissues detailed information can be obtained from the materials and methods as indicated by the quantitative result of immunohistochemistry there was a high abundance of ace2 protein in five tissues including the small intestine duodenum gallbladder kidney and testis figure 2a by contrast the adrenal gland colon rectum and seminal vesicle expressed a low level of ace2 protein figure 2a moreover ace2 protein was not detected in other tissues including the cerebral cortex cerebellum hippocampus caudate thyroid gland nasopharynx bronchus lung oral mucosa salivary gland esophagus stomach liver pancreas and urinary bladder etal based on the rough quantitative result of immunohistochemistry figure 2a next we analyzed the distribution of ace2 protein in human tissues using the ms-based proteomics dataset in hiped the level of ace2 protein was calculated based on the spectral counts the quantitative results suggested that the ace2 protein was enriched in the ovary log10ppm 22 urine 20 pancreatic juice 19 gut fetal 10 kidney 08 testis 07 heart 03 pancreas 03 placenta -03 heart fetal -07 and gallbladder -10 which were ordered by the abundance of ace2 protein figure 2b in particular ace2 protein was positively differentially expressed in that entity in the ovary urine and pancreatic juice as defined by the quantitative result in hiped figure 2b however the unique peptides of ace2 protein cannot be tested in both the fetal ovary and testis figure 2b moreover the ace2 protein also tested negative in human common blood cells including monocyte neutrophil b-lymphocyte t-lymphocyte cd4 t-cells cd8 t-cells nk cells and peripheral blood mononuclear cells figure 2b give ace2 is a membrane and secreted protein we analyzed the level of ace2 mrna in the common blood cells using the public transcriptome data for the common human blood cells sorted by the flow cytometry in hpa blood atlas 23 24 consistent with the proteomic data for ace2 protein ace2 mrna was also tested negative in virtually all blood cells including the basophil eosinophil neutrophil classical monocyte non-classical monocyte intermediate monocyte regulatory t-cells memory cd4 t-cell naive cd4 t-cell memory cd8 t-cell naive cd8 t-cell memory b-cell naive b-cell nk cell and total pbmc et al as indicated by the quantitative results figure 3a indeed such results were further supported by the transcriptome in other different databases including monaco scaled dataset and schmiedel dataset 25 26
supplement figure 1 we also obtained the abundance of ace2 protein from human blood using ms-based proteomics in peptideatlas 27-29 and proximity extension assays-based protein profiling in hpa blood atlas 23 24 based on the spectral counts of ms-based proteomics in the peptideatlas the concentration of ace2 protein in plasma approximately reached to 85 ngl figure 3b further as showed by the quantity results of proximity extension assays-based protein profiling the average concentration of ace2 protein in plasma from male showed a minor higher than those from female across four visits during one year figure 3c with the global outbreak of covid-19 the development of the drugs against sars-cov-2 had become an urgent work accomplishment of the virus life cycle largely depends on host factors therefore targeting the virus-host interactions and host cellular mechanisms are promising treatment options 30 on the theory the agents with the ability to target any step in the virus life cycle can be designed as antiviral drugs cell entry is the first step of cross-species transmission of the virus of which for sars-cov-2 was mediated by the spike proteins on its surface to bind to the ace2 receptor 3 6-9 based on the strategy of targeting ace2 several scientists have screened potential anti-sars-cov-2 drugs 31 32 therefore the comprehensive investigation of ace2 expression in human tissues has implications for understanding the transmission routes of sars-cov-2 and the development of anti-covid-19 drugs previous studies had investigated the distribution of ace2 mrna in human tissues and explored the expression heterogeneity among specific tissue using single-cell rna-sequencing but with a limited size of samples and a lack of determining the protein level of ace2 10-17 however given the great heterogeneity among humans the transcriptome and proteomic dataset with a larger number of samples and a wider range of tissues such as blood cells should be collected to analyze indeed a prior deep proteome and transcriptome of 29 healthy human tissues suggested a strong difference between mrna and protein quantities and that protein expression was often more stable across tissues than that of transcripts 33 therefore protein is a more accurate indicator than the mrna of reflecting the abundance of ace2 in this study we made a comprehensive investigation of the mrna and protein expression of ace2 in human tissues using public transcriptome proteomic datasets and antibody-based protein profiles given blood immune cells are also crucial for combating virus infection we analyzed the expression of ace2 mrna and protein in common blood cells and plasma despite all these databases utilized in this study based on rna-seq there is a minor difference among the transcriptome result obtained from different databases which may be caused by tissue source technical artifacts inherent in the respective methodologies and gene model annotation issues among them in detail the rna data in hap based on surgically removed tissues18 while those in gtex based on postmortem samples 19 20 moreover the mrna without polyadenylation tails are excluded in hpa leading to the absence of many histone genes while which are present in the fantom further the cap analysis gene expression peaks mapping more than 500 base pairs from the transcription start site are absent in the fantom also the peaks mapping more than one location on the genome are removed from fantom 34 of note we found that the lung expresses a low level of ace2 in both mrna and protein which seems to be controversial with the lung as the main tissue with the typical symptoms in response to sars-cov-2 infection such a result can be explained by several reasons as follows specifically and in particular as revealed by prior single-cell rna-sequencing ace2 expression positive is only observed in a small population approximately 1 of type ii alveolar cells while the remaining cell population in lung express a low level of ace2 10 11 which supported previous immunohistochemistry 35 however sars-cov-2 infection remarkably induces the expression of ace2 as an interferon-stimulated gene in human airway epithelial cells 36 37 therefore the type ii alveolar cells would represent a basic target of sars-cov-2 in the lung the infection of sars-cov-2 in type ii alveolar cells upregulates the level of ace2 in the lung and thereby further facilitates the infection of sars-cov-2 indeed we also cannot exclude the possibility that sars-cov-2 used other unknown factors as a receptor that may be highly expressed in lung especially given that previous study had revealed that both tmprss2 and cd147 also partly mediated the cell entry of sars-cov-2 32 38 also a host restriction factor that lowly expressed in the lung for sars-cov-2 cell entry needs to be considered indeed in accordance with previous research 39 we noted that a549 lung alveolar cells expressed a low level as indicated by hpa cell atlas data not show implying the cell line is not an ideal model to study sars-cov-2 in vitro the in vitro model of sars-cov-2 infection should be established using a549 with exogenous ace2 or other primary lung-derived cells such as normal human bronchial epithelial cells 39 both testis and kidney tissues expressed a high level of ace2 mrna and protein which is consistent with previous publications 13 16 such results may explain the damage of testis and the impairment of male gonadal function caused by sars-cov-2 40 moreover both antibody-based ihc and tissue transcriptome data also showed a high abundance of ace2 in the small intestine which is supported by a single-cell transcriptome of revealing that the digestive system may be an important route of sars-cov-2 transmission 12 the high expression level of ace2 in the gastrointestinal tract may explain why most of covid-19 patients show gastrointestinal symptoms in the early stage of the infection 41 however the proteomic data of the small intestine is not available in the hiped by contrast some tissues harbor with a high abundance of ace2 mrna but show a low level of ace2 protein for example ace2 mrna can be detected in the bladder which is consistent with a previous study 13 while no ace2 protein was observed in urinary bladder as indicated by the quantitative result of proteomic data and antibody-based ihc given there was a divergence toward the potential of intrauterine vertical transmission in women who develop covid-19 pneumonia during pregnancy 42-44 we also paid attention to the expression of ace2 in the female reproduction-associated tissues of note the quantitative result of transcriptome supported that ovary virtually did not express ace2 while ovary was the organ with the highest level of ace2 protein as indicated by the proteomic data the placenta also expressed a high level of ace2 protein based on these results the intrauterine vertical transmission potential of sars-cov-2 cannot be underestimated despite uterus was tested negative for ace2 protein of note all the transcriptome in different database revealed no ace2 mrna and protein in the common blood cells including basophil eosinophil neutrophil classical monocytes non-classical monocyte treg gd-t cell mait t-cell memory cd4 t-cell nave cd4 t-cell memory b-cell nave b-cell plasmacytoid dc myeloid dc nk cell and total pbmc suggesting the potential of resistance of immune cell against sars-cov-2 however these results did not suggest that viral particles cannot survival from blood because the public human secretome suggested the concentration of ace2 protein in plasma is approximately 85 ngl indeed according to the description in the latest new coronavirus pneumonia prevention and control program published by the national health commission of china the nucleotides of sars-cov-2 can be tested from the blood sample of patients sars-cov-2 infection also caused a remodeling myeloid in severe covid-19 patients 45 interestingly a prior study reported that covid-19 susceptibility seems to be related to blood group whereas whether such results are associated with the level of ace2 remains uncertain 46 further although our result found that there was virtually no ace2 mrna and protein in the central nervous system cns including the cerebral cortex brain cerebrospinal fluid and cerebellum the possibility of cns infection of sars-cov-2 cannot be neglected indeed accumulating evidence supported the neuroinvasive potential of sars-cov2 47 48 there are also several studies have shown that ace2 enzymatic activity can be detected in human brain tissue and csf samples 49 indicating that ace2 is expressed and functional in the cns of humans the way the tissue is fixed or processed can affect the result of ihc or proteomics of these databases specifically there is no need for an autopsy to study human blood by contrast unlike the data from blood cells for tissue like brain it required autopsy study in summary our study reveals that the tissue distribution of ace2 mrna and protein might differ and their correlation is complex however our study was limited by only analyzed public datasets with a lack of confirmation by experiments nevertheless our study would be beneficial for understanding the risk of different human organs vulnerable to sars-cov-2 infection to obtain the comprehensive information regarding the transcriptome of human tissues we collected the transcriptome from three public databases including the human protein atlas hpa tissues atlas genotype-tissue expression gtex project and functional annotation of mammalian genomes 5 fantom5 project all these databases update irregularly and the version of them we analyzed is the latest in detail the hpa shows the expression of human proteins across tissues and organs based on deep rna-sequencing from 37 major different normal tissue types 18 the mrna data in the hpa tissue atlas provide quantitative data on the average gene expression within an entire tissue to estimate the transcript abundance of each protein-coding gene the hpa integrates rna and protein expression data corresponding to approximately 80 of the human protein-coding genes with access to the primary data for both the rna and the protein analysis on an individual gene level the gtex project includes genotype data from approximately 714 donors and 11688 rna-seq samples across 53 tissues 19 20 rna-seq data from 36 of their tissue types were mapped based on rsemv1222 v7 and the resulting tpm values have been included in hpa for all corresponding genes both hpa and gtex rna-seq tissue of the protein-coding gene is reported as mean ptpm protein-coding transcripts per million corresponding to mean values of the different individual samples from each tissue the fantom5 project provides comprehensive expression profiles using cap analysis of gene expression cage 21 22 which is based on a series of full-length cdna technologies developed in riken cage data for 60 of their tissues were obtained from the fantom5 repository and mapped to ensembl the normalized tags per million for each gene were calculated in hpa the hpa tissue atlas shows the expression of human proteins across tissues and organs based on immunohistochemistry on tissue microarrays containing 44 different tissues 18 50 annotated protein expression profiles were obtained by single antibodies or independent antibodies two or more independent antibodies with non-overlapping epitopes on the same protein for independent antibodies the immunohistochemical data from all the different antibodies were taken into consideration to obtain a comprehensive overview of protein expression patterns in normal human tissues the basic annotation was combined with knowledge-based annotation to determine the rough relative abundance of proteins in these tissues as calculated by hpa tissue atlas basic annotation parameters include an evaluation of i staining intensity negative weak moderate or strong ii fraction of stained cells 25 25-75 or 75 and iii subcellular localization nuclear andor cytoplasmicmembranous knowledge-based annotation was achieved by stringent evaluation of immunohistochemical staining pattern rna-seq data from internal and external sources and available proteingene characterization data with special emphasis on rna-seq all immunohistochemical images are available and the annotation data can be found under primary data in hpa hiped is an integrated proteomics platform residing within genecards which involved 69 normal anatomical entities tissues cells and fluids from four databases including proteomicsdb moped paxdb and maxqb 51 the ppm protein values were calculated for each sample if not provided so by data sources the intensity-based absolute quantification ibaq expression values were divided by the sum of values of each sample and multiplied by 1000000 ibaq is a proxy for protein abundance levels 52 for all samples data was gene centrically aggregated by summing expression values of all isoforms for each gene samples from similar tissues were averaged using geometric mean for better visualization of graphs expression values are drawn on a root scale which is an intermediate between log and linear scales 51 the protein expression images present a protein expression vector for each gene based on normalized abundances in 69 normal human anatomical entities protein differential expression provides a list of anatomical entities for which a gene is positively differentially expressed based on the 69 integrated normal proteomics datasets in hiped genes with fold change value 6 and protein abundance value 01 ppm in an anatomical entity are defined as positively differentially expressed in that entity fold change values were calculated as the ratio between the tested dataset protein abundance and the average of all datasets the blood atlas contains single cell type information on genome-wide rna expression profiles of human protein-coding genes covering 18 cell types obtained by fluorescence-activated cell sorting 23 these cells include various b-cells t-cells nk-cells monocytes and dendritic cells the rna expression for each gene was analyzed by the online tools resided in hpa blood atlas the public transcriptome datasets from schmiedel bj 25 and monaco g 26 were also collected to analyze the mrna expression of ace2 in human common blood cells give ace2 is a membrane and secreted protein in the blood we further analyzed its abundance in the human blood using public ms-based proteomics in peptideatla 27-29 and hpa blood atlas 24 an analysis of the proteins detected in human blood was presented with an estimation of the respective protein concentrations determined with ms-based proteomics and proximity extension assays-based protein profiling hpa blood atlas provides the protein concentration in plasma based on proximity extension assays olink for a longitudinal wellness study covering 86 individuals with four visits during one year at three months intervals protein expression levels are reported as normalized protein expression npx  a database of geopositioned middle east respiratory syndrome coronavirus occurrences rebecca ramshaw e ian letourneau d amy hong y julia hon julia morgan d joshua osborne c shreya shirude maria van kerkhove d simon hay i david pigott m   middle east respiratory syndrome coronavirus mers-cov emerged as a global health concern in 2012 when the first human case was documented in saudi arabia1 now listed as one of the who research and development blueprint priority pathogens cases have been reported in 27 countries across four continents2 imported cases into non-endemic countries such as france great britain the united states and south korea have caused secondary cases35 thus highlighting the potential for mers-cov to spread far beyond the countries where index cases originate reports in animals suggest that viral circulation could be far more widespread than suggested by human cases alone68 to help prevent future incidence of mers-cov public health officials can focus on mitigating zoonotic transfer however in order to do this effectively additional research is needed to determine where spillover could occur between mammals and humans previous literature reviews have looked at healthcare-associated outbreaks9 importation events resulting in secondary cases1011 occurrences among dromedary camels1213 or to summarize current knowledge and knowledge gaps of mers-cov1415 this database seeks fill gaps in literature and build upon existing notification data by enhancing the geographic resolution of mers-cov data and providing occurrences of both mammal and environmental detections in addition to human cases this information can help inform epidemiological models and targeted disease surveillance both of which play important roles in strengthening global health security knowledge of the geographic extent of disease transmission allows stakeholders to develop appropriate emergency response and preparedness activities httpswwwjeeallianceorgglobal-health-security-and-ihr-implementationjoint-external-evaluation-jee inform policy for livestock trade and quarantine determine appropriate demand for future vaccines httpcepinetmission and decide where to deliver them additionally targeted disease surveillance will provide healthcare workers with updated lists of at-risk countries patients with a history of travel to affected regions could then be rapidly isolated and treated thus reducing risk of nosocomial transmission this database is comprised of 861 unique geo-positioned mers-cov occurrences extracted from reports published between october 2012 and february 2018 it systematically captures unique occurrences of mers-cov globally by geo-tagging published reports of mers-cov cases and detections data collection database creation and geo-tagging methods are described below instructions on how to access the database are provided as well with the aim to contribute to future epidemiological analysis all data is available from the global health data exchange16 and figshare17 we identified published reports of mers-cov by searching pubmed web of science and scopus with the following terms middle eastern respiratory syndrome middle east respiratory syndrome merscov and mers the initial search was for all articles published about mers-cov prior to april 30 2017 and was subsequently updated to february 22 2018 these searches were conducted through the university of washington libraries institutional database subscriptions we searched the web of science web of science core collection the subscribed edition includes science citation index expanded 1900-present social sciences citation index 1975-present arts  humanities citation index 1975-present emerging sources citation index 2015-present we searched the standard scopus database and the standard freely available pubmed database these products have a single version that is consistent across institutional subscriptions or access points in total this search returned 7301 related abstracts which were collated into a database before a title-abstract screening was manually conducted fig 1 flowchart articles were removed if they did not contain an occurrence of mers-cov for example vaccine development research or coronavirus proteomic analyses non-english articles were flagged for further review and brought into the full text screening stage the accompanying supplementary file highlight the title and abstract screening process and the inclusion and exclusion criteria full text review was conducted on 1083 sources to meet the inclusion criteria articles must have contained both of the following items 1 a detection of mers-cov from humans animals or environmental sources and 2 mers-cov occurrences tagged with spatial information additionally extractors attempted to prospectively manually remove articles containing duplicate occurrences that were already extracted in the dataset extractors only prospectively manually removed articles if it was clear the articles contained data we were confident had already been extracted and had high-quality data we excluded 885 sources based on full text review in addition we reviewed citations and retroactively added relevant articles to our database if they were not already included we retroactively added and subsequently marked ten articles for extraction using this process in total we extracted 208 peer-reviewed sources reporting detection of mers-cov that included geographic and relevant epidemiological metadata google maps or arcgis23 was used to manually extract location information at the highest resolution available from individual articles we evaluated spatial information as either points or polygons the geography was defined as a point if the location of transmission was reported to have occurred within a 5  5 km area point data are represented by a specific latitude and longitude a point references an area smaller than 5  5 km in order to be compatible with the typical 5  5 km resolution of satellite imagery used for global analyses the geography was defined as a polygon if the location of transmission was less clear but known to have occurred in a general area eg a province or the location of transmission occurred within an area greater than 5  5 km eg a large city we used contextual information to determine location in instances where the authors spelling of a location differed from google maps or arcgis maps provided by authors were digitized using arcgis we used three different types of polygons known administrative boundaries buffers and custom polygons relevant administrative units were sourced from the global administrative unit layers curated by the food and agricultural organization of the un24 for known administrative boundaries of governorates districts or regions and paired with the occurrence record buffers were created to encompass areas in cities and regions without corresponding administrative units to ensure that buffers encompassed the entirety of the area of interest google maps was used to determine the required radius in areas with unspecified boundaries eg table mountain national park and the border region between saudi arabia and uae arcgis was used to generate custom polygons which were assigned a unique code within a defined shapefile for ease of re-identification this database is publicly available online1617 each of the 861 rows represents a unique occurrence of mers-cov rows containing an index unspecified or imported case represent a single case of mers-cov rows containing mammal and secondary cases may represent more than one case but are still unique geospatial occurrences table 1 shows an overview of the content available in the publicly available dataset in addition online-only table 1 lists occurrences by geography origin 405 shape type and publication and online-only table 2 provides citations of the datanid a unique identifier assigned to each publication that was extractedtitle title of the publicationauthor articles authorsdoi articles doiabstract articles abstract if availablesourcetitle journal in which the article was publishedyear articles publication yearsource database where article was foundpmidifapplicable pmid if the article is from pubmedfulltextlinkifincluded link to the full text if availablefileid reference to pdf in format firstauthoryear eg smith2017occid unique identifier assigned to each occurrence of mers-cov a single pdf may represent more than one occurrence each row will have its own occid starting at 1 and numbered consecutively to 883organismtype what type of organism tested positive for mers-cov human mammal or environmentalorganismspecific specifies the exact organism that tested positive for mers-cov names are made consistent with wilson and reeder 2005 mammal species of the world25pathogen name the pathogen identified eg mers-cov bat coronaviruses and other mers-cov-like pathogenspathogennote miscellaneous notes regarding pathogenpatienttype index unspecified na secondary import or absentindex any human infection of mers-cov resulting after direct contact with an animal and no reported contact with a confirmed mers-cov case or healthcare settingunspecified cases that lacked sufficient epidemiological evidence to classify them as any other status eg serosurvey studiesna non-applicable field case was not a patient eg mammalsecondary defined as any cases resulting from contact with known human infections cases reported after the index case can be assumed to be secondary cases unless accompanied by specific details of likely independent exposure to an animal reservoirimport cases that were brought into a non-endemic country after transmission occurred elsewhereabsent suspected cases ultimately confirmed negative for mers-cov18transmissionroute zoonotic direct unspecified or animal-to-animalzoonotic transmission occurred from an animal to a humandirect only relevant for human-to-human transmissionunspecified lacked sufficient epidemiological evidence to classify a human case as zoonotic or directanimal-to-animal transmission occurred from an animal to another animal19clinical describes whether the mers-cov occurrence demonstrated clinical signs of infection denoted by yes no or unknownyes clinical signs of infection were presentreported clinical signs among humans may range from mild eg fever cough to severe eg pneumonia kidney failure clinical signs among camels include nasal dischargeno clinical signs of infection were not presentreportedunknown subjects may or may not have been demonstrating clinical signs of infection for example some authors did not explicitly mention symptoms but individuals reportedly sought medical care another example being when a diagnostic serosurvey was conducted during an ongoing outbreak the term unknown was used when articles lacked sufficient evidence for extractors to definitively label as yes or no20diagnostic describes the class of diagnostic method that was used pcr serology or reported21diagnosticnote more detailed information related to the specific test used eg rk39 igg or igm serology22serosurvey describes the context if serological testing was useddiagnostic testing of symptomatic patientsexploratory historic exposure determined among healthy asymptomatic individuals23country iso3 code for country in which the case occurred24origin open-ended field to provide more details on the specific in-country location of mers-cov case25problemgeography this field was utilized if the mers-cov case was reported in a location that could cause uncertainty when determining exact geographic occurrence eg hospital abattoir26lat latitude measured in decimal degrees27long longitude measured in decimal degrees28latlongsource the source from which latitude and longitude were derived29locconfidence states the level of confidence that researchers had when assigning a geographic location to the mers-cov case good or bad an answer of good meant the article stated clearly that the case occurred in a specific geographic location and no assumptions were required on part of the researcher an answer of bad meant the article did not clearly state the specific geographic location of the mers-cov case but the researcher was able to infer the location of occurrence the field sitenotes was utilized to detail the logic behind researchers decisions when inference was required30shapetype the geographic shape type assigned to the mers-cov occurrence point or polygon31polytype if the mers-cov occurrence was assigned a shapetype of polygon was it admin gaul custom or buffer32bufferradius if a mers-cov occurrence was assigned a buffer what is the radius in km33gaulyearorcustomshapefile file path used to reach the necessary shape file in arcgis users of this dataset can find custom shapefiles created for this dataset at httpscloudihmewashingtoneduindexphpsdgoykyqnbjg54f2download34polyid a standardized and unique identifier assigned to each gaul shapefile35polyfield which type of polygon was used to geo-position the occurrence eg if admin1 polygon was used enter adm1code36sitenotes miscellaneous notes regarding the site of occurrence37monthstart month that the occurrences began if the article provided a specific month of illness onset the month was assigned a number from 112 1  january 2  february etc if the article did not provide a specific month of illness onset then researchers assigned a value of na38monthend month that the occurrences ended defined as the date a patient tested negative for mers-cov if the article provided a specific month for recovery the month was assigned a number from 112 1  january 2  february etc if the article did not provide a specific month of symptom onset then researchers assigned a value of na39yearstart year that the occurrences began if the year of illness onset was not provided in the article the ihme standard was usedyearstart  publication year  340yearend year that the occurrences ended if the article did not provide a specific year for recovery the ihme standard was usedyearend  publication year  141yearaccuracy if years were reported this field was assigned a value of 0 if assumptions were required this field was assigned a value of 1 figures 24 show the geographic distribution of the mers-cov occurrence database with distinctions made by epidemiological and geographic metadata all data extracted from the original search october 2012 to april 30 2017 was reviewed independently by a second individual to check for accuracy challenging extractions from the updated search may 1 2017 to february 22 2018 were selected for group review during bi-weekly team meetings upon extraction completion all data were checked to ensure they fell on land and within the correct country while the protocol implemented above was designed to reduce the amount of subjective decisions made by extractors total elimination was not possible wherever a subjective decision had to be made the extractor utilized the various notes fields in order to document the logic behind decisions these decisions were subsequently reviewed by other extractors the techniques described here can be applied to collect and curate datasets for other infectious diseases as has been previously demonstrated with dengue20 and leishmaniasis18 additionally since these data were collected independently through published reports of mers-cov occurrence they may be used to build upon existing notification data2627 our ability to capture occurrences in this dataset is contingent on the data contained within published literature therefore this dataset does not represent a total count of all cases instead this datasets value lies within its geo-precision data were extracted with a focus on obtaining the highest resolution possible these data may be merged with other datasets such as who26 or oie27 surveillance records and are intended to complement not replace these resources together published reports and notification data can provide a more comprehensive snapshot of current disease extent and at-risk locations an important consideration whether using the literature data alone or in combination with other databases is the potential for duplication various pieces of metadata can be used to evaluate where potential duplicates could lie such as common date fields monthstart monthend yearstart yearend or consistent geographic details lat long polyid shapetype or shared epidemiological tags patienttype researchers may wish to consider further steps such as fuzzy matching of geographic data eg matching a point with an overlapping buffer or temporal data eg matching a precise month with an overlapping month interval we acknowledge this duplicate-removal process will not catch all matching records but it will likely catch several we recommend this approach because it will allow researchers to remove several duplicates without erroneously deleting any two occurrences that are truly unique ie not duplicates essentially we recommend a sensitive approach above a more specific approach as the latter simply risks culling too many records that arent actually duplicates when merging with other databases consistency in metadata tagging is essential for the who disease outbreak news data feed2627 for instance nomenclature for case definitions is slightly different with who definitions of community acquired and not reported comparable to index and unspecified respectively in addition it is important to recognize what information is beyond the scope of these additional databases again when comparing to the who dataset it is important to recognize that serologically positive cases do not meet the case definition used in the who database these adjustments need to be identified on a dataset-to-dataset basis this database can be combined with other covariates eg satellite imagery to produce environmental suitability models of mers-cov infection risk and potential spillover on both global and regional scales as achieved with other exemplar datasets2831 this information can be useful in resource allocation aimed at improving disease surveillance and contribute towards a better understanding of the factors facilitating continued emergence of index cases the addition of sampling techniques and prevalence data may improve this dataset researchers were ultimately unable to add these data due to inconsistencies in the way literature reported sampling techniques and prevalence date by geography an attempt to extract these data using the current approach would have led to sporadic inclusion of this information and would not have been comprehensive for the entire dataset moving forward we recommend authors report sampling technique and prevalence data at the highest resolution geography possible as seen in miguel et al32 we encourage continued presentation of paired epidemiological and geographic metadata that would allow for more detailed analysis in the future this database may also be utilized in clinical settings to provide an evidence-base for diagnoses when used in conjunction with patient travel histories additionally it can be used to identify geographies for surveillance particularly areas where mers-cov has been documented in animals but not humans eg ethiopia and nigeria identifying locations for surveillance will in turn inform global health security while models will increase the resolution at which these questions can be addressed datasets such as this provide an initial baseline a major limitation of this database is the potential for sampling bias which stems from higher frequency of disease reporting within countries where there exists strong healthcare infrastructure and reporting systems this database does not attempt to account for such biases which must be addressed in subsequent modelling activities where such biases are of consequence similarly another limitation is potential duplicate documentation of singular occurrences this can happen when the same occurrence is assigned different geographies eg point polygon in multiple publications even though extractors made efforts to prospectively manually identify duplicate occurrences this was challenging because the process relied upon papers providing sufficient details for extractors to determine a duplicate occurrence eg geography patient demographics dates of occurrence diagnostic methods etc however the majority of papers did not report such details for each occurrence in those instances it was impossible for extractors to discern whether occurrences may have been duplicates from a previous artic le even case studies inconsistently reported patient details and demographic information these are some examples of challenges faced by extractors when we attempted to identify duplicates without sufficient contextual clues extractors lacked evidence to determine duplicity and thus likely extracted some unique occurrences more than once despite efforts to remove duplicate occurrences from the database it is possible that some remain geographic uncertainty is similarly problematic for analyses such as this in some cases polygons as opposed to points are utilised as a geographic frame of reference reflecting the uncertainty in geotagging in the articles themselves for some occurrences there is a strong assumption that the geography listed corresponds to the site of infection while the use of 5 km  5 km as the minimum geographical unit allows for some leeway in this precision it is possible that even with the point data often corresponding to household clusters these may not map directly with true infection sites this must be considered in any subsequent geospatial analysis finally this database represents a time-bounded survey of the literature while all efforts were made to be comprehensive within this period articles and therefore data will continue to be published efforts to streamline ongoing collection processes are still to be fully realized33 regardless we hope that this dataset provides a solid baseline for further iteration  feeling like it is time to reopen now covid-19 new normal scenarios based on reopening sentiment analytics jim samuel  rahman mokhlesur g ali g nawaz yana samuel alexander pelaez  the coronavirus pandemic has created complex challenges and adverse circumstances this research discovers public sentiment amidst problematic socioeconomic consequences of the lockdown and explores ensuing four potential sentiment associated scenarios the severity and brutality of covid-19 have led to the development of extreme feelings and emotional and mental healthcare challenges this research identifies emotional consequences -the presence of extreme fear confusion and volatile sentiments mixed along with trust and anticipation it is necessary to gauge dominant public sentiment trends for effective decisions and policies this study analyzes public sentiment using twitter data time-aligned to covid-19 to identify dominant sentiment trends associated with the push to reopen the economy present research uses textual analytics methodologies to analyze public sentiment support for two potential divergent scenarios -an early opening and a delayed opening and consequences of each present research concludes on the basis of exploratory textual analytics and textual data visualization that tweets data from american twitter users shows more trust sentiment support than fear for reopening the us economy with additional validation this could present a valuable time sensitive opportunity for state governments the federal government corporations and societal leaders to guide the nation into a successful new normal future  the anxiety stress financial strife grief and general uncertainty of this time will undoubtedly lead to behavioral health crises -coe and enomoto mckinsey on the covid-19 crisis 1  covid-19 has become a paradigm shifting phenomenon across domains and disciplines affecting billions of people worldwide directly or indirectly the current coronavirus pandemic disaster has led to escalating emotional and mental health issues with significant consequences and this presents a serious challenge to reopening and recovery initiatives 2  in the united states us alone covid-19 has infected well over 125 million people and killed over 80000 and continues to spread and claim more lives 3  moreover as a result of the lockdown present research uses lockdown as the term representing the conditions resulting from state and federal government novel coronavirus response regulations and advisories restricting government organizational personal travel and business functionalities over 30 million people lost their jobs along with a multi-trillion dollar economic impact 4  furthermore these alarming numbers are growing everyday there is tremendous dissatisfaction among common people due the continued physical material and mental health challenges presented by the lockdown evidenced by the growing number of protests across the us 5  there appears to be a significant sentiment and a strong desire in people to go back to work satisfy basic physical mental and social needs and eagerness to earn money as shown in exploratory public sentiment graph in fig 1  however there is also a significant sentiment to stay safe and many prefer the stay-at-home lockdown measures to ensure lower spread of the coronavirus 6  while politicians may have vested interests state governments and the federal government cannot ignore public sentiment therefore to a consequential measure the reopening of america and its states and regions will be influenced by perceived public sentiment hence it is important to address the question what is the dominant public sentiment in america concerning the reopening and in some form going forward into a new normal new normal is a term being used to represent the potentially transformed socioeconomic systems 7  as result of covid-19 issues amidst the likelihood of multiple future waves of the coronavirus pandemic there is a fear that the current coronavirus wave will see a spike in covid-19 infections with reopening and associated loosening of lockdown restrictions the present study analyzed public sentiment using tweets from the first nine days of the month of may 2020 with a keyword reopen from users with country denoted as usa to gauge public sentiment along eight key dimensions of anger anticipation disgust fear joy sadness surprise and trust as visualized in fig 1  twitter data and tweets text corpus have been widely used in academic research and by practitioners in multiple disciplines including education healthcare expert and intelligent systems and information systems 8 9 10 11 12  sentiment analysis with tweets presents a rich research opportunity as hundreds of millions of twitter users express their messages ideas opinions feelings understanding and beliefs through twitter posts sentiment analysis has gained prominence in research with the development of advanced linguistic modeling frameworks and can be performed using multiple well recognized methods and tools such as r with readily available libraries and functions and also through the use of custom textual analytics programming to identify dominant sentiment behavior or characteristic trait 13 14 15  tweets analytics have also been used to study and provide valuable insights on a diverse range of topics from public sentiment politics and pandemics to stock markets 16 17 18  there are strong circumstantial motivations along with urgent time-sensitivity driving this research article firstly in the us alone we have seen over 89000 deaths and over 14 million covid-19 cases at the point of writing this manuscript and the numbers continue to increase secondly there have been significant economic losses and mass psychological distress due to unprecedented job loss for tens of millions of people these circumstantial motivations led us to our main research motivation discovery of public sentiment towards reopening the us economy the key question we seek to address is what are the public sentiments and feelings about reopening the us economy public sentiment trends insights would be very useful to gauge popular support or the absence thereof for any and all state level of federal reopening initiatives scientists researchers and physicians are suggesting that the reopening process should be on a controlled phased and watchful basis the general recommendation is it should be done into three phases 19  phase 1 slow down the virus spread through complete lockdown and very strict measures for instance mandatory stay-at-home orders which have been active in many states phase 2 reopen on a state-by-state business-by-business and block-byblock basis with caution people would still be required to maintain social distancing use ppe personal protective equipment and strict public hygiene 20  all of these would need to be implemented while protecting the most vulnerable sections of the population using potential strategies such as the relax-at-home or safe-at-home mode moreover the states would be expected to increase testing tracing and tracking of covid-19 cases and ensure ample ppe supplies phase 3 it would be possible for people to go back to near-normal pre-coronavirus lifestyles when proven vaccines andor antibodies become commonly available and mass-adoption sets in early research shows that 90 of transmissions have taken place in closed environments namely home workplace restaurants social gatherings and public transport and such prevalence in critical social spaces contributed to increased fear sentiment 21 22  from a current sentiments analysis perspective since a large ratio of people need to start work return to their businesses and jobs and restart the economy a quick reopening is perceived as being strongly desirable however there is fear regarding a potential second outbreak of the covid-19 pandemic and this presents a cognitive dilemma with implications for mental health and emotional conditions information and information formats have an impact on human sentiment behavior and performance and it is possible to associate underlying feeling or belief to expressed performance and communication 23  the present research addresses the covid-19 public sentiment trend identification challenge by generating insights on popular sentiment about reopening the economy using publicly available tweets from users across the united states these publicly downloadable tweets were filtered with reopen as the keyword and insights were generated using textual data visualization starting with the word cloud figure 2 to gain a quick overview of the textual corpus as such textual visualizations have the potential to provide both cross-sectional and narrative perspectives of data 24  textual analytics were used to discover most frequently used words phrases and prominent public sentiment categories the present research is exploratory in nature and applied in its focus given the urgency of the covid-19 situation in the united states and worldwide this study has reluctantly forfeited the intellectual luxury of time consuming adoption and integration of theoretical frameworks which would satisfy academic rigor but would forego contributing practically to critically time-sensitive and desperately needed covid-19 recovery and reopening solutions consequentially this research is part of our covid-19 solutions research stream which has enthusiastically embraced the generation of insights using rapid exploratory analytics and logical induction with timely and significantly practical implications for policy makers local state and federal governments organizations and leaders to understand and pre-pare for sentiment-sensitive scenarios this research has thus aimed to discover the most critical insights optimized to time constraints through sentiment-sensitive reopening scenarios analysis the rest of the paper is organized as follows section 2 highlights the scenario analysis of past and current events and public sentiment section 3 demonstrates the adopted method for analyzing public sentiment section 4 is dedicated for in-depth discussion pointing out limitations and opportunities of this research finally section 5 concludes this paper extant research has illustrated the dramatic growth in public fear sentiment using textual analytics for identifying dominant sentiments in coronavirus tweets 17  public fear sentiment was driven by a number of alarming facts as described in the subsections below to start with we list a number of initiatives aimed at understanding explaining and predicting various aspects of the coronavirus phenomena a number of works made early prediction of the spread of covid-19 25 26  zhong et al 25 made the early prediction of the spread of the coronavirus using simple epidemic model called sir susceptible-infected-removed dynamic model the model works with two key parameters the infection rate and the removal rate the infection rate is the number of infections by one infective by unit time and the removal rate is the ratio of the removed number to the number of infectives where the removed number includes recovered and dead infectives y aboelkassem 26 used a hill-type mathematical model to predict the number of infections and deaths and the possible re-opening dates for a number of countries the model works based on the three main estimated parameters the steady state number the saturation point the number of days when the cases attain the half of the projected maximum and hill-exponent value li et al 27 categorized covid-19 related social media post into seven situational information categories they also identify some key features in predicting the reposted amount of each categorical information the predicting accuracy of different supervised learning methods are different the accuracy of svm support vector machine naive bayes and random forest are 54 45 and 65 respectively in the us concurrent to the physical healthcare problem extant research observes the mass fear sentiment about cornonavirus and covid-19 phenomena to be on a significant growth curve from the time the study started tracking the sentiment in the february and climbing steeply towards march 2020 17  according to center for disease control and prevention cdc this was also around the time that a massive number of people started seeking medical attention for covid-19 conditions 28  fig 3a shows the percentage of patients visits for covid-19-like illnesses cli and influenza-like illnesses ili compared to the total number of emergency department visits from december 1 2019 to april 26 2020 in the unites states source cdc 28  it clearly indicates that a significant number of people began using emergency facilities for cli and ili from around march of 2020 and this corresponds with the growth fear and panic sentiments associated with covid-19 however by late april we see the emergency visits curve relaxing along with a decline in the number of new infections in many states in the us fig 3b exhibits covid-19-associated weekly hospitalizations per 100000 of the us population among different age categories from march 04 to may 02 2020 in the us it shows that from the second week of march a significant number of people aged over 65 years needed to be hospitalized and the hospitalizations count peaks by the middle of april it clearly shows that this age group is amongst the most vulnerable to the covid-19 outbreak the age group of 50-64 years follows as the second most impacted and the age group of 18-49 years follows as being the third most impacted however covid-19 had a very limited impact on the 0-17 years age group healthcare experts and researchers continue to monitor develop and apply solutions that are helping physical recovery from a current mental healthcare and sentiment tracking perspective we did not find any reports highlighting changes to early stage covid-19 fear and panic sentiment nor clear updates on sentiment trends from extant literature hence having identified an important gap this present research identifies changes in public sentiment by collecting and analyzing twitter data for the first part of may 2020 as described in the methods section  the whole is often greater than the sum of its parts -this adage holds true regarding the adverse collective psychological sentimental and mental healthcare impact of the coronavirus and covid-19 phenomena on human society and societal structures while there was a significant growth in fear and anxiety on the individual level collectively as a society these took the shape of panic and despair as evidenced in panic-buying driven shortages of items in super markets for which there were no supply chain disruptions indicating that these shortages were driven by adverse public sentiment certain american states and regions were more drastically impacted than the others covid-19 had a severe impact on new york and new jersey followed by massachusetts illinois michigan and california as shown in fig 4 source worldometers 3  so far new york and new jersey have seen the maximum number of deaths 4a interestingly if we notice the death of despair prediction for 2020-2029 as shown in fig 4b  new york and new jersey are not in the most affected list instead some states which perhaps have less job opportunity and higher older population will suffer the most source well being trust  the robert graham center 29 the prediction shows that the most likely states to bear a negative impact in the long run will be new mexico nevada wyoming oregon florida and west virginia due to the looming socioeconomic fallout of covid-19 this implies a longer term and more complex mental healthcare challenge as states begin their journey to recovery -it will be vital for governments and relevant organizations to track understand and be sensitive to shifts in public sentiment at the local and national levels the coronavirus pandemic has caused significant global disruptions leading to continuing losses valued at trillions of us dollars due to the closure of many businesses and entire industries such as hospitality and air travel however confusion is rampant due to counter efforts by governments nationalized monetary policy interventions and large scale financial assistance to individual citizens organizations and businesses for example the us government has successfully provided multi-trillion dollar stimulus checks small business assistance and a wide range of concessions and equity market support mechanisms such initiatives make it difficult for investors and individuals to gauge fundamental value of many assets as government policy intervention can support systemic market shifts and disrupt forecasts risks in the global financial market have elevated substantially due to investors anxiety in this pandemic crisis and assets price variability 30 31 32  data analysis on daily covid-19 cases and stock market returns for 64 countries demonstrated a strong negative return with increasing confirmed cases 33  the stock market reacted strongly in the early days of confirmed cases compared to a certain period after the initial confirmed cases in contrast the reaction of individual stock markets is closely related to the severity of the local outbreaks which leads to economic losses high volatility and unpredictable financial markets and tweets textual analytics have been used to gauge associated sentiment 32 18  all of these covid-19 effects lead to a delicate market equilibrium immersed in a fair degree of confusion positively supported by expectation of government intervention and limited by the negative consequences of covid-19 also covid-19 has caused a severe interruption in the functioning of businesses and institutions leading to loss of employment diminished income opportunities and disruption of labor markets leading to significant increase in distress 34  a prolonged pandemic may lead to mass unemployment and irreversible business failures 32  according to the international labor organization ilo the global unemployment rate will increase by 313 subsequently it will increase the underemployment and reduce the economic activities such as global tourism 35  adams-prassl et al 34 studied the comparative job losses and found that about 18 and 15 of people lost their jobs in early april in the us and uk respectively whereas only 5 people lost their job in germany they also predicted that the probability of losing the job of an individual is about 37 in the us and 32 in the uk and 25 in germany in may evidently people are worried and anxious about their livelihood and future income driving a sense of urgency for reopening the economy fig 5 demonstrates how the unemployment situation in the us is worsened by the covid-19 pandemic source us department of labor 4  by the mid of march 2020 many people started losing their jobs and by late april around 16 of the population became unemployed as of now over 30 million people have lost their jobs and this number continues to grow as the lockdown continues the global economic downturn caused by covid-19 is estimated to be worst since the great recession in 2008 35  covid-19 related shutdown is expected to affect 20-25 output decline in advanced economies eg us canada germany italy french with a 33 drop in consumer expenditure and around 2 decline in annual gdp growth rate 36  it is projected that annual global gdp growth will be reduced to 24 in 2020 with a negative growth rate in the first quarter of 2020 from a weak rate of 29 in 2019 30  compared to the pre-covid-19 period it is estimated that about 22 of the us economy would be threatened 24 of jobs would be in danger and total wage income could be reduced by around 17 37  thus far this study has used secondary data and extant research to motivate inform and direct the research focus towards current sentiment analytics on the subject of reopening the us economy previous sections have summarized key aspects of the extensive socioeconomic damage caused by the coronavirus pandemic and highlighted associated psychological and sentiment behavior challenges for the purposes of the main data analysis this research uses a unique twitter dataset of public data specifically collected for this study using a custom date range and filtered to be most relevant to the reopening discussion while public sentiment has changed from apathy disregard and humor in the earliest stages of the coronavirus pandemic to fear in february and march of 2020 and despair in march and april of 2020 yet there is a lack of clarity on the nature of public sentiment surrounding reopening of the economy the analysis of twitter data uses textual analytics methods that includes discovery of high frequency key words phrases and word sequences that reflect public thinking on the topic of reopening these publicly posted key words phrases and word sequences also allow us to peek into the direction of evolving public sentiment anecdotal tweets provide insights into special cases and they provide a peek into influential tweets and logical inflection points in the final parts of the data analysis the study provides insights into dominant current sentiment with sentiment analysis using the r programming language associated sentiment analysis libraries r packages and lexicons  the present study used twitter data from may 2020 to gauge sentiment associated with reopening 293597 tweets with 90 variables were downloaded with a date range from 04302020 to 05082020 using the rtweet package in r and associated twitter api using the keyword reopen this follows standard process for topic specific data acquisition and the dataset was saved in rds and csv formats for exploratory data analysis 38 39  the complete data acquisition and analysis were performed using the r programming language and relevant packages the dataset was cleaned filtered of potential bot activity and subset by country to ensure a final subset of clean tweets with country tagged as us it is possible that the process omitted other tweets from the us which were not tagged by country as belonging to the us furthermore for ethical and moral purposes we used custom code to replace all identifiable abusive words with a unique word text abuvs appended with numbers to ensure a distinct sequence of characters while we wanted to avoid the display of abusive words in an academic manuscript we still believed it to be useful to retain the implication that abusive words were used for further analysis after the filtering cleaning and stopwords processes a final dataset consisting of 2507 tweets and twenty nine variables was used for all the twitter data analysis textual analytics and textual data visualization in this paper a visual summary of the key words in the textual corpus of the filtered tweets is provided in the word cloud visualization in fig 2  the text component of the dataset consisting of filtered tweets only was used to create a text corpus for analysis and data visualization word frequency and n-grams analysis were used to study the text corpus of all the tweets to discover dominant patterns and are summarized in fig 6  word frequency analysis revealed an anticipated array of high frequency words including economy states businesses covid open back work country reopening plan and governor n-grams which focus on the identification of frequently used word pairs and word sequences revealed interesting patterns the most frequent bigrams two word sequences included open economy reopen country social distancing time reopen states reopen and want reopen these largely indicate positive sentiment towards reopening the most frequent trigrams three word sequences included get back work people want reopen stay home order and want reopen country these trigrams also indicate medium to strong support for the reopening the most frequent quadgrams four word sequences included cant happen forever goin worse lets get and constitutional rights must stop quadgrams reveal more complex sentiment with positive but weak support for reopening for example cant happen forever most likely implies that the lockdown cannot last indefinitely and constitutional rights must stop most likely implying that intrusive lockdown measures are not appreciated  descriptive analysis was used to explore the data and tables 1 and 2 summarize the reopening twitter data features associated with sentiment analysis table 1 a ranks the dominant twitter user names mentioned in the reopening tweets data and table 1b ranks the leading hash tags used in the data the text of the tweets was also analyzed in conjunction with other variables in the dataset to gain insights into behavioral patterns which included grouping by technology device used to post tweet and analysis of key words usage by such technological classification we grouped tweets into two technology-user classes iphone users and android users to explore potential behavioral aspects extant research support such a classification of technology users for analyzing sentiment psychological factors individual differences and technology switching 40 17 41 42  we identified 1794 twitter for iphone users and 621 twitter for android users in our dataset and ignored smaller classes such as users of web client technologies for the purposes of relative analysis we normalized the technological groupings so as to ensure comparison of ratio intrinsic to each group and to avoid the distortion caused by unequal number of users in the two technology-user classes our analysis and grouped data visualizations revealed some interesting patterns as summarized in fig 7  twitter for iphone users had a marginally higher ratio of reopen mentions while they were at par with twitter for android users in their references to business and time urgency words twitter for iphone users tended to use more abusive words while twitter for android users tended to post more tweets referencing work trump politics covid-19 and economy negative sentiment is usually tagged to the use of abusive words positive sentiment is currently associated with reopening words and either positive or negative sentiment could be associated with political work business and economy words subject to context and timing this reveals that technology user groups may differ in their sentiment towards reopening and that by itself could merit additional research focus it is important to connect textual analytics discoveries such as word frequencies and n-grams sequences to context and demonstrate potential ways in which the popular words and word sequences are being used to that purpose this section presents and discusses a few notable tweets associated with the n-grams and descriptive textual analytics name mentions abusive words and hashtags in the tweets displayed in this paper have been deleted or replaced to maintain confidentiality research ethics and relevance and spellings though incorrect have been retained as posted originally we observed tweets with high positive sentiment and emotional appeal such as what a beautiful morning to reopen the economy and ready to use common sense and reopen our country some of the tweets focused on humor more importantly when do the bars reopen day 50 i find it amusing that skinheads are the ones calling to reopen hair salons the first weekend the bars reopen is going to kill more people than the coronavirus and first karaoke song when the bars reopen will be a whole new world a large number of tweets referenced jobs work and businesses such as then tell the states to reopen that is the only way to create jobs then tell the states to reopen that is the only way to create jobs how about you just reopen the state we are begging you to open up the economy but you dont care our jobs wont be there if you keep this going and i dont want to get it but we must all reopen and get back to work many tweets were political history will also show that during the pandemic the democrats did nothing to reopen the country or economy but continued to collect a paycheck while 30 was unemployed trump thinks he needs the states to reopen for his reelection and celebrities were not spared of the negative sentiment melinda gates multibillionaire hanging out comfortably at home insists america not reopen the economy until at least january and until america implements paid family medical leave outoftouch and bill gates laughs when he hears about the economy falling because he wants you to die some tweets expressed skepticism this is not hard the economy wont get better even if you open up everything because consumer consumption is based on confidence economics 101 yall america aint ready to reopen the need to reopen the economy is definitely evidence that capitalism will kill us all and what happens when the 20 win and stores reopen and the other 80 still refuse to show up frustration was evident in some tweets i never want to hear the words reopen and economy ever again and some tweets appealed to logic the cure cant be worse than the virus its time to reopen america if they havent been preparing by now its their problem many others have spent their time getting ready to reopen but we dont have proof that will happen when is a good time to reopen it will always be risky and more will be devistated if we dont reopen follow the protocol set out and get us back to work also some quoted caution i do believe there will be serious soul searching in a few weeks as states reopen and coronavirus case numbers explode while other tweets emphasized individual rights it is really past time to reopen our country and to allow us citizens our constitutional rights and reopen let owner make a living no one is being forced to go there bring back choice as evident many strong complex and diverse emotions are expressed in these tweets examples and it is nearly impossible to manually estimate cumulative sentiment classes or scores for large tweets datasets however with the development of standardized sentiment scoring and classification technologies it has become efficient to perform sentiment analysis on large and unstructured data and current research leverages r tools to perform sentiment analysis on reopening data the scaling up of computing technologies over the past decade has made it possible for vast quantities of unstructured data to be analyzed for patterns including the identification of human sentiment expressed in textual data sentiment analysis is one of the main research insights benefits from textual analytics as it extracts potentially intended sentiment meaning from the text being analyzed early stage past research used custom methods and researcher defined protocols to identify both sentiment and personality traits such as dominance through the analysis of electronic chat data and standardized methods to assign positive and negative sentiment scores 14 43 44 45  sentiment analysis assigns sentiment scores and classes by matching keywords and word sequences in the text being analyzed with prewritten lexicons and corresponding scores or classes for this research we used r and well known r packages syuzhet and sentimentr to classify and score the reopening tweets dataset 46 47  the r package syuzhet was used to classify the tweets into eight sentiment classes as shown in fig 1  syuzhet also measures positive and negative sentiment by a simple sum of unit positive and negative values assigned to the text and therefore a single complex enough tweet may simultaneously be scored as having a positive score of 2 and a negative score of 1 sentir measures words and associated word sequence nuances providing a score ranging from around -1 to 1 and the values could be less than -1 or greater than 1 the final sentiment score from sentiment package is a summation of sentiment values assigned to parts of the sentence or textual field and can be less than -1 or more than 1 as shown in fig 9  an analysis of the reopening tweets displayed 4827 of positive sentiment 3682 of negative sentiment and 1492 of neutral sentiment with sentiment the sentiment analysis combing scores from sentiment and the classification provided by syuzhet highlighting trust and anticipation reflected a largely positive sentiment towards reopening the economy sentiment score barplot total sentiment score  there is a high level of uncertainty regarding future events and questions surrounding the effectiveness of available healthcare facilities in protecting the populace against a potential second wave of coronavirus remain there is a serious concern that an unfettered and undisciplined reopening of the us economy may lead to rapid spreading of the coronavirus and a corresponding steep increase in covid-19 cases and deaths however it is also clear that states cannot keep businesses and services closed indefinitely this situation needs to be addressed from multiple perspectives and there are numerous efforts underway to develop solutions for phased openings and preparation for the new normal the present study seeks to leverage the insights discovered through timely sentiment analytics of the reopening tweets data and apply the findings to the open now including planned and phased openings versus open indefinitely later including advocacy of maintaining complete shutdown by analyzing four potential new these new normal scenarios are highlighted and discussed on a ceteris-paribus basis that is we only consider sentiment variance and reopening timing holding all else equal such as the progression of covid-19 healthcare and social distancing protocols and all other necessary precautions preparations and ongoing intensive deep sanitization cycles physical-distance protocols and associated personal and community paraphernalia 48  extant research has demonstrated the validity of using tweets sentiment analysis to understand human behavior 49  sentiment analysis of the reopening tweets data has indicated dominant sentiment trends for trust and anticipation and a larger proportion of positive sentiment scenario a is a valuable new normal setting from a leadership and policy perspective as it provides the enthusiasm and public support that is required for the massive efforts that will be needed to wind the economy back up into action a positive and supportive public attitude will also help the government and public and private organizations in implementing health protocols more effectively than if positive sentiment were missing scenario b in contrast will be a missed new normal opportunity where in spite of positive public sentiment trends there is failure to reopen in a timely manner there are risks associated with any decision or strategy that could be applied however the risk of losing the support of positive public sentiment is too significant to be ignored there is a likelihood that the positive sentiment and forward looking energy to reopen restart businesses rejoin work and push ahead may be dominated by fear panic and despair once again due to prolonged socioeconomic stress and financial loss it will remain important for societal leaders and responsible agencies to seize positive sentiment trends indicating support for reopening and make the most of it especially in the absence of deterministic healthcare and socioeconomic solutions if negative public sentiment were to dominate then it would have the potential to hinder both a quick reopening and any effective reopening at any later stage many tweets have expressed extreme negative sentiment such as abusive word people reopen the whole freaking country and let everybody die  and we are all going to die interestingly sentiment analysis showed that though the number of positive sentiment tweets were higher the negativity of the negative sentiment score tweets were more extreme than the positiveness of the positive sentiment score tweets the extreme negative tweet had a sentiment score of 151 and the extreme positive sentiment score was 136 implying that though fewer tweets were negative the intensity of their linguistic expression was higher scenario c would lead to a volatile start to the new normal scenario as it would lead to a reopening without adequate public sentiment support reopening now with the absence of a dominant positive public sentiment trend could generate a number of adverse effects including failure of businesses that attempt to reopen it is still possible that some quick reopening associated successes and positive information flows reopening now under scenario c may still lead to a limited measure of new normal success as the economy restarts and businesses and individuals are empowered to create economic value however scenario d ceteris-paribus has the potential to lead to the most adverse new normal scenarios scenario d is a combination of dominant negative public sentiment trends where sentiment categories such as fear and sadness dominate and negative sentiment scores peak along with a delayed opening without time constraints indefinite lockdown has never been an option and the strategy has been to flatten the curve through lockdown and distancing measures so as to create a buffer time zone for healthcare facilities to prepare and cater to the infected and for governments to make plans and preparations for optimal post-reopening new normal scenarios scenario d has the potential to lead to growth in negative public sentiment trends creating long term mental healthcare challenges coupled with rapidly deteriorating economic fundamentals in summary current reopening data analytics indicate a positive public sentiment trend which allows for the the more favorable outcomes a and b which can lead to optimal new normal scenarios which maximize the benefits of reopening while limiting related risks the present research has a few limitations which must be kept in mind for any application of the public sentiment trends insights and new normal scenarios presented in this research these limitations also present opportunities for future studies and further extension of this research post-covid-19 reopening and recovery are complex challenges with significant uncertainties and unknowns -this is a new systemic crisis for which the world has no established solutions or proven models to depend on needless to say any reopening endeavor or the absence thereof any action or inaction are all fraught with significant risks we identify and discuss some of these risks from a sentiment association perspective the subsections below provide elaborations on the limitations of this research risks and opportunities for future work  there are two areas of limitations of this research quality of data and variance in sentiment analysis tools twitter data has been used extensively in research and in practice however the data is susceptible to bot activity and also to data errors caused voluntarily or inadvertently by users who provide inaccurate data for example table 2 a  b are both location variables and apart from the fact that they are poorly tagged they do not provide a reliable interpretation of actual location of the user extensive cleaning and data preparation are necessary which is often time and resource consuming especially with textual data furthermore though large volumes of public twitter data can be acquired with reasonable effort the quality of data can be affected by bot activity repeat posts and spam posts though the reopening tweets data used for the present research was cleaned and well prepared prior to analysis following standard processes yet the likelihood that the algorithmic processes did not successfully address all issues remains secondly the tools available for sentiment analysis are subject to a measure of error and are subject to the scope of the underlying lexicons that is also the reason why using multiple lexicons could lead to somewhat different results depending on the context and the complexity of the underlying textual corpus 50  this limitation in sentiment analysis tools can usually be mitigated by analyzing a larger number of tweets this research was intended to be exploratory and directional in nature and therefore multiple data sources were not used ideally public sentiment must be gauged through multiple listening mechanisms using data from multiple social media platforms and public communications to provide a better representation of the population for sentiment analysis fear became a prominent public sentiment as awareness of the seriousness and devastating effects of the coronavirus pandemic increased 17  this sentiment was not unjustified due to diverse risks associated with the pandemic covid-19 has a higher transmissibility with a reproduction number r0 of 20 -647 average r0 is 358 which indicates that the disease can be transmitted to 20 -647 people from an average infected person 51 52  this transmissibility rate is higher than recent infection diseases such as sars severe acute respiratory syndrome and ebola which have a reproduction number of 2 5 51  hence covid-19 is highly contagious especially within enclosed spaces such as trains buses restaurants crowded factory floors indoor markets and similar spaces however it is also well known that covid-19 mainly impacts only the vulnerable parts of the population commonly known as those with preexisting conditions and the elderly with weak immune systems and this awareness has led to growing concerns and protests for reopening businesses and workplaces around the world it is a public health issue to assess the safety of workplaces and estimate their likelihood to transmit contagious diseases rapidly through a variety of activities eg customer and patient dealings close contact interaction with colleagues 53 54 55  for example healthcare workers 90 are exposed more than once a month and 75 are exposed more than once a week bear a higher risk of getting infected and thus may constitute a sub-segment for sentiment analysis which can aid mental health evaluation besides some other occupations eg police firefighters couriers social workers daycare teachers and construction workers have a higher number of exposed workers in the united states self-isolation can significantly reduce icu beds requirements and flatten the disease outbreak curve with a r0 of 25 and without self-isolation 38 times the number of icu beds would be required in the us to treat critically affected people 56  in contrast about 20 of self-isolation by infected persons could reduce icu beds by 484 with a ro of 2 selfisolation can reduces icu bed requirements by 735 knowledge of infectious disease transmission in workplaces social distancing and stay at home practices are critical safeguards from rapid spread of infections 53  thus for reopening workplaces and sustaining the economy it is crucial to adopt appropriate protective measures ie ppe mandatory influenza symptoms sick leave besides adequate workplace settings ie emergency preparedness risk mitigation plans personal hygiene to reduce risk and spreading of covid-19 57 56  sentiment analysis can help track and manage public sentiment as well as local or groups sentiments subject to availability of suitable and timely data and thus contribute to risk mitigation to prevent rapid transmission of covid-19 most of the affected countries around the world implemented varying forms of lockdown policies eg quarantine travel ban social distancing with significant economic consequences 58 32 52  during this lockdown period people were forced to stay at home and many lost their jobs leading to significant emotional upheavals two recent surveys demonstrated that numerous small businesses have shut down due to covid-19 59 60  collecting data from 5800 small businesses bartik et al 59 found that about 43 of them are temporarily closed and on average they reduced their employee counts by 40 as compared to the beginning of the year according to the us chamber of commerce about 24 of all small businesses are already closed and another 40 of those who have not closed are quite likely to close by june of 2020 despite these alarming numbers about 46 of those surveyed believe that within a period of six to twelve months the us will return to a state of normalcy 60  about 25 of all workers in the united states who are employed in information technology administration financial and engineering sectors can perform work from home 61  in contrast about 75 of workers employed in healthcare industrial manufacturing retail and food services etc are unable to perform work from home due to the nature of thier work which mandates physical presence consequently a large portion of the workers 1084 m are at risk of adverse health outcomes reopening the economy will exacerbate the situation with a higher potential for covid-19 transmission not reopening the economy has the potential to create irreversible socioeconomic damage which can subsequently lead to greater loss of lives and more pain along with diminished long term capabilities to fight the coronavirus pandemic and other crises should they persist of arise sentiment analysis and public sentiment trends based scenario analysis can inform and enlighten decision makers to make better decisions and thus help mitigate risks in crisis disaster and pandemic circumstances technology supported sentiment analysis presents numerous opportunities for value creation through serving as a support mechanism for high quality decision making and for increasing awareness leading to risks mitigation for researchers and academicians this study presents a new sub-stream of research which provides a way to use sentiment analysis in crisis management scenario analysis extant research has demonstrated that information facets and categories can influence human thinking and performance and hence can impact feelings this presents an opportunity for future research to perform sentiment analysis with multiple information facets at varying levels to examine potential behavioral variations 62  this study also provides pandemic specific public sentiment insights to researchers specifically interested in pursuing the coronavirus pandemic and covid-19 studies academicians can expand the current study using additional sentiment analysis tools and customized crisis relevant sentiment analysis lexicons there is also an easy opportunity for researchers to gain rapid insights by repeating this study for future time periods and thus help develop a sequence of reopening and recovery relevant public sentiment analysis studies such an approach would also be very useful for practitioners who have a strong need for recognizing market sentiment for a wide range of business decisions practitioners can make use of this study in at least two ways the first is a direct application of the positive public sentiment exploratory findings of this study after additional validation and due diligence to inform and improve organizational decisions and the second is to utilize the logic of methods presented in this research for situation specific and localized sentiment trends based scenario analysis related to organizational decisions for example the four new normal scenarios schema can be customized and adapted to business situations where modified sentiment estimation methods can be used to gauge specific consumer segment sentiment and used to inform products or services design manufacturing or process and marketing decisions the positive sentiment  negative sentiment x action 1 now  action 2 later matrix from the sentiment scenario analysis section provides a conceptual scenario analysis framework which has high potential for scaling and adaptation we shall draw from the heart of suffering itself the means of inspiration and survival churchill 48 sentiment analysis is a valuable research mechanism to support insights formation for the reopening and recovery process sentiment analysis tools can be effectively used to gauge public sentiment trends from social media data especially given the high levels of trust in social media posts among many networks 63  this study discovered positive public sentiment trends for the early part of may 2020 specifically the study discovered high levels of trust sentiment and anticipation sentiment mixed with relatively lower levels of fear and sadness positive sentiment count dominated negative sentiment count though negative tweets used more extreme language while sentiment analysis provides insights into public feelings it is only one part of a complex set of contributions which will be required to effectively move into the new normal sentiment analysis based on the tweets data used in this study appears to indicate public sentiment support for reopening the message that it is time to reopen now was prominent in the reopening tweets data furthermore we believe that this research contributes to the sentiment component of the recent call for cyberpsychology research and well developed sentiment analysis tools and models can help explain and classify human behavior under pandemic and similar crisis conditions 64 65  positive public support sentiment will remain critical for successful reopening and recovery and therefore additional initiatives tracking sentiment and evaluating public feelings effectively will be extremely useful sentiments can be volatile and localized and hence technological and information systems initiatives with real-time sentiment tracking and population segmentation mechanisms will provide the most valuable public sentiment trends insights with additional research and validation 1  the research strategy utilized in this paper can be easily replicated with necessary variations and applied on future data from multiple social media sources to generate critical and timely insights that can help form an optimal new normal  a multi-dimensional big data storing system for generated covid-19 large-scale data using apache spark manar elmeiligy a ali desouky i el sally elghamrawy m  the ongoing outbreak of coronavirus disease had burst out in wuhan china specifically in december 2019 covid-19 has caused by a new virus that had not been identified in human previously this was followed by a widespread and rapid spread of this epidemic throughout the world daily the number of the confirmed cases are increasing rapidly number of the suspect increases based on the symptoms that accompany this disease and unfortunately number of the deaths also increase therefore with these increases in number of cases around the world it becomes hard to manage all these cases information with different situations if the patient either injured or suspect with which symptoms that appeared on the patient therefore there is a critical need to construct a multi-dimensional system to store and analyze the generated large-scale data in this paper a comprehensive storing system for covid-19 data using apache spark css-covid is proposed to handle and manage the problem caused by increasing the number of covid-19 daily css-covid helps in decreasing the processing time for querying and storing covid-19 daily data css-covid consists of three stages namely inserting and indexing storing and querying stage in the inserting stage data is divided into subsets and then index each subset separately the storing stage uses set of storing-nodes to store data while querying stage is responsible for handling the querying processes using apache spark in css-covid leverages the performance of dealing with large-scale data of the coronavirus disease injured whom increase daily a set of experiments are applied using real covid-19 datasets to prove the efficiency of css-covid in indexing large-scale data  coronavirus disease covid-19 1 is an infection disease that firstly reported in wuhan hubei province china in december 2019 and then rapidly spreading across all over the world this virus was previously named as 2019-ncov and it is a positive enveloped single-strand rna virus it also shares a lot of similarities with two other coronaviruses the mers-cov middle east respiratory syndrome and sars-cov severe acute respiratory syndrome 6  china officially declared the epidemic as an outbreak on january 20 when obvious human-to-human transmissions were ascertained with reagent probes and primers distributed to local agencies on that day immediately following the declaration massive actions were taken the next day to curb the epidemic at wuhan and soon spread to the whole country from central to local government including all sectors from business to factories and to schools 6  on february 23 2020 wuhan city and other cities along with the main traffic lines around wuhan were locked down rigorous efforts were devoted to 1 identify the infected and bring them to treatment in hospitals for infectious diseases 2 locate and quarantine all those who had contact with the infected 3 sterilize environmental pathogens 4 promote mask use and 5 release to the public of number of infected suspected under treatment and deaths on a daily basis covid-19 is of critical concern for public health 7 8 9  health care providers should be updated regarding public health and covid-19 outbreaks affecting their communities to promptly make correct decisions 9 10  this would enable them to offer improved services in an efficient manner which is crucial in the current situation 9  most health care providers depend on the center of disease control and prevention cdc to be informed on disease outbreaks or to be notified of new infectious covid-19 9  however we still do not have infectious diseases under control especially novel covid-19 11  in march 26 2020 there have been 85377 confirmed cases and 1293 deaths in the united state 1  globally more than 531000 people are infected with more than 24000 deaths that reported by world health organization who 2 in april 11 2020 number of the infected people around the world are 1696860 in at least 184 countries 102642 of deaths and the recovered people are about 376106 3 we collect and curated individual-level data from national provincial and municipal health reports as well as additional information from online reports to analyze and track the covid-19 4  to keep on touch with more information about the daily cases you can visit 5  unfortunately this rapidly increasing of the number of cases since march 26 2020 until april 11 2020 will cause a problem with storing all these data with this huge growing of the infected people around the world many researchers 18 19 20 21 22 23 exploit bid data and intelligent tools to monitor covid-19 disease in different ways to help the data science researchers for devolving suitable tools for monitoring covid-19 the data must be easily presented and stored in this context there is an urgent need to create a proper indexing systems to manage store analyze and query the large-scale of the generated data easily these systems main goal is to reduce the time taken for data preparation by presenting suitable schemas design and formatting in this work a comprehensive storing system for covid-19 data using apache spark css-covid is proposed this system consists of three stages inserting and indexing stage storing stage and querying stage in the inserting and indexing stage our system uses hdfs hadoop distributed file system to divide the dataset into set of rdd subsets with help of driving program and cluster manager apache spark components in storing stage set of storing-nodes are used to index the data using r-tree algorithm and store the indexed data while the querying stage uses two different nodes receiving-node and replying-node that help to improve the retrieving process the rest of paper is organized as the following section 2 describes the structure of css-covid and explains the all the stages in detail section 3 presents the results of the experiments finally section 5 concludes the paper css-covid is proposed to improve the performance for both inserting and querying for the emerging coronavirus dataset covid-19 overall structure for the proposed system is showed at figure 1 css-covid uses apache spark engine as its internal structure to increase the processing performance and decrease the computing time apache spark 12 13 is a big data distributed processing framework composed of an in-memory data engine that can perform tasks up to one hundred times faster than multi-stage jobs rdd is a programming abstraction that represents an immutable collection of objects that can be split across a computing cluster css-covid consists of three stages inserting and indexing storing and querying stages each stage is responsible for specific function to preserve the parallel computing the subsections have detailed explanation about each part of this system as we mentioned above the information of infected people of emerging coronavirus infection or the suspected cases are rapidly increased therefore we have to use efficient method to store this information efficiently to speed up the processing performance by inserting and storing data quickly in the proposed css-covid the insertion is performed based on apache spark worker nodes rdds are the main logical data units in spark rdd is acronym for resilient distributed datasets which is a programming abstraction that represents an immutable collection of objects that can be split across a computing cluster rdd is built using parallelized transformations filter join or groupby that could be traced back to cover the rdd data operations on rdds can also be split across the cluster and executed in a parallel batch process provide too fast and scalable parallel processing in the beginning of this stage the data is inserted in hadoop distributed file system hdfs until it divided into set of rdds each rdd consists of subset of data which will have indexed by the sparks worker node based on the r-tree algorithm 14  r-tree is variant of r-tree 15  which used to index multi-dimensional data next after the worker node index its subset the indexed data are send to a new storing-node to store these r-trees by managing with both of cluster manager and driver program  in this stage the cluster manager control the worker nodes while they indexing the data when the worker node index its subset of data the cluster manager creates new storing-node and sends the created r-tree into this new storing-node each storing-node has single r-tree that created by single worker node we have required about 25 storing-nodes in our proposal to completely store the dataset figure   3 illustrates the storing process using css-covid  this part of css-covid system is responsible for the querying processes the total process cycle is illustrated in figure 4  in the beginning the user sends the query to the receiving-node the receivingnode get the query from the used and generates an id for each query to distingue the queries come from different users next the receiving-node sends the query and the generated its id to the replying-node then the replying-node store the query id in its table and then send the query to the different storingnodes in the store stage in addition to the replying-node send the query to the cache memory to reduce the answering required time finally when the replying-node obtains the answers from the storingnodes and the cache memory it arranges the answers based on the query id and send the answers to the user css-covid system can serve two different kinds of queries the query can be either k nearest neighbor knn query or range query in this section set of experiments will examine to evaluate the proposed system we use a real dataset  query id query answers 1 2 n  in this section we will evaluate the consuming time of our proposal css-covid using different merging dataset consumes the about 320 secs to insert and index more than one million record indexing data using css-covid in this experiment we will evaluate the knn querying process using different datasets with different figure 6  each dataset elapsed time with different k is showed as we can observe with increasing the required number of k the elapsed time used not much large than the small number of k for example the time required when k  4000 not much large than when k  2500  in this section we will show the used space by different datasets after insert the whole dataset and index and covid-19-outside-hubei 17 as we can observe from figure 8 there is no large different used space by different size dataset that because using r-tree for indexing the data which preserve the used space in addition to using distributed storing-nodes with fixed size of indexed data also preserve the used space to be small as possible  as we know the percentage of accuracy of the query answers is an important factor to evaluate the performance of the indexing systems here we will evaluate the accuracy of the answers that reply to the users figure 9 shows the accuracy percentage using different datasets with different sizes three datasets are used which are covid-19-merging covid-19-inside-hubei and covid-19-outside-hubei as illustrated in figure 9  the percentage of the query answers range between 98 -997 which is a perfect value of any indexing system here we have to mentioned that the used type of query for this experiment is k nearest neighbour query knn with k  3000  in this paper we propose a new indexing system called css-covid comprehensive storing system for covid-19 data using apache spark which is used for indexing and storing large-scale data css-covid is interested to manage and handle the data of the coronavirus disease covid-19 injured people nowadays the number of the injured people is daily increases awesomely which make the process of indexing analyzing and storing all those patient information is very difficult in addition to there are also lots of suspected cases in this epidemic which have many information for each one unfortunately the number of deaths also increase daily which need to manage and analyze all these different cases by describing the problem above with that huge number of information we became in persistent need to create an efficient indexing system to handle store and query this information easily css-covid is consists of three stages inserting and indexing stage storing stage and querying stage in inserting stage we use hdfs to store the dataset and the apache spark worker nodes to index the data using r-tree algorithm in storing stage set of distributed nodes called storing-nodes used to store single created r-tree finally in querying stage receiving-node and replying-node are work together to handle the different type of querying processes each css-covid stage work in parallel to improve the system performance in addition set of experiments are applied to prove that the proposal css-covid is an efficient indexing system for indexing large-scale data  pro-russian biases in anti-chinese tweets about the novel coronavirus autumn toney akshat pandey wei guo david broniatowski aylin caliskan  the recent covid-19 pandemic which was first detected in wuhan china has been linked to increased anti-chinese sentiment in the united states recently broniatowski et al found that foreign powers and especially russia were implicated in information operations using public health crises to promote discord -including racial conflict -in american society broniatowski et al 2018  this brief con-  siders the problem of automatically detecting changes in overall attitudes that may be associated with emerging information operations via artificial intelligence accurate analysis of these emerging topics usually requires laborious manual analysis by experts to annotate millions of tweets to identify biases in new topics we introduce extensions of the word embedding association test from caliskan et al to a new domain caliskan et al 2017  this practical and unsupervised method is applied to quantify biases being promoted in information operations analyzing historical information operations from russias interference in the 2016 us presidential elections we quantify biased attitudes for presidential candidates and sentiment toward muslim groups we next apply this method to a corpus of tweets containing anti-chinese hashtags we find that roughly 1 of tweets in our corpus reference russian-funded news sources and use anti-chinese hashtags and beyond the expected anti-chinese attitudes we find that this corpus as a whole contains pro-russian attitudes which are not present in a control twitter corpus containing general tweets additionally 4 of the users in this corpus were suspended within a week these findings may indicate the presence of abusive account activity associated with rapid changes in attitudes around the covid-19 public health crisis suggesting potential information operations the world health organization has declared that a rapid increase of information published on the internet surrounding the 2019-ncov virus to be an infodemic with disinformation widespread throughout multiple social media platforms who 2020  concomitant with the spread of unverified information several governments have expressed concerns gabrielle 2020 that state-sponsored information operations are using this infodemic to sow panic and fear russian influence operations have previously used public health issues such as hiv infection and vaccination broniatowski et al 2018  as wedge issues to promote discord including racial divisions and a pro-russian policy agenda walter et al 2020  here we find that tweets pertaining to covid-19 and expressing anti-chinese sentiment seem to express pro-russian sentiment these preliminary findings showcase a novel artificial intelligence ai tool an extension of word embedding association test weat caliskan et al 2017  that researchers studying malicious information operations may use to further guide research specifically our findings point to the need for future work to determine whether this pro-russian sentiment is the result of genuine grassroots attitudes or coordinated inauthentic behavior eg a trolling campaign the detection of malicious information operations is complicated by the high velocity and volume of social media posts since manual identification and annotation of online content requires specialized area expertise and does not scale gorwa et al 2020  here we select a practical unsupervised ai method that identifies biases present in large text corpora and extend it to emerging domains specifically we choose word lists to represent information operation biases that are present in emerging domains that do not have historical samples our technique can be applied to new trending topics including those reflecting suspected information opera-tions when annotated data is not readily available we validate our method by measuring known anti-muslim negative biases and negative associations with a major presidential candidate during the 2016 us elections both of which are present in word embeddings trained on corpora generated by a known russian influence campaign twi 2018 howard et al 2019  we next collect twitter data with tweets containing anti-chinese hashtags over the course of one week to investigate potential biased associations present in this public dataset since the covid-19 outbreak originated in wuhan china we expected and found indications of expressions of anti-chinese biases we further demonstrate that these biases are associated with expressions of panic and negative sentiment surprisingly the same corpus associates russia with expressions of calm and positive sentiment caliskan et al show that human-like biases and veridical information are embedded in the statistical regularities of language that are captured by word embeddings caliskan et al 2017  word embeddings are vector space representations of semantics learned via the distributional hypothesis extending caliskan et als weat we examine biases in word embeddings trained on tweets related to covid-19 weat quantifies human-like biases and cultural stereotypes between two target groups and two sets of polar attributes caliskan et al use these tests in word embeddings to replicate biased associations documented by the implicit association test iat by using word sets in the iat greenwald et al 1998  we extend the weat to our research domain by creating bias tests for political leaders and countries calm-panic trustworthy-untrustworthy and pleasant-unpleasant calm-panic addresses the extent to which text may express panic within a group whereas trustworthy-untrustworthy addresses the extent to which text might frame a group as untrustworthy finally we include the original pleasant-unpleasant bias test to measure general negative bias against an opposing group kurdi et al and werntz et al provide word sets that represent the polar extremes of trustworthy untrustworthy panic and calm sentiments that we use in our bias tests kurdi et al 2019 werntz et al 2016  to distinguish between general bias against china and bias against china during the covid-19 outbreak we use three sources of twitter data i twitter-g a general large-scale twitter control corpus that reflects baseline biases ii covid-g a general public dataset of tweets that mentioned common coronavirus hashtags created from 12 march 2020 until 22 march 2020 coronavirus coronavirusoutbreak coronaviruspandemic covid19 covid 19 smith 2020  and iii covid-ac a set of tweets collected between 11 march and 18 march 2020 that contain hashtags discussing the covid-19 pandemic and targeting china and wuhan many of which expressed anti-chinese sentiment chinavirus wuhan wuhanvirus chinavirusoutbreak wuhancoronavirus wuhaninfluenza wuhansars chinacoronavirus wuhan2020 chinaflu wuhanquarantine chinesepneumonia coronachina wohan for twitter-g we use the pre-trained global vectors for word representation glove twitter word embeddings which are widely used twitter word embeddings pennington et al 2014  we used the glove twitter word embeddings with 27 billion tokens and 200-dimensional vectors the twitter-g word embeddings contain similar bias scores to the reported scores by caliskan et al showing that they capture known human-like biases and cultural stereotypes caliskan et al 2017  we generate word embeddings using the glove algorithm for the covid-g and covid-ac corpora since glove is known to capture semantics most accurately in order to validate our methods we use two twitter datasets twi 2018 with ground truth information on bias associations i ru-disinfo a corpus that contains russian information operation tweets released by twitter in january 2019 and ii ira-disinfo a twitter corpus released in october 2018 that contains tweets traced to russias internet research agency ira these tweets were collected by twitter if they were flagged to be involved with state-backed information operations consistent with our experimental datasets we generated word embeddings using the glove algorithm for the two validation datasets recently broniatowski et al showed that russian trolls weaponized health communication to promote discord over twitter broniatowski et al 2018  moreover russian trolls were responsible for interfering with the 2016 us presidential election mueller and cat 2019  given this pre- vious documentation of russian information operations we choose to analyze sentiment towards russia expressed in these tweets we implement the calm-panic bias test and pleasant-unpleasant bias test across the twitter-g covid-g and covid-ac word embeddings to compare results and identify bias shifts in statistics our bias score is known as an effect size and is defined by cohens d according to this statistic an absolute value greater than or equal to 080 indicates a high effect size the statistic can take on values between 2 and 2 accordingly as shown in table 1  we identify biases with high effect size d  080 that are statistically significant p  005 indicating a clear bias in the set of tweets we first validated our method using the ru-disinfo and ira-disinfo word embeddings for the ru-disinfo word embeddings we implemented the truthworthy-untrustworthy bias test to measure the association of the winning presidential candidate candidate a whom us government sources determined was characterized by russian information operations as more trustworthy than the opposing candidate b mueller and cat 2019  specifically the trustworthy-untrustworthy bias experiment produced an effect size of d  127 p  0023 using the ru-disinfo word embeddings consistent with prior research showing that russian information operations characterized candidate b as deceitful and untrustworthy woolley and howard 2018 bovet and makse 2019  as a further validation we examined how this known russian trolling operation characterized muslims since a significant body of prior work analyzing russian information operations woolley and howard 2018 hindman and barash 2018 del vicario et al 2016 lazer et al 2018 bovet and makse 2019 have indicated that these campaigns express anti-muslim sentiment we found that the ira-disinfo word embeddings associated muslim words with calm and anti-muslim words with panic here the calm-panic bias experiment produced an effect size d  122 p  0026 thus our validation experiments demonstrate that our unsupervised method generates results consistent with expectations we next examined anti-chinese and pro-russian biases in the covid-ac corpus we find a strong pro-russian and anti-chinese bias in the calm-panic bias test with an effect size of d  131 p  10 2  using the covid-ac word embeddings the covid-g word embeddings also contain a significant but smaller effect d  085 p  0045 finally in the twitter-g word embeddings bias drastically moves to the opposite direction to d  086 p  0047 in this control dataset russia is associated with panic whereas china is associated with calm we find that at least 1 of tweets in our corpus reference russian-funded news sources and use anti-chinese hashtags additionally 4 of the users in this corpus were suspended one week after data collection in order to investigate the scope of pro-russian biases we ran calm-panic and pleasant-unpleasant weat tests for numerous countries country-x on covid-ac these russia vs country-x weat tests indicated positive pro-russian biases associating russia with calm and positive valence some of these results were not statistically significant since we were not able to identify 8 words to represent many countries accurately for the weat test covid-ac embeddings are trained on a small corpus thus identifying 8 neutral words in covid-acs dictionary to represent countries such as bulgaria greece singapore and taiwan accurately is a challenging task aforementioned countries were not affected by the covid-19 outbreak as intensely compared to russia during our data collection period nevertheless observing consistent pro-russian biases across all of our experiments suggests further investigation into information operations might provide useful insights our practical unsupervised method used to detect and measure implicit biases in text corpora is an implementation of the weat caliskan et al 2017  given a text corpus from a domain of interest we generate word embeddings to automatically discover implicit associations by extending weat weat produces a normalized bias score effect size d by measuring the relative association of two social groups with two polar attributes in any input word embeddings the association is quantified by cosine similarity then the onesided permutation test p -value measures the unlikelihood of the null hypothesis which is the probability that a random permutation of the attribute words would produce the observed difference in sample means we list the word sets used in experiments to extend the weat methodology in table 2 in the appendix we systematically selected neutral words to represent russia and china as social groups using a representative word and the corresponding hashtag of the word the calm panic pleasant unpleasant trustworthy and untrustworthy attribute sets are selected from prior work in social psychology greenwald et al 1998 kurdi et al 2019 werntz et al 2016  when word embeddings are trained on a small corpus or the word sets are considerably small less than 8 words the bias score may be insignificant adding more words to the category and attribute sets increases the significance of the weats results both calm and panic were represented with 4 words in prior work werntz et al 2016  since some of those words were not present in our word embeddings we added synonyms and antonyms to represent each attribute set with 5 words twitter-gs dictionary did not contain many of the hashtags we used in the covid-19 domain consequently we excluded the hashtags while representing russia and china in weat for twitter-g instead we added four random major city names from russia and china to enhance the representation the word lists used in all experiments our source code and word embeddings will be made available on our public github project page we collected 852 955 tweets related to covid-19 with hashtags listed in the results section that were deemed as potentially spreading anti-chinese sentiment on twitter the dataset was collected between 11 march and 18 march 2020 to generate case-insensitive covid-acembeddings caseinsensitive covid-g embeddings were trained on a general public dataset of tweets that mentioned common coronavirus hashtags caseinsensitive ru-disinfo and ira-disinfo embeddings were trained on datasets with tweets linked to russia and ira these datasets are available in twitters transparency report on information operations twi 2018 case-insensitive twitter-g embeddings were trained on 2 billion random tweets therefore we used twitter-g to obtain control results that are highly likely to reflect the baseline biases caliskan et al 2017  we examined a set of tweets containing anti-chinese hashtags pertaining to covid-19 covid-ac to determine if these tweets contained any hallmarks of potential malicious information operations using an unsupervised ai method to quantify biases expressed on twitter our novel approach allows for real-time bias analysis of a given text corpus without requiring expert annotated data generating word embeddings we are able to implement an extended weat to measure bias associations for calm-panic and trustworthy-untrustworthy which may indicate the presence of information operations used to cause confusion and distrust in targeted groups we validate our method on twitter data linked to known russian and ira information operations selecting word sets that represent targeted information campaigns during the 2016 us presidential election we quantify biases towards political candidates and muslims in these prior information operations we quantify pro-russian and anti-chinese biases in addition to associations of china with fear and panic in recent covid-19 related public twitter data various domains can apply this practical method by selecting the desired opposing groups eg russia vs china to discover and measure the present biases these methods could be used to characterize attitudes on several different social media platforms in advance of major world events such as the upcoming us presidential election or the quickly evolving covid-19 outbreak by automatically identifying emerging biases if unexpected biases are detected researchers might then examine whether these could be artificially and deliberately introduced to the public sphere  title identification of a pangolin niche for a 2019-ncov-like coronavirus through an extensive meta-metagenomic search lamia wahba nimit jain andrew fire z karen artiles l matthew mccoy j dae-eun jeong  in numerous instances tracking the biological significance of a nucleic acid sequence can be augmented through the identification of environmental niches in which that sequence is present many metagenomic datasets are now available with deep sequencing of samples from diverse biological niches while any individual metagenomic dataset can be readily queried using web-based tools meta-searches through all such datasets are less accessible in this brief communication we demonstrate such a meta-meta-genomic approach examining close matches to the wuhan coronavirus 2019-ncov in all high-throughput sequencing datasets in the ncbi sequence read archive accessible with the keyword virome in addition to the homology to bat coronaviruses observed in descriptions of the 2019-ncov sequence 12 we note a strong homology to numerous sequence reads in a metavirome dataset generated from the lungs of deceased pangolins reported by liu et al 3 our observations are relevant to discussions of the derivation of 2019-ncov and illustrate the utility of meta-metagenomic search tools in effective and rapid characterization of potentially significant nucleic acid sequences introduction  in numerous instances tracking the biological significance of a nucleic acid sequence can be augmented through the identification of environmental niches in which that sequence is present many metagenomic datasets are now available with deep sequencing of samples from diverse biological niches while any individual metagenomic dataset can be readily queried using web-based tools meta-searches through all such datasets are less accessible in this brief communication we demonstrate such a meta-meta-genomic approach examining close matches to the wuhan coronavirus 2019-ncov in all high-throughput sequencing datasets in the ncbi sequence read archive accessible with the keyword virome in addition to the homology to bat coronaviruses observed in descriptions of the 2019-ncov sequence 1 2  we note a strong homology to numerous sequence reads in a metavirome dataset generated from the lungs of deceased pangolins reported by liu et al 3  our observations are relevant to discussions of the derivation of 2019-ncov and illustrate the utility of meta-metagenomic search tools in effective and rapid characterization of potentially significant nucleic acid sequences in the early years of nucleic acids sequencing biological hypothesis generation and discovery were greatly aided by the aggregation of the majority of published dna and rna sequences into public sequence databases search tools capable of interrogating the ever-expanding databases were facilitated by creative algorithm development and software engineering and by the ever-increasing capabilities of computer hardware and the internet as of the early 2000s sequencing methodologies and computational technologies advanced in tandem so that homology results from a novel sequence were generally available quickly and without substantial cost with the development of larger-scale massively parallel sequencing methodologies the time and resources to search all extant sequence data became untenable for most studies although creative approaches involving curated databases and feature searches ensured that many key features of novel sequences remained readily accessible at the same time the nascent field of metagenomics began with numerous studies highlighting the power of survey sequencing of dna and rna from samples as diverse as the human gut and antarctic soil 4 5  as the diversity and sizes of such datasets expand the utility of searching them with a novel sequence increases meta-metagenomic searches are currently under-utilized--in principle such searches would involve direct access to sequence data from a large set of metagenomic experiments on a terabyte scale along with software able to search for similarity to a query sequence neither of these aspects of meta-metagenomic searches is infeasible with current data transfer and processing speeds in this communication we report the results of searching the recently-described 2019-ncov coronavirus sequence through a set of metagenomic datasets with the tag virome computing hardware a linux workstation used for the bulk analysis of metagenomic datasets employs an 8-core i7 intel microprocessor 128g of random access memory 12tb of conventional disk storage and 1tb of ssd storage additional analysis of individual alignments were carried out with standard consumer-grade computers sequence data all sequence data for this analysis were downloaded from the national center for biotechnology information ncbi website with individual sequences downloaded through a web interface and metagenomic datasets downloaded from the ncbi sequence read archive sra using the sra-tools package version 291 the latter sequence data were downloaded as sra files using the prefetch tool with extraction to readable format fastagz using the ncbi fastq-dump tool each of these manipulations can fail some fraction of the time in the former case due to network issues and for unknown reasons in the latter case thus the workflow continually requests sra files with prefetch until at least some type of file is obtained and attempts to unpack into fastagz format until at least one such file is obtained from each sra file metagenomic datasets for analysis were chosen through a keyword search of the sra descriptions for virome the virome keyword search will certainly not capture every metagenomic dataset with viral sequences and likewise not capture every virus in the short sequence read archive these datasets were downloaded between january 27 and january 31 2020 with up to 16 threads running simultaneously total download time prefetch was approximately 2 days with a similar time required for conversion to gzipped fasta files a total of 9014 sequence datasets were downloaded and converted to fastagz files although a small fraction of these contained very little data only a few reads or reads of at most a few base pairs most files contained large numbers of reads and the total dataset consists of 25tb of compressed sequence data with total bases covered approaching 10tb search software for rapid identification of close matches among large numbers of metagenomic reads we used a simple dictionary based on the 2019-ncov sequence ncbi mn9089473wuhan-hu-1 and its reverse complement querying every 8th k-mer along the individual reads for matches to the sequence as a reference and to benchmark the workflow further we included several additional sequences in the query vaccinia virus an arbitrary segment of a flu isolate the full sequence of bacteriophage p4 and a number of putative polinton sequences from caenorhabditis briggsae  the relatively small group of k-mers being queried 10 6  allows a rapid search for homologs this was implemented in a python script run using the pypy accelerated interpreter we stress that this is by no means the most comprehensive or fastest search for large datasets but it rapidly finds any closely matching sequence with the downloading and conversion of the data rather than the search being rate limiting alignment of reads with the human 2019-ncov virus reads from the positive pangolin datasets were mapped to the 2019-ncov genome with bwa-mem version 0712 using default settings for paired-end mode 6  alignments were visualized with the integrated genomics viewer igv tool 9  assessment of nucleotide similarity between 2019-ncov pangolin metavirome reads and closely related bat coronaviruses all pangolin metavirome reads that aligned to the 2019-ncov genome with bwa-mem after adapter trimming with cutadapt were used for calculation the bat coronavirus genomes were aligned to the 2019-ncov genome in a multiple sequence alignment using the web-interface for clustal omega  httpswwwebiacuktoolsmsaclustalo  with default settings we note that sequence insertions with respect to the 2019-ncov genome in either the pangolin metavirome reads or the bat coronavirus genomes are not accounted for in the similarity traces shown in figure 1b  to identify biological niches that might harbor viruses closely related to 2019-ncov we searched through publicly available metaviromic datasets we were most interested in viruses with highly similar sequences as these would likely be most useful in forming hypotheses about the origin and pathology of the recent human virus we thus set a threshold requiring matching of a perfect 32-nucleotide segment with a granularity of 8 nucleotides in the search ie interrogating the complete database of k-mers from the virus with k-mers starting at nucleotide 1 9 17 25 33 of each read from the metagenomic data for a perfect match this would catch any perfect match of 39 nucleotides or greater with some homologies as short as 32 nucleotides captured depending on the precise phasing of the read all metagenomic datasets with the keyword virome in ncbi sra as of january 2020 were selected for analysis in a process that required approximately 2 days each for downloading and conversion to readable file formats and one day for searching by k-mer match on a desktop workstation computer i7 8-core together the datasets included information from 9014 ncbi short read archive entries with in total 6210 10 individual reads and 8410 12 base pairs despite the relatively large mass of data the 32-nucleotide k-mer match remains a stringent measure with spurious matches to the 30 kb 2019-ncov genome expected at only 1 in 310 14  positive matches among the metagenomic datasets analyzed were relatively rare with the vast majority of datasets 89949014 or 998 showing no matched 32-mers to 2019-ncov of the datasets with matched k-mers one was from an apparent synthetic mixture of viral sequences while the remaining were all from vertebrate animal sources the matches were from five studies two bat-focused studies one bird-focused study one small-animal-and-rodent focused study and a study of pangolins 3  table 1  the abundance and homology of viruses within a metagenomic sample are of considerable interest in interpreting possible characteristics of infection and relevance to the query virus from the quick k-mer search an initial indicator can be inferred from the number of matching reads and k-mer match counts for those reads  table 1 supplementary table 1  for the 2019-ncov matches amongst the available metagenomic datasets the strongest and most abundant matches in these analyses come from the pangolin lung metaviromes the matches are observed throughout the 2019-ncov query sequence and many of the matching reads show numerous matching 32-mer sequences the vast majority of matches were in two lung samples with small numbers of matches in two additional lung datasets no matches were detected for five additional lung datasets and no matches were seen in eight spleen samples and a lymph node sample further analysis of coverage and homology through alignment of the entire metagenomic datasets revealed an extensive if incomplete coverage of the 2019-ncov genome  figure 1a  percent nucleotide similarity can be calculated for pangolin metavirome reads aligning to 2019-ncov  figure 1b  and these segmental homologies consistently show strong matches approaching but still overall weaker than the similarity of the closest known bat coronavirus ratg13 meta-meta-genomic searching can provide unique opportunities to understand the distribution of nucleic acid sequences in diverse environmental niches as metagenomic datasets proliferate and as both the need and capability to identify pathogenic agents through sequencing grows meta-metagenomic searching may prove extremely useful in tracing the origins and spreading of causative agents in the example we present in this paper such a search identifies a number of niches with sequences matching the genome of the recent 2019-ncov virus in so doing raising a number of relevant points for the origin of 2019-ncov before describing the details of these points however it is important to stress that while environmental clinical and animal-based sequencing is valuable in understanding how viruses traverse the animal ecosphere static sequence distributions cannot be used to construct a virus full transmission history amongst different biological niches so even were the closest relative of a virus causing disease in species x to be found in species y we cannot define the source of the outbreak or the directions of transmission as some viruses may move more than once between hosts the sequence of a genome at any time may reflect a history of selection and drift in several different host species this point is also accentuated in the microcosm of our searches for this work when we originally obtained the 2019-ncov sequence from the posted work of wu et al we recapitulated their result that bat-sl-covzc45 was the closest related sequence in ncbis non-redundant nrnt database several pangolin metavirome sequences-which are not currently in the nrnt database-are more closely related to 2019-ncov than bat-sl-covzc45 an assumption that the closest relative of a sequence identifies the origin would at that point have transferred the extant model to zoonosis from pangolin instead of bat to complicate such a model an additional study from zhou et al 2 described a previously unpublished coronavirus sequence designated ratg13 with much stronger homology to 2019-ncov than either bat-sl-covzc45 or the pangolin reads from liu et al while this observation certainly shifts the discussion legitimately toward a possible bat-borne intermediate in the chain leading to 2019-ncov it remains difficult to determine if any of these are true intermediates in the chain of infectivity the match of 2019-ncov to the pangolin coronavirus sequences also enables a link to substantial context on the pangolin samples from liu et al with information on the source of the rescued animals from smuggling activity the nature of their deaths despite rescue efforts and the accompanying pathology that work describes analyses indicating several coronaviruses present in two of the pangolin lungs as well as other viral species the pangolins appear to have died from lung-related illness which may have involved the 2019-ncov closely-related virus notably however two of the deceased pangolin lungs had much lower coronavirus signals while five showed no signal with sequencing depths in the various lungs roughly comparable although it remains possible that the coronavirus was the primary cause of death for these animals it is also possible as noted by liu et al 3  that the virus was simply present in the tissue with mortality due to another virus a combination of infectious agents or other exposures during the course of this work the homology between 2019-ncov and pangolin coronavirus sequences in a particular genomic subregion was also noted and discussed in an online forum virologicalorg with some extremely valuable sharing of analyses and insights matthew wong and colleagues bring up the homology to the pangolin metagenomic dataset in this thread and appears to have encountered it through a more targeted search than ours several models are discussed in this forum in parallel to discussions in the virology community as to the source of 2019-ncov the availability of numerous paths both targeted and agnostic toward identification of natural niches for pathogenic sequences will remain useful to the scientific community and to public health as will vigorous sharing of ideas data and discussion of potential origins and modes of spread for epidemic pathogens competing interests the authors declare that they have no competing interests funding this study was supported by the following programs grants and fellowships human frontier science program hfsp to dej stanford genomics training program 5t32hg000044-22 pi m snyder to mjm and r35gm130366 to azf authors contributions all authors contributed equally and author order was selected randomly  totalreads totalbases hitreads hitkmers  coaid covid-19 healthcare misinformation dataset limeng cui dongwon lee  as the covid-19 virus quickly spreads around the world unfortunately misinformation related to covid-19 also gets created and spreads like wild fire such misinformation has caused confusion among people disruptions in society and even deadly consequences in health problems to be able to understand detect and mitigate such covid-19 misinformation therefore has not only deep intellectual values but also huge societal impacts to help researchers combat covid-19 health misinformation therefore we present coaid covid-19 healthcare misinformation dataset with diverse covid-19 healthcare misinformation including fake news on websites and social platforms along with users social engagement about such news coaid includes 1896 news 183564 related user engagements 516 social platform posts about covid-19 and ground truth labels the dataset is available at httpsgithubcomcuilimengcoaid  covid-19 is believed to be caused by a novel coronavirus called sars-cov-2 which was initially discovered in wuhan china in december 2019 and has quickly spread throughout the world who declared the outbreak a public health emergency of international concern on january 30 2020 16 and characterized covid-19 as a pandemic on march 11 2020 18  as of may 6 2020 more than 35 million cases of covid-19 and almost 250000 deaths worldwide have been reported to who 17  common symptoms of covid-19 includes cough shortness of breath fever sore throat and loss of taste or smell 7  the incubation period is estimated to extend to 14 days with a median time of 51 days 11  mortality for covid-19 3-4 appears higher than that for seasonal influenza 01 15  until now there has been no known vaccine or specific cure for covid-19 14  while covid-19 situation has gotten severely worse misinformation related to covid-19 has rapidly risen and caused serious social disruptions as well one the one hand fake cures for covid-19 have seriously threatened peoples lives for example an arizona man was dead and his wife was hospitalized after the couple ingested a form of chloroquine to prevent covid-19 1  on the other hand the prevalent misinformation is disrupting social order for example 77 cell phone towers have been set on fire due to the conspiracy that 5g mobile networks can spread covid-19 2  in recent years in mitigating these problems many computational methods have been developed to auto-detect diverse types of misinformation in different genres 20 26 23 3  however debunking covid-19-related misinformation exhibits its own set of unique challenges first aided by the fear of the unknown misinformation about emerging diseases can spread more quickly than ordinary misinformation does before being debunked 3  second when one keeps  seeing a piece of fake news in the news feed she tends to think that it is true even if she had doubts before 4  third after people were convinced by misinformation myths correction message from authority may be ineffective or even reduce peoples beliefs in other facts about an epidemic 2  therefore it is not clear how easy or difficult for researchers to build satisfactory computational models to auto-detect covid-19-related misinformation to aid these computational efforts we have constructed a benchmark dataset named as coaid covid-19 healthcare misinformation dataset coaid includes confirmed fake and true news articles from the fact-checked or reliable websites and posts at social platforms we also conducted quick data analysis to demonstrate the utility of coaid we contrasted the distinctive features between misinformation and facts about covid-19 and ran several state-of-the-art misinformation detection methods on coaid to show where solutions currently lie our aim is to call out for public attention to the problems of covid-19-related misinformation and work together to help develop accurate detection and deterrence of such misinformation according to the oxford english dictionary both misinformation 5 and disinformation 6 are either wrong or misleading information but disinformation is spread deliberately while misinformation is not necessarily literature provides more fine-grained definitions of various examples of misinformation on social media including rumor fake news hoax satire propaganda and even conspiracy 22 13  such that 1 fake news is the presentation of fake or misleading claims as news where the claims are misleading deliberately 8  2 hoax is deliberately fabricated falsehood with the intention to deceive a certain group of the population 8  3 rumor is unverified but relevant information that arise in contexts such as danger or potential threat that helps people make sense and manage risk 6  in this work we do not differentiate between misinformation and disinformation as it is virtually impossible to computationally determine ones intention we also interchangeably use fake news and misinformation in this work coaid comprehensively collects various misinformation examples across different platforms the construction of fake news dataset aim to extract useful features that can better distinguish fake news from true ones researchers have proposed a series of methods to extract news and verify the truthfulness of the news there exists several benchmark datasets for fake news detection that contain the linguistic features of news liar 25 is collected from political fact-checking website politifact it has 128k human labeled short statements from politifact api and each statement includes statement content speaker and context the truthfulness of each statement is evaluated by an editor from politifact by using detailed judgments and fine-grained labels pants-fire false barely true half-true mostly-true and true fa-kes 21 includes the news articles around the syrian war the authors first identified the major events in the syrian war from vdc violations documentation center and then retrieved 804 news articles related to the events from the following several news outlets the truthfulness of each article is justified by comparing the articles against the vdc dataset dhoju et al 5 proposed a method to collect a healthcare news dataset they first identified reliable who cdcnihncbi mainstream media outlets and unreliable media outlets dr melissa zimdarss list fact-checking websit snopescom and then used news article scraping tool newspaper3k 7 to crawl the news articles finally they used google cloud natural language api to filter out non-health news articles besides the news content other researchers also collected user engagement features of the news such as user engagement on online social media ghenai et al 9 collected 126 unproven cancer treatments that are scrutinized by an oncologist from medical sources dc science wikipedia and pubmed topic keywords were generated from the treatments and used to find related tweets fakenewsnet 24 is collected from fact-checking websites politifact and gossipcop news contents and the related ground truth labels are crawled from the two websites and the author collected social engagement through twitters advanced search api the dataset includes news id url article title tweet ids of tweets sharing the news and scripts to crawl news contents and social engagement it contains 1056 articles from politifact and 22864 articles from gossipcop each article is labeled as fake or true fakehealth 4 is collected from healthcare information review website health news review which reviews whether a news is satisfactory from 10 criteria daily publication of new content on this website was ceased at the end of 2018 the authors crawled users replies retweets and profiles by using article url and article title by using twitter api in addition existing datasets often focus only on the news coverage or articles and little attention has been paid to social media posts however fake social media posts can easily go viral on multiple social platforms and cause serious social disruptions for example a fake video on youtube claiming that a us patent on the coronavirus was applied in 2006 quickly went viral on multiple social media platforms 8  causing much confusion and discomfort as shown in table 1  coaid dataset not only includes both true and fake news but also has short claims and social media posts compared with news articles a claim is much shorter-only one or two sentences-such as eating garlic prevents infection with the new coronavirus in this work we term myth and rumor as fake claim in order to contrast those with true claim our dataset collects all possible features of interests including multi-modal information and user engagement data in addition coaid can be updated automatically to fetch the latest news the statistics of covid-19 news dataset is shown in table 2  in this section we introduce how we have collected healthcare misinformation related to covid-19 their reliable ground truth labels and associated user engagement features and how we manage to update the dataset automatically we have collected both facts and misinformation related to covid-19 including news articles and claims the publishing dates of the collected information range from december 1 2019 to may 1 2020 the topics include covid-19 coronavirus pneumonia flu 9  lock down stay home quarantine and ventilator  news article an example of news article is shown in figure 1 a we identified reliable media outlets and fact-checking websites and obtained urls of misinformation ie fake news and facts ie true news to collect true news articles we have crawled from 9 reliable media outlets that have been cross-checked as reliable including healthline 10  sciencedaily 11  nih 12 national institutes of health mnt 13  claim to collect claims of one or two sentences we referred to the who official website 26  who official twitter account 27 and mnt 28  we specifically separated true and fake claims for example only older adults and young people are at risk is a fake claim while 5g mobile networks do not spread covid-19 is a true claim after we obtained all urls to true and fake news related to covid-19 we used the newspaper3k 29 to fetch their corresponding title content abstract and keywords in total we have obtained 135 fake news articles 1568 true news articles 27 fake claims and 166 true claims we used twitter api to fetch user engagement data of both facts and misinformation figure 1 b shows user tweets related to the news in figure 1 a and figure 1 c shows user replies under a tweet  tweets we used the titles of news articles as the search queries and specified start and end dates to get the tweets discussing about the news in question in a certain period the retrieved user engagement features include user id tweets replies favorites retweets and location in total we obtained 9218 tweets about fake articles 87324 tweets about true news articles 457 tweets about fake claims and 6342 tweets about true claims which is summarized in table 2   replies we further obtained users replies of each tweet by using tweet ids in total we obtained 5721 replies under fake articles 64115 replies under true news articles 623 tweets about fake claims and 9764 tweets under true claims which is summarized in table 2  in addition to traditional mass media on social media individual users act as the producers of user generated contents forming the so-called self media an example of such self-media is shown in figure 2  as both true and fake news spread through self-media as well we have also collected both fake and true posts originated from five well-known social media platforms-facebook twitter instagram youtube and tiktok-also fact-checked by lead stories politifact factcheckorg checkyourfact afp fact check and health feedback we only count once for each distinctive fake post for example if a post was originally posted by a facebook user and then posted again by another user on twitter it is counted as 1 ie no duplicate fake posts the numbers in parentheses in table 2 represent the total number of posts before duplicates were removed per each post we only crawled its title without news content and listed the posts under claim in table 2  coaid can be updated automatically with the latest news and tweets we record the timestamp of the most recently added data each time and use it as the start date for the next search in summary we list the description of the extracted features in  coaid includes multi-modal news related ground truth labels and user engagement the detailed statistics of the dataset are shown in table 2  in this section we perform preliminary data analysis in order to illustrate the features of coaid then we perform several fake news detection methods to evaluate the challenges of covid-19 healthcare misinformation detection we use vader 10 to analyze users sentiments in tweets related to fake and true news articles we exclude the tweets that are completely neutral and plot the sentiment scores of tweets related to fake articles in figure 3 a and true news articles in figure 3b  respectively the bars show the numbers of tweets with different negative and positive degrees while the scatter plot shows the negative and positive scores the intensity of the hive represents the density of data points we can see from figure 3 that tweets related to fake news are more negative and have stronger sentiment polarities than those related to true news articles next we analyze the top 30 frequent hashtags in tweets related to fake and true news articles respectively for a more intuitive view we delete the most frequent hashtags coronavirus covid19 covid19 and covid as they appear almost in every tweet we show the frequency of hashtags in tweets related to fake and true news articles in figure 4 a and figure 4b  respectively we find that the hashtag distributions of tweets about fake and true news articles are quite different while the hashtags in tweets about true news articles are mainly related to healthcare those in tweets about fake news cover more diverse topics including conspiracy bioweapon and fake cure vitaminc in figure 5  next we list ten most common claims of covid-19 on twitter and their corresponding frequencies we merge similar claims into one such as eating garlic prevents infection with the new coronavirus and garlic protects against coronaviruses we can see that the most frequently discussed claim is covid-19 is just like the flu followed by covid-19 is airborne and the new coronavirus can be transmitted through mosquito bites we then plot the daily counts of tweets related to two claims covid-19 is just like the flu and 5g mobile networks spread covid-19 in figure 6 a and 6b respectively we can see that there were most tweets about covid-19 is just like the flu at the beginning of the global outbreak which is around march 12 2020 note that this topic is mentioned almost everyday the tweets about 5g mobile networks spread covid-19 were mostly on the april 10 2020 when 5g towers were being set on fire but afterward the number declined as the claim was debunked to demonstrate the main utility of m here we conduct comparative experiments on the misinformation detection task we include both simple methods and state-of-the-art methods as baselines we consider the following baseline methods for covid-19 misinformation detection  csi 20  csi is a hybrid deep learning-based misinformation detection model that utilizes information from article content and user response the article representation is modeled via an lstm model with the article embedding via doc2vec 12 and user response  samev 3  same uses news image content metadata and users sentiments for fake news detection as the majority of news does not have a cover image the visual part of the model is ignored and the baseline is termed as samev  han 27  han has two levels of attention mechanisms applied at the word and sentence-level enabling it to attend differently to more and less important content for document classification  defend 23  defend utilizes the hierarchical attention network as han on article content and a co-attention mechanism between article content and user comment for misinformation detection to evaluate the performance of misinformation detection algorithms we use the following metrics which are commonly used to evaluate classifiers in related areas pr-auc precision recall and f1 score we implement all models with keras we randomly use the labels of 75 news articles for training and predict the remaining 25 we set the hidden dimension of all models to 100 the word embeddings are initialized by glove 19 and the dimension of pre-trained word embeddings is set to 100 for deep learning methods we use adam with a minibatch of 50 articles on the dataset and the training epoch is set as 10 for a fair comparison we use the cross-entropy loss for all methods finally we run each method five times and report the average score in table 4  from table 4  we can see that state-of-the-art methods perform better than simple methods as they incorporate signals from user engagement better capturing contextual information however as the dataset is quite imbalanced the models tend to generate many fake positive cases thus the recall and f1 values are far from being satisfactory as the proportion of true and fake information is likely to be even more skewer practical detection solutions need to handle this type of imbalance setting more in addition with regard to healthcare news it is often more scarce and rare for lay persons to give discriminating comments due to lack of professional knowledge for example who tweeted there is no evidence that regularly rinsing the nose with saline solution has protected people from infection with the new coronavirus the majority of comments for the tweet are unrelated useless and even includes hate speech and other misinformation about  in this paper we present a comprehensive covid-19 misinformation dataset coaid which contains news articles user engagement and social platform posts we describe how we collected the dataset in addition we conduct brief data analysis to show the distinctive features between misinformation and facts and demonstrate the future research directions through a fake news detection task over several state-of-the-art methods we hope researchers to find covid-19 useful for their research and together contribute to flatten the curve of covid-19  predicting the growth and trend of covid-19 pandemic using machine learning and cloud computing journal pre-proof predicting the growth and trend of covid-19 pandemic using machine learning and cloud computing end users highlights predicting the growth and trend of covid-19 pandemic using machine learning and cloud computing predicting the growth and trend of covid-19 pandemic using machine learning and cloud computing a r t i c l e i n f o shreshth tuli shikhar tuli rakesh tuli sukhpal gill singh  propose a novel scheme to predict the impact of covid-19 pandemic  design a model based on cloud computing and machine learning for real-time prediction  show improved prediction accuracy compared to baseline method  highlight key future research directions and emerging trends a b s t r a c t the outbreak of covid-19 coronavirus namely sars-cov-2 has created a calamitous situation throughout the world the cumulative incidence of covid-19 is rapidly increasing day by day machine learning ml and cloud computing can be deployed very effectively to track the disease predict growth of the epidemic and design strategies and policy to manage its spread this study applies an improved mathematical model to analyse and predict the growth of the epidemic an ml-based improved model has been applied to predict the potential threat of covid-19 in countries worldwide we show that using iterative weighting for fitting generalized inverse weibull distribution a better fit can be obtained to develop a prediction framework this has been deployed on a cloud computing platform for more accurate and real-time prediction of the growth behavior of the epidemic a data driven approach with higher accuracy as here can be very useful for a proactive response from the government and citizens finally we propose a set of research opportunities and setup grounds for further practical applications  the novel coronavirus disease was first reported on 31 december 2019 in the wuhan hubei province china it started spreading rapidly across the world 1  the cumulative incidence of the causitive virus sars-cov-2 is rapidly increasing and has affected 196 countries and territories with usa spain italy uk and france being the most affected 2  world health organization who has declared the coronavirus outbreak a pandemic while the virus continues to spread 3  as on 4 may 2020 a total of 3581884 confirmed positive cases have been reported leading to 248558 deaths 2  the major difference between the pandemic caused by cov-2 and related viruses like severe acute respiratory syndrome sars and middle east respiratory syndrome mers is the ability of cov-2 to spread rapidly through human contact and leave nearly 20 infected subjects as symptom-less carriers 4  moreover various studies reported that the disease caused by cov-2 is more dangerous for people with weak immune system the elderly people and patients with life threatening diseases like cancer diabetes neurological conditions coronary heart disease and hivaids are more vulnerable to severe effects of covid-19 5  in the absence of any curative drug the only solution is to slow down the spread by exercising social distancing to block the chain of spread of the virus this behavior of cov-2 requires developing robust mathematical basis for tracking its spread and automation of the tracking tools for on line dynamic decision making there is a need for innovative solutions to develop manage and analyse big data on the growing network of infected subjects patient details their community movements and integrate with clinical trials and pharmaceutical genomic and public health data 6  multiple sources of data including text messages online communications social media and web articles can be very helpful in analyzing the growth of infection with community behaviour  wrapping this data with machine learning ml and artificial intelligence ai researchers can forecast where and when the disease is likely to spread and notify those regions to match the required arrangements travel history of infected subjects can be tracked automatically to study epidemiological correlations with the spread of the disease some community ml 12 can be utilized to handle large data and intelligently predict the spread of the disease cloud computing 13 can be used to rapidly enhance the prediction process using high-speed computations 7  novel energy-efficient edge systems can be used to procure data in order to bring down power consumption in this paper we present a prediction model deployed using fogbus framework 14 for accurate prediction of the number of covid-19 cases the rise and the fall of the number of cases in near future and the date when various countries may expect the pandemic to end we also provide a detailed comparison with a baseline model and show how catastrophic the effects can be if poorly fitting models are used we present a prediction scheme based on the ml model which can be used in remote cloud nodes for real-time prediction allowing governments and citizens to respond proactively finally we summarize this work and present various research directions the rest of the paper is organized as follows section 2 presents the prediction model and performance comparison section 3 provides discussions on the results biases implementation and possible deviations in future section 4 provides research opportunities and emerging trends finally section 5 concludes the work and describes the future research opportunities machine learning ml and data science community are striving hard to improve the forecasts of epidemiological models and analyze the information flowing over twitter for the development of management strategies and the assessment of impact of policies to curb its spread various datasets in this regard have been openly released to the public yet there is a need to capture develop and analyse more data as the covid-19 grows worldwide 15 16  the novel coronavirus is leaving a deep socio-economic impact globally due to the ease of virus transmission primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes countries which are densely populated need to be on a higher alert 17  to gain more insight on how covid-19 is impacting the world population and to predict the number of covid-19 cases and dates when the pandemic may be expected to end in various countries we propose a machine learning model that can be run continuously on cloud data centers cdcs for accurate spread prediction and proactive development of strategic response by the government and citizens the dataset used in this case study is the our world in data by hannah ritchie 2  the dataset is updated daily from the world health organization who situation reports 3  more details about the dataset are available at httpsourworldindataorgcoronavirus-source-data the ml models are built to make a good advanced prediction of the number of new cases and the dates when the pandemic might end to provide fail-safe computation and quick data analysis we propose a framework to deploy these models on cloud datacenters as shown in figure 1 in a cloud based environment the government hospitals and private health-centers continuously send their positive patient count population density average and median age weather conditions health facilities etc are also to be integrated for enhancing the accuracy of the predictions for this case study we used three instances of single core azure b1s virtual machines with 1-gib ram ssd storage and 64-bit microsoft windows server 2016 4  we used the healthfog 12 framework leveraging the fogbus 14 for deploying multiple analysis tasks in an ensemble learning fashion to predict various metrics like the number of anticipated facilities to manage patients and the hospitals we analyzed that the cost of tracking patients on a daily basis amortized cpu consumption and cloud execution is 37 and only 12 usd per day as the dataset size increases computationally more powerful resources would be needed many recent works have suggested that the covid-19 spread follows exponential distribution 18 19 20  as per empirical evaluations and previous datasets on sars-cov-1 virus pandemic many sources have shown that data corresponding to new cases with time has large number of outliers and may or may not follow a standard distribution like gaussian or exponential 21 22 23 24  in recent study by data-driven innovation laboratory singapore university of technology and design sutd 5  the regression curves were drawn using the susceptible-infected-recovered model 25 and gaussian distribution was deployed to estimate the number of cases with time however in the previously reported studies on the earlier version of the virus namely sara-cov-1 the data was reported to follow generalized inverse weibull giw distribution 26 better than gaussian as shown in figure 2 details of robust weibull fitting follow in the next section detailed comparison for sars-cov-2 has been described in the next section this fits the following function to the data here   denotes the number of cases with  where  0 is the time in number of days from the first case and    0   are parameters of the model now we can find the appropriate values of the parameters  and to minimize the error between the predicted cases     and the actual cases   this can be done using the popular machine learning technique of levenberg-marquardt lm for curve fitting 27  however as various sources have suggested in initial stages of covid-19 the data has many outliers and noise this makes it hard to accurately predict the number of cases thus we propose an iterative weighting strategy and call our fitting technique robust fitting a diagrammatic representation of the iterative weighting process is shown in figure 3  the main idea is as follows we maintain weights for all data points   in every iteration   starting from 0 as  first we fit a curve using the lm technique with weights of all data points as 1 thus 0  1   second we find the weight corresponding to every point for the next iteration  1  as simply in the above equation we first take   function defined as         for the distances of all points along axis from the curve   this is to have a higher value for points far from the curve and near 0 value for closer points this is then standardized by dividing with max value over all points and subtracted from 1 to get a weight corresponding to each point this weight is then standardized using function so that sum of all weights is 1 the curve is fit again using lm method now with the new weights 1  the algorithm converges when the sum total deviation of all weights becomes lower than a threshold value to find the best fitting distribution model for the data corresponding to covid-19 we studied the data on daily new confirmed covid cases five sets of global data on daily new covid-19 cases were used to fit parameters of different types of distributions finally we identified the best performing 5 distributions the results are shown in table 1  we observe that using the iteratively weighted approach the inverse weibull function fits the best to the covid-19 dataset as compared to the iterative versions of gaussian beta 4-parameter fisher-tippet extreme value distribution and log normal functions when applied to the same dataset iterative weibull showed an average mape of 12 lower than non-iteratively weighted weibull a step-by-step algorithm for iteratively weighted curve fitting using the giw distribution called robust weibull is given in algorithm 1 to compare the proposed robust weibull fitting model we use the baseline proposed by jianxi luo from sutd 3 table 2 shows the model predictions of the spread of the covid-19 for every major country for which sufficient data was available and model fits had 2  05 using the proposed model as shown in the table the proposed model performs significantly better than the baseline as shown in figure 4 6  the predictions of the baseline gaussian model deployed by sutd are overoptimistic following such models could lead to premature uplifting of the lockdown causing adverse effect on management of the epidemic having better fit models as proposed here could help plan a better strategy based on more accurate predictions and future scenarios 6 curves and predictions of all countries have been given in appendix figure 7 shows the total predicted number of cases for all countries across the globe here we have neglected those countries where the data is insufficient for making predictions or the number of days for data is less than 30 as shown in figure 4 explained in model section the fit curve can be used to predict the number of cases that will have to be dealt by the country assuming the same trend continues the figure illustrates that the maximum number of total cases will be in the north america region the number of cases will also be high in the european continent russia and eastern asia including china the original epicenter of the disease the model was also applied to the data corresponding to the number of deaths with time figure 5 shows curves corresponding to both new cases and deaths across the world using the predicted total deaths the expected mortality rate can be calculated as 100  predicted total deaths  the predicted mortality rates of the world and few countries are shown in figure 6  this section discuss about the biases in data integration details with tracking systems and trend possibilities in the future the outbreak of sars-cov-2 and its corresponding diseases covid-19 has received diverse responses from different countries countries like india china and australia have imposed partial to full nation-wide lock-downs leading to mixed repercussions 28 29 30 31  other countries like sweden have imposed little to no restrictions such factors definitely affect the distribution of cases and hence the curve parameters moreover there is bias in data due to diverse travel histories and contact demographic histories of people from wuhan 32  reports from health systems in wuhan are overwhelmed and the only possible way of quantifying spread of coronavirus is through cumulative cases in each country 33  the proposed giw model is applied separately to each country to fit the model parameters to the distribution of new cases with time the parameters themselves incorporate predicting the growth and trend of covid-19 pandemic the biases from travel histories of citizens and migrants lock-downs and social distancing measures taken specifically by each country having a holistic models that can take these indicators as quantified inputs to generate curve without having any training data would require development and collection of large datasets such models can be explored in future with efficient and up-to-date tracking mechanisms the spread of the disease can be traced once authorities have information on the spread of the virus relevant decisions can be made including locking down target areas and increasing testing measures in adjacent areas only with systematic and planned testing can we mitigate the negative effects of the spread of this disease 34  government institutions can utilize cloud services to deploy such frameworks feeding data from such tracking sensors and predict in near-real time the number of cases in the near future 35  further if we frequently update the dataset and utilize other demographic indicators like population density temperatures and age distribution in the proposed model we can make more reliable and accurate predictions for the last expected case this enables the authorities to lift the lock-down in a phased manner thus keeping a check on the post-lockdown rise in cases currently travel and group activities have been restricted world-over as lock-downs are lifted the number of new cases and deaths might change significantly from the proposed predicted trends other factors like virus mutations 36 would also affect the distribution in future hence continuous work is required to ensure accurate predictions are made and correct measures can be taken table 2 predictions and error comparisons country wise predictions using robust weibull model and error comparison between robust weibull and baseline gaussian model we predict the total number of cases that will be reached and the last case date ie when the model predicts new cases  1 we also predict the date when the total number will reach 97 of the total expected cases such data is critical to prepare the healthcare services in advance  all data uptil 4 may 2020 has been used to generate the prediction results shown in figure 8  the covid-19 pandemic has opened several new directions of research for the current and future pandemics the prominent research opportunities are described as follows 1 incorporating other indicators important parameters like population density distribution of age individual and community movements level of healthcare facilities available strain type and virulence of the virus etc need to be included in the regression model to further enhance the prediction accuracy 2 integrating with other time series models models like arima 37 can be integrated with weibull function for further time series analysis and predictions 3 predicting protein structure of cov-2 ai can be utilized to predict the structure and function of various proteins associated with cov-2 and their interaction with the host human proteins and cellular environment the contribution of various socio-economic variables that determine the vulnerability spread and progression of the epidemic can be predicted by developing suitable algorithms this can help efficiently decide resource allocation in large countries with limited healthcare resources 4 analyzing social media data using ai we can also explore and analyze social media data for real time collection of epidemiological data related to covid-19 15  5 contact-less treatment and drug delivery using robotics ai based robots can be used to perform contactless delivery and treat patients remotely to reduce involvement of medical staff with infected people further there have been considerable improvements in air quality across the globe due to covid-19 enforced lock-downs 6 climate change there have been considerable improvements in air quality across the globe due to covid19 enforced lock-downs however there is a prevailing conjecture of the revenge pollution following these lockdowns 38  more extensive studies considering age distributions and demographics with other characteristics van be studied as part of future work 7 risk assessment the risk of severe disease related with covid-19 for people with different age can be predicted using ai using such algorithms proactive measures can be taken to prevent virus being spread to sensitive groups of the society 8 real time sensors and visual imaging ai based proactive measures can be taken to prevent the spread of the virus to sensitive groups in the society real time sensors can be used for example in traffic camera or surveillance which track covid-19 symptoms based on visual imaging and tracking apps and inform respective hospitals and administrative authorities for punitive action 39  tracking needs to cover all stages from ports of entries to public places and hospitals 40  the research directions and challenges are summarized in figure 9  in this study we have discussed how improved mathematical modelling machine learning and cloud computing can help to predict the growth of the epidemic proactively further a case study has been presented which shows the severity of the spread of cov-2 in countries worldwide using the proposed robust weibull model based on iterative weighting we show that our model is able to make statistically better predictions than the baseline the baseline gaussian model shows an over-optimistic picture of the covid-19 scenario a poorly fitting model could lead to a non optimal decision making leading to worsening of public health situation our prediction model is available online at httpsgithubcomshreshthtulicovid-19-prediction the dataset used for this work is the our world dataset available at httpsgithubcomowidcovid-19-dat atreemasterpublicdata few interactive graphs can be seen at httpscollaborationcoraltelecomcovid predicting the growth and trend of covid-19 pandemic   exploratory analysis of covid-19 tweets using topic modeling umap and digraphs catherine ordun allen booz  hamilton sanjay purushotham edward raff  this paper illustrates five different techniques to assess the distinctiveness of topics key terms and features speed of information dissemination and network behaviors for covid19 tweets first we use pattern matching and second topic modeling through latent dirichlet allocation lda to generate twenty different topics that discuss case spread healthcare workers and personal protective equipment ppe one topic specific to us cases would start to uptick immediately after live white house coronavirus task force briefings implying that many twitter users are paying attention to government announcements we contribute machine learning methods not previously reported in the covid19 twitter literature this includes our third method uniform manifold approximation and projection umap that identifies unique clustering-behavior of distinct topics to improve our understanding of important themes in the corpus and help assess the quality of generated topics fourth we calculated retweeting times to understand how fast information about covid19 propagates on twitter our analysis indicates that the median retweeting time of covid19 for a sample corpus in march 2020 was 287 hours approximately 50 minutes faster than repostings from chinese social media about h7n9 in march 2013 lastly we sought to understand retweet cascades by visualizing the connections of users over time from fast to slow retweeting as the time to retweet increases the density of connections also increase where in our sample we found distinct users dominating the attention of covid19 retweeters one of the simplest highlights of this analysis is that early-stage descriptive methods like regular expressions can successfully identify high-level themes which were consistently verified as important through every subsequent analysis  monitoring public conversations on twitter about healthcare and policy issues provides one barometer of american and global sentiment about covid19 this is particularly valuable as the situation with covid19 changes every day and is unpredictable during these unprecedented times twitter has been used as an early warning notifier emergency communication channel public perception monitor and proxy public health surveillance data source in a variety of disaster and disease outbreaks from hurricanes 1  terrorist bombings 2  tsunamis 3  earthquakes 4  seasonal influenza 5  swine flu 6  and ebola 7  in this paper we conduct an exploratory analysis of topics and network dynamics of covid19 tweets since january 2020 there have been a growing number of papers that analyze twitter activity during the covid19 pandemic in the united states we provide a sample of papers published since january 1 2020 in table i  chen et al analyzed the frequency of 22 different keywords such as coronavirus corona cdc wuhan sinophobia and covid-19 analyzed across 50 million tweets from january 22 2020 to march 16 2020 8  thelwall also published an analysis of topics for english-language tweets from march 10-29 2020 9  singh et al 10 analyzed distribution of languages and propogation of myths sharma et al 11 implemented sentiment modeling to understand perception of public policy and cinelli et al 12 compared twitter against other social media platforms to model information spread our contributions are applying machine learning methods not previously analyzed on covid19 twitter data mainly uniform manifold approximation and projection umap to visualize lda generated topics and directed graph visualizations of covid19 retweet cascades topics generated by lda can be difficult to interpret and while there exist coherence values 22 that are intended to score the interpretability of topics they continue to be difficult to interpret and are subjective as a result we apply umap a dimensionality reduction algorithm and visualization tool that clusters documents by topic vectorizing the tweets using term-frequency inverse-document-frequency tf-idf and plotting a umap visualization with the assigned topics from lda allowed us to identify strongly localized and distinct topics we then visualized retweet cascades which describes how a social media network propagates information 23  through the use of graph models to understand how dense networks become over time and which users dominate the covid19 conversations in our retweeting time analysis we found that the median time for covid19 messages to be retweeted is approximately 50 minutes faster than h7n9 messages during a march 2013 outbreak in china possibly indicating the global nature volume and intensity of the covid19 pandemic our keyword analysis and topic modeling were also rigorously explored where we found that specific topics were triggered to uptick by live white house briefings implying that covid19 twitter 14 30990645 jan 1 -apr 4 2020 x medford et al 15 126049 jan 14 -jan 28  2020 x x x x singh et al 10 2792513 jan 16  2020 -mar 15  2020 x x x x lopez et al 16 6468526 jan 22 -mar 13  2020 x x x cinelli et al 12 1187482 jan 27 -feb 14 2020 x x x kouzy et al 17 673 feb 27  2020 x x alshaabi et al 18 unknown mar 1 -mar 21 2020 x x sharma et al 11 30800000 mar 1 2020 -mar 30 2020 x x x x x x x chen et al 8 8919411 mar 5 2020 -mar 12  2020 x schild 19 222212841 nov 1 2019 -mar 22  2020 x x x x yang et al 20 unknown mar 9 2020 -mar 29  2020 x x ours 23830322 mar 24 -apr 9  2020 x x x x x yasin-kabir et al 21 100000000 mar 5 2020 -apr 24  2020 x x x x users are highly attuned to government broadcasts we think this is important because it highlights how other researchers have identified that government agencies play a critical role in sharing information via twitter to improve situational awareness and disaster response 24  our lda models confirm that topics detected by thelwall et al 9 and sharma et al 11  who analyzed twitter during a similar period of time were also identified in our dataset which emphasized healthcare providers personal protective equipment such as masks and ventilators and cases of death this paper studies five research questions 1 what high-level trends can be inferred from covid19 tweets 2 are there any events that lead to spikes in covid19 twitter activity 3 which topics are distinct from each other 4 how does the speed of retweeting in covid19 compare to other emergencies and especially similar infectious disease outbreaks 5 how do covid19 networks behave as information spreads the paper begins with data collection followed by the five stages of our analysis keyword trend analysis topic modeling umap time-to-retweet analysis and network analysis our methods and results are explained in each section the paper concludes with limitations of our analysis the appendix provides additional graphs as supporting evidence ii data collection similar to researchers in table i  we collected twitter data by leveraging the free streaming api from march 24 2020 to april 9 2020 we collected 23830322 173 gb tweets note in this paper we refer to the twitter data interchangeably as both dataset and corpora and refer to the posts as tweets our dataset is a collection of tweets from different time periods shown in table v  using the twitter api through tweepy a python twitter mining and authentication api we first queried the twitter track on twelve query terms to capture a healthcare-focused dataset icu beds ppe masks long hours deaths hospitalized cases ventilators respiratory hospitals covid and coronavirus for the keyword analysis topic modeling and umap tasks we analyzed non-retweets that brought the corpus down to 5506223 tweets in the time-to-retweet and network analysis we included retweets but selected a sample out of the larger 238 million corpus of 736561 tweets our preprocessing steps are described in the data analysis section that follows prior to applying keyword analysis we first had to preprocess the corpus on the text field first we removed retweets using regular expressions in order to focus the text on original tweets and authorship as opposed to retweets that can inflate the number of messages in the corpus we use no-retweeted corpora for both the keyword trend analysis and the topic modeling and umap analyses further we formatted datetime to utc format removed digits short words less than 3 characters extended the nltk stopwords list to also exclude coronavirus covid19 19 covid removed https hyperlinks removed  signs for usernames removed non-latin characters such as arabic or chinese characters and implemented lower-casing stemming and tokenization finally using regular expressions we extracted tweets that table vi and the frequencies of tweets per minute here in table ii  the greatest rate of tweets occurred for the tweets consisting of the term mask mean 55044 in table ii  followed by hospital mean 32370 and vent mean 24811 tweets of less than 10 mean tweets per minute came from groups about testing positive being in serious condition exposure cough and fever this may indicate that people are discussing the issues around covid19 more frequently than symptoms and health conditions in this dataset we will later find out that several themes consistent with these keyword findings are mentioned in topic modeling to include personal protective equipment ppe like ventilators and masks and healthcare workers like nurses and doctors lda are mixture models meaning that documents can belong to multiple topics and membership is fractional 25  further each topic is a mixture of words where words can be shared among topics this allows for a fuzzy form of unsupervised clustering where a single document can belong to multiple topics each with an associated probability lda is a bag of words model where each vector is a count of terms lda requires the number of topics to be specified similar to methods described by syed et al 26  we ran 15 different lda experiments varying the number of topics from 2 to 30 and selected the model with the highest coherence value score we selected the lda model that generated 20 topics with a medium coherence value score of 0344 roder et al 22 developed the coherence value as a metric that calculates the agreement of a set of pairs and word subsets and their associated word probabilities into a single score in general topics are interpreted as being coherent if all or most of terms are related our final model generated 20 topics using the default figure 2 and include the terms generated and each topics coherence score measuring interpretability similar to the high-level trends inferred from extracting keywords themes about ppe and healthcare workers dominate the nature of topics the terms generated also indicate emerging words in public conversation including hydroxychloroquine and asymptomatic our results also show four topics that are in non-english languages in our preprocessing we removed non-latin characters in order to filter out a high volume of arabic and chinese characters in twitter there exists a tweet object metadata field of lang for language to filter tweets by a specific language like english eng however we decided not to filter against the lang element because upon observation approximately 25 of the dataset consisted of an undefined language tag meaning that no language was indicated although it appears to be a small fraction removing even the undefined tweets would have removed several thousand tweets some of these tweets that are tagged as undefined are in english but contain hashtags emojis and arabic characters as a result we did not filter out for english language leading our topics to be a mix of english spanish italian french and portuguese although this introduced challenges in interpretation we feel it demonstrates the global nature of worldwide conversations about covid19 occurring on twitter this is consistent with what singh et al singh et al 10 reported as a variety of languages in covid19 tweets upon analyzing over 2 million tweets as a result we labeled the four topics by the language of the terms in the respective topics spanish topic 1 portuguese topic 14 italian topic 16 and french topic 19 we used google translate to infer the language of the terms when examining the distribution of the 20 topics across the corpora in figure 2  topics 18 potus 12 casedeathnew 13 maskppeventil and 2 likelookwork were the top five in the entire corpora for each plot we labeled each topic with the first three terms of each topic for interpretability in our trend analysis we summed the number of tweets per minute and then applied a moving weighted average of 60 minutes for topics march 24 -march 28 and 60 minutes for topics march 30 to april 8th we provided two different plots in order to visualize smaller time frames such as march 24 of 44 minutes compared to figure 3 and figure 4 show similar trends on a time-series basis per minute across the entire corpora of 5506223 tweets these plots are in a style of broken axes 2 to indicate that the corpora are not continuous periods of time but discrete time frames which we selected to plot on one axis for convenience and legibility we direct the reader to table v for reference on the start and end datetimes which are in utc format so please adjust accordingly for time zone the x-axis denotes the number of minutes where the entire 2 httpsgithubcombendichterbrokenaxes corpora is 8463 total minutes of tweets figure 3 shows that for the corpora of march 24 25 and 28 the topics denoted in hash-marked lines focused on topic 18 potus and topic 13 maskppeventil trended greatest for the later time periods of march 30 march 31 april 4 5 and 8 in figure 4  topic 18 potus and topic 13 maskppeventil also in hash-marked lines continued to trended high it is also interesting that topic 18 was never replaced as the top trending topic across a span of 17 days april 8 2020 also includes early hours of april 9 2020 est potentially as this may have been a proxy for active government listening the time series would temporally decrease in frequency during overnight hours between the we applied change point detection in the time series of tweets per minute for topic 18 in the datasets march 24 2020 april 3 -4 2020 april 5 -6 2020 and april 8 2020 to identify whether the live press briefings coincided with inflections in time using the ruptures python package 27 containing a variety of change point detection methods we used binary segmentation 28  a standard method for change point detection given a sequence of data y 1n  y 1   y n  the model will have m changepoints with their positions  1m   1    m  each changepoint position is an integer between 1 and n  1 the m changepoints split the time series data into m  1 segments with the ith segment containing y   i1  1   i  changepoints are identified by minimizing a cost function c for a given segment where f m is a penalty to prevent overfitting where twice the negative log-likelihood is a commonly used cost function binary segmentation detects multiple changepoints across the time series by repeatedly testing on different subsets of the sequence it checks to see if a  exists that satisfies cy 1  cy  1n     cy 1n  if not then no changepoint is detected and the method stops but if a changepoint is detected the data are split into two segments consisting of the time series before figure 7 blue and after figure 7 pink the changepoint we can clearly see in figure 7 that the timing of the white house briefing indicates a changepoint in time giving us the intuition that this briefing influenced an uptick in the the number of tweets we provide additional examples in the appendix our topic findings are consistent with the published analyses on covid19 and twitter such as 10 who found major themes of healthcare and illness and international dialogue as we noticed in our four non-english topics they are also similar to by thelwall et al 9 who manually reviewed tweets from a corpus of 12 million tweets occurring earlier and overlapping our dataset march 10 -29 similar topics from their findings to ours includes lockdown life politics safety messages people with covid-19 support for key workers work and covid-19 factsnews further our dataset of covid19 tweets from march 24 to april 8 2020 occurred during a month of exponential case growth by the end of our data collection period the number of cases had increased by 7 times to 427460 cases on april 8 2020 29  the key topics we identified using our multiple methods were representative of the public conversations being had in news outlets during march and april including term-frequency inverse-document-frequency tf-idf 34 is a weight that signifies how valuable a term is within a document in a corpus and can be calculated at the n-gram level tf-idf has been widely applied for feature extraction on tweets used for text classification 35 36  analyzing sentiment 37  and for text matching in political rumor detection 23 with tf-idf unique words carry greater information and value than common high frequency words across the corpus tf-idf can be calculated as follows where i is the term j is the document and n is the total number of documents in the corpus tf-idf calculates the term frequency tf ij multiplied by the log of the inverse document frequency n dfi  the term frequency tf ij is calculated as the frequency of i in j divided by all terms i in given j the inverse document frequency is n dfi is the log of the total number of documents j in the corpus divided by the number of documents j containing term i using the scikit-learn implementation of tfidfvectorizer and setting maxfeatures to 10000 we transformed our corpus of 5506223 tweets into a r nk sparse dimensional matrix of shape 5506223 10000 note prior to fitting the vectorizer our corpus of tweets was pre-processed during the keyword analysis stage we chose to visualize how the 20 topics grouped together using uniform manifold approximation and projection umap 38  umap is a dimension reduction algorithm that finds a low dimensional representation of data with similar topological properties as the high dimensional space it measures the local distance of points across a neighborhood graph of the high dimensional data capturing what is called a fuzzy topological representation of the data optimization is then used to find the closest fuzzy topological structure by first approximating nearest neighbors using the nearest-neighbor-descent algorithm and then minimizing local distances of the approximate topology using stochastic gradient descent 39  when compared to t-distributed stochastic neighbor embedding t-sne umap has been observed to be faster 40 with clearer separation of groups due to compute limitations in fitting the entire high dimensional vector of nearly 55m records we randomly sampled one million records we created an embedding of the vectors along two components to fit the umap model with the hellinger metric which compares distances between probability distributions as follows we visualized the word vectors with their respective labels which were the assigned topics generated from the lda model we used the default parameters of nneighbors  15 and mindist  01 figure 6 presents the visualization of the tf-idf word vectors for each of the 1 million tweets with their labeled topics umap is supposed to preserve local and global structure of data unlike t-sne that separates groups but does not preserve global structure as a result umap visualizations intend to allow the reader to interpret distances between groups as meaningful in figure 6 each topic is colorcoded by its respective topic the umap plots appear to provide further evidence of the quality and number of topics generated our observations is that many of these topic clusters appear to have a single dominant color indicating distinct grouping there is strong local clustering for topics that were also prominent in the keyword analysis and topic modeling time series plots a very distinct and separated mass of purple tweets represents the 100 na topic which is an undefined topic this means that the lda model outputted equal scores across all 20 topics for any single tweet as a result we could not assign a topic to these tweets because they all had uniform scores but this visualization informs us that the contents of these tweets were uniquely distinct from the others examples of tweets in this 100 na cateogry include see democrats are always guilty of whatever why are people still getting in cruise ships thank you mike you are always helping others and sponsoring anchors media shows we cannot let this womans brave and courageous actions go to waste chinaliedpeopledied chinaneedstopay i wish people in this country would just stay the hell home instead of going to the beach other observations reveal that the mask-related topic 10 in purple and potentially a combination of 8 and 9 in red are distinct from the mass of noisy topics in the center of the plot we can also see distinct separation of aqua-colored topic 18 potus and potentially topics 5 and 6 in yellow we refer the reader to other examples where umap has been leveraged for twitter analysis to include darwish et al 41 for identifying clusters of twitter users with controversial topic similarity vargas 42 for event detection political polarization by darwish et al 41 and estimating political leaning of users by 43  retweeting is a special activity reserved for twitter where any user can retweet messages which allows them to disseminate their messages rapidly to their followers further a highly retweeted tweet might signal that an issue has attracted attention in the highly competitive twitter environment and may give insight about issues that resonate with the public 44  whereas in the first three analyses we used no retweets in the time-series and network modeling that follows we exclusively use retweets we began by measuring time-toretweet wang et al 1 calls this response time and used it to measure response efficiency and speed of information dissemination during hurricane sandy wang analyzed 986579 tweets and found that 67 of re-tweets occur within 1 h 1  we researched how fast other users retweet in emergency situations such as what spiro 45 reported for natural disasters and how earle 46 reported as 19 seconds for retweeting about an earthquake we extracted metadata from our corpora for the tweet user and entities objects for reference we direct the reader to the twitter developer guide that provides a detailed overview of each object 47  due to compute limitations we selected a sample that consisted of 736561 tweets that included retweets from the corpora of march 24 -28 2020 however since we were only focused on retweets out of the corpus of 736561 tweets we reduced it to 567909 77 that were only retweets the metadata we used for both our time-to-retweet and directed graph analyses in the next section included 1 createdat string -utc time when this tweet was created 2 text string -the actual utf-8 text of the status update see twitter-text for details on what characters are currently considered valid 3 from the user object the idstr string -the string representation of the unique identifier for this user 4 from the retweetedstatus object tweet -the cre-atedat utc time when the retweet was created 5 from the retweetedstatus object tweet -the idstr which is the unique identifier for the retweeting user we used the corpus of retweets and analyzed the time between the tweet createdat and the retweeted createdat here the rtobject is the datetime in utc format for when the message that was retweeted was originally posted the twobject is the datetime in utc format when the current tweet was posted as a result the datetime for the rtobject is older than the datetime for the current tweet this measures the time it took for the author of the current tweet to retweet the originating message this is similar to kuang et al 48 who defined response time of the retweet to be the time difference between the time of the first retweet and that of the origin tweet further spiro et al 45 calls these waiting times the median time-to-retweet for our corpus was 287 hours meaning that half of the tweets occurred within this time less than what wang reported as 10 hour and the mean was 123 hours figure 9 shows the histogram of the number of tweets by their time to retweet in seconds and figure 10 shows it in hours further we found that compared to the 2013 avian influenza outbreak h7n9 in china described by zhang et al 49 covid19 retweeters sent more messages earlier than h7n9 zhang analyzed the log distribution of 61024 h7n9related posts during april 2013 and plotted reposting time of messages on sina weibo a chinese twitter-like platform and one of the largest microblogging sites in china figure 12  zhang found that h7n9 reposting occurred with a median time of 222 minutes ie 37 hours and a mean of 8520 minutes ie 142 hours compared to zhangs study we found our median retweet time to be 287 hours about 50 minutes faster than the reposting time during h7n9 of 37 hours when comparing figure 11 and figure 12  it appears that covid19 retweeting does now completely slow down until 278 hours later 10 4 seconds for h7n9 it appears to slow down much earlier by 10 seconds unfortunately few studies appear to document retweeting times during infectious disease outbreaks which made it hard to compare how covid19 retweeting behavior against similar situations further the h7n9 outbreak in china occurred seven years ago and may not be a comparable set of data for numerous reasons chinese social media may not represent similar behaviors with american twitter and this analysis does not take into account multiple factors that imply retweeting behavior to include the context the users position and the time the tweet was posted 44  we also analyzed what rapid retweeters or those retweeting messages even faster than the median in less than 10000 seconds were saying in figure 21 we plotted the top 50 tf-idf features by their scores for the text of the retweets it is intuitive to see that urls are being retweeted quickly by the presence of https in the body of the retweeted text this is also consistent with studies by suh et al 50 who indicated that tweets with urls were a significant factor impacting retweetability we found terms that were frequently mentioned during the early-stage keyword analysis and topic modeling mentioned again cases ventilators hospitals deaths masks test american cuomo york president china and news when analyzing the descriptions of the users who were retweeted in figure 21  we ran the tf-idf vectorizer on bigrams in order to elicit more interpretable terms user accounts whose tweets were rapidly retweeted appeared to describe themselves as political news-related or some form of social media account all of which are difficult to verify as real or fake vii network modeling we analyzed the network dynamics of nine different time periods within the march 24 -28 2020 covid19 dataset and visualized them based on their speed of retweeting these types of graphs have been referred to as retweet cascades which describes how a social media network propagates information 23  similar methods have been applied for visualizing rumor propogation by jin et al 23 we wanted to analyze how covid19 retweeting behaves at different time points we used published disaster retweeting times to serve as benchmarks for selecting time periods as a result the graphs in figure 8 are plotted by retweeting time of known benchmarks -the median time to retweet after an earthquake which implies rapid notification the median time to retweet after a funnel cloud has been seen all the way to a one-day or 24 hour time period we did this to visualize a retweet cascade of fast to slow information propogation we used median retweeting times published spiro et al 45 for the time it took users to retweet messages based on hazardous keywords like funnel cloud aftershock and mudslide we also used the h7n9 reposting time which zhang et al 49 published of 37 hours we generated a directed graph for each of the nine time periods where the network consisted of a source which was the author of the tweet user object the idstr and a target which was the original retweeter shown in table iv  the goal was to analyze how connections change as the retweeting speed increases the nine networks are visualized in figure 8  graphs were plotted using networkx and drawn using the kamada kawai layout 51  a force-directed algorithm we modeled 700 users for each graph we found that more nodes became too difficult to interpret the size of the node indicates the number of degrees or users that it is connected to it can mean that the node has been retweeted by others several times or it can also mean that the node itself has been retweeted by others several times the density of each network increases over time shown in figure 8 and figure 13  very rapid retweeters in the time it takes to retweet after an earthquake start off with a sparse network with a few nodes in the center being the focus of retweets in figure 8a  by the time we reach figure 8d  the retweeted users are much more clustered in the center and there are more connections and activity the top retweeted user in our median time network figure 8g  was a news network and tweeted the team took less than a week to take the ventilator from the drawing board to working prototype so that it can by 24 hours out in figure 8h  we see a concentrated set of users being retweeted and by figure 8i  one account appears to dominate the space being retweeted 92 times this account was retweeting the following message several times she was doing chemotherapy couldnt leave the house because of the threat of coronavirus so her line sisters in addition the number of nodes generally decreased from 1278 in earthquake time to 1067 in one week and the density also generally increased shown in table iv these retweet cascade graphs provide only an exploratory analysis network structures like these have been used to predict virality of messages for example memes over time as the message is diffused across networks 52  but analyzing them further could enable 1 an improved understanding about how covid19 information diffusion is different than other outbreaks or global events 2 how information is transmitted differently from region to region across the world and 3 what users and messages are being concentrated on over time this would support strategies to improve government communications emergency messaging dispelling medical rumors and tailoring public health announcements there are several limitations with this study first our dataset is discontinuous and trends seen in figure 3 and figure 4 where there is an interruption in time should be taken with caution although there appears to be a trend between one discrete time and another without the missing data it is impossible to confirm this as a trend as a result it would be valuable to apply these techniques on a larger and continuous corpus without any time breaks we aim to repeat the methods in this study on a longer continuous stream of twitter data in the near future next the corpus we analyzed was already pre-filtered with thirteen track terms from the twitter streaming api that focused the dataset towards healthcare related concerns this may be the reason why the high level keywords extracted in the first round of analysis were consistently mentioned throughout the different stages of modeling however after review of similar papers indicated in table i  we found that despite having filtered the corpus on healthcare-related terms topics still appear to be consistent with analyses where corpora were filtered on limited terms like coronavirus third the users and conversations in twitter are not a direct representation of the us or global population the pew research foundation found that only 22 of american adults use twitter 53 and that this group is different from the majority of us adults because they are on average younger more likely to identify as democrats more highly educated and possess higher incomes 54  the users were also not verified and should be considered as a possible mixture of human and bot accounts fourth we reduced our corpus to remove retweets for the keyword and topic modeling anlayses since retweets can obscure the message by introducing virality and altering the perception of the information 55  as a result this reduced the size of our corpus by nearly 77 from 23820322 tweets to 5506223 tweets however there appears to be variability in terms of consistent corpora sizes in the twitter analysis literature both in table i fifth our compute limitations prohibited us from analyzing a larger corpus for the umap time-series and network modeling for the lda models we leveraged the gensim mul-ticorelda model that allowed us to leverage multiprocessing across 20 workers but for umap and the network modeling we were constrained to use a cpu however as stated above visualizing more than 700 nodes for our graph models was unintepretable applying our methods across the entire 238 million corpora for umap and the network models may yield more meaningful results sixth we were only able to iterate over 15 different lda models based on changing the number of topics whereas syed et al 26 iterated on 480 models to select coherent models we believe that applying a manual gridsearch of the lda parameters such as iterations alpha gamma threshold chunksize and number of passes would lead to a more diverse representation of lda models and possibly more coherent topics seven it was challenging to identify papers that analyzed twitter networks according to their speed of retweets for public health emergencies and disease outbreaks zhang et al 49 points out that there are not enough studies of temporal measurement of public response to health emergencies we were lucky to find papers by zhang et al 49 and spiro et al 45 who published on disaster waiting times chew et al 62 and szomszor et al 6 have published about twitter analysis in h1n1 and the swine flu respectively chew analyzed the volume of h1n1 tweets and categorized different types of messages such as humor and concern szomszor correlated tweets with uk national surveillance data and tang et al 63 generated a semantic network of tweets on measles during the 2015 measles outbreak to understand keywords mentioned about news updates public health vaccines and politics however it was difficult to compare our findings against other disease outbreaks due to the lack of similar modeling and published retweet cascade times and network models we answered five research questions about covid19 tweets during march 24 2020 -april 8 2020 first we found highlevel trends that could be inferred from keyword analysis second we found that live white house coronavirus briefings led to spikes in topic 18 potus third using umap we found strong local clustering of topics representing ppe healthcare workers and government concerns umap allowed for an improved understanding of distinct topics generated by lda fourth we used retweets to calculate the speed of retweeting we found that the median retweeting time was 287 hours fifth using directed graphs we plotted the networks of covid19 retweeting communities from rapid to longer retweeting times the density of each network increased over time as the number of nodes generally decreased lastly we recommend trying all techniques indicated in table i to gain an overall understanding of covid19 twitter data while applying multiple methods for an exploratory strategy there is no technical guarantee that the same combination of five methods analyzed in this paper will yield insights on a different time period of data as a result researchers should attempt multiple techniques and draw on existing literature models were calculated using the ruptures python package we also applied exponential weighted moving average using the ewm pandas function we applied a span of 5 for march 24 2020 and a span of 20 for april 3 -4 datasets april 5 -6 datasets and april 8 -9 datasets our parameters for binary segmentation included selecting the l2 model to fit the points for topic 18 using 10 nbkps breakpoints   collective response to the media coverage of covid-19 pandemic on reddit and wikipedia nicol gozzi michele tizzani michele starnini fabio ciulla daniela paolotti andr panisson nicola perra  the exposure and consumption of information during epidemic outbreaks may alter risk perception trigger behavioural changes and ultimately affect the evolution of the disease it is thus of the uttermost importance to map information dissemination by mainstream media outlets and public response however our understanding of this exposure-response dynamic during covid-19 pandemic is still limited in this paper we provide a characterization of media coverage and online collective attention to covid-19 pandemic in four countries italy united kingdom united states and canada for this purpose we collect an heterogeneous dataset including 227 768 online news articles and 13 448 youtube videos published by mainstream media 107 898 users posts and 3 829 309 comments on the social media platform reddit and 278 456 892 views to covid-19 related wikipedia pages our results show that public attention quantified as users activity on reddit and active searches on wikipedia pages is mainly driven by media coverage and declines rapidly while news exposure and covid-19 incidence remain high furthermore by using an unsupervised dynamical topic modeling approach we show that while the attention dedicated to different topics by media and online users are in good accordance interesting deviations emerge in their temporal patterns overall our findings offer an additional key to interpret public perceptionresponse to the current global health emergency and raise questions about the effects of attention saturation on collective awareness risk perception and thus on tendencies towards behavioural changes  in the next influenza pandemic be it now or in the future be the virus mild or virulent the single most important weapon against the disease will be a vaccine the second most important will be communication 1  this evocative sentence was written in may 2009 by john m barry in the early phases of what soon after become the h1n1 2009 pandemic in his essay barry summarised the mishandling of the deadly 1918 spanish flu highlighting the importance of precise effective and honest information in the onset of health crises eleven years later we find ourselves dealing with another pandemic the cause is not a novel strain of influenza but these words are unfortunately still extremely relevant in fact as the sars-cov-2 sweeps the world and the vaccine is just a far vision of hope the most important weapons to reduce the burden of the disease are non-pharmaceutical interventions 2 3  social distancing became paramount gatherings have been cancelled mobility within and across countries have been dramatically reduced while such measures have been enforced to different extents across nations they all rely on compliance their effectiveness is linked to risk and susceptibility perception 4  thus the information that citizens are exposed to is fundamental history repeats itself and we seem not be able to learn from our past mistakes as happened in 1918 despite early evidences from china 5 6  the virus was first equated by many to the normal seasonal flu as happened in 1918 many national and regional governments organised campaigns aimed at boosting social activities and thus local economies actively trying to convince people that their cities were safe and that  andrepanissonisiit  nperragreenwichacuk the spreading was isolated in far away locations for example the hashtag milanononsiferma milan does not stop was coined to invite citizens in milan to go out and live normally free aperitifs were offered in venice in hindsight of course is easy to criticise the initial response in italy in fact the country has been one of the first to experience rapid growth of hospitalizations 7  however the mayor of london twelve days before the national lockdown and few days after the extension of the cordon sanitaire to the entire country in italy affirmed via his official facebook page we should carry on doing what weve been doing 8  more in general in several western countries the news coming from others reporting worrying epidemic outbreaks were not considered as relevant for the internal situation this initial phase aimed at conveying low local risk and boosting confidence about national safety has been repeated at different times across countries a series of surveys conducted in late february provide a glimpse of the possible effects of these approaches they report that citizens of several european countries despite the grim news coming from asia were overly optimistic about the health emergency placing their risk of infections to be 1 or less 9  as happened in 1918 the countries that reacted earlier rather than later were able to control the virus with significant less victims 10 11 12 13 14  history repeats itself but the context often is radically different in 1918 news circulated slowly via news papers controlled by editorial choices and of course words of mouth in 2009 we witnessed the first pandemic in the social media era newspapers and tv were still very important source of information but twitter facebook youtube wikipedia started to become relevant for decentralized news consumption boosting peer discussions and misinformation spread today these platforms and websites are far more popular integral part of society and instrumental pieces of the national and international news circulations together with traditional news media they are the principal sources of information for the public as such they are fundamental drivers of people perception opinions and thus behaviours this is particularly relevant for health issues for example about 60 of adults in the usa consulted online sources to gather health information 15  with respect to past epidemics and pandemics studies on traditional news coverage of the 2009 h1n1 pandemic highlighted the importance of framing and its effect on peoples perception behaviours such as vaccination intent stigmatisation of cultures at the epicentre of the outbreak and how these factors differ across countriescultures 16 17 18 19 20 21  during zika epidemic in 2016 public attention was synchronised across us states driven by news coverage about the outbreak and independently of the real local risk of infection 22  with respect to covid-19 pandemic itself a recent study clearly shows how google searches for coronavirus in the usa spiked significantly right after the announcement of the first confirmed case in each state 23  several studies based on twitter data also highlight how misinformation and low quality information about covid-19 although overall limited spread before the local outbreak and rapidly took off once the local epidemic started in the current landscape this has the potential to boost irrational unscientific and dangerous behaviours 24 25 26  on the other hand despite some important limitations 27  modern media has become a key data source to observe and monitor health in fact posts on twitter 28 29 30 31 32 33  facebook 34  and reddit 35 36  page views in wikipedia 37 38 and searches on google 39 40 have been used to study nowcast and predict the spreading of infectious diseases as well as the prevalence of noncommunicable illnesses therefore in the current full-fledged digital society information is not only key to inform peoples behaviour but can be used to develop an unprecedented understanding of such behaviours as well as of the phenomena driving them the context where covid-19 is unfolding is thus very heterogeneous and complex traditional and social media are integral parts of our perception and opinions have the potential to trigger behaviour change and thus influence the pandemic spreading such complex landscape must be characterized in order to understand the public attention and response to media coverage here we tackle this challenge by assembling an heterogeneous dataset which includes 227 768 news and 13 448 youtube videos published by traditional media 278 456 892 views of topical wikipedia pages 107 898 submissions and 3 829 309 comments from 417 541 distinct users on reddit as well as epidemic data in four different countries italy united kingdom united states and canada first we explore how media coverage and epidemic progression influence public attention and response to achieve this we analyze news volume and covid-19 incidence with respect to wikipedia page views volume and reddit comments our results show that public attention and response are mostly driven by media coverage rather than disease spreading furthermore we observe typical saturation and memory effects of public collective attention moreover using an unsupervised topic modeling approach we explore the different topics framed in traditional media and in reddit discussions we show that while attentions of news outlets and online users towards different topics are in good accordance interesting deviations emerge in their temporal patterns also we highlight that at the end of our observation period general interest grows towards topics about the resumption of activities after lockdown the search for a vaccine against sars-cov-2 acquired immunity and antibodies tests overall the research presented here offers insights to interpret public perceptionresponse to the current global health emergency raises interrogatives about the effects of attention saturation on collective awareness risk perception and thus on tendencies towards behavioural changes how is collective attention shaped by news media coverage and epidemic progression to tackle this important question we collected an heterogeneous dataset that includes covid-19 related news articles and youtube videos published online by mainstream information media relevant posts and relative discussion of geolocalized reddit users and country-specific views to wikipedia pages related to covid-19 for italy united kingdom united states and canada see subsections a b c of methods and materials for details this choice aims to provide an overview of media coverage and a proxy of public attention and response on the one hand the study of news articles and videos allows us to estimate the exposure of the public to covid-19 pandemic in traditional news media on the other hand the study of users discussions and response on social media through reddit and information seeking through wikipedia page views allows us to quantify the reaction of individuals to both the covid-19 pandemic and news exposure as mentioned in the introduction previous studies showed the usefulness of social media internet use and search trends to analyze health-related information streams and monitor public reaction to infectious diseases 41 42 43 44 45  hence we consider volume of comments of geolocalized users on the subreddit rcoronavirus 1 to explore the public discussion in reaction to media covering the epidemic in the various countries while we consider the number of views of relevant wikipedia pages about covid-19 pandemic to quantify users interest it is important to stress how reddit and wikipedia provide different aspects of online users behaviour and collective response in fact while reddit posts can be regarded as a general indicator of the online discussion surrounding the global health emergency the number of access to covid-19 related wikipedia pages is a proxy of health information seeking behaviour hisb hisb is the act through which individuals retrieve and acquire new knowledge about a specific topic related to health 46 47  and it is likely to be triggered on a population scale by a disrupting event such as the threaten of a previously unknown disease 48 49  normalized weekly volume of news articles and youtube videos news reddit comments reddit wikipedia views wikipedia related to covid-19 pandemic and covid-19 incidence covid inc in different countries our analysis starts by comparing in figure 1  the weekly volume of news and videos published on youtube wikipedia views and reddit comments of geolocalized users in comparison with the weekly covid-19 incidence in the four countries considered it can be seen how as covid-19 spreads both media coverage and public interest grow in time however public attention quantified by the number of reddit comments and wikipedia views sharply decreases after reaching a peak despite the volume of news and covid-19 incidence remaining high furthermore the peak in public attention consistently anticipates the maximum media exposure and maximum covid-19 incidence the correlation between media coverage public attention and the epidemic progression is quantified more in details in figure 2  the plot shows that news coverage of each country is strongly correlated with covid-19 incidence both global and domestic and slightly less with the volume of reddit comments and wikipedia views which in turn are much less correlated with covid-19 incidence both global and domestic this holds for all countries under consideration and highlights how the disease spreading triggers media coverage and how the public response is more likely driven by such news exposure in each country rather than covid-19 progression beyond these observations it is interesting to notice from figure 2 that italy is the only country where news volume shows higher correlation with domestic rather than global incidence this suggests that italian media coverage follows more closely the internal evolution rather than the global one at odds with respect to other countries this is probably due to italy being the location of the first covid-19 outbreak outside asia this observation is supported by figure 3  showing the citation share of italian locations by italian news media before and after the first covid-19 death was confirmed in italy on 20200220 after this date italian locations represent about 74 of all places cited by italian media in our dataset with an increase of 45 with respect to the same statistics calculated before similar effects though generally less intense can be observed also in the other countries therefore while media coverage is generally well synchronized with the global covid-19 incidence the media attention gradually shifts towards the internal evolution of the pandemic as soon as domestic outbreaks erupt arguably this may have played an important role in individual risk perception we can speculate that re-framing the emergency within a national dimension had the potential to amplify the perceived susceptibility of individuals 50 51 and thus increase the adoption of behavioural changes 4 52  indeed previous studies showed how at the beginning of february 2020 people were overly optimistic regarding the risks associated with the new virus circulating in asia and how their perception sharply changed after first cases were confirmed in their countries 9 53  to explore more systematically the relationship between media coverage public attention and epidemic progression we consider a linear regression model to nowcast separately for each country collective public attention quantified with the number of comments by geolocalized reddit users or visits to relevant wikipedia pages given the volume of media coverage or the covid-19 incidence as independent variables we include also memory effects in the public attention by considering an exponential decaying term in the news time series 22 see subsection d of methods and materials for details we compare the three models where the independent variables are the domestic incidence the news volume the news volume plus a memory term by using the akaike information criterion aic 54 and coefficient of determination r 2  we found that the model considering only covid-19 incidence has much less predictive power than the ones considering me- dia coverage see table ii in methods and material this enforces the idea that collective attention is mainly driven by media coverage rather than covid-19 incidence in addition we found that including memory effects improves significantly the model performance not surprisingly the coefficients of the memory effects term reported in table i are negative for all countries this implies that public attention actually saturates in response to news exposure and gives us the chance to quantify the rate at which this phenomenon happens the results presented so far are in very good accordance with findings obtained in previous contexts related to epidemics and pandemics indeed a similar media-driven spiky unfolding of public attention measured through the information seeking and public discussions of online users has been observed during the 2009 h1n1 influenza pandemic 57 58  the 2016 zika outbreak 59  the seasonal flu 60 and during more localized public health emergency such as the 2013 measles outbreak in netherlands 61  our findings confirm the central role of media showing how media exposure is capable of shaping and driving collective attention during a national and global health emergency media exposure is an important factor that can influence individual risk perception as well 62 63 64  the timing and framing of the information disseminated by media can actually modulate the attention and ultimately the behaviour of individuals 2  this becomes an even greater concern in a context where the most effective strategy to fight the spreading are containment measures based on individuals behaviour for this reason in the next section we characterize media coverage and online users response more specifically in terms of content produced and consumed  while collective attention and media coverage are well correlated in terms of volume the content and topics discussed by media and consumed by online users may not be as synchronized 65 66  to shed light on this issue we adopt an unsupervised topic modeling approach to extract prevalent topics in the news articles mentioned and discussed on reddit often indeed users on reddit post a submission containing a news article and discussion unfolds in comments under such submission differently from the first part and to provide a comprehensive overview of the topics discussed here we do not take into account any geographical context nonetheless in the supplementary information we provide some insights also on the specific topics discussed by users in different countries we characterize the main topics discussed on reddit by considering all submissions that include a news article in english we then apply a topic modelling approach on the content of this news article set specifically we extract topics by means of non-negative matrix factorization nmf 67  a popular method for this kind of tasks see subsection e of methods and materials for details in this way we extract the n  64 most relevant topics in the news shared on reddit as a second step we apply the model trained on the reddit news to the set of articles published by mainstream media that is we characterize the news published by media in terms of the topics discussed on reddit this choice allows us to directly compare the topics covered by media with the public discussion around such news exposure a complete list of the 64 topics extracted with the most frequent words is provided in the supplementary information we consider the number of articles published on a certain topic as a proxy of general interest of traditional media towards it while we measure the collective interest of reddit users by the number of comments under the news articles on a specific topic figure 4 shows an overview of the topics extracted and a comparison of the interest of media and reddit users we find a diverse and heterogeneous set of topics among others we recognize topics about the global spreading of the virus outbreaks who cdc covid-19 symptoms treatment hospitals and care facilities symptoms medical treatment medical staff care facilities the economic impact of the pandemic and responses from the governments to the upcoming crisis economy money different societal aspects sports religious services education and also the possible interven- overall the attention of traditional media and reddit users towards different topics are in good accordance indeed in figure 4 we represent the difference between interest share towards different topics in media and reddit submissions that is we compute the percentage share of attention dedicated by news outlets and reddit users to each topic and we subtract these two quantities we observe a maximum absolute mismatch in interest share of 261 nonetheless we observe that reddit users are slightly more interested to topics regarding health symptoms medical treatment non-pharmaceutical interventions and personal protective equipment social distancing face masks studies and information on the epidemic research surveys santa clara study cdc and also to specific public figures such as anthony fauci interestingly the santa clara study topic refers to the discussion about a controversial scientific paper suggesting that a much higher fraction of the population in the santa clara county was infected respect to what originally thought 68  since the study suggests a lower mortality rate the preprint has been quickly leveraged to support protest against lockdowns 2  while substantial flaws have been detected in the scientific methodology of the paper 3  the topics overview presented so far does not take into account any temporal dynamics of interest however topics showing a similar overall statistics may present a mismatch in temporal patterns hence in the following we take into account the temporal evolution of interest towards different topics in figure 5 we represent each topic as a single point its x-coordinate y-coordinate indicates when such topic reached 50 of its total relevance in news outlets on reddit during the analysis interval see subsection e of methods and materials for a formal definition of the relevance of a topic therefore topics at the bottom left became relevant very early in the public discussion among these we recognize themes centred on early covid-19 outbreaks ie chinese japanese iranian and italian outbreaks the events related to cruise ships specific countries ie israel singapore and malaysia and also topics regarding early health issues such as symptoms confirmed cases and the cdc on the contrary topics in the top right became relevant toward the end of the analysis interval early may reasonably we find here topics about the resumption of activities after lockdown ie reopening the feasibility and timing of a possible vaccine against sars-cov-2 ie vaccine and discussions regarding acquired immunity and antibodies tests ie immunity in-between we find all other topics clustered around end of march and mid-april 2020 the period when the general discussion surrounding covid-19 pandemic aroused sharply as also shown in figure 1  note that the diagonal plotted as a dashed line in figure 5 separates topics according to their temporal evolution above below the diagonal we find topics whose interest on reddit grows slowly quickly with respect to the media coverage therefore above the diagonal the interest of reddit users is mainly triggered by media exposure while below it the interest grows faster and declines rapidly despite sustained media exposure while the top-left and bottom-right regions are empty indicating that as a first approximation temporal patterns of attention by traditional media and reddit users are well-synchronized interesting deviations from the diagonal are observable for example above the diagonal one can find mainly topics related to various outbreaks economics and politics for which the interests on reddit follows the media coverage below the diagonal we observe topics more related to everyday life such as schools medical staff care facilities and lockdown for which the attention on reddit accelerates with respect to media coverage and then declines rapidly note that our view of topics discussed on reddit is limited since we only consider topics from news articles shared in submissions and do not explicitly take into account content expressed in comments this ensures a proper comparison with topics extracted from news published and explains the absence of points in the bottom right corner of figure 5  in this work we characterized the response of online users to both media coverage and covid-19 pandemic progression as a first step we focused on the impact of media coverage on collective attention in different countries characterized as volumes of country-specific wikipedia pages views and comments of geolocalized reddit users we showed that collective attention was mainly driven by media coverage rather than epidemic progression rapidly saturated and decreased despite media coverage and covid-19 incidence remaining high this trend is very similar to that observed during other outbreaks 57 58 59 60 61  also we showed how media coverage sharply shifted to the domestic situation as soon as the first death was confirmed in the home country discussing the implications for re-shaping individuals perception of risk 9 53  as a second step we focused on the dynamics of content production and consumption we modeled topics published in mainstream media and discussed on reddit showing that reddit users were generally more interested in health data regarding the new disease and interventions needed to halt the spreading with respect to media exposure by taking into account the dynamics of topics extracted we show that while their temporal patterns are generally synchronized the public attention for topics related to politics and economics is mainly triggered by media exposure while the interests for topics more related to daily life accelerates on reddit with respect to media coverage of course our research comes with limitations first we characterized the exposure of individuals to covid-19 pandemic by considering only news articles and youtube videos published online by major news outlets however individuals are also exposed to relevant information through other channels with television on top of these 69  second a 2013 pew internet study found that reddit users are more likely young males 70  showing that around 15 of male internet users aged between 18 and 29 declare to use reddit compared to the 5 of women in the same age range and to the 8 of men aged between 30 and 49 similarly informal surveys proposed to users showed that most of respondents were males in their late teens to mid-20s and that female users were very much in the minority 71  furthermore reddit is much more popular among urban and suburban residents rather than individuals living in rural areas 70  besides socio-demographic biases other works suggested also that reddit has become more and more a self-referential community reinforcing the tendency to focus on its own contents rather than external sources 72  thus perceptions interests and behaviours of reddit users may differ from those of the general population a similar argument may be raised for wikipedia searches indeed the usage of internet especially for information seeking purposes can vary across people with different socio-demographic backgrounds 73 74 75 76  finally our view on online users reaction is partial indeed we do not consider other popular digital data sources such as for example twitter the reason behind this choice is twofold first many studies already characterized public response during the current and past health emergencies through the lens of twitter 25 43 45 58 59 77 78 79 80  second several studies have reported high prevalence of bots as drivers of low quality information and discussions on covid-19 on this platform 24 25 81 82 83  thus careful and challenging extra steps would be necessary to isolate identify and distinguish organic discussionsreactions possibly originated from traditional media from those sparked by social bots we leave this for future work in conclusion our work offers further insights to interpret public response to the current global health emergency and raises questions about possible undesired effects of communication on one hand our results confirm the pivotal role of media during health emergencies showing how collective attention is mainly driven by media coverage therefore since people are highly reactive to the news they are exposed to in the beginning of an outbreak the quality and type of information provided might have critical effects on risk perception behaviours and ultimately on the unfolding of the disease on the other hand however we found that collective online attention saturates and declines rapidly despite media exposure and disease circulation remaining high attention saturation has the potential to affect collective awareness perceived risk and ultimately propensity towards virtuous individual behavioural changes aimed at mitigating the spreading furthermore especially in case of unknown viruses attention saturation might exacerbate the spreading of low quality information which is likely to spread in the early phases of the outbreak when the characteristics of the disease are uncertain future works are needed to characterize the actual effects of attention saturation on human perceptions during a global health emergency our findings suggest that public health authorities should consider to reinforce specific communication channels such as social media platforms in order to compensate the natural phenomenon of attention saturation indeed these channels have the potential to create a more durable engagement with people through a continuous loop of direct interactions currently we see public health authorities issuing regularly declarations on social media however the cdc didnt even have a twitter account in 2009 during h1n1 pandemic the account was created in may 2010 while this is just an example it underlines how we are relatively new to communicating such global health emergencies through social medias therefore there is great need to further reinforce and engage people through these channels alongside public health authorities should consider to strengthen additional communication channels an example can be represented by participatory surveillance platforms all over the world such as influenzanet flu near you and flutracking 84 85 86  which have the potential of delivering in-depth targeted information to individuals during public health emergencies to promote the exchange of information between people and public health authorities with the potential to enhance the level of engagement in the community 87  in this section we provide general information about the data sets collected and the methods used we collect news articles using news api a service that allows to download articles published online in a variety of countries and languages 88  for each of the country considered we download all relevant articles published online by selected sources in the period 20200207 -20200515 we select relevant articles considering those citing one of the following keywords coronavirus covid19 covid-19 ncov-19 sars-cov-2 note that for each article we have access to title description and a preview of the whole text in total our dataset consists in 227 768 news 71 461 published by italian 63 799 by uk 82 630 by us and 9 878 by canadian media additionally we collect all videos published on youtube by major news organizations in the four countries under investigation via their official youtube channels using the official api 89  in doing so we download title and description of all videos and select as relevant those that mention one of the following keywords coronavirus virus covid covid19 sars sars-cov-2 sarscov2 the reach of each channel measured by number of subscribers varies quite drastically from more than 9 million for cnn usa to about 12 thousand for ansa italy in total the youtube dataset consist of 13 448 videos 3 325 by italian 3 525 by british 6 288 by american ans 310 by canadian channels it is important to underline that while there is a good overlap between the sources of news articles and videos some do not match this is due to the fact that not all news organizations run a youtube channel and others do not produce traditional articles in the supplementary information we provide a complete list of news outlets and youtube channels considered reddit is a social content aggregation website where users can post comment and vote content it is structured in subcommunities ie subreddits centered around a variety of topics reddit has already proven to be suitable for a variety of research purposes ranging from the study of user engagement and interactions between highly related communities 90 91 to post-election political analyses 92  also it has been used to study the impact of linguistic differences in news titles 93 and to explore recent web-related issues such as hate speech 94 or cyberbullying 95 as well as health related issues like mental illness 96  also providing insights about the opioid epidemics 77 97  we used the reddit api to collect all submissions and comments published in reddit under the subreddit rcoronavirus from 15022020 to 15052020 after data cleanup by removing entries deleted by authors and moderators we keep only submissions with score  1 to avoid spam we remove comments with less than 10 characters and with more than 3 duplicates to avoid using automatic messages from moderation final data contains 107 898 submissions and 3 829 309 comments from 417 541 distinct users for the submissions we then selected entries with links to english news outlets the content of the urls was extracted using the available implementation 4 of the method described in 98  resulting in 66 575 valid documents reddit does not provide any explicit information about users location therefore we use self reporting via regular expression to assign a location to users reddit users often declare geographical information about themselves in submissions or comment texts we used the same approach as described in 97  that found the use of regular expressions as reliable resulting in high correlation with census data in the us although we acknowledge a potential higher bias at country level due to heterogeneities in reddit population coverage and users demographics we selected all texts containing expressions such as i am from or i live in and extracted candidate expressions from the text that follows the expression to identify which ones represented country locations by removing inconsistent self reporting we were able to assign a country to 789 909 distinct users from which 41 465 have written at least one comment in the subreddit rcoronavirus 13 811 from usa 6 870 from canada 3 932 from uk and 445 from italy wikipedia has become a popular digital data source to study health information seeking behaviour 57 99  and to monitor and forecast the spreading of infectious diseases 100 101  here we use the wikimedia api 102 to collect the number of visits per day of wikipedia articles and the total monthly accesses to a specific project from each country we consider the language as indicative of a specific country suggesting the relevant projects for our analysis to be in english and italian ie enwikipedia and itwikipedia respectively we choose the articles directly related to covid-19 and the ones in the see also section of each page at the time of the analysis 20200207 -20200515 including country-specific articles see supplementary information for full list of web pages considered except for the italian where the language is highly indicative of the location the number of the access to english pages are almost evenly distributed among english-speaking countries to normalize the signal related to each country we weight the number of daily accesses to a single article from a specific project p s p d with the total number of monthly accesses from a country c to the related wikipedia project t c p d such that the daily page views from a given wikipedia project and country is where the denominator is the total number of views of the wikipedia specific project the total volume of views at day d from a country c is then given by the sum over all the articles a and projects p namely y c d  ap y c ap d above we showed how media coverage covid-19 incidence and public attention are correlated even across four different countries to move a step forward in this analysis we considered a linear regression model that predicts for each country the public response given the news exposure to include memory effects in the public response to media coverage we consider also a modified version of this simple model in which we weight cumulative news articles volume time series with an exponential decaying term 22  formally we define the new variable where  is a free parameter that sets the memory time scale and is tuned with cross-validation more details in the supplementary information these two models are compared to a linear regression that considers only covid-19 incidence to predict public collective attention then the models considered are where y t can be either the volume of reddit comments of geolocalized users or country specific wikipedia visits and u t is the error term in table ii we report the results of the three regressions in terms of akaike information criterion aic 54  we observe that the model considering both news volume and memory effects is generally the better choice while the model considering covid-19 incidence only is the worst topic modeling has emerged as one of the most effective methods for classifying clustering and retrieving textual data and has been the object of extensive investigation in the literature many topic analysis frameworks are extensions of well known algorithms considered as state-of-the-art for topic modeling latent dirichlet allocation lda 103 is the reference for probabilistic topic modeling nonnegative matrix factorization nmf 67 is the counterpart of lda for the matrix factorization community although there are many approaches to temporal and hierarchical topic modeling 104 105 106  we choose to apply nmf to the dataset and then build time-varying intensities for each topic using the articles publication date starting from a dataset d containing the news articles shared in reddit we extract words and phrases with the methodology described in 107  discarding terms with frequency below 10 to form a vocabulary v with around 60k terms each document is then represented as a vector of term counts in a bag-of-words approach we apply tf-idf normalization 108 and extract a total of k  64 topics through nmf where 2 f is the frobenius norm and x  r dv is the matrix resulting form tf-idf normalization subject to the constraint that the values in w  r dk and h  r kv must be nonnegative the nonnegative factorization is achieved using the projected gradient method with sparseness constraints as described in 109 110  the matrix h is then used as a transformation basis for other datasets eg with a new matrix x we fix h and calculate a new w according to eq 3 for each topic k we build a time series s k for each dataset d where s t k is the strength of topic k at time t for the news outlets dataset s is the set of all documents shared at time t in news outlets for reddit we weight each shared document by its number of comments and s is the set of all documents shared at time t in reddit and c i is the number of comments associated to document i finally we define the relevance of a topic as the integral in time of the strength therefore given t 0 and t f as the startend of our analysis interval and given k  the coordinates of figure 5 are the t 12 such that authors would like to thank the startup quick algorithm for providing the platform httpscovid19scopsai scopshome where the data collected during covid-19 pandemic were visualized in real-time dp and mt acknowledge support from the lagrange project of the institute for scientific interchange foundation isi foundation funded by fondazione cassa di risparmio di torino fondazione crt mt acknowledges support from epipose -epidemic intelligence to minimize covid-19s public health societal and economical impact h2020-sc1-phe-coronavirus-2020 call ms and ap acknowledge support from the research project casa nel parco por fesr 1420 -canp -cod 320 -16 -piattaforma tecnologica salute e benessere funded by regione piemonte in the context of the regional platform on health and wellbeing ap acknowledges partial support from intesa sanpaolo innovation center the funders had no role in study design data collection and analysis decision to publish or preparation of the manuscript ng acknowledges support from the doctoral training alliance contributions ng ms dp ap and np conceptualized the study ng np ap and mt collected the data ng ap and fc performed analyses ng ms and np wrote the initial draft of the manuscript ng and ap provided visualization all authors ng np dp ms ap mt fc discussed the research design reviewed edited and approved the manuscript  from left to right ratio between uk media interest and general media interest for different topics ratio between uk reddit users interest and general reddit users interest for different topics differences between these two quantities for different topics in the fisrt two plots topics to the left of the dashed line on 1 are less discussed in by uk mediausers with respect to the general discussion while topics to the right are more discussed in the last plot positive negative bars indicated that uk reddit users pay generally more less attention to that topic with respect to uk media figure 7  from left to right ratio between us media interest and general media interest for different topics ratio between us reddit users interest and general reddit users interest for different topics differences between these two quantities for different topics in the fisrt two plots topics to the left of the dashed line on 1 are less discussed in by us mediausers with respect to the general discussion while topics to the right are more discussed in the last plot positive negative bars indicated that us reddit users pay generally more less attention to that topic with respect to us media figure 8  from left to right ratio between canadian media interest and general media interest for different topics ratio between canadian reddit users interest and general reddit users interest for different topics differences between these two quantities for different topics in the fisrt two plots topics to the left of the dashed line on 1 are less discussed in by canadian mediausers with respect to the general discussion while topics to the right are more discussed in the last plot positive negative bars indicated that canadian reddit users pay generally more less attention to that topic with respect to canadian media  automatic detection of coronavirus disease covid-19 in x-ray and ct images a machine learning based approach sara kassani hosseinzadeh peyman kassasni hosseinzadeh michal wesolowski j kevin schneider a ralph deters  the newly identified coronavirus pneumonia subsequently termed covid-19 is highly transmittable and pathogenic with no clinically approved antiviral drug or vaccine available for treatment the most common symptoms of covid-19 are dry cough sore throat and fever symptoms can progress to a severe form of pneumonia with critical complications including septic shock pulmonary edema acute respiratory distress syndrome and multi-organ failure while medical imaging is not currently recommended in canada for primary diagnosis of covid-19 computer-aided diagnosis systems could assist in the early detection of covid-19 abnormalities and help to monitor the progression of the disease potentially reduce mortality rates in this study we compare popular deep learning-based feature extraction frameworks for automatic covid-19 classification to obtain the most accurate feature which is an essential component of learning mobilenet densenet xception resnet inceptionv3 inceptionres-netv2 vggnet nasnet were chosen amongst a pool of deep convolutional neural networks the extracted features were then fed into several machine learning classifiers to classify subjects as either a case of covid-19 or a control this approach avoided task-specific data pre-processing methods to support a better generalization ability for unseen data the performance of the proposed method was validated on a publicly available covid-19 dataset of chest x-ray and ct images the densenet121 feature extractor with bagging tree classifier achieved the best performance with 99 classification accuracy the second-best learner was a hybrid of the a resnet50 feature extractor trained by lightgbm with an accuracy of 98  a series of pneumonia cases of unknown etiology occurred in december 2019 in wuhan hubei province china on december 31 2019 27 unexplained cases of pneumonia were identified and found to be associated with so called wet markets which sell fresh meat and seafood from a variety of animals including bats and pangolins the pneumonia was found to be caused by a virus identified as severe acute respiratory syndrome coronavirus 2 sars-cov-2 with the associated disease subsequently termed coronavirus disease 2019 covid-19 figure 1  the illustration of covid-19 created at the centers for disease control and prevention cdc 10  the protein particles e s and m are located on the outer surface of the virus particle the spherical viral particles colorized blue contain cross-sections through the viral genome seen as black dots 11  processing techniques and deep learning algorithms could assist physicians as diagnostic aides for covid-19 and help provide a better understanding of the progression the disease hemdan et al 13 developed a deep learning framework covidx-net to diagnose covid-19 in x-ray images a comparative study of different deep learning architectures including vgg19 densenet201 resnetv2 inceptionv3 inceptionresnetv2 xception and mo-bilenetv2 is provided by authors the public dataset of x-ray images was provided by dr joseph cohen 14 and dr adrian rosebrock 15  the provided dataset included 50 x-ray images divided into two classes as 25 normal cases and 25 positive covid-19 images hemdans results demonstrated vgg19 and densenet201 models achieved the best performance scores among counterparts with 9000 accuracy barstugan et al 16 proposed a machine learning approach for covid-19 classification from ct images patches with different sizes 1616 3232 4848 6464 were extracted from 150 ct images different hand-crafted features such as grey level co-occurrence matrix glcm local directional pattern ldp grey level run length matrix glrlm grey-level size zone matrix glszm and discrete wavelet transform dwt algorithms were employed the extracted features were fed into a support vector machine svm 17 classifier on 2-fold 5-fold and 10-fold cross-validations the best accuracy of 9877 was obtained by glszm feature extractor with 10-fold cross-validation wang and wong 18 designed a tailored deep learning-based framework covid-net developed for covid-19 detection from chest x-ray images the covid-net architecture was constructed of combination of 11 convolutions depth-wise convolution and the residual modules to enable design deeper architecture and avoid the gradient vanishing problem the provided dataset consisted of s a combination of covid chest x-ray dataset provided by dr joseph cohen 14  and kaggle chest x-ray images dataset 19 for a multi-class classification of normal bacterial infection viral infection non-covid and covid-19 infection obtained accuracy of this study was 835 in a study conducted by maghdid et al 20  a deep learning-based method and transfer learning strategy were used for automatic diagnosis of covid-19 pneumonia the proposed architecture is a combination of a simple convolutional neural network cnn architecture one convolutional layer with 16 filters followed by batch normalization rectified linear unit relu two fully-connected layers and a modified alexnet 21 architecture with the feasibility of transfer learning the proposed modified architecture achieved an accuracy of 9400 ghoshal and tucker 22 investigated the diagnostic uncertainty and interpretability of deep learning-based methods for covid-19 detection in x-ray images dropweights based bayesian convolutional neural networks bcnn were used to estimate uncertainty in deep learning solutions and provide a level of confidence of a computer-based diagnosis for a trusted clinician setting to measure the relationship between accuracy and uncertainty 70 posterioranterior pa lung x-ray images of covid-19 positive patients from the public dataset provided by dr joseph cohen 14 were selected and balanced by kaggles chest x-ray images dataset 19  to prepare the dataset all images were resized to 512512 pixels a transfer learning strategy and real-time data augmentation strategies were employed to overcome the limited size of the dataset the proposed bayesian inference approach obtained the detection accuracy of 9286 on x-ray images using vgg16 deep learning model hall et al 23 used a vgg16 architecture and transfer learning strategy with 10-fold crossvalidation trained on the dataset from dr joseph cohen 14  all images were rescaled to 224224 pixels and a data augmentation strategy was employed to increase the size of dataset the proposed approach achieved an overall accuracy 961 and overall area under curve auc of 9970 on the provided dataset farooq and hafeez 24 proposed a fine-tuned and pre-trained resnet-50 architecture covid-resnet for covid-19 pneumonia screening to improve the generalization of the training model different data augmentation methods including vertical flip random rotation with angle of 15 degree along with the model regularization were used the proposed method achieved the accuracy of 9623 on a multi-class classification of normal bacterial infection viral infection non-covid-19 and covid-19 infection dataset the main motivation of this study is to present a generic feature extraction method using convolutional neural networks that does not require handcrafted or very complex features from input data while being easily applied to different modalities such as x-ray and ct images another primary goal is to reduce the generalization error while achieving a more accurate diagnosis the contributions are summarized as follows  deep convolutional feature representation 25 26 27 is used to extract highly representative features using state-of-the-art deep cnn descriptors the employed approach is able to discriminate between covid-19 and healthy subjects from chest x-ray and ct images and hence produce higher accuracy in comparison to other works presented in the literature to the best of our knowledge this research is the first comprehensive study of the application of machine learning ml algorithms 15 deep cnn visual feature extractor and 6 ml classifier for automatic diagnoses of covid-19 from x-ray and ct images  to overcome the issue of over-fitting in deep learning due to the limited number of training images a transfer-learning strategy is adopted as the training of very deep cnn models from scratch requires a large number of training data  no data augmentation or extensive pre-processing methods are applied to the dataset in order to increase the generalization ability and also reduce bias toward the model performance  the proposed approach reduces the detection time dramatically while achieving satisfactory accuracy which is a superior advantage for developing real or near real-time inferences on clinical applications  with extensive experiments we show that the combination of a deep cnn with bagging trees classifier achieves very good classification performance applied on covid-19 data despite the limited number of image samples  finally we developed an end to end web-based detection system to simulate a virtual clinical pipeline and facilitate the screening of suspicious cases the rest of this paper is organized as follows the proposed methodology for automatically classifying covid-19 and healthy cases is explained in section 2 the dataset description experimental settings and performance metrics are given in section 3 a brief discussion and results analysis are provided in section 4 and finally the conclusion is presented in section 5 few studies have been published on the application of deep cnn feature descriptors to x-ray and ct images each of the cnn architectures is constructed by different modules and convolution layers that aid in extracting fundamental and prominent features from a given input image briefly in the first step we collect available public chest x-ray and ct images in the next step we pre-processed the provided dataset using standard image normalization techniques to improve the quality of visual information of the input data once input images are prepared we fed them into the feature extraction phase with the state-of-the-art cnn descriptors to extract deep features from each input image for the training phase the generated features are then fed into machine learning classifiers such as decision tree dt 28  random forest rf 29  xgboost 30  adaboost 31  bagging classifier 32 and lightgbm 33  finally the performance of the proposed approach is evaluated on test images the concept of transfer learning has been introduced for solving deep learning problems arising from insufficiently labeled data or when the cnn model is too deep and complex aiming to tackle these challenges studies in a variety computer vision tasks demonstrated the advantages of transfer learning strategies from an auxiliary domain in improving the detection rate and performance of a classifier 34 35 36  in a transfer learning strategy we transfer the weights already learned on a cross-domain dataset into the current deep learning task instead of training a model from scratch with the transfer learning strategy the deep cnn can obtain general features from the source dataset that cannot be learned due to the limited size of the dataset in the current task transfer learning strategies have various advantages such as avoiding the overfitting issue when the number of training samples is limited reducing the computational resources and also speeding up the convergence of the network 37 38 effective feature extraction is one of the most important steps toward learning rich and informative representations from raw input data to provide accurate and robust results the small or imbalanced size of the training samples poses a significant challenge for the training of a deep cnn where data dimensionality is much larger than the number of samples leading to over-fitting although various strategies eg data augmentation 39  transfer learning 40 and fine-tuning 41  may reduce the problem of insufficient or imbalance training data the detection rate of the cnn model may degrade due to the over-fitting issue since the overall performance obtained by a fine-tuning method in the initial experiments for this study was not significant we employed a different approach inspired by 25 26 27 known as deep convolutional feature representation in this method we used pre-trained well-established cnn models as a visual feature extractor to encode the input images into a feature vector of sparse descriptors of low dimensionality then the computed encoded feature vectors produced by cnn architectures are fed into different classifiers ie machine learning algorithms to yield the final prediction this lower dimension vector significantly reduces the risk of over-fitting and also the training time different robust cnn architectures such as mobilenet densenet xception inceptionv3 inceptionresnetv2 resnet vggnet nasnet are selected for feature extraction with the possibility of transfer learning advantage for limited datasets and also their satisfying performances in different computer vision tasks 42 43 44 45  figure 3  illustrates the visual features extracted by vggnet architecture from an x-ray image of a covid-19 positive patient in order to evaluate the performance of our feature extracting and classifying approach we used the public dataset of x-ray images provided by dr joseph cohen available from a github repository 14  we used the available 117 chest x-ray images and 20 ct images 137 images in total of covid-19 positive cases we also included 117 images of healthy cases of x-ray images from kaggle chest x-ray images pneumonia dataset available at 19 and 20 images of healthy cases of ct images from kaggle rsna pneumonia detection dataset available at 46 to balance the dataset with both positive and normal cases figure 4 shows examples of confirmed covid-19 images extracted from the provided dataset the x-ray images of confirmed covid-19 infection demonstrate different shapes of pure ground glass also known as hazy lung opacity with irregular linear opacity depending the disease progress 12   the images within the dataset were collected from multiple imaging clinics with different equipment and image acquisition parameters therefore considerable variations exist in images intensity the proposed method in this study avoids extensive pre-processing steps to improve the generalization ability of the cnn architecture this helps to make the model more robust to noise artifacts and variations in input images during feature extraction phase hence we only employed two standard pre-processing steps in training deep learning models to optimize the training process  resizing the images in this dataset vary in resolution and dimension ranging from 365465 to 1125859 pixels therefore we re-scaled all images of the original size to the size of 600450 pixels to obtain a consistent dimension for all input images the input images were also separately resized to 331331 pixels and 224224 pixels as required for nasnetlarge and nasnetmobile architectures respectively  image normalization for image normalization first we re-scaled the intensity values of the pixels using imagenet mean subtraction as a pre-processing step the imagenet mean is a pre-computed constant derived from the imagenet database 21  another essential pre-process step is intensity normalization to accomplish this we normalized the intensity values of all images from 0 255 to the standard normal distribution by min-max normalization to the intensity range of 0 1 which is computed as where x is the pixel intensity x min and x max are minimum and maximum intensity values of the input image in equation 1 this operation helps to speed up the convergence of the model by removing the bias from the features and achieve a uniform distribution across the dataset to measure the prediction performance of the methods in this study we utilized common evaluation metrics such as recall precision accuracy and f1-score according to equations 2 3 4 5 true positive tp is the number of instances that correctly predicted false negative fn is the number of instances that incorrectly predicted true negative tn is the number of negative instances that predicted correctly while false positive fp is the number of negative instances incorrectly predicted given tp tn fp and fn all evaluation metrics were calculated as follows recall or sensitivity is the measure of covid-19 cases that are correctly classified recall is critical especially in the medical field and is given by precision or positive predictive value is defined as the percentage of correctly classified labels in truly positive patients and is given as accuracy shows the number of correctly classified cases divided by the total number of test images and is defined as f1-score also known as f-measure is defined as the weighted average of precision and recall that combines both the precision and recall together f-measure is expressed as diagnostic imaging modalities such as chest radiography and ct are playing an important role in confirming the primary diagnosis from the polymerase chain reaction pcr test for covid-19 medical imaging is also playing a critical in monitoring the progression of the disease and patient care extracting features from radiology modalities is an essential step in training machine learning models since the model performance directly depends on the quality of extracted features motivated by the success of deep learning models in computer vision the focus of this research is to provide an extensive comprehensive study on the classification of covid-19 pneumonia in chest x-ray and ct imaging using features extracted by the stateof-the-art deep cnn architectures and trained on machine learning algorithms the 10-fold cross-validation technique was adopted to evaluate the average generalization performance of the classifiers in each experiment for all cnns the network weights were initialized from the weights trained on imagenet the windows based computer system used for this work had an intelr coretm i7-8700k 37 ghz processors with 32 gb ram the training and testing process of the proposed architecture for this experiment was implemented in python using keras package with tensorflow backend as the deep learning framework backend and run on nvidia geforce gtx 1080 ti gpu with 11gb ram table 1 and figure 5 summarize the accuracy performance of six machine learning algorithms namely dt rf xgboost adaboost bagging classifier and lightgbm on the feature extracted by deep cnns each entry in table 1  is in the format    where  is the average classification accuracy and  is standard deviation analyzing table 1 the topmost result was obtained by bagging classifier with a maximum of 9900  009 accuracy on features extracted by desnsenet121 architecture with feature extraction time of 9306 seconds and training time of 30748 seconds in table 5  which is the highest result reported in the literature for covid-19 classification of this dataset it is also inferred from table 1 that the second-best result obtained by resnet50 feature extractor and lightgbm classifier with feature extraction time of 0960 seconds and training time of 10206 seconds in table 5  with an overall accuracy of 9800  009 comparing the first and second winners among all combinations the classification accuracy of densenet121 with bagging is slightly better 1 than resnet50 with lightgbm while the training time of the second winner is tempting almost 30 times better than the first winner in terms of accuracy although bagging is a slow learner it has the lowest standard deviation and hence is more stable than other learners the results also demonstrate that the detection rate is worst on the features extracted by resnet101v2 trained by the adaboost classifier with 7600  032 accuracy figure 5 and figure 6 demonstrate box-plot distributions of deep cnns feature extractors and classification accuracy from the 10-fold cross-validation circles in figure 5 represent outliers in tables 2 3  table 4  comparison of classification f1-score metric of different machine learning models the bold value indicates the best result underlined value represents the second-best result of the respective category trained visual feature extractor so far was desnsenet121 mobilenet and inceptionv3 rather than counterpart architectures for covid-19 image classification although the approach presented here shows satisfying performance it also has limitations classifying more challenging instances with vague low contrast boundaries and the presence of artifacts some examples of these cases are illustrated in figure 7  finally comparison of the feature extraction time using deep cnn models and training with ml algorithms are shown in table 5 and after training a model the pre-trained weights and models can be used as predictive engine for cad systems to allow an automatic classification of new data a web-based application was implemented using standard web development tools and techniques such as python javascript html and flask web framework figure 9 shows the output of our web-based application for covid-19 pneumonia detection this web application could help doctors benefit from our proposed method by providing an online tool that only requires uploading an x-ray or ct image the application then provides the physician with a simple covid-19 positive or covid-19 negative observation it should be noted that this application has yet to be clinically validated is not yet approved for diagnostic use and would simply serve as a diagnostic aid for the medical imaging specialist the proposed method is generic as it does not need handcrafted features and can be easily adapted requiring minimal pre-processing the provided dataset is collected across multiple sources with different shape textures and morphological characteristics the transfer learning strategy has successfully transferred knowledge from the source to the target domain despite the limited dataset size of the provided dataset during the proposed approach we observed that no overfitting occurs to impact the classification accuracy adversely however our study has some limitations the training data samples are limited extending the dataset size by additional data sources can provide a better understanding on the proposed approach also employing pre-trained networks as feature extractors requires to rescale the input images to a certain dimension which may discard valuable information although the proposed methodology achieved satisfying performance with an accuracy of 9900 the diagnostic performance of the deep learning visual feature extractor and machine learning classifier should be evaluated on real clinical study trials the ongoing pandemic of covid-19 has been declared a global health emergency due to the relatively high infection rate of the disease as of the time of this writing there is no clinically approved therapeutic drug or vaccine available to treat covid-19 early detection of covid-19 is important to interrupt the human-to-human transmission of covid-19 and patient care currently the isolation and quarantine of the suspicious patients is the most effective way to prevent the spread of covid-19 diagnostic modalities such as chest xray and ct are playing an important role in monitoring the progression and severity of the disease in covid-19 positive patients this paper presents a feature extractor-based deep learning and machine learning classifier approach for computer-aided diagnosis of covid-19 pneumonia several ml algorithms were trained on the features extracted by well-established cnns architectures to find the best combination of features and learners considering the high visual complexity of image data proper deep feature extraction is considered as a critical step in developing deep cnn models the experimental results on available chest x-ray and ct dataset demonstrate that the features extracted by desnsenet121 architecture and trained by a bagging tree classifier generates very accurate prediction of 9900 in terms of classification accuracy  a novel and reliable deep learning web-based tool to detect covid-19 infection from chest ct-scan abdolkarim saeedi maryam saeedi  maghsoudi arash  the corona virus is already spread around the world in many countries and it has taken many lives furthermore the world health organization who has announced that covid-19 has reached the global epidemic stage early and reliable diagnosis using chest ct-scan can assist medical specialists in vital circumstances in this work we introduce a computer aided diagnosis cad web service to detect covid-19 online one of the largest public chest ct-scan databases containing 746 participants was used in this experiment a number of well-known deep neural network architectures consisting of resnet inception and mobilenet were inspected to find the most efficient model for the hybrid system a combination of the densely connected convolutional network densenet in order to reduce image dimensions and nu-svm as an anti-overfitting bottleneck was chosen to distinguish between covid-19 and healthy controls the proposed methodology achieved 9080 recall 8976 precision and 9061 accuracy the method also yields an auc of 9505 ultimately a flask web service is made public through ngrok using the trained models to provide a restful covid-19 detector which takes only 39 milliseconds to process one image the source code is also available at httpsgithubcomkilj4edencovidweb based on the findings it can be inferred that it is feasible to use the proposed technique as an automated tool for diagnosis of covid-19  so far six specimens of the coronavirus have been detected and with the most recent case the number has risen to seven corona virus disease covid-19 is a novel undiscovered type of coronavirus to humans reporting of the virus was first in wuhan china since then it was spread rapidly and widely in china and cases have been reported in many other countries fever cough and difficulty breathing are usual signs symptoms tend to start about after five days but can vary between two fourteen days human coronavirus is transmitted through infected droplets distributed by coughing or sneezing or by contacting with contaminated skin and surfaces on january 30 th  the world health organization who declared the 2019-20 coronavirus outbreak a public health emergency of international concern and a pandemic on march 11 th 2020 according to who reports more than 323 million are affected by the coronavirus which yields in 228394 deaths by the end of april 2020 1  the typical diagnostic approach is from a nasopharyngeal swab by real-time reverse transcription polymerase chain reaction rrt-pcr due to the limited number of test kits and the amount of time to get the response ct-scan based detection can be a promising tool deep learning in the field of automatic diagnosis of different disorders has made considerable improvements and it is able to become a fast accurate and more accessible method to help medical professionals in critical conditions wang et al 2 have analyzed 1119 ct-scan images using transfer learning their results demonstrate 793 accuracy zhao et al 3  have provided a ct-scan public database they have used a pre-trained deep learning conventional neural network cnn based on the chest x-ray images for discrimination between 247 covid-19 patients and healthy controls which results in 847 accuracy they used transfer learning and data augmentation to avoid overfitting in another study wang et al 4 have combined two publicly available datasets a total 16756 images where covidx dataset consists of 76 covid-19 images from 53 patients and the other dataset is related to rsna pneumonia detection challenge they have developed a new neural network architecture covid-net architecture in order to detect covid-19 based on chest x-ray images their results revealed 924 accuracy on the covidx dataset xu et al 5 experimented with 219 images from 110 patients and 224 influenza-a ct-scan images they trained a 3-dimensional cnn to segment the infected areas aiming to differentiate between covid-19 and influenza-a patients right now as the models and methods for evaluating covid-19 is progressing very quickly the need to develop a fast and reliable diagnosis technique with the ability to be used anywhere in the realworld applications seems necessary in this paper we propose a fast pipeline using a combination of deep and machine learning algorithms the densenet121 presents remarkable results based on the nu-svm classifier the introduced hybrid model is also compared to other common deep learning methods to assert comprehensive empirical evidence for the reliability of the proposed combination the models are also hosted on an online web service to be to perform live detection based on chest ct images by providing the source code and a detailed explanation we further insist on providing enough information for the community to keep on with the pandemic the dataset used in this study which is publicly available 6 consists of 349 patients with confirmed covid and 397 healthy subjects for processing purposes images were resized to 224 224 3 as suggested in the article 3 images were also normalized for better compensation with learning method 7  figure 1 demonstrates two covid and non covid samples  densenets are an extension of convolutional neural networks the main idea behind densenet is to reduce parameters as well as having a boost to computational efficiency by using all of the outputs form the previous layers as inputs to the next ones also this model has a less tendency to overfit 8 compared to other techniques such as resnet 9 or fractalnet 10  figure 2 shows the architecture of this network also equation 1 shows the formula for a densely connected layer where is the output of the current layer represented by with a non-linear transformation denoted by   as it can be observed the current layer reuses all the features from 0 to  1 layers in this work densenet121 was chosen due to less overfitting with respect to other available variations 8  in order to design deeper networks and address the budget challenges inception neural networks were implemented through reducing the dimensionality via usage of 1  1 stacked convolutions the aim is to get multiple kernel sizes inside the network instead of sequentially stacking them ordering each to work at the same stage initial version of inception architecture generally known as googlenet which was introduced by szegedy et al 11  extra modifications were built on the early model by presenting batch normalization inceptionv2 and factorization inceptionv3 with aiming to reduce computational expanse and parameters by keeping the network performance stable 12  resnet residual network architecture was released in 2015 and won multiple competitions with resnet152 known as the deepest structure for some time resnet50 is a 50 layered deep learning neural network the network is composed of five stages which each of these stages includes convolutional and identity blocks main idea behind resnet is to identity mapping layers that skips one or several layers therefore in each stage in addition to the results obtained from previous layer the data itself is also fed to the next layer which makes deeper models trainable with an easier optimization process 9  mobilenet is a fast and lightweight deep neural network which is appropriate for mobile and embedded vision applications 13  this architecture employs depth-wise separable convolutions which uses a depth-wise operation followed by a point-wise convolution the idea of depth-wise separable convolution is that instead of the usual convolution operation -where each filter is applied to all input features and then the output is provided by the combination of them-the process is comprised of two steps once the filter has been applied to a feature map the results are not combined immediately in the second step a 1  1 convolution layer performs the combination of feature maps yielded in the previous step this method greatly reduces the number of parameters compared to a traditional network with identical depth 26 nu-svm svm was first introduced by vapnik et al in 1963 14 as a linear classifier this method uses geometrical parameters instead of statistical therefore its a non-parametric classifier in general svm is used for two or multi-class classification and regression problems for n training samples each demonstrated with    referred to as feature vector and is set of labels the idea behind the svm is to find the optimal hyperplane with maximum margin in order to differentiate between classes if the samples are not linearly separable non-linear kernel svm projects the data into n-dimensional space and the decision boundary is determined in that space the equation of the hyperplane is as follows where x represents each sample the on hyperplane is a vector perpendicular to decision boundary b is bias is kernel function for transformation of dataset to n-dimensional space nu-svm is distinguished from conventional c-svm by means of regularization parameter c varies between zero to infinity and hardly can be estimated while operates between zero and one optimization function for minimizing the loss can be obtained by following equation in this equation  01 is the regulator term or the nu value small allows margin constrains to be simply rejected whereas large makes the constrains hard to reject for considering noise and overlapping of samples in dataset parameter is used nu-svm is optimized using bayesian optimization which is a hyperparameter tuning approach which builds a probabilistic objective function using the bayes theorem and consequently selects a few candidate parameters to be evaluated with the main problem accuracy was chosen to maximize the classifier efficiency  in this study 10-fold cross validation was used to evaluate the generalization ability of the model in this technique the dataset is divided into n approximately compliment subsets n-1 folds regarded as training set and the remaining fold is test set the process will repeat until every fold is once used as the test set finally the average score of all n-folds presents the performance rate one advantage of this technique is that the whole dataset is used for training and testing therefore each sample appears in the training and testing sets and reduce the loss of information each performance component can be defined by equations below in this work a flask web service is built based on the trained model the implementation is in a such way that a chest ct image is uploaded to the backend through an html form making the image ready for preprocessing consequently the proposed pipeline is evaluated on the image and the prediction is then shown to the user with the json format to increase the prediction speed models are preloaded before running the service figure 3 demonstrates this procedure in a web browser  in this work divergent deep learning algorithms were inspected with the nu-svm figure 4  represents the block diagram of entire process all of the processing is done using python and a number of frameworks namely tensorflow keras scikit-learn common deep structures consisting of densenet121 resnet50 v1v2 inceptionv3 and mobilenet v1v2 were used after resizing images phase to derive features from the images the extracted maps were applied to svm in order to classify samples in two categories initially feature maps were extracted from the model resulting in a 1024-dimensional feature map which was is reshaped for a better representation and a sample is shown in  figure  5  the performances achieved for 10-fold cross validation includes of accuracy recall precision f1-score and auc for each deep learning methods are demonstrated in table 2  figure 6 displays the fold variations of each performance metric over the 10-folds for the densenet121 model as it can be seen 92 of the covid-19 misclassified as healthy controls and 1024 of the normal participants incorrectly categorized as respiratory patients figure 7 shows the roc plot for the proposed method as it can be seen the densenet121 model is consistent over all folds resulting in an area under curve of 95 which is a motivation to enhance ct based techniques in our method a combined densenet-nu-svm system was proposed for processing ct-scan images with database consisting of 349 covid and 397 non-covid 746 patients which is more dependable compared to most of the studies which had less than a hundred covid patients 2 3 4 5  densenet121 was used to reduce parameter space by extracting local features and nu-svm was used for classification according to table 2  the implemented system yielded 9061 percentage accuracy and 9080 percentage sensitivity which is superior compared to the performance rates of other popular architectures 9505 auc was also achieved which demonstrates the hybrid system monitoring ability in addition the suggested diagnostic approach was developed as a web-service program which simply gives the results of covid diagnosis to the user after uploading a chest ct-scan image the advantage of the proposed methodology is a reasonable accuracy as well as online usability figure 8 a disadvantage would be the small dataset size in which since deep learning models training process requires a considerable number of samples and small datasets lead to overfitting combining non biased dnn derived features with nu-svm could overcome this issue and perform well with classifying previously unseen data  in this research a pipeline was introduced to detect covid-19 easily and accurately different pretrained models were compare and the mot significant approach was deployed as a restful flask web service with the aim of online covid-19 detection metrics included an accuracy of 9061 9080 sensitivity and 9505 auc the experimental results demonstrate that the proposed diagnostic approach has the ability of monitoring covid-19 moreover it may be possible to build more reliable models with this approach using larger databases and use them anywhere in the world  statistical analysis and visualization of the potential cases of pandemic coronavirus r muthusami k saritha   diseases and bacteria or viruses which cause them often have different names the human immunodeficiency virus hiv for example induces the acquired immunodeficiency disease aids the virus that triggers the current outbreak is called coronavirus 2 a serious acute respiratory syndrome shortened to sars-cov-2 the illness shortened to covid-19 is called coronavirus disease the world health organization and the international committee on taxonomy of viruses ictv 1 gave these names in public speaking the who also refers to the virus as the virus accountable for covid-19 or the covid-19 virus the outbreak was first reported in wuhan city china wuhan is the capital of the hubei province and has a population of around 11 million chinese authorities reported a cluster of related pneumonia cases in the town on 29 december 2019 a novel coronavirus which was later called sars-cov-2 soon confirmed to cause these cases 27 the first covid-19 cases outside of china were found in thailand on january 13 and in japan on january 16 8 the chinese government put the city of wuhan and other cities in the area on lockdown on january 23 covid-19 has since spread to several more countries-cases have been recorded in all regions of the world it grew into a global pandemic by march 2020 and was announced by the who as such 911 while people sometimes refer to the virus that causes covid-19 as the coronavirus several different coronaviruses do exist the word refers to a group of viruses specific to humans coronaviruses cause about 30 percent of all cold cases 12 corona is latin for crownthis group of viruses is named because under an electron microscope its surface looks like a crown as the outbreak of the novel sarscov2 is increasingly spreading in china and beyond threatening to become a global pandemic epidemiological data need to be interpreted in such a way that the model of statistical data analysis and visualization can increase the understanding of situation among the mass population in the coming days 13 14 the world health organization who johns hopkins university researchers and other agencies all maintain dataset on the number of confirmed infected cases deaths and disease recoveries all data obtained in this research work is from johns hopkins university and is freely accessible via the github repository the dataset covered the period from 22 january 2020 to 17 april 2020 which includes time-series and aggregated data 15 we statistically analyzed our dataset with various methods of data analysis and visualized those data to provide a proper understanding of the covid19 outbreak worldwide our exploit analysis was carried out by johns hopkins university with the 2019 coronavirus dataset januaryapril 2020 here between 22 january 2020 and 17 april 2020 we present an effort to visualize and analyze the results covid19 has so far propagated nearly 185 countriesregions 83 citiesprovinces have been registered and 264 separate geographical locations combined using time-series data it estimated the number of individual cases such as confirmed infected deaths and recovered around the globe and the top 10 countries in the world as of 17 april 2020 the united states and spain are among the top ten countries in the world further to the discussion on different cases such as confirmed illnesses deaths and recovery in those countries as seen in the fig 1 worldwide the total confirmed infected cases are 2152646 and the global average rate is 038 with a standard deviation of 215 the global average rate of the top 10 countries is 773 with a standard deviation of 849 in this circumstance the us ranked first with a total of 667801 the global percentage is 3102 and with a total of 184948 the global percentage of 859 spain is second the estimated number of deaths worldwide is 143800 with a global average of 361 for this situation the us occupied the first place with 32916 counts and with 22170 counts italy was second in the top 10 countries around the world the total number of cases recovered is 542107 in the world in this scenario germany ranked first with a total of 77000 with a total of 74797 spain ranked second and the us ranked fourth with a total of 54703 in the top 10 countries of the world from a statistical data analysis it can be understood that 5 of deaths and 8 of recoveries occurred in reported cases in the united states in spain 10 of deaths and 40 of recoveries occurred in confirmed cases we also explore time-series data using visual data analysis to provide a clear and understandable outcome of this extreme outbreak of covid19 this segment will analyze various time-series data using several visual data analysis approaches with the r programming language we have created a graph and given awareness of how sarscov2 spread around the globe from 22 january 2020 to 17 april 2020 it allows individuals to grasp the epidemiological essence of covid19 figure 1 indicates that the confirmed infected cases have been crossed by 2000000 cases around the globe many cases such as death recovery and active have also been shown new cases reported on a single day do not actually represent new cases on that day as the number of confirmed infected cases or deaths announced by any organizationincluding who ecdc johns hopkins university and othersdoes not reflect the total number of new cases or deaths on that day this is due to the long chain of reporting that occurs between a new case or death and its inclusion in national or international statistics regression and generalized linear models glm of data from the covid-19 time series are used to analyze confirmed infected deaths and recovered cases the fitted models have yielded better statistical results the findings shown below represent all three cases in the usa from the models results obtained on the confirmed case the exponential model coefficients are  0807 and 017 the glm poisson model coefficients are 3469 and 0119 and the glm gamma model coefficients are  0433 and 017 both of which are statistically significant as shown in fig 2 in case of death the exponential model coefficients are  2774 and 0144 the glm poisson model coefficients are  2424 and 0151 both of which are statistically reasonable in the recovered case the exponential model coefficients are  2204 and 0137 the glm poisson model coefficients are  2864 and 0163 both of which are statistically significant from the findings it can be understood that all cases such as confirmed infected deaths and recovered are linearly increased the same thing is reflected in the upper part of the graph ie the output of linear and generalized linear models regression and generalized linear models of the covid-19 time series were also used to analyze confirmed infected death and recovered cases in spain the fitting models have provided better statistical results the findings shown below reflect all three cases in spain here too the count has risen linearly in the confirmed case the exponential model coefficients are  2278 and 0185 the glm poisson model coefficients are 4159 and 0093 both of which are statistically significant in case of death the exponential model coefficients are  2919 and 0152 the glm poisson model coefficients are 1329 and 0104 both of which are statistically fine in the recovered case the exponential model coefficients are  2876 and 0165 the glm poisson model coefficients are 0914 and 0124 both of which are statistically appropriate in the event of an outbreak of an infectious disease it is necessary not only to track the number of deaths but also the rate of increase in the number of deaths if there is a fixed number of deaths over a fixed period of time we call that linear growth but if they continue to double within a fixed time span we call it exponential growth based on the results looking at the rate of death growth we have understood that it is linear growth in the us and spain figure 3 indicates that changes every day occurred in confirmed cases between 22 january 2020 and 17 april 2020 from the usa and spain by this we will conclude that the reported cases will accelerate on 20 march 2020 and that the last day of change is 31451 in the us in spain the confirmed case rises linearly from 03 march 2020 to 15 april 2020 the last day of change is 7304 it is clear that the real-time analysis of these data is extremely useful in documenting the epidemiological behavior of this severe disease we believe that this method of data analysis will certainly increase understanding of the situation and inform behavior this study examined three separate categories of data including confirmed infected death and recovered cases across the globe for the period from 22 january to 17 april 2020 it will also include a comparative overview of all the cases reported in the united states and spain nevertheless we are discussing various cases internationally in order to explain the various cases identified over a particular time span after review 2152646 confirmed cases of covid19 occurred worldwide on 17 april 2020 in the us where the highest count is 667801 the global percentage is 3102 death cases were 143800 across the globe 668 with the us top count being 32916 493 the cases recovered were 542107 around the globe 2518 with germany at the top of the list with a total of 77000 cases the visual analysis of the growth rate of confirmed infected deaths and recovered cases between the us and spain is another investigation the goal of this article on covid-19 is to summarize existing research collect relevant data and make it possible for readers to make sense of the published data and early research on the coronavirus outbreak much of our work focuses on known problems for which we can link with well-established research and evidence on covid-19 the research presented here is based on statistical and visual data analysis methods with the aid of a dataset provided by john hopkins university the research was done with r studio 125033 and r 40 beta versions of the windows 10 operating system each and every description of the different cases of covid19 is documented here between 22 january 2020 and 17 april 2020 we are now also observing the harmful outbreak of the sarscov2 virus to the world this is extremely troubling in this analysis we examined the top 10 countries most affected and comprehensive reported cases of the united states and spain in conclusion the dataset covid-19 2019-ncov from the johns hopkins csse data repository 22 january 2020 to 17 april 2020 was used for our experiment it has supported us to generate and disseminate detailed information to the scientific community and to the public especially at the peak phase in order to understand the growth and impact of the novel coronavirus nevertheless knowledge of this novel sarscov2 virus remains minimal among the general population around the globe raw data published from different sources are not adequately capable of offering an insightful understanding of covid19 as a consequence of sarscov2 a user-friendly data analysis platform would also be more effective in recognizing the epidemic of this severe disease the informative graphics of the visualization platform provide an intuitive interface and a simple view of all raw data hopefully in the coming days we will continue to track the epidemiological data of this outbreak that we have used in this study and from other official sources  public opinions towards covid-19 in california and new york on twitter xueting wang canruo zou zidian xie dongmei li  287 words main text 4017 words all rights reserved no reuse allowed without permission abstract background with the pandemic of covid-19 and the release of related policies discussions about the covid-19 are widespread online social media becomes a reliable source for understanding public opinions toward this virus outbreak  in december 2019 a novel and contagious coronavirus disease of 2019 covid-19 has been reported in china and continued spreading out massively the cause of this pandemic is due to a novel coronavirus called sars-cov-2 according to the centers for disease control and prevention cdc the coronavirus could transmit between persons through close contact 1  on january 30 2020 covid-19 became a public health emergency of international concern 2  by june 22 2020 the coronavirus has been confirmed in six continents with nearly nine million covid-19 cases the number of confirmed cases in the us has been grown rapidly since early march on march 7 new york declared a state of emergency after 89 confirmed cases and since then the confirmed cases continued increasing at an alarming rate new york state was facing severe challenges in health care including a shortage of protective gear and medical equipment hospital overcrowding and paramedic shortages meanwhile california as the most populous state in the us was also facing the viral transmission but less severe than what ny faced discussions about the covid-19 pandemic are widespread online especially through social media which offers powerful public platforms where people can share their opinions with large crowds twitter is an online microblogging and networking platform and users can post and all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint interact messages known as tweets in past few years twitter data has been increasingly used for research previous study had used twitter data to reveal insights from user-generated tweets about ebola through textual analytics 3  the outbreak of coronavirus in the us has created an uproar in discussions about the pandemic on twitter a recent research collected a dataset of 9 million twitter posts using the keywords coronavirus and discussed the dynamics of coronavirus on twitter 4  however little work has been reported on social media data analysis that focused on covid-19 in the united states since its outbreak on march 2020 with the growth of the number of confirmed covid-19 cases and the announcement of covid-19 related policies mentions of coronavirus-related topics and sentiments might vary over time its important to understand how the perceptions of the pandemic related issues evolve over time and whether they vary with different geolocations thus in this study we aimed to investigate the differences in sentiments and public opinions towards the outbreak of covid-19 for twitter users between new york state and california state in the united states and how these are related to the number of cases and policy changes our study provides valuable information about public attitude changes over time towards the emergence and outbreak of covid-19 which could guide future research on exploring public concerns towards the pandemic we obtained covid-19-related twitter posts from march 5 2020 to april 2 2020 via twitter streaming api to generate only covid-19-related posts we used a list of keywords for twitter streaming-corona corona covid19 covid19 covid coronavirus coronavirus coronavirus and ncov the dataset was then filtered by the same keyword all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint set to remove any unrelated posts another filtering process was to remove the promotional and commercial posts since corona is a beer brand in the market its possible that the discussions in the posts about corona were not related to the coronavirus the keywords for filtering are dealer deal supply beer drink drank drunk store promo promotion etc to identify tweets from california and new york the last data filtering process is to separate tweets into two datasets -one dataset contains posts only from california state another only from new york state based on the geolocation information associated with each tweet the keywords used to filter locations such as los angeles ca monroe new york are scraped from wikipedia website list of cities and towns 5  to prevent from missing locations while filtering all the county and city names are considered into the keyword lists by converting itemset into data frames and dropping the duplicates we removed any duplicated items that share the same id number the number of confirmed covid-19 cases in california and new york states each day from march 5 2020 to april 2 2020 were downloaded from the website of 1point3acrescom 6 in order to explore the trend of covid-19-related tweets posted between march 5 2020 and april 2 2020 we calculated the number of tweets by days and hours for each state to convert the date into the correct time zone we parsed the date time into readable date format with the corresponding time zone we calculated the average number of tweets posted by each hour within a 24-hour period the vader valence aware dictionary and sentiment reasoner was used for the sentiment analysis which is a tool that is commonly used for sentiment analysis of social media data 7  all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint we applied the vadersentiment package in python to calculate the sentiment score for each tweet and calculated the average sentiment scores by each day and each hour for the text in each post we extracted the compound score which is normalized between -1 negative extreme and 1 positive extreme we calculated the average sentiment score within each hour in a 24-hour period for 29 days in addition a two-sample t-test was conducted to test whether there is a significant difference between average sentiment scores in california and new york based on the sentiment scores per minute topic modeling is typically used to discover abstract topics in documents through statistical modeling 8  to understand the most frequent topics that twitter users were discussing about covid-19 we applied latent dirichlet allocation lda for topic modeling analysis to build the lda model several steps were conducted  remove noises -emails newline extra spaces distracting single quote and urls  tokenization -split texts into sentences and words lower case the words and remove punctuation  create bigram and trigram models -convert two or three words frequently occurring together in the document eg difficultybreathing magicjohnson catchingfeelings  remove stop words downloaded from nltk and make bigrams and trigrams  lemmatization using spacy -lemmatize the words to a normal form  create dictionary and corpus as lda input to identify the optimal number of topics we calculated the coherence scores from 2 topics to 15 topics and considered the number of topics with the highest coherence score as the optimal number of topics after building the lda model we applied pyldavis to visualize the all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint information contained in the topic models its a python built-in package that helps understand the meaning of each topic the prevalence of each topic and the correlation among topics according to sievert and shirleys study result the optimal value of the weight parameter is 06 in the pyldavis method 9  in our analysis we adjusted the parameter to 06 and extracted the top ten frequent words in each topic the summary of each topic was manually conducted and discussed among all coauthors which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020 which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint a sentiment analysis was conducted to understand public attitudes toward the number of covid-19 cases and the policy changes we calculated average sentiment scores for covid-19 tweets from california and new york between march 5 2020 and april 2 2020 overall both states had negative sentiment scores towards covid-19 the average sentiment score for california was -0042 standard error  00004 while for new york it was -0033 standard error  00003 a two-sample t-test showed that the difference in average sentiment scores between california and new york was significant p-value  0001 the daily average sentiment scores in both new york and california varied over time  figure 3  in the 24-hour period new york showed a clear peak on the hourly average sentiment score with the highest sentiment score of -0018 at 1100am supplemental figure 2  in contrast california did not show a clear peak in addition twitter users from both states showed more negative sentiments towards covid-19 from midnight to early morning all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint  the lda topic modeling was applied to understand the general topics in covid-19 tweets based on the coherence scores we found that with new york data the optimal number of topics was 13 while with california data it was 14 we then summarized each topic with top 10 frequent words table 1  all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint table 1  as shown in table 1  topics discussed in covid-19 tweets in both new york and california were very similar among which the most prevalent topic was protective measures in addition which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint concerns about the covid-19 pandemic and government actions such as public concerns and government and policy were other popular topics in both states in this study we showed that from march 5 to april 9 in 2020 the number of covid-19 tweets in california was almost twice of that in new york california had a significantly lower sentiment score towards covid-19 than new york in addition we showed that the sentiment scores towards covid-19 in both california and new york varied over time and the policy announcements and number of confirmed cases might be the major drives for these sentiment changes twitter users from both states discussed similar topics with the protective measures as the most popular topic followed by pandemics impact and government policy while we showed that the sentiment scores varied over time in both california and new york the highest and lowest sentiment scores correlated with some important policy announcements and the severity of covid-19 on march 7 governor cuomo declared a state of emergency in new york and the sentiment score on that day was the lowest -0093 one possible explanation is that people expressed serious concerns about the covid-19 pandemic the word emergency along with sick leave business worker suggests that people were worried about their businesses and not being able to work meanwhile on the same day the average sentiment score in california was the lowest which coincide with the policies including the largest school district was closed and san francisco banned large group gatherings another interesting observation is that on march 25 2020 the average sentiment scores in both states reached the highest point the major event happened on that day was that a 2 trillion coronavirus relief package was passed to address the economic impacts caused by the pandemic all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg101101202007 1220151936 doi medrxiv preprint it can be inferred that since the pandemic has forced a lot of people losing their jobs and not being able to maintain their livings the announcement of relief bill could relieve some financial stresses which led to less negative sentiment scores in both states in addition we found that the number of confirmed cases and deaths might correlate with the publics sentiment for instance on march 7 the number of confirmed cases in california reached 100 10  on march 29 new york surpassed 1000 death tolls 11  and california surpassed 100 death tolls 12  these numbers might be the drives for more negative sentiment scores in both states since new york had much more confirmed cases and deaths than california in march 2020 we would expect that the average sentiment score in new york would be lower than in california however our results showed that california had a lower sentiment score -0042 than new york -0033 and the difference is significant one explanation for this is that on march 21 2020 new york began gathering ventilators across the state the federal disaster assistance might bring people who were in pessimistic situations some hopes a recent research by wallethub about the unemployment rate showed that new york had a 156969 increase in the unemployment claims since the start of the covid-19 pandemic and california had a 114354 increase much less than new york 13  thus the 2 trillion relief bill happened on march 25 could provide people in new york financial support more significantly another reasonable speculation could be that as the most populous state in the us california people might be more worried about the potential high infection rate our result on topic modeling analysis showed that there was no major difference between the topics discussed in california and new york however the popularity of each topic was different between two states the topic that had the highest token percentage in both states was related to protective measures they shared the same keywords such as work stop home all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint stay hand mask it can be inferred that these tweets were talking about staying home washing hands and wearing masks the second prevalent topic in new york was about the impact that pandemic brought including keywords such as business travel worker since new york had far more confirmed cases and deaths the negative impacts were more severe and people might discuss more about the harm to businesses and traveling restrictions in contrast tweets from california talked less about working impacts according to the topic modeling keywords presumably due to less serious situations topics related to government and policy seem to be the next mostly discussed categories keywords including trump president and crisis appeared in tweets from both states with high frequencies other than the prevention and impacts of covid-19 people paid significant attention to the government policy and presidents actions which might partially explain the fluctuation of sentiment scores hospital environment and medical support were other popular topics in both states although social media provides first-hand users perspectives it does not provide controlled variables we dont have the user demographic information including age gender and education etc twitter users cant represent the whole population since about only 20 of us adults are currently using twitter furthermore our study only focused on the beginning of covid-19 epidemic in the us and the story might evolve later this study used social media data from twitter to analyze the sentiment scores and public opinions towards the emergence of covid-19 in california and new york states in the united states at the beginning of the covid-19 epidemic this study showed how covid-19 affected peoples attitudes from different states in a timely manner the results from this study could provide guidelines for future social media research on the associations between the outbreak of covid-19 and public opinions as well as valuable suggestions on future pandemic research all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020 which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted july 14 2020  httpsdoiorg1011012020071220151936 doi medrxiv preprint  compres a dataset for narrative structure in news effi levi guy mor shaul shenhav tamir sheafer  this paper addresses the task of automatically detecting narrative structures in raw texts previous works have utilized the oral narrative theory by labov and waletzky to identify various narrative elements in personal stories texts instead we direct our focus to news articles motivated by their growing social impact as well as their role in creating and shaping public opinion we introduce compres -the first dataset for narrative structure in news media we describe the process in which the dataset was constructed first we designed a new narrative annotation scheme better suited for news media by adapting elements from the narrative theory of labov and waletzky complication and resolution and adding a new narrative element of our own success then we used that scheme to annotate a set of 29 english news articles containing 1099 sentences collected from news and partisan websites we use the annotated dataset to train several supervised models to identify the different narrative elements achieving an f 1 score of up to 07 we conclude by suggesting several promising directions for future work  automatic extraction of narrative structures from texts is a multidisciplinary field of research combining discourse and computational theories which has been receiving increasing attention over the last few years examples include modeling narrative structures for story generation gervs et al 2006  using unsupervised methods to detect narrative event chains chambers and jurafsky 2008 and detecting content zones baiamonte et al 2016 in news articles using semantic features to detect narreme boundaries in fictitious prose delmonte and marchesini 2017  identifying turning points in movie plots papalampidi et al 2019 and using temporal word embeddings to analyze the evolution of characters in the context of a narrative plot volpetti et al 2020  a recent and more specific line of work focuses on using the theory laid out in labov and waletzky 1967 and later refined by labov 2013 to characterize narrative elements in personal experience texts swanson et al 2014 relied on labov and waletzky 1967 to annotate a corpus of 50 personal stories from weblogs posts and tested several models over hand-crafted features to classify clauses into three narrative clause types orientation evaluation and action ouyang and mckeown 2014 constructed a corpus from 20 oral narratives of personal experience collected by labov 2013  and utilized logistic regression over hand-crafted features to detect instances of complicating actions while these works concentrated their effort on detecting narrative elements in personal experience texts we direct our focus to detecting narrative structure in news stories the social impact of news stories distributed by the media and their role in creating and shaping of public opinion incentivized our efforts to adapt narrative structure analysis to this domain to the best of our knowledge ours is the first attempt to automatically detect the narrative elements from labov 2013 in news articles in this work we introduce compres -a new dataset of news articles annotated with narrative structure for this purpose we adapted two elements from the narrative theory presented in labov and waletzky 1967  labov 1972 labov   2013  namely complication and resolution while adding a new narrative element success to create a new narrative annotation scheme which is better suited for informational text rather than personal experience we used this scheme to an-notate a newly-constructed corpus of 29 english news articles containing a total of 1099 sentences each sentence was tagged with a subset of the three narrative elements or in some cases none of them thus defining a novel multi-label classification task we employed two supervised models in order to solve this task a baseline model which used a linear svm classifier over a bag-of-words feature representation and a complex deep-learning model -a fine-tuned pre-trained state-of-the-art language model roberta-based transformer the latter significantly outperformed the baseline model achieving an average f 1 score of 07 the remainder of this paper is organized as follows section 2 gives a theoretical background and describes the adjustments we have made to the scheme in labov 2013 in order to adapt it to informational text section 3 provides a complete description of the new dataset and of the processes and methodologies which were used to construct and annotate it along with a short analysis and some examples for annotated sentences section 4 describes the experiments conducted on the dataset reports and discusses our preliminary results finally section 5 contains a summary of our contributions as well as several suggested directions for future work 2 narrative analysis the study of narratives has always been associated in one way or another with an interest in the structure of texts ever since the emergence of formalism and structuralistic literary criticism propp 1968 and throughout the development of narratology genette 1980 fludernik 2009 chatman 1978 rimmon-kenan 2003  narrative structure has been the focus of extensive theoretical and empirical research while most of these studies were conducted in the context of literary analysis the interest in narrative structures has made inroads into social sciences the classical work by labov and waletzky 1967 on oral narratives as well as later works labov 1972 labov  2013  signify this stream of research by providing a schema for an overall structure of narratives according to which a narrative construction encompasses the following building blocks labov 1972 labov  2013  these building blocks provide useful and influential guidelines for a structural analysis of oral narratives despite the substantial influence of labov and waletzky 1967 labov 2013  scholars in the field of communication have noticed that this overall structure does not necessarily comply with the form of news stories thornborrow and fitzgerald 2004 bell 1991 van dijk 1988  and consequently proposed simpler narrative structures thornborrow and fitzgerald 2004  in line with this stream of research our coding scheme was highly attentive to the unique features of news articles a special consideration was given to the variety of contents forms and writing styles typical for media texts for example we required a coding scheme that would fit laconic or problem-driven short reports too short for full-fledged labovian narrative style as well as complicated texts with multiple story-lines moving from one story to another we addressed this challenge by focusing on two out of labovs six elementscomplicating action and resolution providing answers to the potential question and then what happened labov 2013  we consider these two elements to be the most fundamental and relevant for news analysis there are several reasons for our focus on these particular elements first it goes in line with the understanding that worth-telling stories usually consist of protagonists facing and resolving problematic experiences eggins and slade 2005  from a macro-level perspective this can be useful to capture or characterize the plot type of stories shenhav 2015  moreover these elements resonate with what is considered by entman 2004 to be the most important framing functions -problem definition and remedy our focus can also open up opportunities for further exploration of other important narrative elements in media stories such as identifying villainous protagonists who are expected to be strongly associated with the complication of the story and who are expected to be instrumental to a successful resolution shenhav 2015  in order to adapt the original complicating action and resolution categories to news media content we designed our annotation scheme as follows complicating action -hence complication -was defined in our narrative scheme as an event or series of events that point at problems or tensions resolution refers to the way the story is resolved or to the release of the tension an improvement from -or a manner of -coping with an existing or hypothetical situation was also counted as a resolution we did that to follow the lack of a closure which is typical for many social stories shenhav 2015 and the often tentative or speculative notion of future resolutions in news stories thornborrow and fitzgerald 2004  we have therefore included in this category any temporary or partial resolutions the transitional characteristic of the resolution brought us to subdivide this category into yet another derivative category defined as success unlike the transitional aspect of the resolution which refers implicitly or explicitly to a prior situation this category was designed to capture any description or indication of an achievement or a good and positive state here we describe the process of constructing compres our dataset of news articles annotated with narrative structures the dataset contains 29 news articles comprising 1099 sentences an overview of the dataset is given in table 1  we started by conducting a pilot study for the purpose of formalizing an annotation scheme and training our annotators for this study samples were gathered from print news articles in the broad domain of economics published between 1995 and 2017 and collected via lexisnexis we used these articles to refine elements from the theory presented in labov and waletzky 1967 labov 2013  into a narrative annotation scheme which is better suited for news media as detailed in section 22 as well as perform extensive training for our annotators the result was a multi-label annotation scheme containing three narrative elements complication resolution and success following the conclusion of the pilot study we used the samples which were collected and manually annotated during the pilot to train a multi-label classifier for this task by fine-tuning a roberta-base transformer liu et al 2019  this classifier was later used to provide labeled candidates for the annotators during the annotation stage of the compres dataset in order to optimize annotation rate and accuracy the pilot samples were then discarded the news articles for the compres dataset were sampled from 120 leading news and partisan websites in the english language all published between 2017 and 2020 the result is a corpus of 29 news articles comprising a total of 1099 sentences with an average of 393 sentences per article and a standard deviation of 218 and an average of 222 tokens per sentence with a standard deviation of 130 the articles are semantically diverse as they were sampled from a wide array of topics such as politics economy sports culture health for each article in the corpus additional meta-data is included in the form of the article title and the url from which the article was taken for future reference the news articles content was extracted using diffbot the texts were scraped and split into sentences using the punkt unsupervised sentence segmenter kiss and strunk 2006  some remaining segmentation errors were manually corrected following the pilot study section 31 a code book containing annotation guidelines was produced for each of the three categories in the annotation scheme -complication resolution and success -the guidelines provide  a general explanation of the category  select examples of sentences labeled exclusively with the category we employed a three-annotator setup for annotating the collected news articles first the model which was trained during the pilot stage section 31 was used to produce annotation suggestions for each of the sentences in the corpus each sentence was then separately annotated by two trained annotators according to the guidelines described in section 341 each annotator had the choice to either accept the suggested annotation or to change it by adding or removing any of the suggested labels disagreements were later decided by a third expert annotator the project lead table 2 reports inter-coder reliability scores for each of the three categories averaged across pairs of annotators the raw agreement in percentage between annotators and cohens kappa coefficient accounting for chance agreement artstein and poesio 2008  categories vary significantly in their prevalence in the corpus their respective proportions in the dataset are given in table 1  the categories are unevenly distributed complication is significantly more frequent than resolution and success this was to be expected considering the known biases of newsworthiness towards problems crises and scandals and due to the fact that in news media resolutions often follow reported complications table 3 reports pairwise pearson correlations  coefficient between the categories a minor negative correlation was found between complication and success   026 and a minor positive correlation was found between resolution and success   022 these were not surprising as success is often associated with resolving some complication however complication and resolution were found to be completely uncorrelated   001 which -in our opinion -indicates that the success category does indeed bring added value to our narrative scheme in table 5 we display examples of annotated sentences from the compres dataset note that all the possible combinations of categories exist in the dataset table 4 summarizes the occurrences of each of the possible category combinations in the dataset the fact that the dataset is composed of full coherent news articles allows the analysis of a range of micro meso and macro stories in narrative texts for example an article in the dataset concerning the recent coronavirus outbreak in south korea 1 opens with a one-sentence summary tagged with both complication and resolution south koreas top public health official hopes that the country has already gone through the worst of the novel coronavirus outbreak that has infected thousands inside the country complication this problem-solution or in this case hopeful solution plot structure reappears in the same article but this time it is detailed over a series of sentences more than 7300 coronavirus infections have been confirmed throughout south korea killing more than 50 complication the south korean government has been among the most ambitious when it comes to providing the public with free and easy testing options success the sequence starts with two sentences tagged with complication followed by two additional ones tagged with both complication and resolution and concludes with a sentence tagged as success this example demonstrates a more gradual transition from problem through solution to success we randomly divided the news articles in the dataset into training validation and test sets while keeping the category distribution in the three sets as constant as possible the statistics are given in table 7  the training set was used to train the supervised model for the task the validation set was used to select the best model during the training phase further details are given in sections 42 and the test set was used to evaluate the chosen model and produce the results reported in section 45 for our baseline model we used unigram counts bag-of-words as the feature representation we first applied basic pre-processing to the texts sentences were tokenized and lowercased numbers were removed and contractions expanded all the remaining terms were used as the features we utilized a linear svm classifier with the documentterm matrix as input and employed the one-vs-rest strategy for multilabel classification the validation set was used to tune the c hyperparameter for the svm algorithm via a random search on the interval 0 1000 in order to choose the best model in addition to the baseline model we experimented with a deep-learning model fine-tuning a pre-trained language model for our multi-label classification task we used the roberta-base transformer liu et al 2019 as our base language model utilizing the transformers python package wolf et al 2019  we appended a fully connected layer over the output of the language model with three separate sigmoid outputs one for each of the narrative categories in order to fine-tune it to our task the entire deep model was fine-tuned for 5 epochs and evaluated against the validation set after every epoch as well as every 80 training steps the checkpoint with the best performance smallest loss on the validation set was used to choose the best model finally we tested the effect of data augmentation in our setup both models were re-trained with augmented training data via back-translation back-translation involves translating training samples to another language and back to the primary language thus increasing the size of the training set and potentially improving the generalization capacity of the model shleifer 2019  for this purpose we used google translate as the translation engine translation was performed to german and back to english discarding translations that exactly match the original sentence following the augmentation the training set size almost  sentence comp res suc 1 it is no surprise then that the sensational and unverified accusations published online this week stirred a media frenzy 2 america would lose access to military bases throughout europe as well as nato facilities ports airfields etc 3 how did some of the biggest brands in care delivery lose this much money 4 bleeding from the eyes and ears is also possible after use idph said the gentrification project which concluded this year included closing more than 100 brothels and dozens of coffee shops where cannabis can be bought and trying to bring different kinds of businesses to the area his proposal to separate himself from his business would have him continue to own his company with his sons in charge instead hospitals are pursuing strategies of market concentration the south korean government has been among the most ambitious when it comes to providing the public with free and easy testing options 9 the husband and wife team were revolutionary in this fast-changing industry called retail 10 with its centuries-old canals vibrant historic center and flourishing art scene amsterdam takes pride in its cultural riches 11 mr trump chose to run for president he won and is about to assume office as the most powerful man in the world 12 soon after her administration announced a set of measures intended to curb misconduct 13 voter suppression is an all-american problem we can fight -and win 14 though many of his rivals and some of his jamaican compatriots have been suspended for violations bolt has never been sanctioned or been declared to have tested positive for a banned substance 15 the utah mans mother laurie holt thanked mr trump and the lawmakers for her sons safe return adding i also want to say thank you to president maduro for releasing josh and letting him to come home 16 they were fortunate to escape to america and to make good lives here but we lost family in kristallnacht 17 historically such consolidation and price escalation has enabled hospitals to offset higher expenses  we report our test results in table 6  first we observe that the deep models significantly outperformed the baseline models an average f 1 score of 07 compared to 03904 which represents an increase of 75 in performance the improvement is evident for every one of the narrative categories but is particularly substantial for the success category -an f 1 score of 056 compared to 015 constituting an increase of 373 one plausible explanation we can offer has to do with the nature of our success category while the complication and resolution categories seem to be constrained by sets of generic terminologies the definition of success is more content-oriented and thus highly sensitive to specific contexts for example linguistically speaking the definition of the success of an athlete in never being tested positive for a banned substance see sentence 14 in table 5  is very different from the definition of success in the cultural context of the art scene of a city sentence 10 in table 5  generally the performance for each category appears to reflect the proportion of instances belonging to each category see table 1  this is most evident in the baseline models -f 1 scores of 061 04 and 015 in the svm model and f 1 scores of 061 043 and 017 in the augmented svm model for complication resolution and success respectively however in the deep models this behavior seems to be less extreme in the augmented roberta model the f 1 score for the success category is higher by 005 compared to the resolution category despite being less frequent in the dataset we also observe that the success category consistently exhibit notably higher precision than recall across all models possibly due to the smaller number of samples encountered by the classifier during training this is generally true for the resolution category as well except in the case of the roberta model though to a lesser extent interestingly the data augmentation procedure does not seem to have any effect on model performance both in the case of the baseline model an increase of 001 in the average f 1 score as well as the case of the deep model case no change in the average f 1 score we introduced compres -the first dataset for narrative structure in news media motivated by the enormous social impact of news media and their role in creating and shaping of public opinion we designed a new narrative structure annotation scheme which is better suited to informational text specifically news articles we accomplished that by adapting two elements from the theory introduced in labov and waletzky 1967 labov 2013  -complication and resolutionand adding a new element success this scheme was used to annotate a set of 29 articles containing 1099 sentences which were collected from news and partisan websites we tested two supervised models on the newly created dataset a linear svm over bag-of-words baseline classifier and a fine-tuned pre-trained roberta-base transformer and performed an analysis of their performances with respect to the different narrative elements in our annotation scheme our preliminary results -an average f 1 score of up to 07 -demonstrate the potential of supervised learning-methods in inferring the narrative information encoded into our scheme from raw news text we are currently engaged in an ongoing effort for improving the annotation quality of the dataset and increasing its size in addition we have several exciting directions for future work first we would like to explore incorporating additional elements from the narrative theory in labov 2013 to our annotation scheme for example we believe that the evaluation element may be beneficiary in encoding additional information over existing elements in the context of news media such as the severity of a complication or the finality of a resolution a related interesting option is to add completely new narrative elements specifically designed for informational texts and news articles such as actor-based elements identifying entities which are related to one or more of the currently defined narrative categories for instance as mentioned in 22 we may add indications for villainous protagonists strongly associated with complications in the story and are expected to be instrumental to a successful resolution another direction which we would like to explore includes enriching the scheme with clauselevel annotation of the different narrative elements effectively converting the task from multilabel classification to a sequence prediction one -detecting the boundaries of the different narrative elements in the sentence alternatively we could introduce additional layers of information which will encode more global narrative structures in the text such as inter-sentence references between narratively-related elements eg a resolution referencing its inducing complication or even between narrativelyrelated articles eg different accounts of the same story   mike thelwall   the covid-19 pandemic in 2020 was recognised because scientists already knew about coronaviruses the origins of coronaviruses were also already known zoonotic with specific animal carriers moreover an understanding of virus mutations had led to an expectation that new coronaviruses could emerge and that their virulence could differ from those already found thus whilst the virulence of covid-19 and timing of its occurrence could not be predicted in advance its emergence was a recognised possibility in addition prior coronavirus research had identified a set of symptoms from previous outbreaks tested a range of treatments experimented with vaccines and implemented preventative measures thus biomedical and public health investigations of covid-19 had a body of prior coronavirus research to draw upon assessing the extent to which covid-19 differs from prior diseases might help speed new biomedical and public health research for example this is made explicit in some papers such as repurposing antivirals as potential treatments for sars-cov-2 from sars to covid-19 gmez-ros lpez-agudelo  ramrez-malule 2020  it seems likely that this is a general trend so older coronavirus research will be attracting substantial new attention in 2020 but evidence is needed to confirm this there are currently three known coronaviruses diseases that can have a serious impact on humans other coronaviruses are mild in humans or only infect some species of animals  sars severe acute respiratory syndrome is caused by the coronavirus sars-cov also known as sars-cov-1 sarsr-cov it was first identified in 2003 and there has been no outbreak since then 8437 people have been reported infected with an 11 death rate httpswwwwhointcsrsarscountry20030711en  mers middle east respiratory syndrome is caused by the mers coronavirus mers-cov it was first identified in saudi arabia in 2012 and by january 2020 2500 people had been reported infected with a 35 death rate httpwwwemrowhointhealth-topicsmers-covmers-outbreakshtml  covid-19 is caused by the coronavirus sars-cov-2 and emerged in december 2019 at the time of writing it had infected many more people than the previous two coronaviruses was more infectious for human-to-human transmission had a lower death rate but much higher death toll it has previously been called 2019-ncov and 20192020 novel coronavirus despite the above-mentioned likelihood of prior coronavirus research being more useful in 2020 there is no evidence yet to check whether interest in research specific to sars and mers has increased due to covid-19 a positive result -even though highly expectedwould empirically validate the importance of ongoing research into diseases related to potential pandemics eg coronaviruses ebolaviruses flaviviridae viruses this article addresses this issue and compares the current academic impact of covid-19 research with prior coronavirus research to assess their current relative importance it is not clear whether older coronavirus research would be more impactful this seems like a possibility because it may be more foundational and higher quality due to more time to plan and execute conversely research focusing on covid-19 may be more relevant to the 2020 pandemic the research questions are as follows  rq1 is sars and mers research from before 2020 more cited in 2020 than expected for its age  rq2 is sars and mers research from before 2020 having more scholarly impact than 2020 covid-19 research 2 background bibliometric studies of coronaviruses some bibliometric studies have investigated the influence of coronavirus research mostly characterising the number and type of publications indexed in relevant scholarly databases since coronaviruses have many variants and could be the primary focus of a paper or less central to a research project each study has operationalised its sample in different ways and there is not a single agreed method all seem to have been produced in 2020 in the context of covid-19 a range of studies have shown that there is a rapid rate of covid-19 research publishing and that both mers and sars are relevant to this emerging set in detail and discussed chronologically the specific findings are as follows one study found 8732 articles and 1028 reviews with the title term coronavirus by february 9 2020 in the web of science the journal of virology 9 was the single most common source and both sars and mers were identified as relevant keywords tao zhou yao et al 2020  by february 29 2020 183 publications matching the query covid-19 were indexed in pubmed a third of which reported original research lou tian niu et al 2020  in pubmed and the world health organisation who covid-19 research database there had been 564 observational or interventional investigations about covid-19 by march 18 2020 chahrour assi bejjani et al 2020  an investigation of web of science wos publications matching a set of covid-19 queries on 1 april 2020 found keyword related to both mers and sars to be associated suggesting that early research had often made connections with the two prior diseases hossain 2020  by 7 april the covid-19 coverage of a range of scholarly databases had been increasing at an increasing rate since january 2020 torres-salinas 2020 on 9 april 12109 papers had been indexed by scopus matching the query coronavirus with sudden increases associated with each of sars mers and covid-19 haghani bliemer goerlandt  li 2020  see also  danesh  ghavidel 2020  a collection of 2958 articles and 2797 preprints from scopus arxiv biorxiv and medrxiv by april 23 2020 was created by a set of inclusive queries with unspecified manual checking afterwards latif usman manzoor et al 2020  topic modelling applied to this dataset was used extracted sets of ten topics for different slices of the data but none included sars or mers scopus queries for covid-19 in titles abstracts or keywords two days later april 25 found 3513 documents and a keyword-based topic visualisation included mers hamidah sriyono  hudha 2020  also on april 25 the number of covid-19 documents indexed by pubmed was experiencing exponential-like growth kambhampati vaishya  vaish 2020  a similar investigation with a wider range of queries found both sars and mers represented in a topic map dehghanbanadaki seif vahidi et al 2020  one prior bibliometric study has compared sars mers and covid-19 papers until march 25 2020 similar document types were published for each disease both sars and mers research volume had decreased over time and covid-19 research was more cited higher field normalised citation counts this paper adopted an inclusive search strategy finding 7272 sars documents and 2199 mers documents hu chen wang et al 2020  thus this analysis may be dominated by studies that are related to the three diseases without being primarily about them some research has compared bibliometric trends for a variety of diseases including coronaviruses a comparison of sars mers avian flu ebola hivaids hepatitis b  c flu and swine flu found that short-lasting epidemics were uniquely associated with rapid increases and declines in both publication volumes and citation rates around the critical years kagan moran-gilad  fire 2020  another study compared covid-19 with sars ebola avian flu h1n1 and zika publications in wos by 9 april 2020 adding papers from pubmed and the chinese national knowledge infrastructure for covid-19 it found that all four prior epidemics were associated with rapid increases in publication volumes with slower declines research into all diseases covered a wide range of subject areas zhang zhao sun huang  glnzel 2020  the research design was to gather studies about coronaviruses from before 2020 and compare their impact with that of studies published in 2020 there is a large curated relevant dataset the covid-19 open research dataset cord-19 which is a collection of papers designed for data mining and scientometrics but takes a broad approach by including some non-covid-19 publications colavizza costas traag van eck van leeuwen  waltman 2020  this was not used because of its broad remit the scholarly database dimensions was chosen in preference to the web of science pubmed or scopus for its rapid indexing of academic documents and wider coverage of covid-19 than other scholarly indexes torres-salinas 2020 kousha  thelwall 2020  for the basic sample dimensions was searched for documents matching coronavirus weekly from 21 march 2020 to 30 may 2020 using the query below this single term was chosen rather than set of coronavirus-related keywords and phrases to give a narrow focus on the virus the earliest result was from 2008 which seems to be an api limitation since the web version has results from 1950 search publications for coronavirus return publications basics  extras mendeley was queried for each document matching the above query in order to count the number of registered readers of the document reader counts were checked each week immediately after the dimensions queries mendeley readers have moderate or strong correlations with citation counts in all or almost all academic fields thelwall 2017b and recent scholarly articles usually have at least one mendeley reader zahedi costas  wouters 2014  early mendeley reader counts have a high correlation with later scopus citation counts thelwall 2018  but appear about a year before them maflahi  thelwall 2018 thelwall 2017a  so they are preferable to citation counts as an indicator of academic impact this is also true for covid-19 research kousha  thelwall 2020  most people registering documents in mendeley are academics or phd students although there are also some masters students librarians and other professionals mohammadi thelwall haustein  larivire 2015  people usually register an article in mendeley because they have read it or intend to read it mohammadi thelwall  kousha 2016  so all evidence points to it being a citation-like academic impact indicator with a small element of educational impact mendeley documents were searched for using an authortitle query and a separate doi query with the results combined to identify readers of all variants of a document in mendeley zahedi haustein  bowman 2014  the mendeley reader counts were not field normalised because the documents fit within a relatively narrow topic and trends over time are clearer with the raw reader count data four subsets were extracted to identify the influence of different types of research the first three sets of documents were based on the inclusion of a human coronavirus-related keyword in the title the fourth encapsulated any mention of coronaviruses generally or a specific human coronavirus in article titles although an article can be about these topics without containing a disease or virus name in the title for example by being published before a formal name was assigned eg a pneumonia outbreak associated with a new coronavirus of probable bat origin or research targeting an aspect of the disease without needing to specify it eg the psychological impact of quarantine and how to reduce it rapid review of the evidence and first respiratory transmitted food borne outbreak from 2020 this method seemed to be effective at eliminating peripherally relevant papers for example many papers in the complete set of matches of the original dimensions coronavirus query were about other viruses or about viruses in general but mentioned coronaviruses as an example or as part of a list eg fighting misconceptions to improve compliance with influenza vaccination among health care workers  mers journal articles with mers mers-cov or middle east respiratory syndrome in their titles  sars journal articles with sars sarsr-cov sars-cov-1 or sars-cov or severe acute respiratory syndrome in their titles the results from 2020 were manually checked to remove false matches that were mentions of covid-19 before it had been named  covid-19 journal articles containing covid-19 covid19 covid2019 sars-cov-2 2019-ncov 2019 coronavirus coronavirus disease 2019 or wuhan in their titles the inclusion of wuhan did not generate false matches because the original dataset was captured with a coronavirus query  coronavirus journal articles containing any of the above or the word coronavirus in their titles this encapsulates the three human coronaviruses and the generic name for the virus family but not names for other coronaviruses documents that did not match the dimensions classification for journal articles were excluded some of the journal articles may have been news items editorials letters short articles or reviews rather than standard journal articles these were retained because short form contributions seem to play an important role in infectious disease research kousha  thelwall 2020  the rate of increase for each subset was calculated with the percentage increase in average mendeley readers over about two months from the first date checked in april to the last date in may 2020 although the start date could have been earlier at 21 march 2020 there was a higher rate of increase at the end of march april was chosen as the starting point as a conservative step in case the end of march increase was due to a technical cause as captured by dimensions the volume of research about sars has slowly decreased since 2011 figure 1  with a projected decrease for 2020 even though the 2020 data contains only a quarter of a year articles recorded in mendeley on 21 march 2020 in contrast the volume of research about mers increased to 201516 then decreased even allowing for the 2020 data containing only a quarter of a year figure 1  the number of journal articles matching dimensions coronavirus queries and with a title containing sars mers or coronavirus and with a mendeley record on 21 march 2020 sars documents exclude those with sars- in the title the average readership for the three sets of articles is very approximately constant irrespective of year for all sets except with a substantial increase in 2020 other factors being equal older articles should be more cited and have more readers because interest should accrue over time thus the relatively static average numbers of readers per article until 2019 suggests a moderate tendency for newer articles to be more read in all three categories in addition articles published in 2020 attracted substantially more readers than articles published earlier for all datasets the articles published in 2020 have already attracted far more readers on average than articles published before figures 3 4 5 6  for mers figure 4  articles written when the disease was first identified tend to have attracted more readers presumably due to their use in follow-up research the percentage increase in mendeley readers over a two-month period was calculated for each data set and year figure 7  multiplying by 6 would give an estimated 12 month annual increase in mendeley readers if the rate was constant over the year for reference a 17 increase over two months would equate to an annual increase of over 100 for almost all datasets and years the two-month increase was above 17 indicating an over 100 annual increase in mendeley readers this figure can be benchmarked against expected increases in mendeley readers based on prior information about the rate at which citations increase citations can expect to continue to accumulate in the long term so new citations for old articles are to be expected for example the cited half-life of the most common source the journal of virology is 89 years according to the 2019 clarivate journal citation reports so the typical virology article should have attracted half of its final citation count after nine years nevertheless in the life sciences the annual number of citations that a paper attracts seems to peak after two years then gradually decrease adams 2005  this is an old reference but the average age of cited biomedical literature has been approximately constant since the 1950s larivire archambault  gingras 2008 and in biology the peak for more cited papers published in 1990 is at 4 years with the peak for less cited papers from 1990 being at 2 years parolo pan ghosh huberman kaski  fortunato 2015  consistent with an overall average of 3 years to the citation peak thus after three years the annual percentage increase in citations should be substantially below 100 and after four years it should be below 50 since mendely readers accumulate a year earlier than citations maflahi  thelwall 2018 thelwall 2017  the annual mendeley reader count percentage increases should be substantially below 100 after two years and below 50 after three years thus sars mers and coronavirus research for every year from 2008 to 2017 has attracted an abnormally large increase in academic attention during april and may 2020 this is especially the case for sars research it is to be expected that the rate of increase is highest for the newest articles with two anomalous exceptions 2009 and mers in 2012 -only one paper the reason for the high readership counts for sars papers from 2020 compared to sars papers from previous years figure 3 can be deduced by reading their titles table 1  the top 17 sars papers from 2020 also mentioned covid-19 in their titles thus the high readership rate for 2020 is probably due to sars being mentioned in the context of its implications for covid-19 366 gold nanoparticle-adjuvanted s protein induces a strong antigen-specific igg response against severe acute respiratory syndrome-related coronavirus infection but fails to induce protective antibodies and limit eosinophilic infiltration in lungs 340 deja vu or jamais vu how the severe acute respiratory syndrome experience influenced a singapore radiology departments response to the coronavirus disease covid-19 epidemic 333 inactivation of three emerging viruses -severe acute respiratory syndrome coronavirus crimean-congo haemorrhagic fever virus and nipah virus -in platelet concentrates by ultraviolet c light and in plasma by methylene blue plus visible light 324 identification of potential cross-protective epitope between 2019-ncov and sars virus 277 a high atp concentration enhances the cooperative translocation of the sars coronavirus helicase nsp13 in the unwinding of duplex rna 236 aerosol and surface stability of hcov-19 sars-cov-2 compared to sars-cov-1 171 long-term bone and lung consequences associated with hospital-acquired severe acute respiratory syndrome a 15-year follow-up from a prospective cohort study 84 evaluation of an octahydroisochromene scaffold used as a novel sars 3cl protease inhibitor 37 references to covid-19 are in bold readers are bold when there is no reference to covid-19 the reason for the high readership counts for mers papers from 2020 compared to sars papers from previous years  figure 4  can again be deduced by reading their titles table 2  for mers most articles mentioning covid-19 including its earlier names have more readers than most articles not mentioning it in its title with some exceptions thus again the high readership rate for mers is partly due to research using it to illuminate covid-19 properties usually by comparisons or in the form of learning lessons from it there are four exceptions in terms of highly read articles that do not mention covid-19 three are about treatments for mers that mention the antiviral medication remdesivir which has also been suggested elsewhere as a potential treatment for covid-19 a review of mers research is also widely read 604 readers three papers mentioning covid-19 284 222 readers are not in the top 18 one is a short editorial and the other two are short letters  the main limitation of this study is its restriction to articles mentioning the diseases in their titles articles could be primarily about them without including their names in the titles if they use an uncommon or early name variant a second limitation is the use of mendeley reader counts as a source of academic impact evidence mendeley may be less used in china and this could skew the results away from studies that were more read in china also its use as an academic impact source may be misleading if users employ it for non-academic purposes such as personal safety during the pandemic the use of a two-month period to assess changes is also a restriction since the rate of increase of readers might speed or slow in the rest of 2020 the results show apparently for the first time that older sars and mers research has generated substantial new attention in april-may 2020 which is almost certainly due to new covid-19 research nevertheless sars and mers research from before 2020 has had far less academic impact at least as reflected by mendeley reader counts than articles from 2020 that have reviewed or situated prior sars and mers research in the context of covid-19 this issue does not seem to have been investigated for other groups of related diseases thus whilst studies of sars and mers have informed covid-19 research this has occurred disproportionately through new articles that have explicitly made connections with covid-19 or that have translated sars and mers research into implications for covid-19 the results confirm that older sars and mers research is proving useful for covid-19 but also suggest that research interpreting sars and mers studies for covid-19 performs a useful role in academia in future when new diseases emerge that are variants of known diseases researchers may therefore need to prioritise publishing reviews of prior research targeting a new disease at its early stages as a service to scientists researching the new disease these reviews may save valuable time by reducing the need for researchers and clinicians to rely on the source material to draw conclusions about lessons for the new disease the processed data used to produce the graphs are available in the supplementary material httpsdoiorg106084m9figshare12442616  knowledge synthesis from 100 million biomedical documents augments the deep expression profiling of coronavirus receptors a venkatakrishnan j arjun puranik akash anand david zemmour xiang yao xiaoying wu ramakrishna chilaka dariusz murakowski k kristopher standish bharathwaj raghunathan tyler wagner enrique garcia-rivera hugo solomon abhinav garg rakesh barve anuli anyanwu-ofili najat khan venky soundararajan  the covid-19 pandemic demands assimilation of all available biomedical knowledge to decode its mechanisms of pathogenicity and transmission despite the recent renaissance in unsupervised neural networks for decoding unstructured natural languages a platform for the real-time synthesis of the exponentially growing biomedical literature and its comprehensive triangulation with deep omic insights is not available here we present the nferx platform for dynamic inference from over 45 quadrillion possible conceptual associations extracted from unstructured biomedical text and their triangulation with single cell rna-sequencing based insights from over 25 tissues using this platform we identify intersections between the pathologic manifestations of covid-19 and the comprehensive expression profile of the sars-cov-2 receptor ace2 we find that tongue keratinocytes airway club cells and ciliated cells are likely underappreciated targets of sars-cov-2 infection in addition to type ii pneumocytes and olfactory epithelial cells we further identify mature small intestinal enterocytes as a possible hotspot of covid-19 fecal-oral transmission where an intriguing maturation-correlated transcriptional signature is shared between ace2 and the other coronavirus receptors dpp4 mers-cov and anpep -coronavirus this study demonstrates how a holistic data science platform can leverage unprecedented quantities of structured and unstructured publicly available data to accelerate the generation of impactful biological insights and hypotheses the nferx platform single-cell resource -httpsacademianferxcom  since december 2019 the sars-cov-2 virus has been rapidly spreading across the globe the associated disease has been declared a pandemic by the world health organization who with over 350000 confirmed cases and 15000 deaths across nearly every country as of march 23 2020 1  the constellation of symptoms ranging from acute respiratory distress syndrome ards to gastrointestinal issues is similar to that observed in the 2002 severe acute respiratory syndrome sars epidemic and the 2012 middle east respiratory syndrome mers outbreak sars mers and covid-19 are all caused by coronaviruses cov deriving their name from the crown-like spike proteins protruding from the viral capsid surface coronavirus infection is driven by the attachment of the viral spike protein to specific human cell-surface receptors ace2 for sars-cov-2 and sars-cov 2-4  dpp4 for mers-cov 5 and anpep for specific -coronaviruses 6  in addition to these receptors the protease activity of tmprss2 has also been implicated in viral entry 7 8  in a recent clinical study of covid-19 patients from china 48 of the 191 infected patients studied had comorbidities such as hypertension and diabetes 9  epidemiological and clinical investigations on covid-19 patients are also suggesting fecal viral shedding and gastrointestinal infection 10 11 12  in the case of the earlier sars epidemic multiple organ damage involving lung kidney and heart was reported 13  the mechanisms by which various comorbidities impact the clinical course of infections and the reasons for the observed multi-organ phenotypes are still not well understood thus there is an urgent need to conduct a comprehensive pan-tissue profiling of ace2 the putative human receptor for sars-cov-2 a deep profiling of ace2 expression in the human body demands a platform that synthesizes biomedical insights encompassing multiple scales modalities and pathologies described across the scientific literature and various omics siloes with the exponential growth of scientific eg pubmed preprints grants translational eg clinicaltrialsgov and other eg patents biomedical knowledge bases a fundamental requirement is to recognize nuanced scientific phraseology and measure the strength of association between all possible pairs of such phrases such a holistic map of associations will provide insights into the knowledge harbored in the worlds biomedical literature while unsupervised machine learning has been advanced to study the semantic relationships between word embeddings 14 15 and applied to the material science corpus 16  this has not been scaled-up to extract the global context of conceptual associations from the entirety of publicly available unstructured biomedical text additionally a principled way of accounting for the distances between phrases captured from the ever-growing scientific literature has not been comprehensively researched to quantify the strength of local context between pairs of biological concepts given the propensity for irreproducible or erroneous scientific research 17  which reflects as truths semi-truths and falsities in the literature any local or global signals extracted from this unstructured knowledge need to be seamlessly triangulated with deep biological insights emergent from various omics data silos the nferx software is a cloud-based platform that enables users to dynamically query the universe of possible conceptual associations from over 100 million biomedical documents including the covid-19 open research dataset recently announced by the white house 18 figure 1  an unsupervised neural network is used to recognize and preserve complex biomedical phraseology as 300 million searchable tokens beyond the simpler words that have generally been explored using higher dimensional word embeddings previously 14  our local context score is derived from pointwise mutual information content between pairs of these tokens and can be retrieved dynamically for over 45 quadrillion possible associations our global context score is derived using word2vec 14  as the cosine distance between 180 million word vectors projected in a 300 dimensional space  figure 1a figure s1  in order to assess the veracity of these conceptual associations derived from biomedical literature it is absolutely essential to enable triangulation with structured data sources including gene and protein expression datasets to address this need and empower the scientific community we built a single cell resource which harnesses these local and global score metrics to enable seamless integration of literature-derived associations with the analysis of transcriptomes from over 1 million individual cells from over 25 human and mouse tissues  figure 1b  here we use this first-in-class resource to conduct a comprehensive expression profiling of ace2 across host tissues and cell types and discuss how the observed expression patterns correlate with the pathogenicity and viral transmission shaping the ongoing covid-19 pandemic  figure 1c  to systematically profile the transcriptional expression of ace2 across tissues and cell types we triangulated single cell rnaseq-based measurements with literature-derived signals to automatically delineate novel emerging and known expression patterns  figure 2b  table s1  this approach immediately highlights renal proximal tubular cells and small intestinal enterocytes among the cell types that most robustly express ace2 detection in 40 of cells these cell types are also moderately to strongly associated with ace2 in the literature the strong intestinal ace2 expression is particularly interesting given the emerging clinical reports of fecal shedding and persistence post-recovery which may reflect a fecal-oral transmission pattern 10-12  conversely pancreatic pp cells gamma cells pancreatic alpha cells and keratinocytes show similarly robust ace2 expression but have not been strongly associated with ace2 in the literature this combination suggests either a biological novelty or an experimental artifact we note that the strong ace2 expression in pancreatic cell types is derived from only one murine study figure s2a 19  while ace2 expression is not observed in gamma or alpha cells from scrna-seq of human pancreatic islets  figure s2b 20 21 22  while we cannot determine the validity of either observation this example demonstrates how knowledge synthesis can automatically surface discordant biological signals for further evaluation surprisingly cells from respiratory tissues were notably absent among the populations with highest ace2 expression by scrna-seq  figure 2b  this observation was corroborated by complementary gene expression analysis of over 250000 bulk rna-seq samples from gtex 23 24 and the gene expression omnibus geo along with protein expression analysis from healthy tissue proteomics and immunohistochemistry ihc datasets 25 26 27  where lung and other respiratory tissues consistently show lower ace2 expression compared to the digestive tract and kidney figure s3a-d  however the respiratory transmission of covid-19 along with the disease symptomatology and well-documented viral shedding in respiratory secretions 28 strongly indicates that sars-cov-2 indeed infects and replicates within these tissues this would suggest that ace2 is likely expressed in the respiratory epithelium and so we prioritized the respiratory and digestive tracts for further knowledge synthesis-augmented scrna-seq analysis we also applied the single cell resource to analyze several other human and mouse tissues including heart adipose liver pancreas blood spleen bone marrow thymus testis prostate bladder ovary uterus placenta brain and retina an example use case describing the functionalities of the single cell app and a summary of ace2 expression across these tissues are given in the supplemental text and figures s4-19  next we classified 105 respiratory cell populations from eight independent studies based on their expression of and literature-derived associations to ace2  figure 3a  consistent with the low levels of ace2 in respiratory tissues by bulk rna-seq proteomics and ihc  figure s3a-d  we found that ace2 expression is detected in fewer than 10 of all cell types recovered from these studies however as mentioned above we believe that even low ace2 expression levels in these respiratory cells may be sufficient for covid-19 pathogenesis we found that club cells formerly known as clara cells were consistently among the highestexpressing respiratory cell types figures 3a-b  literature-derived local and global scores suggest that this ace2-club cell connection is underappreciated with a few documents discussing these concepts together nferx link we also found that ace2 is detected in type ii pneumocytes in multiple studies although the percentage of expressing cells ranges from only 05-7 figures 3a-b  this relatively low expression which may be deemed inconsequential if viewed in isolation is strongly supported by knowledge synthesis that highlights an existing association between ace2 and type ii pneumocytes figures 3a-b  indeed multiple studies have demonstrated ace2 expression in these cells 29 30 31 32 33 34 35  further ace2 expression in bulk rnaseq of gtex lung samples n  578 is strongly correlated to markers of type ii pneumocytes with all seven surfactant protein-encoding genes among the top 4 of transcriptional correlations to ace2 out of the 19000 genes expressed at  1 tpm in gtex lung samples hypergeometric p-value  11x10 -10   figure s20  our scrnaseq analysis also shows that ace2 is expressed in small fractions of ciliated airway cells and epithelial cells of the nasal cavity figures 3a-b  while no staining is observed for ace2 in nasopharynx samples from the human protein atlas hpa ihc dataset  figure s21  a previous ihc study did report the staining of ace2 in nasal and oral mucosa and the nasopharynx 33  this expression is consistent with the high sars-cov-2 viral loads detected in nasal swab samples 28  intriguingly mild degeneration of olfactory epithelium was observed in an immunosuppressed animal model infected with sars-cov 36  these observations are correlated with emerging reports of anosmiahyposmia loss of smell in otherwise asymptomatic covid-19 patients from south korea and other countries 37  such emerging clinical evidence emphasizes the need for further investigation into olfactory ace2 expression via scrna-seq and other modalities taken together these scrna-seq analyses and triangulation to literature synthesis confirm that type ii pneumocytes are a likely target of sars-cov-2 infection while also highlighting club cells ciliated cells and olfactory epithelial cells as additional potential sites of infection we then classified 136 gastrointestinal cell types from nine scrna-seq studies based on their expression of and literature associations to ace2  figure 4a  these studies encompassed samples from the upper mid and lower gi tracts including tongue esophagus stomach small intestine and colon 19 38 39 40 41 42  this analysis highlights a robust expression of ace2 in tongue keratinocytes that has not been strongly documented in the literature as evidenced by the weak local context score between ace2 and keratinocytes  figure 4b  in fact we found no previous reports of ace2 expression in keratinocytes and only one recent report suggesting ace2 expression in the human tongue based on a combination of bulk rna-seq and a scrna-seq dataset which has not been made publicly accessible 43  we propose that a subset of ace2  tongue keratinocytes may serve as a novel site of sars-cov-2 entry and highlight the need to generate additional gene and protein expression data from human tongue samples to further evaluate this hypothesis emerging reports of loss of taste dysgeusia in otherwise non-symptomatic covid-19 patients may warrant further study of the tongue in this pathology 44  we also found that ace2 is highly expressed in both human and murine small intestinal enterocytes confirming an association which has been moderately appreciated in literature as indicated by our literature derived local score between ace2 and enterocytes however to our knowledge the transcriptional heterogeneity of ace2 among enterocyte populations has never been explored in this context we found that ace2 shows an increase in expression correlated with the maturation of murine small intestinal enterocytes with minimal expression in stem cells and transit amplifying cells in contrast to most robust expression in mature enterocytes  figure  4  to the best of our knowledge this is the first demonstration that ace2 expression synchronously increases over the course of enterocyte maturation the recognition of such intratissue heterogeneity is necessary to specify the cell types which are most likely responsible for the proposed fecal-oral transmission of covid-19 12  to determine whether the maturation-correlated expression pattern is unique to ace2 we computed cosine similarities between the ace2 gene expression vector cp10k values in 6000 small intestinal enterocytes and that of the 15700 other genes detected in this study  figure  5a  for this analysis the vector space is constituted of the individual cells as the dimensions using the gene expression values to construct the vectors see methods interestingly we found that anpep the established entry receptor for hcov-229e showed the third highest cosine similarity to ace2  figure 5b  further dpp4 -the entry receptor for mers coronavirus -is also among the top 1 of similarly expressed genes by this metric figure 5b  we confirmed that both of these genes do indeed show a maturation-correlated transcriptional pattern similar to that of ace2  figure 5c-d  highlighting an unexpected shared pattern of transcriptional heterogeneity among known coronavirus receptors in a cell population which may be relevant for viral transmission we then asked whether this shared pattern of transcriptional heterogeneity among coronavirus receptors is observed in the human small intestine indeed among all enterocytes from a human scrna-seq study both anpep and dpp4 were among the top 1 of genes with similar expression vectors to that of ace2  figure s22a-b  we independently validated this observation by computing gene expression correlations from bulk rna-sequencing of human small intestine samples from gtex n  187 which similarly revealed that dpp4 and anpep are among the top 1 of correlated genes to ace2  figure s22c  in fact among all 18500 genes mean expression  1 tpm in gtex small intestine samples dpp4 shows the second highest correlation to ace2 r  095 to our knowledge this is the first demonstration that all known coronavirus entry receptors display highly coordinated and maturation-correlated transcriptional expression patterns in intestinal epithelial cells we propose that the requisite interaction with human proteins displaying a tightly defined expression gradient on apical surfaces of epithelial cells which is shared among known coronavirus strains may have fundamental implications for understanding the evolution lifecycle andor transmission patterns of this family of viruses recent advances in scrna-seq are empowering us to study tissue and cellular transcriptomes at previously unprecedented resolutions several single-cell rna sequencing based efforts such as the human cell atlas are underway to catalog gene expression across tissues and cell types and the raw data from many of these studies are available on public platforms such as the broad institute single cell portal 45 and gene expression omnibus geo analyses of these datasets are of interest to a wide range of researchers but currently prove challenging for all but a few due to the need for specialized workflows and computing infrastructures consequently the widespread use of this data for biomedical research is hampered an issue which is particularly evident in the face of public health crises like the ongoing covid-19 pandemic to address this unmet need the nferx platform single cell resource enables the rapid and interactive analysis of the continually growing scrnaseq datasets by specialists and non-specialists alike furthermore the seamless triangulation of scrna-seq insights with global and local scores derived from the synthesis of accessible biomedical literature creates a truly first-in-class resource by making the resource available to all academic researchers we enable scientists to not only dive deeper into insights that are aligned with existing knowledge but also to prioritize the novel insights which warrant further experimental validation looking forward we plan to automate the integration of the rapidly growing number of scrna-seq studies so that access to the entire worlds knowledge of single cell transcriptomes is just one click away for any researcher as we do so we encourage interactive feedback from the scientific community so that this platform can evolve to optimally support the research needs across the biomedical ecosystem beyond the covid-19 focus on the current study combined with our analyses of bulk rna-seq ihc and proteomics datasets our characterization of the known human coronavirus receptors ace2 dpp4 anpep using the nferx platform single cell resource represents the most comprehensive molecular fingerprint of host factors determining coronavirus infections including covid-19 while this serves as a primer of the deep profiling that is made possible with this resource we also identified several interesting aspects of coronavirus receptor biology which warrant further experimental follow-up we identified tongue keratinocytes and olfactory epithelia as novel ace2-expressing cell populations and thus as important potential sites of sars-cov-2 infection this molecular fingerprint is a striking correlate to emerging clinical reports of dysgeusia 44 and anosmia 37 in covid-19 patients which strongly implicate the gustatory and olfactory systems in sars-cov-2 pathogenesis and human-to-human transmission tongue epithelial cells have also previously been shown to uptake epstein-barr virus 46  and importantly a recent study found that ace2 is appreciably expressed in tongue based on a small number of non-tumor bulk rna-seq samples from tcga 43  this study further showed by scrna-seq that ace2 expression is observed in a subset of the human tongue but not other oral mucosal epithelial cells albeit in only 05 of the recovered epithelial population this data has unfortunately not been released for public consumption but certainly does provide preliminary support for our finding particularly as the listed set of cluster-defining genes for this population sfn krt6a krt10 is consistent with the tongue keratinocyte identify from the tabula muris data set  figure s23  we thus emphasize the imminent need for further generation of multi-omic expression data from large numbers of healthy and diseased human tongue samples drawn from a cohort of wide demographic representation we also observed that expression of ace2 and other coronavirus receptors is intimately linked to the maturation status of small intestinal enterocytes pinpointing the more mature subsets as the most likely cells to harbor sars-cov-2 virus this finding amplifies the potential for fecal-oral transmission of covid-19 10-12 and should motivate further experimental validation to determine whether monitoring of fecal viral loads should be considered clinically for diagnostic or prognostic purposes we further found that this transcriptional mirroring of coronavirus entry receptors was not unique to small intestine but rather also strongly present among renal proximal tubule epithelial cells where ace2 dpp4 and anpep expression tends to be observed in the same cellular subsets these observations suggest the existence of a transcriptional network spanning tissues and cell types which may drive andor regulate coronavirus receptor expression the question of whether coronaviruses have evolved to exploit such a network may be relevant to pursue particularly given that downregulation of ace2 by sars-cov has been reported previously and is associated with poor clinical outcomes 31 47  perhaps other coronaviruses can similarly modulate the expression of their entry receptors to impact the clinical course of the induced disease the emerging picture of the coronavirus life cycle appears to be intricately interwoven with many proteins beyond the primary host receptors for instance a recent structural complex of the sars-cov-2 spike protein with ace2 identified slc6a19 as an interaction partner of ace2 48  further spike proteins from some coronaviruses can interact with ceacam1 49 and sialylated glycans similar to influenza hemagglutinin 50 as host receptors future studies are likely to highlight several other proteins and glycans that constitute the interactome of the coronavirus proteome understanding the expression profiles of the interactome across tissues will provide systems level insights on the cellular dynamics of the functional partners and the regulatory machinery of the host receptor proteins like in the current study the nferx platform will be an excellent resource for unraveling the purported interaction partners for coronavirus receptors and profiling their expression across different tissues and cells constituting the human body overall this study evidences the utility of an integrative data science platform to enable rapid and high-throughput analysis of publicly available data to generate relevant biological insights and scientific hypotheses we hope that by making our biomedical knowledge synthesis-augmented single cell platform publicly accessible we help empower the research community to advance our understanding of the worlds most pressing biomedical challenges such as covid-19 in order to capture biomedical literature based associations the nferx platform defines two scores a local score and a global score as described previously 51  briefly the local score represents a traditional natural language processing technique which captures the strength of association between two concepts in a selected corpus of biomedical literature based on the frequency of their co-occurrence normalized by the frequency of each individual concept throughout the corpus a higher local score between concept x and concept y indicates that these concepts are frequently mentioned in close proximity to each other more frequently than would be expected by chance the global score on the other hand is based on the neural network renaissance that has recently taken place in the natural language processing nlp field to compute global scores all tokens eg words and phrases are projected in a high-dimensional vector space of word embeddings these vectors serve to represent the neighborhood of concepts which occur around a given concept the cosine distance between any two vectors measures the similarity of these neighborhoods and is the basis for our global score metric where concepts which are more similar in this vector space have a higher global score while the global scores in this work are computed in the embedding space of word2vec model it can also be computed in the embedding space of any deep learning model including recent transformer-based models like bert 52  these may have complementary benefits to word2vec embeddings since the embeddings are context sensitive having different vectors for different sentence contexts however despite the context sensitive nature of bert embeddings a global score computation for a phrase may still be of value given the score is computed across sentence embeddings capturing the context sensitive nature of those phrases from a visualization perspective the local score and global score signals are represented in the platform using bubbles where bubble size corresponds to the local score and color intensity corresponds to the global score this allows users to rapidly determine the strength of association between any two concepts throughout biomedical literature we consider concepts which show both high local and global scores to be concordant and have found that these typically recapitulate well-known associations the nferx platform also supports a logical thought engine that enables and conjunction or disjunction and not negation queries -the universal logic gates this engine is referred to as dynamic adjacency and leverages a highly distributed main memory approach that allows the computation of local scores for any type of logical query in real time fundamentally this system allows a user to extract all 100-word fragments of text which meet the specified logical query we then calculate local scores for all other tokens occurring within these fragments which quantifies the likelihood ie odds of each token occurring this frequently within these textual fragments by chance the platform further leverages statistical inference to calculate enrichments based on structured data thus enabling real-time triangulation of signals from the unstructured biomedical knowledge graph various other structured databases eg curated ontologies rna-sequencing datasets human genetic associations protein-protein interactions this facilitates unbiased hypothesisfree learning and faster pattern recognition and it allows users to more holistically determine the veracity of concept associations finally the platform allows the user to identify and further examine the documents and textual fragments from which the knowledge synthesis signals are derived using the documents and signals applications if we have an automated method that given a corpus consisting of text and other structured and semi-structured data as is often the case with biomedical data comes up with a strength of association score between a query entity and all the tokens or entities present in the corpus then that score can be used to obtain a ranking of tokens or entities related to the query there have been other motivations for ranking the association strength of tokensentities eg for use in picking among possible tokens in speech recognition or optical character recognition -ocr generally association scores are based in some way on the co-occurrences of the tokens or referents to the entities in the text within small windows of text co-occurrences have been studied in linguisticsnlp since at least firths maxim that a word is known by the company it keeps one popular traditional measure for association strength between tokens in text is pointwise mutual information or pmi 53  which we consider in several association scores formally for a given corpus an association score is some real-valued function sq t where q is a query tokenentity and t is another tokenentity we discussed association scores as if they were symmetric above but for some convenience later on our formal notion is asymmetric theres a query q and a token t in particular we later extend q to logical combinations of tokensentities the association score need not be symmetric for logical queries it cannot be as t is still restricted to single tokensentities though it often is when q and t are both single tokensentities context of q -all the measures involve the notion of the context of the query q the context of q are those locations in the corpus deemed to be near to q for single token queries follow the typical approach of defining context as those locations in the corpus that are within some fixed number of words w the window size w is a tunable parameter from an occurrence of q in the corpus the dynamic adjacency engine generalizes this notion of context in a natural way to logical queries the context for a logical q is a certain set of fixed-length fragments co-occurrences -this is just the number of times t appears in the context of q traditional pmi -this is logpt  q  pt here pt  q is the number of times t occurs in the context of q ie co-occurrences of t and q divided by the total length of all q contexts in the corpus whereas pt is the number of occurrences of t in the entire corpus divided by the corpus length word2vec cosine distance -the popular word2vec algorithm 5 generates a vector we use 300dimensional vector representation for each token in a corpus the purpose of these vectors is usually to be used as features in downstream nlp tasks but they can also be used for similarity the original paper validates the vectors by testing them on word similarity tasks the association score is the cosine between the vector for q and the vector for t this score only applies to singletoken q exponential mask pmi exppmi -this is our first new proposed score pmi treats every position in a binary way its either in the context of q or not with a window size of say 50 a token which appears 3 words from a query q and a token which appears 45 words from a query q are treated the same we thought it might be useful to consider a measure which distinguishes positions in the context based on the number of words away that position is from an occurrence of q we did this by weighting the positions in the context by some weight between 0 and 1 our weighting is based on an exponential decay which has some nice properties especially when we extend to the case of logical queries local score -this is another new proposed score we find that pmi and exppmi can vary a lot for small samples ie small numbers of co-occurrences occurrences the local score is logcoocc  sigmoidpmi -05 constructed to correct for this we found that this formula too works well empirically exponential mask local score explocalscore -we apply both modifications together the exponential mask score is logweightedcoocc  sigmoidexppmi -05 here weightedcoocc is the sum of the weights of the positions of the corpus evaluation of association scores are further described in the supplementary information the objective of the single cell platform is to enable dynamic visualization and analysis of single cell rna-sequencing data currently there are over 30 scrna-seq studies available for analysis in the single cell app including studies from human donorspatients covering tissues such as adipose tissue blood bone marrow colon esophagus liver lung kidney ovary nasal epithelium pancreas placenta prostate retina small intestine and spleen because no pantissue reference dataset yet exists for humans we have manually selected individual studies to maximally cover the set of human tissues in some cases these studies contain cells from both healthy donors and patients affected by a specified pathology such as ulcerative colitis colon or asthma lung there are also a number of murine scrna-seq studies covering tissues including adipose tissue airway epithelium blood bone marrow brain breast colon heart kidney liver lung ovary pancreas placenta prostate skeletal muscle skin spleen stomach small intestine testis thymus tongue trachea urinary bladder uterus and vasculature note that two of these murine studies tabula muris and mouse cell atlas include 20 tissues each for each study a counts matrix was downloaded from a public data repository such as the gene expression omnibus geo or the broad institute single cell portal  table s1  note that this data has not been re-processed from the raw sequencing output and so it is likely that alignment and quantification of gene expression was performed using different tools for different studies in some cases multiple complementary datasets have been generated from a single publication in these cases we have generated separate entries in the single cell platform while counts matrices have been generated using different technologies eg drop-seq 10x genomics etc and different alignmentpre-processing pipelines all counts matrices were scaled such that each cell contains a total of 10000 scaled counts ie the sum of expression values for all genes equals 10000 in each individual cell all data were uniformly processed using the seurat v3 package 54  in short this pipeline involves the following steps first we identify 2000 variable genes across the given dataset and then perform linear dimensionality reduction by principal component analysis pca using the set of principal components which contribute 80 of variance across the dataset we then do the following i perform graph-based clustering to identify groups of cells with similar expression profiles louvain clustering ii compute umap and tsne coordinates for each individual cell used for data visualization and iii annotate cell clusters note that the three human pancreatic datasets gse81076 gse85241 gse86469 were integrated together in a shared multi-dimensional space using cca canonical correlation analysis and the integration method in the seurat v3 package 54  cell clustering and computation of dimensionality reduction coordinates were performed on this integrated dataset in cases where publicly deposited counts matrices are accompanied by author-assigned annotations for individual cells or clusters we have retained these cell annotations for display in the platform and accompanying analyses for any study which was not accompanied by a metadata file containing cluster annotations we have manually labeled clusters based on sets of canonical cluster-defining genes in these cases we have attempted to leverage annotations and descriptions of gene expression patterns described by study authors in the manuscript text and figures corresponding to the data being analyzed the platform allows users to query any gene in any selected study the corresponding data is displayed in commonly employed formats including a series of violin plots and as a set of dimensionality reduction plots expression is summarized by listing the percent of cells expressing gene g in each annotated cluster and the mean expression of gene g in each cluster to measure the specificity of gene g expression to each cluster c we compute a cohens d value which assesses the effect size between the mean expression of gene g in cluster c and the mean expression of gene g in all other clusters specifically the cohens d formula is given as follows meanc -meanasqrtstdevc 2  stdeva 2  where c represents the cluster of interest and a represents the complement of c ie all other cell clusters note that this is functionally similar to the computation of paired fold change values and p-values between clusters which is frequently used to identify cluster-defining genes within the platform we support the run-time computation of cosine similarity ie 1 -cosine distance between the queried gene and all other genes this provides a measure of expression similarity across cells and can be used to identify co-regulated and co-expressed genes specifically to perform this computation we construct a gene expression vector for each gene g this corresponds to the set of cp10k values for gene g in each individual cell from the selected populations in the selected study for each single-cell dataset we examined the expression of ace2 tmprss2 anpep and dpp4 we generally considered a cell population to potentially express a gene if at least 5 of cells from that cluster showed non-zero expression of this gene for each dataset we show a figure which includes a umap dimensionality reduction plot colored by annotated cell type along with identical plots colored by the expression level of each coronavirus receptor in all individual cells in some cases we also show violin plots from the platform which automatically integrate literature-derived insights to highlight whether there exist textual associations between the queried gene and the tissuecell types identified in the selected study figure 5 expression of all known coronavirus receptors synchronously increase with enterocyte maturation  estimation of evolutionary dynamics and selection pressure in coronaviruses helena maier jane erica bickerton paul britton muhammad munir mart cortey   coronaviruses encode the largest positive sense single-stranded rna genomes known ranging from 27 to 31 kb in length although coronaviruses have been shown to possess proofreading ability 1 relatively high mutation rates mean that coronaviruses are one of the most diverse genetically distinct and recently emerging groups of viruses the emergence of these viruses are mainly triggered by the virus evolution which could occur due to high mutational rates selection pressure on genetic diversity inter- and intra-host selection frequency of recombination and genetic drifts during transmission bottlenecks within subfamily coronaviridae alphacoronaviruses and betacoronaviruses infect and cause diseases in mammals whereas gammacoronaviruses are mainly avian specific 2 bovine coronaviruses bcovs together with human coronavirus oc43 hcov-oc43 equine coronavirus ecov and porcine hemagglutinating encephalomyelitis virus phev belong to the virus species betacoronavirus1 of the lineage a of the genus betacoronavirus 3 bcov causes infections both in respiratory and enteric systems in cattle of all ages like other coronaviruses bcov exhibit high genetic mutations one mutation per genome per replication round 4 5 the nucleotide nt substitutions per site per year were found to be 13  104 61  104 and 36  104 for rna-dependent rna polymerase rdrp s and n genes respectively 68 due to their evolutionary potential bcovs have been isolated from humans bcov-like human enteric coronavirus hecv-4408us94 and a recently isolated canine respiratory coronavirus crcov has also shown a high genetic similarity to betacoronavirus1 9 10 taken together experimental data and mathematical models have reinforced the need for studying coronavirus dynamics and evolution which could provide bases for effective control measures recent availability of quantitative deep-sequencing methodologies has provided data that can be modelled for future prediction of transmission dynamics and to estimate relevant parameters in this protocol we used publically available s gene data on bcov as prototype coronavirus and analyzed to predict epidemiological linkage mutation-prone sites and evolution in the s gene of bcov the same protocol is applicable to other genes of the coronaviruses and viruses of other families to perform in silico analysis of the s genes of bcov the following equipment will be required see
note
1mac os x with minimum 24 ghz processor and 2 gb ramtextedit textwrangler stable release 18 or latestbioedit version v725mrbayes version 322 or latesta perl script for generating suitable file formatsbeast version 180 or latestbeauti version 17 or latesttracer version 16 or latestfigtree v123 or latestan appropriate internet access
 
define objectives see
note
2construct dataset and label it as bcovs genesfas see
note
3open the downloaded file bcovs genesfas in textedit and edit the sequence titles the sequence titles can be arranged depending upon objective in mind and the availability of downstream analysis tools one accepted way of labelling the sequence title will be to arrange them in hostisolateidgenotypecountryyear accession number remove all illegal characters along with empty spaces and replace them with underscoreunderstrike  however do not remove any greater than signs  which will destroy the fasta format and may require rebuilding of the data set 11 to do so use the find and replace with options in the textedit which can be opened with cmdf command in mac os x save the file before closing the datasetopen the file in bioedit and click on accessory application - clustalw multiple alignment save the newly opened aligned file and label it as bcovs genesalignfas see
note
4convert the fas file bcovsgenesalignfas to a nex file bcovs genesalignnex use either a perl script available to freely download at httpsgithubcomdrmuhammadmunirperlblobmasterconvertfastatophylip or using trial version of codoncode aligner wwwcodoncodecomaligner see
note
5move bcovsgenesalignnex file into the folder of mrbayes detailed description of the program can be found on the webpage of the program httpmrbayessourceforgenet briefly open terminal and type mb to start the mrbayes software double click on mrbayes application icon in window the following instructions should appearmrbayes v321 x64bayesian analysis of phylogenydistributed under the gnu general public licensetype help or help command for information on the commands that are availabletype about for authorship and general information about the programmrbayes 
to execute the file into the program type executespacefilename eg execute bcovsgenesalignnex then press enter the message reached end of file indicates successful execution of the file and the program is ready to run in any error either follow the instructions mentioned in the error or rebuilt datasets the most common error is the presence of illegal characters such as pipeline sign  colon  semicolon  slashstrokesolidus  apostrophe   quotation marks         and brackets        among others therefore remove these from the fasta file as described beforeto set the evolutionary model to the gtr substitution type lset nst6 ratesinvgamma after the mrbayes  prompt then press enter the message successfully set likelihood model parameters indicates the success in model setupto set the sample collection 200 from posterior probability distribution diagnostic calculation every 1000 generations and print and sample frequency to 100 type mcmc ngen20000 samplefreq100 printfreq100 diagnfreq1000 after the mrbayes  prompt then press enter program will start calculating the split frequency depending on the speed of the operating system and the size of the dataset note the message average standard deviation of split frequencies if it is below 001 after 2000 generations type yes after continue the analysis yesno prompt to set more generations continue this until the split frequency drops below 001 once reached type no which leads the users to mrbayes  promptto summarize the parameter type sump then press enterto summarize the tree type sumt then press enter this command will save the tree with extension nexcontre ie bcovs genesalignnexcontre in the mrbayes folder where the original file bcovs genesalignnex was kept the tree can be opened and annotated in the figtreeopen the desired file bcovsgenesalignnexcontre after launching figtreelabel your sequences by searching your sequence-tag such as isolate name or country in the search button when taxa is selected similarly select nod or clade to label the respective items see
note
6after annotation save your tree using file - export graphics - pdf or other desired file format from the list - ok path the resulting file can be used for further editing or for presentation 11
 
to analyze the occurrences of synonymous ds and non-synonymous dn substitutions in the s gene use the same fasta file bcovsgenesalignfas that was generated for phylodynamics see
note
7open the snap tool freely available at httpwwwhivlanlgovcontentsequencesnapsnaphtml and paste the sequence or upload the datasetboth accumulated cumulated dn-ds and per codon dn-ds selection sites can be calculated by the generated table of snapsince the selections are calculated on every nucleotide sites under positive or negative selection can be highlighted see
note
8
 
alternatively and to verify the robustness of the data generated by the snap the same alignment can be used to calculate selection pressure using gtr general time reversible substitution model on a neighbor-joining phylogenetic tree by the datamonkey web server freely available at httpwwwdatamonkeyorgdatauploadphpthe program uses the computational engine of the hyphy package 12 to estimate dnds with a variety of evolutionary models and can analyze selection even in the presence of recombination see
note
9
 
within the beast package open beauti program bayesian evolutionary analysis utility and import nexus bcovsgenesalignnex or fasta bcovsgenesalignfas file of the data set remember to execute the data by file - import data - openseveral parameters of the beast run ie the date of the sequences the substitution model the rate variation among sites the length of the mcmc chain can then be adjusted according to specific need 13 see
note
10once all desired parameters are set finally click on the generate beast file to generate xml file which will be used as input for beast analysislabel the file as bcovsgenesalignxml for consistency
 this is a brief explanation in order to run beast program and summarize results using tracermove the xml files bcovsgenesalignxml into the beast folderopen the beast program double-click a white screen on java environment will appear wait for several seconds until a second screen appearschoose the file to analyze in this second screen before beginning the analyses enable the allow overwriting of log files option then press run and the analysis will beginafter few moments depending upon the processing capacity of the operating system and the size of the data the chain will begin to run there will be seven columns that extend vertically every column is one of the parameters that are being estimated however the first and the last column are crucial to observe the first column is the generation being sampled in every moment every chain has ten million steps and the last column shows how many millions of states will be run per hour remember ten million steps per chain depending on the length of the chain the length of the sequences and the number of sequences to be analyzed it may take variable time to complete the runonce the chain has run it is required to store the parameters close the beast window and open the beast folder every time a chain is run two files are generated xml file and several ends log and tre once the first run is complete change the name of the log and tre files for example after the completion of a run for bcovsgenealignxml bcovsgenealignlog and bcovsgenealigntre files will be generated rename these two files to bcovsgenealign1log and bcovsgenealign1trerun the bcovsgenealignxml at least for two more times steps 25finally three different log files and tre files will be available labelled as bcovsgenealign1log bcovsgenealign2log and bcovsgenealign3log these three files contain the estimations of the substitution rate that have to be summarized in tracerto summarize the run open the tracer program and select the option file and input trace file and open the first 1log file from the folder followed by the addition of the second 2log file finally add the third 3log log filethe estimations of the parameters are viewable in the graphic interface select the option combined from the trace files upper left and the estimations that will appear on the traces table are the main estimations for all the parameters see
note
11 generally the desired parameters are
tree model root this is the number of years that passed after the most recent common ancestor tmrca subtract this number from the most modern date to yield the tmrca for the dataset
clock rate this is directly the rate of evolution in substitutionsiteyear

 
this protocol is optimized for mac os x however all the software packages and tools used here are also available for windows which can be installed using recommended methodologies all the software used here are open access which do not require any subscription for any operating systems these software packages are only for demonstration purposes and there may be alternative solutions for the same purpose the overall time of the data analysis depends upon processing power of the operating system and the number and length of sequences in the datasetthe same phylogenetic tree can be used for different interpretations failing to create a proper objective can lead to drawing incorrect conclusions from phylogenetic studies it is therefore essential to define the objective for the downstream analyses before initiating the studyconstruction of datasets depends on the objectives one of the most common interests of bioinformaticians is to determine the epidemiological linking of the query sequence to that of sequences reported from the world and are available in the public domains for this purpose the basic local alignment search tool blast is the most widely used tool primarily owing to its speed of execution search the nucleotide sequences with objective-based keyword such as bovine coronaviruses s gene manual editing and investigations of the downloaded sequences are always suggested notably blast-explorer is primarily aimed at helping the construction of sequence datasets for further phylogenetic study and it can also be used as a standard blast server with enriched output use blast or blast-explorer or other suitable database for construction of datasetsthere are different algorithms for dna sequence alignment with variable degrees of utility in this protocol clustalw was used for simplicity any other algorithm can be used depending upon the preferences and interestnexus format is required input for mrbayes different tools both online and offline can be used to generate appropriate nexus output we have only presented two commonly used and easily achievable methodsdetailed demonstration for tree annotation is described in our earlier publication 11the file used for phylogenetic analysis may contain all available sequences in the public domain which increases the size of the file significantly however depending upon the objective in mind the datasets can be modified accordingly for the larger datasets the compiled data will be emailed to the email address provided once ready this is also important to keep a record for future usethe cut point is calculated to be zero all sites showing cumulated dn-ds values above 0 are under positive pressure whereas values below 0 are under negative pressurethe nature of parameter selection and interpretation is complex and is beyond the scope of this protocol please consult developers published report for thorough understating of the concepts and applications 12the parameters of the beast run are crucial and can determine the nature of output and may heavily influence the results however normally the default parameters are used 13when summarizing the beast results do not use the mean as it appears in the output this is the arithmetic mean instead use the geometric mean that appears in the summary statistic table right upper from a methodological point of view this is much more correct click on every parameter in the tracer table and the summary statistic table for every parameter will change
  building a pubmed knowledge graph jian xu sunkyu kim min song minbyul jeong donghyeon kim jaewoo kang justin rousseau f xin li weijia xu vetle torvik i yi bu chongyan chen islam ebeid akef daifeng li ying ding   experts in healthcare and medicine communicate in their own languages such as snomed ct icd-10 pubchem and gene ontology these languages equate to gibberish for laypeople but for medical minds they are an intricate method of transporting important semantics and consensus capable of translating diagnoses medical procedures and medications among millions of physicians nurses and medical researchers thousands of hospitals hundreds of pharmacies and a multitude of health insurance companies these languages eg genes drugs proteins species and mutations are the backbone of quality healthcare however they are deeply embedded in publications making literature searches increasingly onerous because conventional text mining tools and algorithms continue to be ineffective given that medical domains are deeply divided locating collaborators across domains is arduous for instance if a researcher wants to study ace2 gene related to covid-19 he or she would like to know the following which researchers are currently actively studying ace2 gene what are the related genes diseases or drugs discussed in these articles related to ace2 gene and with whom could the researcher collaborate this is a strenuous position to be in and the aforementioned problems diminish the curiosity directed at the topic many studies have been devoted to building open-access datasets to solve bio-entity recognition problems for example hakala et al1 used a conditional random field classifier-based tool to recognize the named entities from pubmed and pubmed central bell et al2 performed a large-scale integration of a diverse set of bio-entities and their relationships from both bio-entity datasets and pubmed literature although these open-access datasets are predominantly about bio-entity recognition researchers have also been interested in extracting other types of entities and relationships from pubmed including the mapping of author affiliations to cities and their geocodes34 author name disambiguation5 and and author background information collections6 although the focus of previous research has been on limited types of entities the goal of our study was to integrate a comprehensive dataset by capturing bio-entities disambiguated authors funding and fine-grained affiliation information from pubmed literature figure 1 illustrates the bio-entity integration framework this framework consists of two parts 1 bio-entity extraction which contains entity extraction named entity recognition ner and multi-type normalization and 2 integration which connects authors orcid and funding information the process illustrated in fig 1 can be described as follows first we applied the high-performance deep learning method bidirectional encoder representations from transformers for biomedical text mining biobert78 to extract bio-entities from 29 million pubmed abstracts based on the evaluation this method significantly outperformed the state-of-the-art methods based on the f1 score by 051 on average then we integrated two existing high-quality author disambiguation datasets author-ity5 and semantic scholar9 we obtained the disambiguated authors of pubmed articles with full coverage and quality of 9809 in terms of the f1 score next we integrated additional fields from credible sources into our dataset which included the projects funded by the national institutes of health nih10 the affiliation history and educational background of authors from orcid6 and fine-grained region and location information from the mapaffil 2016 dataset11 we named this new interlinked dataset pubmed knowledge graph pkg pkg is by far the most comprehensive up-to-date high-quality dataset for pubmed regarding bio-entities articles scholars affiliations and funding information being an open dataset pkg contains rich information ready to be deployed facilitating the effortless development of applications such as finding experts searching bio-entities analyzing scholarly impacts and profiling scientists careers the ner task recognizes a variety of domain-specific proper nouns in a biomedical corpus and is perceived as one of the most notable biomedical text mining tasks in contrast to previous studies that have built models based on long short-term memory lstm and conditional random fields crfs1213 the recently proposed bidirectional encoder representations from transformers bert14 model achieves excellent performance for most of the nlp tasks with minimal task-specific architecture modifications the transformers applied in bert connect the encoders and decoders through self-attention for greater parallelization and reduced training time bert was designed as a general-purpose language representation model that was pre-trained on english wikipedia and bookscorpus consequently it is incredibly challenging to maintain high performance when applying bert to biomedical domain texts that contain a considerable number of domain-specific proper nouns and terms eg brca1 gene and triton x-100 chemical bert required refinement so bioberta neural network-based high-performance ner modelwas developed its purpose is to recognize the known biomedical entities and discover new biomedical entities first in the ner component the case-sensitive version of bert is used to initialize biobert second pubmed articles and pubmed central articles are used to pre-train bioberts weights the pre-trained weights are then fine-tuned for the ner task while fine-tuning bert biobert we used wordpiece tokenization15 to mitigate the out-of-vocabulary issue wordpiece embedding is a method of dividing a word into several units eg immunoglobulin divided into i mmuno g lo bul in and expressing each unit this technique is effective at extracting the features associated with uncommon words the ner models available in biobert can predict the following seven tags iob2 tags ie inside outside and begin16 x ie a sub-token of wordpiece cls ie the leading token of a sequence for classification sep ie a sentence delimiter and pad ie a padding of each word in a sentence the ner models were fine-tuned as follows81documentclass12ptminimal
				usepackageamsmath
				usepackagewasysym 
				usepackageamsfonts 
				usepackageamssymb 
				usepackageamsbsy
				usepackagemathrsfs
				usepackageupgreek
				setlengthoddsidemargin-69pt
				begindocumentplefttirightsoftmaxtiwtbkquad quad k01ldots 6enddocumentptisoftmaxtiwtbkk016where k represents the indexes of seven tags b i o x cls sep pad p is the probability distribution of assigning each k to token i and documentclass12ptminimal
				usepackageamsmath
				usepackagewasysym 
				usepackageamsfonts 
				usepackageamssymb 
				usepackageamsbsy
				usepackagemathrsfs
				usepackageupgreek
				setlengthoddsidemargin-69pt
				begindocumenttiin rhenddocumenttirh is the final hidden representation which is calculated by biobert for each token i h is the hidden size of ti documentclass12ptminimal
				usepackageamsmath
				usepackagewasysym 
				usepackageamsfonts 
				usepackageamssymb 
				usepackageamsbsy
				usepackagemathrsfs
				usepackageupgreek
				setlengthoddsidemargin-69pt
				begindocumentwin rktimes henddocumentwrkh is a weight matrix between k and ti k represents the number of tags and is equal to 7 and b is a k-dimensional vector that records the bias on each k the classification loss l is calculated as follows2documentclass12ptminimal
				usepackageamsmath
				usepackagewasysym 
				usepackageamsfonts 
				usepackageamssymb 
				usepackageamsbsy
				usepackagemathrsfs
				usepackageupgreek
				setlengthoddsidemargin-69pt
				begindocumentlleftvartheta right-frac1nmathopsum limitsi1nlogleftpleftyi tivartheta rightrightenddocumentl1ni1nlogpyitiwhere  represents the trainable parameters and n is the sequence length first a tokenizer was applied to words in a sentence on a dataset with labels in the conll format17 the wordpiece algorithm was then applied to the sub-words of each word consequently biobert was able to extract diverse types of bio-entities furthermore an entity or two entities with frequently-occurring token interaction would be marked with more than one entity type span 262 for all pubmed abstracts based on the calculated probability distribution we were able to choose the correct entity type when entities were tagged with more than two types according to the probability-based decision rules8 because an entity may be referred to by several synonymous terms synonyms and a term can be polysemous if it refers to multiple entity types polysemy we require a normalization process for the extracted entities however it is a daunting challenge to build a single normalization tool for multiple entity types because there exist various normalization models that depend on the type of entity we addressed this issue by combining multiple ner normalization models into one multi-type normalization model that assigns ids to extracted entities table 1 illustrates the statistics of the proposed normalization model the multi-type normalization model is based on a normalization model per entity type table 1 to improve the number of normalized entities we added the disease names from the polysearch2 dictionary 76001 names of 27658 diseases to the sieve-based entity linking dictionary 76237 names of 11915 diseases we also added the drug names from drugbank18 and the us food and drug administration fda to the tmchem dictionary because there are no existing normalization models for species we normalized species based on dictionary lookup using tmvar 20 we created a dictionary of mutations with normalized mutation names in which a mutation with several names was assigned to one normalized name or id despite a rigorous effort to create global author ids eg orcid and researcherid most articles in pubmed particularly those before 2003 the year in which the field orcid was added into pubmed provide limited author information with respect to last name first initial and affiliation only for first authors before 2014 author information is not effective meta-data to be used directly as a unique identifier because different people may have the same names and the names and affiliations of an individual can change over time and is essential for identifying unique authors in recent decades researchers have made several attempts to solve the and problem using three types of methods the first type of method relies on manual matching of articles with authors by surveying scientists or consulting curricula vitae cvs gathered from the internet19 although this type of method ensures high accuracy a considerable amount of investment in labor is required to collect and code the data which is impractical for huge datasets the second type of method uses publicly-accessible registry platforms such as orcid or google scholar to help researchers identify their own publications which produces a source of highly accurate and low-cost accessible disambiguation of authorship for large numbers of authors however registries cover only a small proportion of researchers2021 which introduces a form of survivor bias into samples the third type of method uses an automated approach to estimate the similarity of author instance feature combinations and identify whether they refer to the same person the features for automated and include author name author affiliation article keywords journal names22 coauthor information23 and citation patterns24 automated methods typically rely on supervised or unsupervised machine learning in which the machine learns how to weigh the various features associated with author names and where to assign a pair of author names either to the same author or to two different authors2526 this type of method can potentially avoid the shortcomings of the previous two types moreover automated methods have been improved to a high level of accuracy after years of development for pubmed automated methods are the optimal choice because they can overcome the shortcomings of the other two methods while simultaneously providing high-quality and results for the entire dataset several scholars have disambiguated the authors using automated methods although the evaluations of these results have exhibited different levels of accuracy and coverage limitations we believe that integrating them with due diligence can yield a high-quality and dataset with full coverage of pubmed articles according to our investigation a high-quality pubmed and dataset with complete coverage can be obtained through the integration of the following two existing and datasetsauthor-ity the author-ity database uses diverse information about authors and publications to determine whether two or more instances of the same name or of highly similar names on different papers represent the same person according to the and evaluation based on the method discussed in the section technical validation the f1 score of author-ity is 9816 which is the highest accuracy result that we have observed however this dataset only covers authors before 2009semantic scholar the semantic scholar database trains a binary classifier to merge a pair of author names and use the pair to create author clusters incrementally according to the and evaluation based on the method discussed in the section technical validation the f1 score of semantic scholar is 9694 which is 122 lower than that of author-ity however it has the most comprehensive coverage of authors because the author-ity dataset has a higher f1 score than the semantic scholar dataset we selected the authors unique id of the author-ity dataset as the primary andid andid is limited by time range containing pubmed papers before 2009 however we supplemented authors after 2009 using the and result from semantic scholar the following steps were applied step 1 we allocated the authors unique id to each author instance according to the author-ity and results such that authors from the author-ity dataset before 2009 have unique author ids step 2 for authors that have the same semantic scholar andid but never appear in the author-ity dataset we generated a new andid to label them for example author pietranico r published two papers in 2012 and 2013 and had two corresponding author instances because all papers that pietranico r published were after 2009 they were not covered by author-ity and therefore had no andid allocated by author-ity however the authors disambiguated correctly by semantic scholar were allocated unique andids in semantic scholar to maintain the consistency in labeling we generated a new andid continuing andids of author-ity to label these two author instances as disambiguated by semantic scholar step 3 for author instances with a unique andid in semantic scholar and in which authors at least one had the same author-ity andid we allocated the author-ity andid to all author instances as their unique id for example maneksha s published three papers in 2007 2009 and 2010 and the first two author instances had a unique author-ity andid however the last one had no author-ity andid because it was beyond the time coverage of the author-ity dataset nevertheless based on the and results of semantic scholar the three author instances had an identical andid therefore the last author instance with no author-ity andid could be labeled with the same id as the other two author instances nih exporter provides data files that contain research projects funded by major funding agencies such as the centers for disease control and prevention cdc the nih the agency for healthcare research and quality ahrq the health resources and services administration hrsa the substance abuse and mental health services administration samhsa and the us department of veterans affairs va furthermore it provides publications and patents citing support from these projects it consists of 49 data fields including the amount of funding for each fiscal year organization information of the pis and the details of the projects according to our investigation nih-funded research accounts for 807 of all grants recorded in pubmed the nih exporter dataset contains a unique piid for each scholar who received nih funding between 1985 and 2018 and his or her pmids of the published articles through the mapping of pmids in nih exporter to pmids in pubmed 1n connections between the pi and articles have been established paving the way for investigating the article details of a specific pi and vice versa furthermore by mapping pi names last name first initial and affiliation to author names that were listed in articles supported by the pis projects a 11 connection between the pi and the andid was established providing a way to obtain pi-related article information regardless of whether the article was labeled with a project id according to its website orcid is a nonprofit organization helping to create a world in which all who participate in research scholarship and innovation are uniquely identified and connected to their contributions and affiliations across disciplines borders and time27 it maintains a registry platform for researchers to actively participate in identifying their own publications information about formal employment relationships with organizations and educational backgrounds orcid provides an open-access dataset called orcid public dataset 20186 which contains a snapshot of all public data in the orcid registry associated with an orcid record that was created or claimed by an individual as of october 1 2018 the dataset includes 7132113 orcid ids of which 1963375 have educational affiliations and 1913610 have employment affiliations as a result of the proliferation of orcid identifiers pubmed has used orcid identifiers as alternative author identifiers since 201328 using the following two steps we could map orcid records to the pubmed authors our first step was to map the author instances in pubmed to an orcid record based on the feature combinations of article doi and author name last name and first initial because the doi is not a compulsory field for pubmed we appended the feature combinations of article titles journals and author names to map the records between the two datasets the result contained many 11 connections between a disambiguated author of pubmed and an orcid record furthermore 11 connections between andid and orcid id and 1n connections between andid and background information education and employment were established the mapaffil 2016 dataset3 resolves pubmed authors affiliation strings to cities and associated geocodes worldwide this dataset was constructed based on a snapshot of pubmed which included the medline and pubmed-not-medline records acquired in the first week of october 2016 affiliations were linked to a specific author on a specific article prior to 2014 pubmed only recorded the affiliation of the first author however mapaffil 2016 covered some pubmed records that lacked affiliations and were harvested elsewhere such as from pmc nih grants the microsoft academic graph and the astrophysics data system all affiliation strings were processed using mapaffil to identify and disambiguate the most specific place names the dataset provides the following fields pmid author order last name first name year of publication affiliation type city state country journal latitude longitude and federal information processing standards fips code the mapaffil 2016 dataset does have a limitation because it does not cover the pubmed data after 2015 covering 629 affiliation instances in pubmed consequently we performed an additional step to improve the fraction of coverage we collected authors who published their first article before 2016 and continued publishing articles after 2015 by their andids the new affiliation instances of the author after 2015 succeeded their corresponding fine-grained affiliation data from the affiliation instances before 2016 fraction of affiliation instance coverage increased to 842 if the author did not change affiliation we also applied an up-to-date open-source library affiliation parser4 to extract additional fine-grained affiliation fields from all affiliation instances including department institution email zip code location and country table 2 summarizes the date coverage and version information of integrated datasets and open-access software used to extract data we built pkg with bio-entities extracted from pubmed abstracts and results of pubmed authors and the integrated multi-source information this dataset is freely available on figshare29 it contains seven comma-separated value csv files named authorlist bioentitiesmain bioentitiesmutation affiliations researcheremployment researchereducation and nihprojects the details are presented in table 3 pubmed raw data are not included into figshare file set because the amount of pubmed raw data is too large and they are not generated or altered by our methods pubmed raw data can be freely downloaded from pubmed website30 we also provide the following download link httpertaccutexasedudatasetsped which contains both the pubmed raw data and pkg dataset to facilitate the application of pkg dataset the statistics of all five types of extracted entities are presented in table 4 each data field is self-explanatory by its name and fields with the same name in other tables follow the same data format that can be linked across tables tables 511 illustrate the field name format and short description of fields for each data file listed in table 3 updating pkg is a complex task because it is subject to the update of different data sources and requires significant computation in the future we hope to refresh pkg quarterly based on pubmed updated files and updated datasets from other sources we may also develop an integrative ontology to integrate all types of entities to validate the performance of the bio-entity extraction we established bert and the state-of-the-art models as baselines then we calculated the entity-level precision recall and f1 scores of these models as evaluation metrics the datasets and the test results of biomedical ner are presented in table 12 in table 12 we report the precision p recall r and f1 f scores of each dataset the highest scores are in boldface and the second-highest scores are underlined sachan et al31 reported the scores of the state-of-the-art models for the ncbi disease and bc2gm datasets presented in table 10 moreover the scores for the 2010 i2b2va dataset were obtained from zhu et al32 single model and the scores for the bc5cdr and jnlpba datasets were obtained from yoon et al13 the scores for the bc4chemd dataset were obtained from wang et al33 and scores for the linnaeus and species-800 datasets were obtained from giorgi and bader34 according to table 12 bert which is pre-trained on the general domain corpus was highly effective on average the state-of-the-art models outperformed bert by 228 in terms of the f1 score however biobert obtained the highest f1 score in recognizing genesproteins diseases and drugschemicals it outperformed the state-of-the-art models by 051 in terms of the f1 score on average we used the multi-type normalization model to assign unique ids to synonymous entities table 13 presents the performance of the multi-type entity normalization model as shown in table 13 with respect to genes and proteins there were 75 different species in the bc3 gene normalization bc3gn test set but gnormplus focused only on seven of these species consequently gnormplus achieved a considerably lower f1 score by 366 on the multispecies test set bc3gn than on the human species test set bc2gn for mutations tmvar 20 achieved f1 scores close to 90 on two corpora osirisv12 and the thomas corpus the validation of author disambiguation remains a challenge because there is a lack of abundant validation sets we applied a method using the nih exporter-provided information on nih-funded researchers to evaluate the precision recall and f1 measures of the author disambiguation35 nih exporter provides information about the principal investigator id piid for each scholar who received nih funding between 1985 and 2018 because applicants established a unique piid and used the piid across all grant applications these piids have extremely high fidelity nih exporter also provides article pmids as project outputs which can be conveniently used as a connection between piids and andid we confirmed the bibliographic information of the nih-funded scientists who received nih funding during the years 19852018 our and evaluation steps were as follows first we collected project data for the years 19812018 in nih exporter including 304782 piid records and the corresponding 331483 projects next we matched the projects to articles acknowledging support by the grant which were also recorded in the nih exporter dataset we matched 214956 of the projects to at least one article and identified 1790949 articles funded by these projects some of these projects 116527 did not match articles and were excluded because the nih occasionally awards a project to a team that includes more than one pi we eliminated the 13154 records that contained multiple pis because they could result in uncertain credit allocation consequently our relevant set of pis decreased to 147027 individuals associated with 1749873 articles and 201802 projects we then connected nih piids from nih exporter to andids using the article pmids and author pis last name plus the initials as a crosswalk this step resulted in 1400789 unique articles remaining associated with 109601 piids and 107380 andids finally we computed precision p based on the number of articles associated with the most frequent andid-to-piid matched over the number of all articles associated with a specific andid36 furthermore we computed recall r based on the number of articles associated with the most frequent piid-to-andid matched over the number of all articles associated with a particular piid36 figure 3 summarizes the precision recall and f1 calculations table 14 illustrates the precision recall and f1 scores for author-ity semantic scholar and our integrated and result as presented in table 14 after integrating the and results of author-ity and semantic scholar we obtained a high-quality integrated and result that outperformed semantic scholar by 115 in terms of the f1 score and had more comprehensive coverage until 2018 than author-ity until 2009 the evaluation results of and might be slightly overestimated the pis of nih grants usually have many publications over a long period and might be more likely to have rich information such as affiliations and email addresses about publications therefore it should be easier to acquire higher performance on and tasks than that of new entrants who published fewer papers and may lack of sufficient information for and furthermore approximately 115 of the author instances cannot be disambiguated since they do not exist in the author-ity or semantic scholar and results which further slightly reduces the performance of and results theoretically however the semantic scholar and results and the and integration are evaluated based on the same baseline dataset with author-ity in this section and the evaluation of author-ity performance using a random sample of articles indicates reliably high quality the recall of the author-ity dataset is 988 the lumping putting two different individuals into the same cluster of the author-ity dataset affects 05 of the clusters and the splitting assigning articles written by the same individual to more than one cluster of the author-ity dataset affects 2 of the articles5 consequently we believe these factors have a limited impact on and performance for dr silberstein 539 bio-entities including 342 diseases 142 drugs 24 genes 17 species and 14 mutations were extracted from 455 articles as depicted in fig 4a headache and migraine were his two most studied diseases reaching 21 and 19 articles respectively in 2004 we trended his research over time on triptans starting with sumatriptan cgrp began to emerge in his publications starting in 2015 we noted the five researchers that have collaborated with dr silberstein through his career and map with pkg their collaborations interactions and institutions over time visualizing the profiles of individual researchers can help to understand the trends in their topics of interest and collaboration patterns to enable an understanding of collaboration factors that may be associated with academic success or scientific discovery for cgrp there are currently 7877 articles by 32392 authors on cgrp dating back to 1982 figure 4b illustrates that there was a dramatic increase in the number of cgrp-related articles from 13 in 1982 to 1209 in 1991 with a steady increase to 1517 in 2018 the trend of the number of authors over time was similar to that of the volume of articles on cgrp as we demonstrated with a previous analysis of the repurposing of aspirin4243 we observe research on cgrp starting at approximately the same time as the research on triptans for the treatment of migraines research on the pathophysiology of migraines identified a central role of the neuropeptide calcitonin gene-related peptide cgrp which is thought to be involved with the dilation of cerebral and dural blood vessels release of inflammatory mediators and the transmission of pain signals44 research on the mechanism of the action of triptansserotonin receptor agonistshas led to an understanding that they normalize elevated cgrp levels which among other mechanisms has led to an improvement in migraine headache symptoms consequently papers in high-impact journals have called for identifying molecules and the development of drugs to directly inhibit cgrp45 which has since led to the development of cgrp inhibitors as a new class of migraine treatment medications a total of 28223 disambiguated authors and 5379 distinct bio-entities of coronavirus articles were used to construct author-bio-entity bipartite network figure 5 illustrated the bipartite network fig 5a and its author projection fig 5b and bio-entity projection fig 5c in fig 5a the author vertices are blue and the bio-entity vertices are pink a link between a bio-entity and an author exists if and only if this bio-entity has been researched by that author connections between two authors or between two bio-entities are not allowed the edge weight is set as the number of papers an author published that mention a bio-entity in fig 5bc the edge weight is set as the number of common neighbors for the author and bio-entity respectively vertices are marked with different colors to show their community attribution figure 5a illustrates a distinct relationship between authors and their focused bio-entities for example the disease sars have been frequently studied by author baric r s yuen kwok-yung and zheng bo-jian in addition to sars baric r s is also interested in coronavirus infection and hbv infection figure 5b depicts the common research interest relationship between authors strong connections between authors may indicate that they collaborated multiple times such as chan kwok hung and yuen kwok-yung who published 69 papers together these connections may also indicate author pairs that have similar research interests but never collaborated such as baric r s and yuen kwok-yung which is crucial for the collaborative commendation similarly the connections between bio-entities in fig 5c indicate that they have been studied by authors with similar research interests which can be further applied to discover the hidden relations between bio-entities  intra-model variability in covid-19 classification using chest x-ray images brian goodwin d corey jaskolski can zhong herick asmani  synthetaic  x-ray and computed tomography ct scanning technologies for covid-19 screening have gained significant traction in ai research since the start of the coronavirus pandemic despite these continuous advancements for covid-19 screening many concerns remain about model reliability when used in a clinical setting much has been published but with limited transparency in expected model performance we set out to address this limitation through a set of experiments to quantify baseline performance metrics and variability for covid-19 detection in chest x-ray for 12 common deep learning architectures specifically we adopted an experimental paradigm controlling for train-validation-test split and model architecture where the source of prediction variability originates from model weight initialization random data augmentation transformations and batch shuffling each model architecture was trained 5 separate times on identical train-validation-test splits of a publicly available x-ray image dataset provided by  results indicate that even within model architectures model behavior varies in a meaningful way between trained models best performing models achieve a false negative rate of 3 out of 20 for detecting covid-19 in a hold-out set while these results show promise in using ai for covid-19 screening they further support the urgent need for diverse medical imaging datasets for model training in a way that yields consistent prediction outcomes it is our hope that these modeling results accelerate work in building a more robust dataset and a viable screening tool for covid-19  the spread of the novel coronavirus which causes covid-19 has caught most of the world off-guard resulting in severely limited testing capabilities for example as of april 15 2020 almost 3 months since the first case in the us only about 33 million tests have been administered 1  which equates to approximately 1 of the us population reverse transcription-polymerase chain reaction rt-pcr is an assay commonly used to test for covid-19 but is available in extremely limited capacity 2  3  in an effort to offer a minimally invasive low-cost covid-19 screen via x-ray imaging ai engineers and data scientists have begun to collect datasets 4 and utilize computer vision and deep learning algorithms 5  all these efforts seek to leverage an available medical imaging modality for both diagnosis and in the future predicting case outcome clinical observations have largely propelled ai research in computer vision for screening covid-19 and these reports cite differentiable lung abnormalities of covid-19 patients from chest ct 6  x-ray 7  8  and even ultrasound 9  current research also shows that covid-19 is correlated with specific biomarkers in x-ray 10  though these recent efforts are valuable in that they will lay the foundation for future work in this area there are significant flaws in the methodology as well as in the behavior of the resultant models much of the initial work on covid-19 prediction from chest x-ray used a training set that included a little over 100 images with 10 test images that were in fact identical to the validation set though such small test data sets do not allow for declaring sweeping diagnostic value statements unfortunately the popular media articles effectively hype the value of these models with hopeful titles like coronavirus neural network can help spot covid-19 in chest x-rays 11  how ai is helping in the fight against covid-19 12  and ai could help spot telltale signs of coronavirus in lung x-rays 13  network weights from these publications are not publicly available for these published models we have responded to this shortcoming by providing pre-trained weights for many of the most common deep learning architectures for computer vision and we have made the code for pre-training freely available to our knowledge this repository of pretrained model weights is the first of its kind in response to the current crisis and the first to report prediction results across multiple architectures on a test set that is held out from the validation and training sets our goal is to facilitate advancement of screening technology for covid-19 and highlight the need for larger more diverse datasets the urgency for a clinical methodology to provide covid-19 screens cannot be understated 14  our hope is twofold 1 that the community advances computer vision for covid-19 detection via x-ray before recommending use in a clinical setting and 2 that pre-trained model weights will help accelerate ongoing development in ai to augment the decision-making process for clinicians during a time where healthcare workers are under a severe amount of stress we carried out a series of experiments to quantify baseline machine-learning performance in detecting covid-19 from chest x-ray images using a series of common openly available neural network architectures computational benchmarking was outside of our experimental approach since it has been extensively studied 15  in this study we focused on quantifying the expected variability in prediction outcomes and sought to quantify the reliability of predictions with respect to the chest x-ray data that is currently available to the public and contains covid-19 positive scans all computational experiments were executed on lambda blade lambdalabscom hardware 8x rtx 8000  nvlink gpus with 48gb gddr6 ram 2x intel xeon gold 5218 processor with 512 gb ram using the pytorch framework 16  an identical train-validation-test split tvts was employed for all experiments and each model architecture was trained 5 separate times creating 5 separate models each we controlled for tvts and model architecture while allowing randomness in weight initialization and data augmentation during batch training for each experiment we designed our approach with the aim to elucidate expected variability in model behavior for covid-19 detection in chest x-ray since covid-19 datasets are becoming more abundant existing sets are constantly growing and evolving and it has become important to cite the data source and the day it was acquired specifically this study uses data from the cohen et al 4 chest x-ray dataset that was acquired on 2020-04-17 this dataset contains three classes 1 healthy 2 community acquired pneumonia cap and 3 covid-19 examples in figure 1  we employed a modified 80-10-10 tvts table i training paradigm it was modified to maximize the number of covid-19 training samples and double the size of the test hold out set used in previous work 5  the dataset version at the time of this study was large enough to accommodate the addition of a covid-19 validation set and test hold out set 2x larger than previous work from 10 hold out images to 20 5   we elected to generate baseline results for the following commonly used architectures resnet-18 -50 -101 -152 17  wideresnet-50 -101 18  resnext-50 -101 19  mobilenet-v1 20  densenet-121 -169 -201 21  adam optimization was used during training on only the last fully connected layer in each network using a batch size of 128 resulting in a mean compute time of 1567-507 secepoch across all architectures since all models have been pre-trained on imagenet we elected to freeze the convolutional layers to retain the higher-level learned features 22  ie all weights were frozen but for the final layer in each network models were trained on chest x-ray images of size 3 x 512 x 512 px all images were assumed to be rgb channels despite their inherent grayscale property therefore no manipulations were made to the networks to uniquely accommodate single channel x-ray imaging data all models were trained for 100 epochs with stopping criteria and weights from lowest validation loss were saved out covid-19 recall was not considered during training all experiments were carried out using weighted cross entropy loss wcel where the contribution to the loss from a given class is weighted based on its representative proportion in the total dataset our decision to use wcel as opposed to cel is based on the objective to achieve a high recall in the underrepresented class the covid-19 class since the ai task is straightforward detect covid-19 stopping criteria was based on plateau of the validation wcel performance metrics were then calculated using only the test holdout set our first iteration of testing not reported in this paper used cel and performance metrics were found to have improved dramatically when models were trained using wcel as reported in this paper validation losses followed a common trend across models during training with a distribution illustrated in figure 2  each training experiment was carried out on an identical tvts of the dataset for each architecture 5 training runs were carried out to gather a small distribution of models and prediction outcomes the class prediction and therefore covid-19 detection was based solely on the maximum value output from the softmax layer 3 total classes sources of variability across all experiments included image augmentation transformations that are based on random draws from a binomial probability distribution shuffling for batch allocation ie each batch id does not contain identical images across all experiments and random weight initialization last layer only datasets were prepared in a manner consistent with wang et al 5 see githubcomlindawanggcovid-net and a data augmentation protocol was implemented given the consistent format of a chest x-ray only modest translational and rotational augmentations were applied with brightness jitter and a possibility of horizontal flip table ii   multiple measures of accuracy and uncertainty were calculated to quantify baseline performance expectations for each network we report statistical tests model performance characteristics and common accuracy metrics with the aim to quantify the expected performance and the variability of model performance given the size of current covid-19 chest x-ray datasets specifically we report type i false pos- 24  this statistical test was carried out using the method described in dietterich 23 for a binary classification task covid-19 v non-covid-19 all statistical analyses were carried out using r 25  and figures were built using ggplot2 26  among all tested models densenet-169 was found to have the highest false negative rate fnr densenet-121 had the highest fnr variance and resnet-18 had the highest mean false positive rate fpr figure 3  all plot labels for model architectures are organized by number of tunable parameters increasing from left to right order reversed in figure 5b  the lowest fnr 015 was achieved by densenet-201 and resnext-101 overall type i and ii error rates varied across models and varied modestly within model architecture  figure 3  networks had consistently lower softmax output probabilities for covid-19 in the event of type i error fp while tp probability distributions consistently extended well into those from fp outputs figure 4  no significant difference across architectures was found between softmax probabilities in the event of a more serious type ii error fn failing to correctly detect covid-19 for the binary covid-19 detection task mean softmax outputs were 0601-0161 and 0213-0135 for fp and fn respectively interestingly 16 5 of all tp predictions had softmax probability outputs below 0667 prediction behavior was often inconsistent between models and those with identical architectures to a significant degree based on mcnemars test  figure 5  p-values of the mcnemars chi-squared test statistic were used to estimate the prediction behavior consistency in the test set only between all models for the binary classification task of detecting covid-19 versus non-covid-19 with the null hypothesis a low p-value suggests that the two models in question have inconsistent prediction behaviors a large portion of model comparisons show low p-values and therefore high prediction inconsistency  figure 5b  similarly low p-values were common when comparing models having identical architectures  figure 5c  if prediction behavior were largely consistent between models distributions shown in figures 5b and c would have large spikes near 10 instead the distribution of p-values is more uniform than expected given that our experimental design controlled for the tvts and model architecture across all experiments it is expected that a high quality dataset would produce a distribution of p-values indicating similar behavior between models with identical architectures figure 5c  mcnamers test suggests that several architectures have reliably inconsistent behavior the most apparent differences are between mobilenet-v2 v wideresnet-101 resnet-152 v wideresnet-101 and resnet-152 v wideresent-50  figure 5a  mobilenet-v2 was found to have the least inter-and intra-model differences which could be explained by its relatively small number of tunable parameters making a more general model fit predictions from densenet-121 models had the most consistency on average with all other trained models including those with identical architecture models sharing the wideresnet-101 architecture had the most intra-model differences followed by resnext-101 comparisons between mobilenet-v2 densenet-121 and resnet-18 indicate that these architectures had the most similar prediction behavior  figure 5b  and their false negative rate fnr values fall in the middle of the pack conversely models with deeper architectures were responsible for the highest fnr values table iii  using the ensemble of models trained during this study the fpr and fnr become 0009 and 020 respectively the ensemble predictions were carried out by simply summing the output from the last layer softmax of each network for a given image in the test hold-out set the model ensemble offers no improvement over best performing models in fnr or fpr but improves the f1 score and multiclass accuracy to 0640 from 0625 and 0894 from 0884 respectively the ai community is responding to the covid-19 crisis and releasing publications at a rapid pace studies show promise in using ai in a clinical setting as a screening tool for diseases like covid-19 while these results suggest that machine-learning techniques should only be used for covid-19 screening not diagnostic purposes our intent with this study was to accelerate existing work for clinical augmentation purposes and provide insight into model selection for covid-19 use cases the use of ai in diagnostic procedures should be limited for use as a decision support tool during screening and diagnoses should not rely on ai results alone given the length of the incubation period and the variability in the symptom onset latency from infection it is difficult to control for the time at which the image was acquired relative to the time of infection however efforts have been made to include an offset in covid-19 imaging datasets by accounting for the number of days since the start of symptoms or hospitalization 4  those who curate these datasets also must deal with inherent ambiguity in medical records such as image acquisition after a few days of symptoms for example cohen et al 4 assume 5 days with the currently available datasets ai engineers rely completely on clinical diagnoses and therefore assume that no false positive images exist ie it is assumed that patients that tested positive for covid-19 are indeed covid-19 positive given an estimated false negative rate for covid-19 tests are high  10 27  perhaps those testing positive can be safely assumed that they are indeed infected with the virus currently false positive rt-qpcr tests are not reported deep learning architectures like those in this study have been translated for use in upper body ct images 28  and current studies demonstrate more reliable predictions than those from chest x-rays 29  30  reasons for this performance relate to the image detail that can be obtained from a chest ct as well as the size of the dataset used by li et al 29  which contained images from 1296 covid-19 patients allowing for a larger hold-out test size than what is currently available in x-ray 127 images at the time of this study 29  while ct scans provide enhanced screens they are less accessible more expensive and less efficient than the chest x-ray due to the preparation time and scan time required consequently ct falls short as a covid-19 screening tool due to limitations including the complex mechanics and calibrations required for 3d geometrical renderings furthermore only 25 ct scanners exist in the united states per million population approx 31  and covid-19 screens place an increased demand on top of existing demand for ct scans not only that but an increased infection risk is an unfortunate corollary for non-covid-19 patients requiring ct scans 3  especially due to the amount of ct essential equipment that interfaces with the patient compared to that for chest x-ray not to mention the increased exposure for clinicians and technicians if covid-19 screens are needed en masse the chest x-ray is a promising low-cost service that requires no moving parts and could be modified to meet a spike in demand while this study suffers from several limitations accuracy metrics on a few tested architectures achieved results either consistent with or better than the current state-ofthe-art in both x-ray and ct studies 5  29  32 - 35  the results of this study though limited to specific neural network architectures suggest that transfer learning provides an efficient means to achieve high accuracy in detecting covid-19 however models remain highly dependent upon model architecture and vary depending on initial conditions and data augmentation steps to better quantify the reliability of ai predictions in this context our next goal is to implement segmentation techniques 36 and carry out a cross-validation protocol for each architecture and use a richer dataset model accuracy metrics indicate that more advancements are necessary before using ai for covid-19 screening via x-ray in our opinion clinicians should not rely on solutions derived from architectures that have high and high variability fnr furthermore inconsistent behavior in predictions between models with identical architectures injects doubt into its efficacy which is unlikely to resolve until dataset limitations are worked out without a more abundant dataset we do not expect deep learning approaches for covid-19 screening to gain the reliability needed for clinical implementation finally our aim is to encourage a focus on advancing the quality of x-ray screens for covid-19 due to its efficiency over other means and to accelerate workflows that seek to leverage ai we have made pre-trained model weights and the code for training freely available to the community through our github repository under a creative commons attribution-noncommercial 40 license covidresearchaidatasets githubcomsynthetaic we expect these model weights to provide significant improvements in model training efficiency as these public datasets continue to grow and evolve we believe that ai results have the potential to achieve a degree of reliability that alleviates skepticism within the medical community regarding the use of chest x-ray and computer vision to screen covid-19 models trained for this study had the task of detecting covid-19 versus community acquired pneumonia and non-covid-19 healthy cases which limits feature space to which the models are exposed future work should include many more image classifications to enable the network to learn features specific to a given pathology which could provide the means to elucidate differentiable features of covid-19 in chest x-ray we aim to solve the data limitation problem in future work through numerical methods and data collection  knowledge synthesis from 100 million biomedical documents augments the deep expression profiling of coronavirus receptors a venkatakrishnan j arjun puranik akash anand david zemmour xiang yao xiaoying wu ramakrishna chilaka dariusz murakowski k kristopher standish bharathwaj raghunathan tyler wagner enrique garcia-rivera hugo solomon abhinav garg rakesh barve anuli anyanwu-ofili najat khan venky soundararajan  the covid-19 pandemic demands assimilation of all available biomedical knowledge to decode its mechanisms of pathogenicity and transmission despite the recent renaissance in unsupervised neural networks for decoding unstructured natural languages a platform for the real-time synthesis of the exponentially growing biomedical literature and its comprehensive triangulation with deep omic insights is not available here we present the nferx platform for dynamic inference from over 45 quadrillion possible conceptual associations extracted from unstructured biomedical text and their triangulation with single cell rna-sequencing based insights from over 25 tissues using this platform we identify intersections between the pathologic manifestations of covid-19 and the comprehensive expression profile of the sars-cov-2 receptor ace2 we find that tongue keratinocytes airway club cells and ciliated cells are likely underappreciated targets of sars-cov-2 infection in addition to type ii pneumocytes and olfactory epithelial cells we further identify mature small intestinal enterocytes as a possible hotspot of covid-19 fecal-oral transmission where an intriguing maturation-correlated transcriptional signature is shared between ace2 and the other coronavirus receptors dpp4 mers-cov and anpep -coronavirus this study demonstrates how a holistic data science platform can leverage unprecedented quantities of structured and unstructured publicly available data to accelerate the generation of impactful biological insights and hypotheses the nferx platform single-cell resource -httpsacademianferxcom  since december 2019 the sars-cov-2 virus has been rapidly spreading across the globe the associated disease has been declared a pandemic by the world health organization who with over 350000 confirmed cases and 15000 deaths across nearly every country as of march 23 2020 1  the constellation of symptoms ranging from acute respiratory distress syndrome ards to gastrointestinal issues is similar to that observed in the 2002 severe acute respiratory syndrome sars epidemic and the 2012 middle east respiratory syndrome mers outbreak sars mers and covid-19 are all caused by coronaviruses cov deriving their name from the crown-like spike proteins protruding from the viral capsid surface coronavirus infection is driven by the attachment of the viral spike protein to specific human cell-surface receptors ace2 for sars-cov-2 and sars-cov 2-4  dpp4 for mers-cov 5 and anpep for specific -coronaviruses 6  in addition to these receptors the protease activity of tmprss2 has also been implicated in viral entry 7 8  in a recent clinical study of covid-19 patients from china 48 of the 191 infected patients studied had comorbidities such as hypertension and diabetes 9  epidemiological and clinical investigations on covid-19 patients are also suggesting fecal viral shedding and gastrointestinal infection 10 11 12  in the case of the earlier sars epidemic multiple organ damage involving lung kidney and heart was reported 13  the mechanisms by which various comorbidities impact the clinical course of infections and the reasons for the observed multi-organ phenotypes are still not well understood thus there is an urgent need to conduct a comprehensive pan-tissue profiling of ace2 the putative human receptor for sars-cov-2 a deep profiling of ace2 expression in the human body demands a platform that synthesizes biomedical insights encompassing multiple scales modalities and pathologies described across the scientific literature and various omics siloes with the exponential growth of scientific eg pubmed preprints grants translational eg clinicaltrialsgov and other eg patents biomedical knowledge bases a fundamental requirement is to recognize nuanced scientific phraseology and measure the strength of association between all possible pairs of such phrases such a holistic map of associations will provide insights into the knowledge harbored in the worlds biomedical literature while unsupervised machine learning has been advanced to study the semantic relationships between word embeddings 14 15 and applied to the material science corpus 16  this has not been scaled-up to extract the global context of conceptual associations from the entirety of publicly available unstructured biomedical text additionally a principled way of accounting for the distances between phrases captured from the ever-growing scientific literature has not been comprehensively researched to quantify the strength of local context between pairs of biological concepts given the propensity for irreproducible or erroneous scientific research 17  which reflects as truths semi-truths and falsities in the literature any local or global signals extracted from this unstructured knowledge need to be seamlessly triangulated with deep biological insights emergent from various omics data silos the nferx software is a cloud-based platform that enables users to dynamically query the universe of possible conceptual associations from over 100 million biomedical documents including the covid-19 open research dataset recently announced by the white house 18 figure 1  an unsupervised neural network is used to recognize and preserve complex biomedical phraseology as 300 million searchable tokens beyond the simpler words that have generally been explored using higher dimensional word embeddings previously 14  our local context score is derived from pointwise mutual information content between pairs of these tokens and can be retrieved dynamically for over 45 quadrillion possible associations our global context score is derived using word2vec 14  as the cosine distance between 180 million word vectors projected in a 300 dimensional space  figure 1a figure s1  in order to assess the veracity of these conceptual associations derived from biomedical literature it is absolutely essential to enable triangulation with structured data sources including gene and protein expression datasets to address this need and empower the scientific community we built a single cell resource which harnesses these local and global score metrics to enable seamless integration of literature-derived associations with the analysis of transcriptomes from over 1 million individual cells from over 25 human and mouse tissues  figure 1b  here we use this first-in-class resource to conduct a comprehensive expression profiling of ace2 across host tissues and cell types and discuss how the observed expression patterns correlate with the pathogenicity and viral transmission shaping the ongoing covid-19 pandemic  figure 1c  to systematically profile the transcriptional expression of ace2 across tissues and cell types we triangulated single cell rnaseq-based measurements with literature-derived signals to automatically delineate novel emerging and known expression patterns  figure 2b  table s1  this approach immediately highlights renal proximal tubular cells and small intestinal enterocytes among the cell types that most robustly express ace2 detection in 40 of cells these cell types are also moderately to strongly associated with ace2 in the literature the strong intestinal ace2 expression is particularly interesting given the emerging clinical reports of fecal shedding and persistence post-recovery which may reflect a fecal-oral transmission pattern 10-12  conversely pancreatic pp cells gamma cells pancreatic alpha cells and keratinocytes show similarly robust ace2 expression but have not been strongly associated with ace2 in the literature this combination suggests either a biological novelty or an experimental artifact we note that the strong ace2 expression in pancreatic cell types is derived from only one murine study figure s2a 19  while ace2 expression is not observed in gamma or alpha cells from scrna-seq of human pancreatic islets  figure s2b 20 21 22  while we cannot determine the validity of either observation this example demonstrates how knowledge synthesis can automatically surface discordant biological signals for further evaluation surprisingly cells from respiratory tissues were notably absent among the populations with highest ace2 expression by scrna-seq  figure 2b  this observation was corroborated by complementary gene expression analysis of over 250000 bulk rna-seq samples from gtex 23 24 and the gene expression omnibus geo along with protein expression analysis from healthy tissue proteomics and immunohistochemistry ihc datasets 25 26 27  where lung and other respiratory tissues consistently show lower ace2 expression compared to the digestive tract and kidney figure s3a-d  however the respiratory transmission of covid-19 along with the disease symptomatology and well-documented viral shedding in respiratory secretions 28 strongly indicates that sars-cov-2 indeed infects and replicates within these tissues this would suggest that ace2 is likely expressed in the respiratory epithelium and so we prioritized the respiratory and digestive tracts for further knowledge synthesis-augmented scrna-seq analysis we also applied the single cell resource to analyze several other human and mouse tissues including heart adipose liver pancreas blood spleen bone marrow thymus testis prostate bladder ovary uterus placenta brain and retina an example use case describing the functionalities of the single cell app and a summary of ace2 expression across these tissues are given in the supplemental text and figures s4-19  next we classified 105 respiratory cell populations from eight independent studies based on their expression of and literature-derived associations to ace2  figure 3a  consistent with the low levels of ace2 in respiratory tissues by bulk rna-seq proteomics and ihc  figure s3a-d  we found that ace2 expression is detected in fewer than 10 of all cell types recovered from these studies however as mentioned above we believe that even low ace2 expression levels in these respiratory cells may be sufficient for covid-19 pathogenesis we found that club cells formerly known as clara cells were consistently among the highestexpressing respiratory cell types figures 3a-b  literature-derived local and global scores suggest that this ace2-club cell connection is underappreciated with a few documents discussing these concepts together nferx link we also found that ace2 is detected in type ii pneumocytes in multiple studies although the percentage of expressing cells ranges from only 05-7 figures 3a-b  this relatively low expression which may be deemed inconsequential if viewed in isolation is strongly supported by knowledge synthesis that highlights an existing association between ace2 and type ii pneumocytes figures 3a-b  indeed multiple studies have demonstrated ace2 expression in these cells 29 30 31 32 33 34 35  further ace2 expression in bulk rnaseq of gtex lung samples n  578 is strongly correlated to markers of type ii pneumocytes with all seven surfactant protein-encoding genes among the top 4 of transcriptional correlations to ace2 out of the 19000 genes expressed at  1 tpm in gtex lung samples hypergeometric p-value  11x10 -10   figure s20  our scrnaseq analysis also shows that ace2 is expressed in small fractions of ciliated airway cells and epithelial cells of the nasal cavity figures 3a-b  while no staining is observed for ace2 in nasopharynx samples from the human protein atlas hpa ihc dataset  figure s21  a previous ihc study did report the staining of ace2 in nasal and oral mucosa and the nasopharynx 33  this expression is consistent with the high sars-cov-2 viral loads detected in nasal swab samples 28  intriguingly mild degeneration of olfactory epithelium was observed in an immunosuppressed animal model infected with sars-cov 36  these observations are correlated with emerging reports of anosmiahyposmia loss of smell in otherwise asymptomatic covid-19 patients from south korea and other countries 37  such emerging clinical evidence emphasizes the need for further investigation into olfactory ace2 expression via scrna-seq and other modalities taken together these scrna-seq analyses and triangulation to literature synthesis confirm that type ii pneumocytes are a likely target of sars-cov-2 infection while also highlighting club cells ciliated cells and olfactory epithelial cells as additional potential sites of infection we then classified 136 gastrointestinal cell types from nine scrna-seq studies based on their expression of and literature associations to ace2  figure 4a  these studies encompassed samples from the upper mid and lower gi tracts including tongue esophagus stomach small intestine and colon 19 38 39 40 41 42  this analysis highlights a robust expression of ace2 in tongue keratinocytes that has not been strongly documented in the literature as evidenced by the weak local context score between ace2 and keratinocytes  figure 4b  in fact we found no previous reports of ace2 expression in keratinocytes and only one recent report suggesting ace2 expression in the human tongue based on a combination of bulk rna-seq and a scrna-seq dataset which has not been made publicly accessible 43  we propose that a subset of ace2  tongue keratinocytes may serve as a novel site of sars-cov-2 entry and highlight the need to generate additional gene and protein expression data from human tongue samples to further evaluate this hypothesis emerging reports of loss of taste dysgeusia in otherwise non-symptomatic covid-19 patients may warrant further study of the tongue in this pathology 44  we also found that ace2 is highly expressed in both human and murine small intestinal enterocytes confirming an association which has been moderately appreciated in literature as indicated by our literature derived local score between ace2 and enterocytes however to our knowledge the transcriptional heterogeneity of ace2 among enterocyte populations has never been explored in this context we found that ace2 shows an increase in expression correlated with the maturation of murine small intestinal enterocytes with minimal expression in stem cells and transit amplifying cells in contrast to most robust expression in mature enterocytes  figure  4  to the best of our knowledge this is the first demonstration that ace2 expression synchronously increases over the course of enterocyte maturation the recognition of such intratissue heterogeneity is necessary to specify the cell types which are most likely responsible for the proposed fecal-oral transmission of covid-19 12  to determine whether the maturation-correlated expression pattern is unique to ace2 we computed cosine similarities between the ace2 gene expression vector cp10k values in 6000 small intestinal enterocytes and that of the 15700 other genes detected in this study  figure  5a  for this analysis the vector space is constituted of the individual cells as the dimensions using the gene expression values to construct the vectors see methods interestingly we found that anpep the established entry receptor for hcov-229e showed the third highest cosine similarity to ace2  figure 5b  further dpp4 -the entry receptor for mers coronavirus -is also among the top 1 of similarly expressed genes by this metric figure 5b  we confirmed that both of these genes do indeed show a maturation-correlated transcriptional pattern similar to that of ace2  figure 5c-d  highlighting an unexpected shared pattern of transcriptional heterogeneity among known coronavirus receptors in a cell population which may be relevant for viral transmission we then asked whether this shared pattern of transcriptional heterogeneity among coronavirus receptors is observed in the human small intestine indeed among all enterocytes from a human scrna-seq study both anpep and dpp4 were among the top 1 of genes with similar expression vectors to that of ace2  figure s22a-b  we independently validated this observation by computing gene expression correlations from bulk rna-sequencing of human small intestine samples from gtex n  187 which similarly revealed that dpp4 and anpep are among the top 1 of correlated genes to ace2  figure s22c  in fact among all 18500 genes mean expression  1 tpm in gtex small intestine samples dpp4 shows the second highest correlation to ace2 r  095 to our knowledge this is the first demonstration that all known coronavirus entry receptors display highly coordinated and maturation-correlated transcriptional expression patterns in intestinal epithelial cells we propose that the requisite interaction with human proteins displaying a tightly defined expression gradient on apical surfaces of epithelial cells which is shared among known coronavirus strains may have fundamental implications for understanding the evolution lifecycle andor transmission patterns of this family of viruses recent advances in scrna-seq are empowering us to study tissue and cellular transcriptomes at previously unprecedented resolutions several single-cell rna sequencing based efforts such as the human cell atlas are underway to catalog gene expression across tissues and cell types and the raw data from many of these studies are available on public platforms such as the broad institute single cell portal 45 and gene expression omnibus geo analyses of these datasets are of interest to a wide range of researchers but currently prove challenging for all but a few due to the need for specialized workflows and computing infrastructures consequently the widespread use of this data for biomedical research is hampered an issue which is particularly evident in the face of public health crises like the ongoing covid-19 pandemic to address this unmet need the nferx platform single cell resource enables the rapid and interactive analysis of the continually growing scrnaseq datasets by specialists and non-specialists alike furthermore the seamless triangulation of scrna-seq insights with global and local scores derived from the synthesis of accessible biomedical literature creates a truly first-in-class resource by making the resource available to all academic researchers we enable scientists to not only dive deeper into insights that are aligned with existing knowledge but also to prioritize the novel insights which warrant further experimental validation looking forward we plan to automate the integration of the rapidly growing number of scrna-seq studies so that access to the entire worlds knowledge of single cell transcriptomes is just one click away for any researcher as we do so we encourage interactive feedback from the scientific community so that this platform can evolve to optimally support the research needs across the biomedical ecosystem beyond the covid-19 focus on the current study combined with our analyses of bulk rna-seq ihc and proteomics datasets our characterization of the known human coronavirus receptors ace2 dpp4 anpep using the nferx platform single cell resource represents the most comprehensive molecular fingerprint of host factors determining coronavirus infections including covid-19 while this serves as a primer of the deep profiling that is made possible with this resource we also identified several interesting aspects of coronavirus receptor biology which warrant further experimental follow-up we identified tongue keratinocytes and olfactory epithelia as novel ace2-expressing cell populations and thus as important potential sites of sars-cov-2 infection this molecular fingerprint is a striking correlate to emerging clinical reports of dysgeusia 44 and anosmia 37 in covid-19 patients which strongly implicate the gustatory and olfactory systems in sars-cov-2 pathogenesis and human-to-human transmission tongue epithelial cells have also previously been shown to uptake epstein-barr virus 46  and importantly a recent study found that ace2 is appreciably expressed in tongue based on a small number of non-tumor bulk rna-seq samples from tcga 43  this study further showed by scrna-seq that ace2 expression is observed in a subset of the human tongue but not other oral mucosal epithelial cells albeit in only 05 of the recovered epithelial population this data has unfortunately not been released for public consumption but certainly does provide preliminary support for our finding particularly as the listed set of cluster-defining genes for this population sfn krt6a krt10 is consistent with the tongue keratinocyte identify from the tabula muris data set  figure s23  we thus emphasize the imminent need for further generation of multi-omic expression data from large numbers of healthy and diseased human tongue samples drawn from a cohort of wide demographic representation we also observed that expression of ace2 and other coronavirus receptors is intimately linked to the maturation status of small intestinal enterocytes pinpointing the more mature subsets as the most likely cells to harbor sars-cov-2 virus this finding amplifies the potential for fecal-oral transmission of covid-19 10-12 and should motivate further experimental validation to determine whether monitoring of fecal viral loads should be considered clinically for diagnostic or prognostic purposes we further found that this transcriptional mirroring of coronavirus entry receptors was not unique to small intestine but rather also strongly present among renal proximal tubule epithelial cells where ace2 dpp4 and anpep expression tends to be observed in the same cellular subsets these observations suggest the existence of a transcriptional network spanning tissues and cell types which may drive andor regulate coronavirus receptor expression the question of whether coronaviruses have evolved to exploit such a network may be relevant to pursue particularly given that downregulation of ace2 by sars-cov has been reported previously and is associated with poor clinical outcomes 31 47  perhaps other coronaviruses can similarly modulate the expression of their entry receptors to impact the clinical course of the induced disease the emerging picture of the coronavirus life cycle appears to be intricately interwoven with many proteins beyond the primary host receptors for instance a recent structural complex of the sars-cov-2 spike protein with ace2 identified slc6a19 as an interaction partner of ace2 48  further spike proteins from some coronaviruses can interact with ceacam1 49 and sialylated glycans similar to influenza hemagglutinin 50 as host receptors future studies are likely to highlight several other proteins and glycans that constitute the interactome of the coronavirus proteome understanding the expression profiles of the interactome across tissues will provide systems level insights on the cellular dynamics of the functional partners and the regulatory machinery of the host receptor proteins like in the current study the nferx platform will be an excellent resource for unraveling the purported interaction partners for coronavirus receptors and profiling their expression across different tissues and cells constituting the human body overall this study evidences the utility of an integrative data science platform to enable rapid and high-throughput analysis of publicly available data to generate relevant biological insights and scientific hypotheses we hope that by making our biomedical knowledge synthesis-augmented single cell platform publicly accessible we help empower the research community to advance our understanding of the worlds most pressing biomedical challenges such as covid-19 in order to capture biomedical literature based associations the nferx platform defines two scores a local score and a global score as described previously 51  briefly the local score represents a traditional natural language processing technique which captures the strength of association between two concepts in a selected corpus of biomedical literature based on the frequency of their co-occurrence normalized by the frequency of each individual concept throughout the corpus a higher local score between concept x and concept y indicates that these concepts are frequently mentioned in close proximity to each other more frequently than would be expected by chance the global score on the other hand is based on the neural network renaissance that has recently taken place in the natural language processing nlp field to compute global scores all tokens eg words and phrases are projected in a high-dimensional vector space of word embeddings these vectors serve to represent the neighborhood of concepts which occur around a given concept the cosine distance between any two vectors measures the similarity of these neighborhoods and is the basis for our global score metric where concepts which are more similar in this vector space have a higher global score while the global scores in this work are computed in the embedding space of word2vec model it can also be computed in the embedding space of any deep learning model including recent transformer-based models like bert 52  these may have complementary benefits to word2vec embeddings since the embeddings are context sensitive having different vectors for different sentence contexts however despite the context sensitive nature of bert embeddings a global score computation for a phrase may still be of value given the score is computed across sentence embeddings capturing the context sensitive nature of those phrases from a visualization perspective the local score and global score signals are represented in the platform using bubbles where bubble size corresponds to the local score and color intensity corresponds to the global score this allows users to rapidly determine the strength of association between any two concepts throughout biomedical literature we consider concepts which show both high local and global scores to be concordant and have found that these typically recapitulate well-known associations the nferx platform also supports a logical thought engine that enables and conjunction or disjunction and not negation queries -the universal logic gates this engine is referred to as dynamic adjacency and leverages a highly distributed main memory approach that allows the computation of local scores for any type of logical query in real time fundamentally this system allows a user to extract all 100-word fragments of text which meet the specified logical query we then calculate local scores for all other tokens occurring within these fragments which quantifies the likelihood ie odds of each token occurring this frequently within these textual fragments by chance the platform further leverages statistical inference to calculate enrichments based on structured data thus enabling real-time triangulation of signals from the unstructured biomedical knowledge graph various other structured databases eg curated ontologies rna-sequencing datasets human genetic associations protein-protein interactions this facilitates unbiased hypothesisfree learning and faster pattern recognition and it allows users to more holistically determine the veracity of concept associations finally the platform allows the user to identify and further examine the documents and textual fragments from which the knowledge synthesis signals are derived using the documents and signals applications if we have an automated method that given a corpus consisting of text and other structured and semi-structured data as is often the case with biomedical data comes up with a strength of association score between a query entity and all the tokens or entities present in the corpus then that score can be used to obtain a ranking of tokens or entities related to the query there have been other motivations for ranking the association strength of tokensentities eg for use in picking among possible tokens in speech recognition or optical character recognition -ocr generally association scores are based in some way on the co-occurrences of the tokens or referents to the entities in the text within small windows of text co-occurrences have been studied in linguisticsnlp since at least firths maxim that a word is known by the company it keeps one popular traditional measure for association strength between tokens in text is pointwise mutual information or pmi 53  which we consider in several association scores formally for a given corpus an association score is some real-valued function sq t where q is a query tokenentity and t is another tokenentity we discussed association scores as if they were symmetric above but for some convenience later on our formal notion is asymmetric theres a query q and a token t in particular we later extend q to logical combinations of tokensentities the association score need not be symmetric for logical queries it cannot be as t is still restricted to single tokensentities though it often is when q and t are both single tokensentities context of q -all the measures involve the notion of the context of the query q the context of q are those locations in the corpus deemed to be near to q for single token queries follow the typical approach of defining context as those locations in the corpus that are within some fixed number of words w the window size w is a tunable parameter from an occurrence of q in the corpus the dynamic adjacency engine generalizes this notion of context in a natural way to logical queries the context for a logical q is a certain set of fixed-length fragments co-occurrences -this is just the number of times t appears in the context of q traditional pmi -this is logpt  q  pt here pt  q is the number of times t occurs in the context of q ie co-occurrences of t and q divided by the total length of all q contexts in the corpus whereas pt is the number of occurrences of t in the entire corpus divided by the corpus length word2vec cosine distance -the popular word2vec algorithm 5 generates a vector we use 300dimensional vector representation for each token in a corpus the purpose of these vectors is usually to be used as features in downstream nlp tasks but they can also be used for similarity the original paper validates the vectors by testing them on word similarity tasks the association score is the cosine between the vector for q and the vector for t this score only applies to singletoken q exponential mask pmi exppmi -this is our first new proposed score pmi treats every position in a binary way its either in the context of q or not with a window size of say 50 a token which appears 3 words from a query q and a token which appears 45 words from a query q are treated the same we thought it might be useful to consider a measure which distinguishes positions in the context based on the number of words away that position is from an occurrence of q we did this by weighting the positions in the context by some weight between 0 and 1 our weighting is based on an exponential decay which has some nice properties especially when we extend to the case of logical queries local score -this is another new proposed score we find that pmi and exppmi can vary a lot for small samples ie small numbers of co-occurrences occurrences the local score is logcoocc  sigmoidpmi -05 constructed to correct for this we found that this formula too works well empirically exponential mask local score explocalscore -we apply both modifications together the exponential mask score is logweightedcoocc  sigmoidexppmi -05 here weightedcoocc is the sum of the weights of the positions of the corpus evaluation of association scores are further described in the supplementary information the objective of the single cell platform is to enable dynamic visualization and analysis of single cell rna-sequencing data currently there are over 30 scrna-seq studies available for analysis in the single cell app including studies from human donorspatients covering tissues such as adipose tissue blood bone marrow colon esophagus liver lung kidney ovary nasal epithelium pancreas placenta prostate retina small intestine and spleen because no pantissue reference dataset yet exists for humans we have manually selected individual studies to maximally cover the set of human tissues in some cases these studies contain cells from both healthy donors and patients affected by a specified pathology such as ulcerative colitis colon or asthma lung there are also a number of murine scrna-seq studies covering tissues including adipose tissue airway epithelium blood bone marrow brain breast colon heart kidney liver lung ovary pancreas placenta prostate skeletal muscle skin spleen stomach small intestine testis thymus tongue trachea urinary bladder uterus and vasculature note that two of these murine studies tabula muris and mouse cell atlas include 20 tissues each for each study a counts matrix was downloaded from a public data repository such as the gene expression omnibus geo or the broad institute single cell portal  table s1  note that this data has not been re-processed from the raw sequencing output and so it is likely that alignment and quantification of gene expression was performed using different tools for different studies in some cases multiple complementary datasets have been generated from a single publication in these cases we have generated separate entries in the single cell platform while counts matrices have been generated using different technologies eg drop-seq 10x genomics etc and different alignmentpre-processing pipelines all counts matrices were scaled such that each cell contains a total of 10000 scaled counts ie the sum of expression values for all genes equals 10000 in each individual cell all data were uniformly processed using the seurat v3 package 54  in short this pipeline involves the following steps first we identify 2000 variable genes across the given dataset and then perform linear dimensionality reduction by principal component analysis pca using the set of principal components which contribute 80 of variance across the dataset we then do the following i perform graph-based clustering to identify groups of cells with similar expression profiles louvain clustering ii compute umap and tsne coordinates for each individual cell used for data visualization and iii annotate cell clusters note that the three human pancreatic datasets gse81076 gse85241 gse86469 were integrated together in a shared multi-dimensional space using cca canonical correlation analysis and the integration method in the seurat v3 package 54  cell clustering and computation of dimensionality reduction coordinates were performed on this integrated dataset in cases where publicly deposited counts matrices are accompanied by author-assigned annotations for individual cells or clusters we have retained these cell annotations for display in the platform and accompanying analyses for any study which was not accompanied by a metadata file containing cluster annotations we have manually labeled clusters based on sets of canonical cluster-defining genes in these cases we have attempted to leverage annotations and descriptions of gene expression patterns described by study authors in the manuscript text and figures corresponding to the data being analyzed the platform allows users to query any gene in any selected study the corresponding data is displayed in commonly employed formats including a series of violin plots and as a set of dimensionality reduction plots expression is summarized by listing the percent of cells expressing gene g in each annotated cluster and the mean expression of gene g in each cluster to measure the specificity of gene g expression to each cluster c we compute a cohens d value which assesses the effect size between the mean expression of gene g in cluster c and the mean expression of gene g in all other clusters specifically the cohens d formula is given as follows meanc -meanasqrtstdevc 2  stdeva 2  where c represents the cluster of interest and a represents the complement of c ie all other cell clusters note that this is functionally similar to the computation of paired fold change values and p-values between clusters which is frequently used to identify cluster-defining genes within the platform we support the run-time computation of cosine similarity ie 1 -cosine distance between the queried gene and all other genes this provides a measure of expression similarity across cells and can be used to identify co-regulated and co-expressed genes specifically to perform this computation we construct a gene expression vector for each gene g this corresponds to the set of cp10k values for gene g in each individual cell from the selected populations in the selected study for each single-cell dataset we examined the expression of ace2 tmprss2 anpep and dpp4 we generally considered a cell population to potentially express a gene if at least 5 of cells from that cluster showed non-zero expression of this gene for each dataset we show a figure which includes a umap dimensionality reduction plot colored by annotated cell type along with identical plots colored by the expression level of each coronavirus receptor in all individual cells in some cases we also show violin plots from the platform which automatically integrate literature-derived insights to highlight whether there exist textual associations between the queried gene and the tissuecell types identified in the selected study figure 5 expression of all known coronavirus receptors synchronously increase with enterocyte maturation  coronet a deep neural network for detection and diagnosis of covid-19 from chest x-ray images asif khan iqbal junaid shah latief mohammad bhat mudasir   the 2019 novel coronavirus or covid-19 first reported in wuhan china in december 2019 belongs to the family of viruses coronavirus cov was called severe acute respiratory syndrome coronavirus 2 sars-cov-2 before it was named covid-19 by world health organization who in february 2020 the outbreak was declared a public health emergency of international concern on 30 january 2020 1 and finally on march 11 2020 who declared covid-19 as pandemic after the outbreak the number of daily cases began to increase exponentially and reached 18 million cases and around 114698 deaths globally by 12 april 2020 the virus has engulfed more than 210 countries among which usa spain and italy are severely hit with 560433 166831 and 156363 active cases and 22115 17209 and 19899 deaths respectively 2 once infected a covid-19 patient may develop various symptoms and signs of infection which include fever cough and respiratory illness like flu in severe cases the infection may cause pneumonia difficulty breathing multi-organ failure and death 23 due to the rapid and increasing growth rate of the covid-19 cases the health system of many advanced countries has come to the point of collapse they are now facing shortage of ventilators and testing kits many countries have declared total lockdown and asked its population to stay indoors and strictly avoid gatherings a critical and important step in fighting covid-19 is effective screening of infected patients such that positive patients can be isolated and treated currently the main screening method used for detecting covid-19 is real-time reverse transcription polymerase chain reaction rrt-pcr 45 the test is done on respiratory samples of the patient and the results can be available within few hours to 2 days an alternate method to pcr screening method can be based on chest radiography images various research articles published in radiology journal 67 indicate that that chest scans might be useful in detecting covid-19 researchers found that the lungs of patients with covid-19 symptoms have some visual marks like ground-glass opacitieshazy darkened spots that can differentiate covid-19 infected patients from non covid-19 infected ones 89 the researchers believe that chest radiology based system can be an effective tool in detection quantification and follow-up of covid-19 cases a chest radiology image based detection system can have many advantages over conventional method it can be fast analyze multiple cases simultaneously have greater availability and more importantly such system can be very useful in hospitals with no or limited number of testing kits and resources moreover given the importance of radiography in modern health care system radiology imaging systems are available in every hospital thus making radiography based approach more convenient and easily available today researchers from all around the world from various different fields are working day and night to fight this pandemic many researchers have published series of preprint papers demonstrating approaches for covid-19 detection from chest radiography images 1011 these approaches have achieved promising results on a small dataset but by no means are production ready solutions these approaches still need rigorous testing and improvement before putting them in use subsequently a large number of researchers and data scientists are working together to build highly accurate and reliable deep learning based approaches for detection and management of covid-19 disease researchers are focusing on deep learning techniques to detect any specific features from chest radiography images of covid-19 patients in recent past deep learning has been very successful in various visual tasks which include medical image analysis as well deep learning has revolutionized automatic disease diagnosis and management by accurately analyzing identifying classifying patterns in medical images the reason behind such success is that deep learning techniques do not rely on manual handcrafted features but these algorithms learn features automatically from data itself 12 in the past deep learning has had success in disease classification using chest radiography image chexnet 13 is a deep neural network model that detects pneumonia from chest x-ray image chexnet achieved exceptional results exceeding average radiologist performance another similar approach called chestnet 14 is a deep neural network model designed to diagnose thorax diseases on chest radiography images the success of ai based techniques in automatic diagnosis in the medical field and rapid rise in covid-10 cases have necessitated the need of ai based automatic detection and diagnosis system recently many researchers have used radiology images for covd-19 detection a deep learning model for covid-19 detection covid-net proposed by wang and wong 10 obtained 835 accuracy in classifying covid-19 normal pneumonia-bacterial and pneumonia-viral classes hemdan et al 15 used various deep learning models to diagnose covid-19 from cheat x-ray images and proposed a covidx-net model comprising seven cnn models apostolopoulos and mpesiana 16 trained different pre-trained deep learning models on a dataset comprising of 224 confirmed covid-19 images and achieved 9875 and 9348 accuracy for two and three classes respectively narin et al 11 trained resnet50 model using chest x-ray images and achieved a 98 covid-19 detection accuracy for two classes however the performance for multi class classification is not known sethy and behera 17 used various convolutional neural network cnn models along support vector machine svm classifier for covid-19 classification their study states that the resnet50 model with svm classifier provided the best performance most recently ozturk et al 18 proposed a deep network based on darknet model their model consists of 17 convolution layers with leaky relu as activation function their model achieved an accuracy of 9808 for binary classes and 8702 for multi-class cases all these techniques except covid-net 10 either perform binary classification normal vs covid-19 or 3class classification normal vs pneumonia vs covid-19 other than covid-net none of the methods discussed above treat pneumonia bacterial and pneumonia viral as separate classes in this study we present a deep learning based approach to detect covid-19 infection from chest x-ray images we propose a deep convolutional neural network cnn model to classify three different types of pneumonia bacterial pneumonia viral pneumonia and covid-19 pneumonia we also implemented binary and 3-class versions of our proposed model and compared the results with other studies in the literature the proposed model is called coronet and will help us identifying the difference between three types of pneumonia infections and how covid-19 is different from other infections a model that can identify covid-19 infection from chest radiography images can be very helpful to doctors in the triage quantification and follow-up of positive cases even if this model does not completely replace the existing testing method it can still be used to bring down the number of cases that need immediate testing or further review from experts deep learning is all about data which serves as fuel in these learning models since covid-19 is a new disease there is no appropriate sized dataset available that can be used for this study therefore we had to create a dataset by collecting chest x-ray images from two different publically available image databases covid-19 x-ray images are available at an open source github repository by joseph et al 19 the authors have compiled the radiology images from various authentic sources radiological society of north america rsna radiopaedia etc of covid-19 cases for research purpose and most of the studies on covid-19 use images from this source the repository contains an open database of covid-19 cases with chest x-ray or ct images and is being updated regularly at the time of writing this paper the database contained around 290 covid-19 chest radiography images pneumonia bacterial pneumonia viral and normal chest x-ray images were obtained from kaggle repository chest x-ray images pneumonia 20 the dataset consists of 1203 normal 660 bacterial pneumonia and 931 viral pneumonia cases we collected a total of 1300 images from these two sources we then resized all the images to the dimension of 224  224 pixels with a resolution of 72 dpi table i
below shows the summary of the prepared dataset figure 1
below shows some samples of chest x-ray images from the prepared dataset in order to overcome the unbalanced data problem we used resampling technique called random under-sampling which involves randomly deleting examples from the majority class until the dataset becomes balanced we used only 310 normal 330 pneumonia-bacterial and 327 pneumonia-viral x-ray images randomly from this chest x-ray pneumonia database convolutional neural network also known as cnn is a deep learning technique that consists of multiple layers stacked together which uses local connections known as local receptive field and weight-sharing for better performance and efficiency the deep architecture helps these networks learn many different and complex features which a simple neural network cannot learn convolutional neural networks are powering core of computer vision that has many applications which include self-driving cars robotics and treatments for the visually impaired the main concept of cnn is to obtain local features from input usually an image at higher layers and combine them into more complex features at the lower layers 21
22 a typical convolutional neural network architecture consists of the following layersa
convolutional layer

convolution layer is the core building block of a convolutional neural network which uses convolution operation represented by  in place of general matrix multiplication its parameters consist of a set of learnable filters also known as kernels the main task of the convolutional layer is to detect features found within local regions of the input image that are common throughout the dataset and mapping their appearance to a feature map the convolution operation is given as1fijikijmniimjnkmn
where i is the input matrix image k is the 2d filter of size m x n and f represents the output 2d feature map here the input i is convolved with the filter k and produces the feature map f this convolution operation is denoted by ikthe output of each convolutional layer is fed to an activation function to introduce non-linearity there are number of activation functions available but the one which is recognized for deep learning is rectified linear unit relu relu simply computes the activation by thresholding the input at zero in other words relu outputs 0 if the input is less than 0 and raw output otherwise it is mathematically given as2fxmax0x

a
subsampling pooling layer

in cnn the sequence of convolution layer is followed by an optional pooling or down sampling layer to reduce the spatial size of the input and thus reducing the number of parameters in the network a pooling layer takes each feature map output from the convolutional layer and down samples it ie pooling layer summarizes a region of neurons in the convolution layer there most common pooling technique is max pooling which simply outputs the maximum value in the input region other pooling options are average pooling and l2-norm pooling
a
fully connected layer

in fully connected layer each neuron from previous layer is connected to every neuron in the next layer and every value contributes in predicting how strongly a value matches a particular class the output of last fully connected layer is then forwarded to an activation function which outputs the class scores softmax and support vector machines svm are the two main classifiers used in cnn softmax function which computes the probability distribution of the n output classes is given as3zkexki1nexn
where x is the input vector and z is the output vector the sum of all outputs z equals to 1 the proposed model coronet uses softmax to predict the class to which the input x-ray image belongs to
 all the layers discussed above are stacked up to make a full cnn architecture in addition to these main layers mentioned above cnn may include optional layers like batch normalization layer to improve the training time and dropout layer to address the overfitting issue coronet is a cnn architecture tailored for detection of covid-19 infection from chest x-ray images it is based on xception cnn architecture 23 xception which stands for extreme version of inception 24 its predecessor model is a 71 layers deep cnn architecture pre-trained on imagenet dataset xception uses depthwise separable convolution layers with residual connections instead of classical convolutions depthwise separable convolution replaces classic n x n x k convolution operation with 1  1 x k point-wise convolution operation followed by channel-wise n x n spatial convolution operation this way the number of operations are reduced by a factor proportional to 1k residual connections are skip connections which allow gradients to flow through a network directly without passing through non-linear activation functions and thus avoiding the problem of vanishing gradients in residual connections output of a weight layer series is added to the original input and then passed through non-linear activation function as shown in figure 3
 coronet uses xception as base model with a dropout layer and two fully-connected layers added at the end coronet has 33969964 parameters in total out of which 33969964 trainable and 54528 are non-trainable parameters architecture details layer-wise parameters and output shape of coronet model are shown in table ii
 to initialize the model parameters we used transfer learning to overcome the problem of overfitting as the training data was not sufficient we implemented three scenarios of the proposed model to detect covid-19 from chest x-ray images first model is the main multi-class model 4-class coronet which is trained to classify chest x-ray images into four categories covid-19 normal pneumonia-bacterial and pneumonia-viral the other two models 3-class coronet covid-19 normal and pneumonia and binary 2-class coronet model covid-19 normal and pneumonia are modifications of the main multi-class model the proposed model coronet was implemented in keras on top of tensorflow 20 the model was pre-trained on imagenet dataset and then retrained end-to-end on prepared dataset using adam optimizer with learning rate of 00001 batch size of 10 and epoch value of 80 for training data shuffling was enabled which involves shuffling the data before each epoch all the experiment and training was done on google colaboratory ubuntu server equipped with tesla k80 graphics card we used 4-fold cross-validation 25 approach to assess the performance of our main 4-class model the training set was randomly divided into 4 equal sets three out of four sets were used to train the cnn model while the remaining set was used for validation this strategy was repeated 4 times by shifting the validation and training sets final performance of the model was reported by averaging values obtained from each fold plots of accuracy and loss on the training and validation sets over training epochs for fold 4 are shown in figure 4
 to check the robustness we tested our proposed model on another dataset prepared by ozturk et al 18 the dataset-2 contains around 500 normal 500 pneumonia and 157 covid-19 chest x-ray images this dataset contains same covid-19 x-ray images as in our prepared dataset however normal and pneumonia x-ray images were collected from chestx-ray database provided by wang et al 26 after slight modification and fine-tuning our proposed model achieved an overall accuracy of 90 the results are illustrated in table vi
and corresponding confusion matrix is given in figure 7
 table vi shows accuracy precision recall and f-measure of coronet on dataset-2 in this study we proposed a deep model based on xception architecture to detect covid-19 cases from chest x-ray images the proposed model was tested on two datasets and performed exceptionally well on both of them our model achieved an accuracy of 895 9459 and 99 for 4-classes 3-classes and binary class classification tasks respectively furthermore our model also achieved an accuracy of 90 on dataset-2 another positive observation from the results is the precision ppv and recall sensitivity for covid-19 cases higher recall value means low false negative fn cases and low number of fn is an encouraging result this is important because minimizing the missed covid-19 cases as much as possible is the main aim of this research the results obtained by our proposed model are superior compared to other studies in the literature table vii
presents a summary of studies conducted in the automated diagnosis of covid-19 from chest x-ray images and their comparison with our proposed model coronet figure 8
shows coronet results on some sample images from the test set wang and wong 10 presented a residual deep architecture called covid-net for detection of covid-19 from chest x-ray images covid-net is one of the early works done on covid-19 which uses deep neural network to classify chest x-ray images into four categories covid normal pneumonia bacterial and pneumonia viral covid-net achieved an accuracy of 835 for four classes table viii
presents performance comparison of covid-net and our proposed model coronet on 4-class classification task apostolopoulos and mpesiana 16 evaluated various state-of-the-art deep architectures on chest x-ray images with transfer learning their best model vgg19 managed to achieve an accuracy of 9348 and 9875 for 3-class and 2-class classification tasks respectively on a dataset consisting of 224 covid-19 700 pneumonia and 504 normal x-ray images narin et al 11 performed same experiment with three different cnn models resnet50 inceptionv3 and inceptionresnetv2 and resnet50 pre-trained on imagenet database achieved best accuracy of 98 for 2-class classification since they did not include pneumonia cases in their experiment it is unknown how well their model would distinguish between covid-19 and other pneumonia cases cnns are deep models which perform automatic feature extraction from input data and based on these extracted features a classifier like softmax performs classification softmax classifier is a default but noncompulsory choice for cnns and can be replaced by any good classifier like support vector machine svm one such experiment was done by sethy and behera 17 they employed resnet50 cnn model along with svm for detection of covid-19 cases from chest x-ray images cnn model acts as feature extractor and svm serves the purpose of classifier their model achieved an accuracy of 9538 on 2-class problem ozturk et al 18 proposed a cnn model based on darknet architecture to detect and classify covid-19 cases from x-ray images their model achieved binary and 3-class classification accuracy of 9808 and 8702 respectively on a dataset consisting of 125 covid-19 500 pneumonia and 500 normal chest x-ray images the promising and encouraging results of deep learning models in detection of covid-19 from radiography images indicate that deep learning has a greater role to play in fighting this pandemic in near future some limitation of this study can be overcome with more in depth analysis which is possible once more patient data both symptomatic and asymptomatic patients becomes available as the cases of covid-19 pandemic are increasing daily many countries are facing shortage of resources during this health emergency it is important that not even a single positive case goes unidentified with this thing in mind we proposed a deep learning approach to detect covid-19 cases from chest radiography images the proposed method coronet is a convolutional neural network designed to identify covid-19 cases using chest x-ray images the model has been trained and tested on a small dataset of few hundred images prepared by obtaining chest x-ray images of various pneumonia cases and covid-19 cases from different publically available databases coronet is computationally less expensive and achieved promising results on the prepared dataset the performance can further be improved once more training data becomes available notwithstanding the encouraging results coronet still needs clinical study and testing but with higher accuracy and sensitivity for covid-19 cases coronet can still be beneficial for radiologists and health experts to gain deeper understandings into critical aspects associated with covid-19 cases for further research in this area we have made the source code trained model and dataset available at httpsgithubcomdrkhan107coronet
 the authors have no conflict of interest to disclose  targeting sars-cov-2 with ai-and hpc-enabled lead generation a first data release other members of the ahlg-sars-cov-2 collaboration yadu babuji ben blaiszik tom brettin kyle chard ryan chard austin clyde ian foster zhi hong shantenu jha zhuozhao li xuefeng liu arvind ramanathan yi ren nicholaus saint marcus schwarting rick stevens hubertus van dam rick wagner  researchers across the globe are seeking to rapidly repurpose existing drugs or discover new drugs to counter the the novel coronavirus disease covid-19 caused by severe acute respiratory syndrome coronavirus 2 sars-cov-2 one promising approach is to train machine learning ml and artificial intelligence ai tools to screen large numbers of small molecules as a contribution to that effort we are aggregating numerous small molecules from a variety of sources using high-performance computing hpc to computer diverse properties of those molecules using the computed properties to train mlai models and then using the resulting models for screening in this first data release we make available 23 datasets collected from community sources representing over 42 b molecules enriched with pre-computed 1 molecular fingerprints to aid similarity searches 2 2d images of molecules to enable exploration and application of image-based deep learning methods and 3 2d and 3d molecular descriptors to speed development of machine learning models this data release encompasses structural information on the 42 b molecules and 60 tb of pre-computed data future releases will expand the data to include more detailed molecular simulations computed models and other products  the coronavirus disease covid-19 pandemic caused by transmissible infection of the severe acute respiratory syndrome coronavirus 2 sars-cov-2 virus 1 2 3 4  has resulted in millions of diagnosed cases and over 353 000 deaths worldwide 5  straining healthcare systems and disrupting key aspects of society and the wider economy in order to save lives and reduce societal effects it is important to rapidly find effective treatments through drug discovery and repurposing efforts here we describe a public data release of 23 molecular datasets collected from community sources or created internally representing over 42 b molecules in addition to collecting the datasets from heterogeneous locations and making them available through a unified interface we have enriched the datasets with additional context that would be difficult for many researchers to compute without access to significant hpc resources for example these data now include the 2d and 3d molecular descriptors computed molecular fingerprints 2d images representing the molecule and canonical simplified molecular-input line-entry system smiles 6 structural representations to speed development of machine learning models this data release encompasses information on the 42 b molecules and 60 tb of additional data we intend to supplement this dataset in future releases with more datasets further enrichments tools to extract potential drugs from natural language text and machine learning models to sift the best candidates for protein docking simulations from the billions of available molecules in the following we first describe the datasets collected the methodology used to generate the enriched datasets and then discuss future directions we have collected molecules from the datasets listed in table 1  each of which has either been made available online by others or generated by our group the collected datasets include some specifically collected for drug design eg enamine known drug databases eg drugbank 7 8  drugcentral 9 10  cureffi 11  antiviral collections eg cas covid-19 antiviral candidate compounds 12  and the lit covid-19 dataset 13  others that provide known decoys dude database of useful decoys and further counterexamples including molecules used in other domains eg qm9 14 15  harvard organic photovoltaic dataset 16 17  by aggregating these diverse datasets including the decoys and counterexamples we aim to allow researchers the maximal freedom to create training sets for specific use cases future releases will include additional data relevant to sars-cov-2 research 32 33 97 545 266 qm9 qm9 subset of gdb-17 14 15 133 885 rep repurposing-related drugtool compounds 34 35 10 141 sav synthetically accessible virtual inventory savi 36 37 265 047 097 sur surechembl dataset of molecules from patents 38 39 17  the data processing pipeline is used to compute different types of features and representations of billions of small molecules the pipeline is first used to convert the smiles representation for each molecule to a canonical smiles to allow for de-duplication and consistency across data sources next for each molecule three different types of features are computed 1 molecular fingerprints that encode the structure of molecules 2 2d and 3d molecular descriptors and 3 2d images of the molecular structure these features are being used as input to various machine learning and deep learning models that will be used to predict important characteristics of candidate molecules including docking scores toxicity and more figure 1  the computational pipeline that is used to enrich the data collected from included datasets after collection each molecule in each dataset has canonical smiles 2d and 3d molecular features fingerprints and images computed these enrichments simplify molecule disambiguation ml-guided compound screening similarity searching and neural network training respectively term description source-key identifies the source dataset see the three-letter keys in table 1  identifier a per-molecule identifier either obtained from the source dataset or if none such is available defined internally smiles a canonical smiles for a molecule as produced by open babel we use open babel v30 42 to convert the simplified molecular-input line-entry system smiles specifications of chemical species obtained from various sources into a consistent canonical smiles representation we organize the resulting molecule specifications in one directory per source dataset each containing one csv file with columns source-key identifier smiles where source-key identifies the source dataset identifier is an identifier either obtained from the source dataset or if none such is available defined internally and smiles is a canonical smiles as produced by open babel identifiers are unique within a dataset but may not be unique across datasets thus the combination of source-key identifier is needed to identify molecules uniquely we obtain the canonical smiles by using the following open babel command we use rdkit 43 table 2  and fingerprint is a base64-encoded representation of the fingerprint in figure 2  we show an example of how to load the fingerprint data from a batch file within individual dataset using python 3 further examples of how to use fingerprints are available in the accompanying github repository 44   we generate molecular descriptors using mordred 45 version 120 the collected descriptors 1800 for each molecule include descriptors for both 2d and 3d molecular features we organize these descriptors in one directory per source dataset each containing one or more csv files each row in the csv file has columns source-key identifier smiles descriptor 1  descriptor n  in figure 3  we show how to load the data for an individual dataset eg ffi using python 3 and explore its shape figure 3-left  and create a tsne embedding 46 to explore the molecular descriptor space figure 3 -right  images for each molecule were generated using a custom script 44 to read the canonical smiles structure with rdkit kekulize the structure handle conformers draw the molecule with rdkitchemdraw and save the file as a png-format image with size 128128 pixels for each dataset individual pickle files are saved containing batches of 10 000 images for ease of use with entries in the format source identifier smiles image in pil format in figure 4  we show an example of loading and display image data from a batch of files from the ffi dataset  providing access to such a large quantity of heterogeneous data currently 60 tb is challenging we use globus 47 to handle authentication and authorization and to enable high-speed reliable access to the data stored on the petrel file server at the argonne leadership computing facilitys alcf joint laboratory for system evaluation jlse access to this data is available to anyone following authentication via institutional credentials an orcid profile a google account or many other common identities users can access the data through a web user interface shown in fig 5  facilitating easy browsing direct download via https of smaller files and high-speed reliable transfer of larger data files to their laptop or a computing cluster via globus connect personal or an instance of globus connect server there are more than 20 000 active globus endpoints distributed around the world users may also access the data with a full-featured python sdk more details on globus can be found at httpswwwglobusorg figure 5  data access with globus all data are stored on globus endpoints allowing users to access move and share the data through a web interface pictured above a rest api or with a python client the user here has just transferred the first three files of descriptors associated with the e15 dataset to an endpoint at uchicago we have released to the community an open resource of molecular structures as canonical smiles descriptors 2d images and fingerprints we hope these data will contribute to the discovery of small molecules to combat the sars-cov-2 virus we expect forthcoming data releases to extend to molecular conformers incorporate the results of natural language processing extractions of drugs from covid-related literature provide the results of molecular docking simulations against sars-cov-2 viral and host proteins and include the trained machine learning models that the team is building to identify top candidates for running various more expensive calculations all data and code links can be found at http2019-ncovgroupgithubiodata subsequent updates will be made available through the same web page and further release papers will be issued as necessary the code for the examples used in this paper can be found at httpsgithubcomglobus-labscovid-analyses the data generated have been prepared as part of the ncov-group collaboration a group of over 200 researchers working to use computational techniques to address various challenges associated with covid-19 we would like to thank all the researchers who helped to assemble the original datasets and who provided permission for redistribution this research was supported by the doe office of science through the national virtual biotechnology laboratory a consortium of doe national laboratories focused on response to covid-19 with funding provided by the coronavirus cares act this research used resources of the argonne leadership computing facility a doe office of science user facility supported under contract de-ac02-06ch11357 additional data storage and computational support for this research project has been generously supported by the following resources petrel data service at the argonne leadership computing facility alcf frontera at the texas advanced computing center tacc comet at the san diego supercomputing center sdsc the work leveraged data and computing infrastructure produced in other projects including exalearn and the exascale computing project 48 doe contract de-ac02-06ch11357 parsl parallel scripting library 49 nsf 1550588 funcx distributed function as a service platform 50 nsf 2004894 globus data services for science authentication transfer users and groups see globusorg for funding chimad materials data facility 51 52 and polymer property predictor database 53 nist 70nanb19h005 and nist 70nanb14h012 for all information unless otherwise indicated this information has been authored by an employee or employees of the uchicago argonne llc operator of the argonne national laboratory with the us department of energy the us government has rights to use reproduce and distribute this information the public may copy and use this information without charge provided that this notice and any statement of authorship are reproduced on all copies while every effort has been made to produce valid data by using this data user acknowledges that neither the government nor uchicago argonne llc makes any warranty express or implied of either the accuracy or completeness of this information or assumes any liability or responsibility for the use of this information additionally this information is provided solely for research purposes and is not provided for purposes of offering medical advice accordingly the us government and uchicago argonne llc are not to be liable to any user for any loss or damage whether in contract tort including negligence breach of statutory duty or otherwise even if foreseeable arising under or in connection with use of or reliance on the content displayed on this site  covid-19 social media sentiment analysis on reopening mohammed ahmed emtiaz rafiqul rabin islam farah chowdhury naz  the novel coronavirus pandemic is the most talked topic in social media platforms in 2020 people are using social media such as twitter to express their opinion and share information on a number of issues related to the covid-19 in this stay at home order in this paper we investigate the sentiment and emotion of peoples in the usa on the subject of reopening we choose the social media platform twitter for our analysis and study the tweets to discover the sentimental perspective emotional perspective and triggering words towards the reopening during this covid-19 pandemic researchers have made some analysis on various social media dataset regarding lockdown and stay at home however in our analysis we are particularly interested to analyse public sentiment on reopening our major finding is that when all states resorted to lockdown in march people showed dominant emotion of fear but as reopening starts people have less fear while this may be true due to this reopening phase daily positive cases are rising compared to the lockdown situation overall people have a less negative sentiment towards the situation of reopening  covid-19 a new strain of coronavirus originated in wuhan china in december 2019 huang et al 2020  on 11 th march 2020 the world health organization announced the covid-19 outbreak as a pandemic this highly infectious covid-19 has crossed boundaries in a speed anybody could have imagined turning the world upside down covid-19 has already claimed more than 03 million lives with around 6 million people infected in more than 200 countries or territories 1 up to may 31 2020 it has shaken the global economic and social structure so badly that resuming normal life is a thing to envision as a measure to control the situation a number of countries have resorted to complete lockdown mitj et al 2020  when the whole world is covid-19 stricken the lockdown has been the only solution to get a hold ghosal et al 2020  since then much of the debate has been going on how the countries should ease the lockdown program as lockdown is not the ultimate solution easing the restrictions is the only way to minimize economic loss and keep the society functional as the whole world is still not free from the deadly clutch of corona and continuing to claim human lives reopen decisions from the government is a huge step to take public sentiment towards this reopen decision can help policymakers and general people to better understand the situation through classified information this covid-19 outbreak has not only caused economic breakdown fernandes 2020 across the world but also left a great impact on human minds to capture human emotions social media websites work as one of the best possible sources de choudhury and counts 2012  it is the fastest way for people to express themselves and thus the news feed is flooded with data reflecting thoughts dominating the peoples minds at this time apart from sharing their thoughts people use these media to disseminate the information during this lockdown people have taken social networks to express their feelings and thus find a way to calm themselves down emotions and sentiments are the driving force that people are sharing in social media if we analyze the social media posts then we can find the insights of those posts along with emotions and sentiments from this kind of analysis we may find what people like what they want and what are the main concerns of them twitter has a large number of daily active users all around the world where people share their thoughts and information regarding any topic of recent concern through this medium it is a searchable archive of human thought the affluence of publicly available data shared through twitter has encouraged researchers to determine the sentiments on almost everything including sentiments towards any product service researchers and practitioners are already giving useful information on issues related to this covid-19 outbreak we have mainly contributed to the issue of the reopening phase giving an insight into peoples reactions coordination of responses from online and from the real-world is sure to reveal promising results to address the current crisis our major finding is that during this reopen phase fear among people is less dominant compared to the previous times when the states made lockdown decisions another finding is that as the reopen phase starts the number of new cases is also increasing people are sharing their thoughts and opinions to combat this situation contributions this paper makes the following contributions  first we generate the word cloud and n-gram representation of tweets to see the insights of twitter users during the reopening phase more details in section 51  second we conduct a shallow analysis to study the sentiment and the emotion in reopening related tweets more details in section 52 and section 53  third we analyze the actual effects of lockdown and reopening using real data more details in section 54  finally we have made our code publicly available for an extensible work in similar areas the structure of this paper is organized as follows section 2 presents a literature review and some considerations of the previous work section 3 contains the information of dataset section 4 describes the experimental setup to analysis the reopening tweet section 5 provides an analysis of the results and findings section 6 discusses some of the challenges and threats to validity finally section 7 concludes the paper with future works researchers all around the world are analyzing twitter data to discover peoples reactions to the corona virus related issues such as lockdown safety measures barkur et al 2020 analyzed twitter data and found that people of india were positive towards the lockdown decision of their government to flatten the curve dubey 2020 made a sentiment analysis on the tweets related to covid19 in some or the other way from march 11 to march 31 on twelve different countries usa italy spain germany china france uk switzerland belgium netherland and australia they found that while most of the countries are taking a positive attitude towards this situation there is also negative emotion such as fear sadness present among people countries specially france switzerland netherlands and the united states of america have shown distrust and anger more frequently compared to other countries rajput et al 2020 made a statistical analysis of twitter data posted during this coronavirus outbreak the number of tweet ids tweeting about coronavirus rose rapidly making several peaks during february and march an empirical analysis of words in the messages showed that the most frequent words were coronavirus covid19 and wuhan the immense number of tweet messages within a period of 2-3 months and the frequency of these words clearly show the threats the global population is exposed to abd-alrazaq et al 2020 aimed to identify the main topics posted by twitter users related to the covid-19 pandemic between february 2 2020 and march 15 2020 users on twitter discussed 12 main topics related to covid-19 that were grouped into four main themes 1 the origin of the virus 2 its sources 3 its impact on people countries and the economy that were represented by six topics deaths fear and stress travel bans and warnings economic losses panic increased racism and 4 the last theme was ways of mitigating the risk of infection cinelli et al 2020  ran a comparative analysis of five different social media platforms twitter instagram youtube reddit and gab about information diffusion about covid-19 they wanted to investigate how information is spread during any critical moment their analysis suggests that information spread depends on the interaction pattern of users engaged with the topic rather than how reliable the information is samuel and ali identified public sentiment using coronavirus specific tweets and provided insights on how fear sentiment evolved over time specially when covid-19 hit the peak levels in the united states as they classified tweets using sentiment analysis the fear sentiment which was the most dominant emotion across the entire tweets and by the end of march its seriousness became clearly evident as the fear curve showed a steep rise we used data from multiple sources the primary source of this analysis is the social platform twitter we used the python library tweepy 2 roesslein 2009  and twitter developers api 3 for collecting the tweet data from twitter we used a unique twitter dataset which is specifically collected for this study using a date range from may 3 2020 to may 15 2020 tweets were extracted using the hashtags covid19 covid corona coronaviras corona-virus covid19-virus and sarscov2 we collected a total of 5703590 tweets from twitter in addition we collected the covid-19 time-series dataset from the github repository cssegisanddatacovid-19 dong et al 2020  maintained by the amazing team at johns hopkins university center for systems science and engineering csse it contains country and statewise daily new cases recovered and death data of covid-19 in this section we will describe the data preprocessing and the experimental setup for sentiment and emotion analysis the data we used from the twitter post between may 3-15 2020 is noisy and unstructured text in nature therefore we applied the following preprocessing steps to clean the raw tweet  url and email removing urls and emails from tweets using regular expression  mention and tagging removing mentions  symbol and word after  symbol and hashtags only  symbol from tweets using regular expression  noisy words removing irrelevant words ie rt amp after manual inspection and stripping non-ascii characters from the tweet  newline and whitespace removing newlines and extra whitespaces from tweets  us-reopen in section 51 -53 we first filtered tweets by locations states of us and then selected tweets having word reopen  stopwords and gazetteers in section 51 for most frequent words we removed words from tweets that are found in the python nltk stopwords and gazetteers corpus 4  we also split the tweets into a list of words using the python nltk tweettokenizer and represent different forms of words to a single word using the python nltk wordnetlemmatizer loper and bird 2002   multi-line tweet in section 52 for emotion analysis we converted the multi-line tweet into a single-line tweet in order to use the corresponding apis we used the python library textblob loria 2018 for finding the sentiments from tweets in textblob sentiments of tweets are analyzed in two perspectives 1 polarity and 2 subjectivity based on tweets this library returns the polarity score and subjectivity score polarity score is float value within the range -1 to 1 where 0 indicates neutral sentiments the positive score represents positive sentiments and the negative score represents negative sentiments subjectivity score is also a float value within the range 0 to 1 where 0 indicates that it is a fact and other values indicate the public opinions we used the python implementation of ibm watson tone analyzer service 5 to detect emotional tones in tweets the toneanalyses api uses the general-purpose endpoint to analyze the emotional tones of a tweet and reports the following seven tones analytical tentative confident joy fear sadness and anger the emotion detection process is summarized as following steps  create a tone analyzer service in ibm cloud for a specific region and api plan  get the api key the service endpoint url and the version of api  authenticate to the api using ibm cloud identity and access management iam  request up to 100 sentences no more than 128 kb size in total for the sentence-level tone analysis  parse the json responses that provide the results for each sentence of the tweet here for our analysis the toneanalyses requires a tweet as a single sentence for detecting sentence-level emotional tones in this paper we seek to answer the following research questions  rq1 what are the most frequent words  rq2 what are the trends of sentiment  rq3 what are the trends of emotion  rq4 what are the effects of reopening the tweets were organized into word clouds to analyze which words have been frequently used by twitter users around the world as word cloud visualization consists of the size and visual emphasis of words being weighted by their frequency of occurrence in the textual corpus we can get insights from the most frequent words occurring from figure 1a  we can see that along with the words covid corona or virus words like new and cases got a large number of mentions no wonder the rising number of new cases everyday around the world contribute to the good number of mentions of these words our main goal is to see the effect of reopening in our analysis thats why we also investigate the most frequent words used in reopen related tweets figure 1b represents only the reopen related most frequent words within the united states we can see that businesses schools gyms economy and protesters are the most frequently used words few tweets are protesting to reopen the economy reopen the economy against shelter in place which is a false choice as schools make plans to reopen after covid19 shutdowns how should we proceed any rush to reopen without adequate testing and tracing will cause a resurgence as we reopen shuttered offices and buildings there is a danger of an outbreak of legionnaire disease from these few tweets we can see that few people are protesting to reopen but most of the people are concerned about the reopening timing aftermath of reopening and thinking about the precaution measures needed after reopening we searched for the most frequent words that appeared in twitter posts of our collected data for this section we only considered actual tweets and ignored all kinds of re-tweets from our dataset after applying the preprocessing steps mentioned in section 41 we also removed hashtags and nonalphabetic characters from tweets we split the rest of the tweet into chunks of n consecutive words next we merged the chunks of all tweets as a flat list where n  1 2 3 after that we count the frequency of each unique chunk and sorted those by frequency the top 15 most frequent chunks known as n-gram are shown in table 1 economic breakdown and necessity to overcome this situation through reopen the plan case testing and phase surely emphasize the safety measures that need to be followed while reopening as a control measure the state and people surely stands for how all states beginning to reopen in some way from 2-grams the social distancing stay home wear mask and contact tracing imply users try to raise awareness through their posts the testing site public health and task force imply the safety measures people and policymakers should abide by to curb the spread during the reopen situation the state begin small business back work and business begin indicate as people are going to resume their work after states are reopening from 3-grams the economic breakdown stockmarket fluctuation business plan and training course are the topic that draws our attention this surely indicates there were numerous posts about how the loss due to this pandemic can be overcome through the strategy of the people who are in charge of the decisions from figure 2a we can see that the highest percentage of emotional tone is analytical347 the second highest tone was joy1735 the next few tones are tentative sadness and confident respectively anger and fear had the lowest percentage in our data collection the analytical tone implies that people shared information and their constructive thoughts through their posts joy implies that people showed a positive attitude towards the situation samuel and ali showed that towards the end of march the fear emotion was the most dominant among all emotions and hit a peak but after the announcement of the lockdown easement program people had less fear from figure 2b we can see that during may 13-15 anger sadness and analytical had high peaks the rest emotional tones confident joy tentative and fear had a steady curve during the time period of our data collection sentiment analysis identies words contextual polarity and subjectivity we used python library textblob loria 2018 for sentiment analysis the details of textblob is available in section 4 fig 3 represents the details of twitter sentiments fig 3a corresponds to the barplot of sentiment polarities of tweets and fig 3b corresponds to the daily counts of polarities it can be seen that the majority of tweets have a neutral sentiment 4366 followed by positive 3989 sentiments also the daily trend of positive and negative tweets follows the same pattern so people are taking the reopen decision positively fig 3c indicates that people are mostly posting their own opinion and fig 3d shows an interesting point that day by day people are posting their own opinion rather than facts we saw that many peoples are protesting for reopening states and countries we also saw it in fig 1b  we investigated the reopening effect in the united states fig 4a and b shows that after reopening the new covid19 cases are increasing on the other hand fig 4c and d shows that lockdown helps to reduce the number of new covid19 cases lockdown also helps to reduce the new cases in other countries lau et al 2020  it would be better if the lockdown continues for few more days or if the states or country decide to reopen then they can take proper precautions before the reopen and people need to strictly maintain the instructions from health organizations the followings are the main challenges and threats to the validity of our study limited data we only conducted a shallow analysis of the twitter post from may 3 2020 to may 15 2020 therefore our results may not reflect the actual effect we leave the large scale evaluation as future work noisy data the dataset contains posts from social media ie twitter that are noisy and unstructured having mis-spelling and unknown words are also common in the post additionally the dataset contains other languages but written using the english alphabet that can negatively impact the analysis there are many posts whose length is very small ie around 3 words therefore dataset clean-ing is challenging and important as it has a direct impact on the analysis internal validity we used some open source toolchains in this paper and some issues may exist in those apis however to reduce the issue in our implementation other collaborators reviewed the code and manually inspected some results in this work we try to get insights into public reaction as the reopen phase starts there has been figure 4  effect of reopen this figure represents the daily confirmed cases from 4 different states of the united states we used two kinds of shaded regions to indicate the lockdown and reopen times red shaded area represents the lockdown period and the green shaded area represents the reopen period some analysis from social media data about how people are reacting in the lockdown time we make an effort to understand whether there is a change in public sentiment from the lockdown phase to the reopen phase we have made our analysis on twitter data on reopening related issues during this covid-19 outbreak from our analysis we have found that even though people are showing a positive attitude to reopen their areas to make the economy functional they are also urging the authority in concern to make plans for avoiding the highly predictable second wave from real data we have seen that the new cases are increasing as the reopen phase starts given this situation the coordination of responses from online and from the real world is sure to reveal promising results to address the current crisis we want to put stress on the fact that along with traditional public health surveillance infoveillance studies can play a vital role source code we have released the source code for public dissemination and can be found at httpsgithubcomemtiaz-ahmedcovid19-twitter-reopen  single-cell analysis of ace2 expression in human kidneys and bladders reveals a potential route of 2019-ncov infection wei lin longfei hu yan zhang joshua ooi d ting meng peng jin xiang ding longkai peng lei song zhou xiao xiang ao xiangcheng xiao qiaoling zhou ping xiao jue 6 fan yong zhong  since december 2019 a novel coronavirus named 2019 coronavirus 2019-ncov has emerged in wuhan of china and spread to several countries worldwide within just one month apart from fever and respiratory complications acute kidney injury has been observed in some patients with 2019-ncov in a short period of time angiotensin converting enzyme ii ace2 have been proposed to serve as the receptor for the entry of 2019-ncov which is the same for severe acute respiratory syndrome coronavirus sars to investigate the possible cause of kidney damage in 2019-ncov patients we used both published kidney and bladder cell atlas data and an independent unpublished kidney single cell rna-seq data generated in-house to evaluate ace2 gene expressions in all cell types in healthy kidneys and bladders our results showed the enriched expression of all subtypes of proximal tubule cells of kidney and low but detectable levels of expression in bladder epithelial cells these results indicated the urinary system is a potential route for 2019-ncov infection along with the respiratory system and digestion system our findings suggested the kidney abnormalities of sars and 2019-ncov patients may be due to proximal tubule cells damage and subsequent systematic inflammatory response induced kidney injury beyond that laboratory tests of viruses and related indicators in urine may be needed in some special patients of 2019-ncov  in late december 2019 a cluster of mystery pneumonia cases emerged in wuhan a city in the middle south of china deep sequencing analysis and etiological investigations then confirmed the pathogen was a type of a newly identified coronavirus which had been labelled as 2019 novel coronavirus 2019-ncov the rapid spread of 2019-ncov caused the outbreak of pneumonia in china and also other countries making it a severe threat to international public health security within a short period of time 1 2 3  based on bioinformatics analyses it has been shown that 2019-ncov genome is 88 identical with two bat-derived severe acute respiratory syndrome sars-like coronaviruses bat-sl-covzc45 and bat-sl-covzxc21 79 with sars-cov and 50 with mers- single-cell rna sequencing scrna-seq has been extensively applied in 2019-ncov research due to its capability to profile gene expressions for all cell types in multiple tissues unbiasedly at a high resolution in addition to well-known alveolar type 2 cells in lung expression of ace2 has also been investigated in liver cholangiocytes esophagus epithelial cells and absorptive enterocytes of ileum and colon by scrna-seq 11 12 13  these results demonstrated the potential utility of scrna-seq to unmask the potential target cell types of 2019-ncov since the urinary system infection and its potential aftermath could be essential to the patient care during and after the infection here we used two scrna-seq transcriptome data in healthy kidneys and one dataset in heathy bladders to investigate the expression patterns of cell types in the urinary system gene expression matrices of scrna-seq data from normal kidneys of three healthy donors were downloaded from the gene expression omnibus gse131685 we reproduced the downstream analysis using the code provided by the author in the original paper we obtained the scrna-seq data of healthy bladder tissues of three bladder cancer patients from the gene expression omnibus gse108097 to be consistent with the kidney datasets we applied harmony 14 to integrate samples and performed downstream analysis using seurat v3 clustering analysis was done by first reducing the gene expression matrix to the first 20 principal components and then using resolution 03 for the graph-based clustering we annotated the cell types afresh using the canonical markers listed in the supplementary table of the original paper kidney samples were obtained at a single site by wedge and needle biopsy of living donor kidneys after removal from the donor and before implantation in the recipient kidney biopsy samples were cleaned with sterile pbs after acquisition the fresh kidney tissue was immediately transferred into the gexscope tissue preservation solution singleron biotechnologies at 2-8c the samples were digested in 2ml gexscope tissue dissociation solution singleron biotechnologies at 37c for 15min in a 15ml centrifuge tube with continuous agitation after washed with hanks balanced salt solution hbss for three times and cut into approximately 1-2 mm pieces subsequently cell debris and other impurities were filtered by a 40-micron sterile strainer corning the cells were centrifuged at 1000 rpm and 4c for 5 minutes cell pellets were resuspended into 1ml pbs hyclone to remove red blood cells 2 ml gexscope red blood cell lysis buffer singleron biotechnologies was added to the cell suspension and incubated at 25c for 10 minutes the mixture was then centrifuged at 1000 rpm for 5 min and the cell pellet resuspended in pbs cells were counted with tc20 automated cell counter bio-rad the single cell suspension was proceeded to the single cell library preparation and sequencing as previously described 15  to be consistent with the public data used above we applied a similar downstream analysis workflow 14  we modified the number of principal components to 20 and used resolution 06 to obtain comparable cell type clustering results with the public kidney data cell types were annotated based on the canonical markers in the literature 14 16  by analyzing the public single-cell transcriptome dataset of normal human kidney cells from three donors 14  we found ace2 expression distributed across multiple cell types notably ace2 was mostly enriched in proximal tubule cells including both convoluted tubule and straight tubule fig 1a-d  the other nephron subtypes such as collecting duct and distal tubule as well as immune cells all showed extremely low gene expressions to validate this result we further analyzed normal kidney samples from 2 healthy donors the 4736 cells were classified into 9 cell types which were highly overlapped with the public data including 7 nephron specific subtypes based on the canonical marker genes fig 2a  the proximal tubule pt cubn slc13a3 slc22a8 were classified into proximal convoluted tubule and proximal straight tubule based on different marker expression level 14 16  as observed in the public dataset we observed upregulated ace2 expressions in all proximal tubule cells comparing to other cell types  figure 2d  quantitatively we found about 5-15 of both straight and convoluted proximal tubule cells expressing ace2 previous immunohistochemistry analysis 17 indicated a complex spatial distribution of ace2 protein expression concentrated in the brush border of the proximal tubules based on the public bladder dataset of 12 cell types  figure 3a however previous analysis of sars patients indicated sars virus was able to survive in urines on detectable levels 18 19  the detection of sars virus in urine implied the possibility of virus releasing from infected bladder epithelial cells in agreement with our analysis above however the detailed mechanism of renal involvements of 2019-ncov and sars-ncov is still unclear the etiology of aki in sars seems to be multifactorial and still uncertain which was thought to have a certain correlation with the high expression of ace2 in the kidney 24 25  based on our single-cell analysis in both normal kidneys and bladders we found both detectable levels of ace2 in kidney and bladder kidney pt cells have higher expression percentages than bladder epithelial cells which may indicate that kidney is more susceptible to 2019-ncov infection than bladder comparing to the expression levels of ace2 in the digestion system 11 the lower expression of ace2 in the renal and urinary system could suggest fewer patients with renal related symptoms which positively correlates with the observations for sars patients acute kidney injury which has been proved to be a predictor of high mortality in sars patients 20  may also lead to difficulty of treatment worsening conditions and even be an the copyright holder for this preprint which was not peer-reviewed is the  httpsdoiorg10110120200208939892 doi biorxiv preprint the copyright holder for this preprint which was not peer-reviewed is the  httpsdoiorg10110120200208939892 doi biorxiv preprint the copyright holder for this preprint which was not peer-reviewed is the  httpsdoiorg10110120200208939892 doi biorxiv preprint  estimation of the transmission risk of the 2019-ncov and its implication for public health interventions biao tang xia wang qian li nicola bragazzi luigi sanyi tang yanni xiao jianhong wu   coronaviruses are enveloped single-stranded positive-sense rna viruses belonging to the family of coronaviridae 1 they cause generally mild respiratory infections even though they are occasionally lethal since their discovery and first characterization in 1965 2 three major large-scale outbreaks have occurred caused by emerging highly pathogenic coronaviruses namely the severe acute respiratory syndrome sars outbreak in 2003 in mainland china 3 the middle east respiratory syndrome mers outbreak in 2012 in saudi arabia 45 and the mers outbreak in 2015 in south korea 67 these outbreaks have resulted in more than 8000 and 2200 confirmed sars and mers cases respectively 8 recently a fourth coronavirus outbreak has occurred in wuhan the capital city of the hubei province and the seventh largest city of peoples republic of china 91011  since 31 december 2019 when the wuhan municipal health commission reported 27 cases of viral pneumonia including 7 critically ill cases the pneumonia outbreak has received considerable global attention a novel coronavirus was identified as the causative agent by the chinese authorities on 7 january 2020 and on 10 january 2020 the world health organization who designated the novel coronavirus as 2019-ncov on the same day the who released a wide range of interim guidance for all countries on how they can get prepared for coping with this emergency including how to monitor for potentially infected people collect and test samples manage patients control and mitigate the burden generated by the infection in health centers maintain the right drug supplies and effectively communicate with the lay public regarding the new virus 12 by the morning of 23 january 2020 more than 571 confirmed cases with 17 deaths had been reported in other parts of mainland china and in various countries including south korea japan thailand singapore the philippines mexico and the united states of america as of 6 february 2020 0245 gmt 28276 cases of which 3863 are in critical condition and 565 deaths had been reported  the transmission potential often measured in terms of the basic reproduction number the outbreak peak time and value and duration under current and evolving intervention measures remain unclear and warrant further investigation  on 20 january 2020 the chinese government revised the law provisions concerning infectious diseases to add the 2019-ncov as a class b agent a pathogen that can cause an epidemic outbreak on the same day public health officials announced a further revision to classify the novel virus as a class a agent a pathogen that can cause an epidemic in a short time some non-pharmaceutical interventions npis including intensive contact tracing followed by quarantine of individuals potentially exposed to the disease and isolation of infected symptomatic individuals were implemented but their effectiveness during the early stage is questionable  quantifying the effectiveness of these interventions is of crucial importance for wuhan as well as for other cities in their preparedness and rapid response to the importation of infected cases with the arrival of the spring festival massive traveling is expected to mobilize a large segment of the population by which the novel coronavirus may be broadly reseeded  extreme unprecedented measures have been taken for example on 23 january 2020 the chinese authorities introduced travel restrictions affecting five cities wuhan huanggang ezhou chibi and zhijiang effectively shutting down the movement of more than 40 million people  however how these expensive and resource-intensive measures can contribute to the prevention and control of the infection in these cities and other parts of the country and how long these travel restrictions should be maintained remain to be determined in the context of a novel coronavirus affecting a nave population estimation of the basic reproduction number is important for determining the potential and severity of an outbreak and providing critical information for designing and implementing disease outbreak responses in terms of the identification of the most appropriate evidence-based interventions mitigation measures and the determination of the intensity of such programs in order to achieve the maximal protection of the population with the minimal interruption of social-economic activities 8  as recognized by the who 13 mathematical models especially those which are timely play a key role in informing evidence-based decisions by health decision- and policy-makers to the best of our knowledge only a few mathematical models have so far been publicly released including a bats-hosts-reservoir-people transmission network model and a returning traveler study aimed to compute underestimated coronavirus cases 1415  no study has focused on the practical implications of public health interventions and measures therefore the present study was undertaken to fill in this gap of knowledge we obtained data of laboratory-confirmed 2019-ncov cases which occurred in mainland china from the who situation report the national health commission of the peoples republic of china and the health commission of wuhan city and hubei province 16171819 data information includes the cumulative number of reported cases as shown in figure 1a and the quarantined and released population as shown in figure 1b the data were released and analyzed anonymously since the identification of the 2019-ncov on 10 january 2020 some cases were ruled out and the cumulative number of reported cases per day was 41 from 10 to 15 january 2020 to obtain the relatively reliable data we used the exponential growth law to deduce the number of reported cases per day from 31 december 2019 to 10 january 2020 called datarev2 or from 10 to 15 january 2020 called datarev1 based on the 41 cases on that date as shown in figure 1a  by inferring the effectiveness of intervention measures including quarantine and isolation figure 1b we estimated the required effectiveness of these interventions in order to prevent the outbreak here we propose a deterministic susceptible-exposed-infectious-recovered seir compartmental model based on the clinical progression of the disease epidemiological status of the individuals and intervention measures figure 2 we parameterized the model using data obtained for the confirmed cases of 2019-ncov in mainland china and estimated the basic reproduction number of the disease transmission in more detail we investigated a general seir-type epidemiological model which incorporates appropriate compartments relevant to interventions such as quarantine isolation and treatment we stratified the populations as susceptible s exposed e infectious but not yet symptomatic pre-symptomatic a infectious with symptoms i hospitalized h and recovered r compartments and further stratified the population to include quarantined susceptible sq isolated exposed eq and isolated infected iq compartments  with contact tracing a proportion q of individuals exposed to the virus is quarantined the quarantined individuals can either move to the compartment eq or sq depending on whether they are effectively infected or not 20 while the other proportion 1  q consists of individuals exposed to the virus who are missed from the contact tracing and move to the exposed compartment e once effectively infected or stay in compartment s otherwise let the transmission probability be  and the contact rate be constant c then the quarantined individuals if infected or uninfected move to the compartment eq or sq at a rate of cq or 1  cq those who are not quarantined if infected will move to the compartment e at a rate of c1q the infected individuals can be detected and then isolated at a rate di and can also move to the compartment r due to recovery  the transmission dynamics are governed by the following system of equations 1sccq1siasqec1qsiaeieiiia1eaasq1cqsiasqeqcqsiaqeqhiiqeqhhriiaahh
where  is the derivative with respect to time and the other parameters are summarized in table 1  given the model structure with quarantine and isolation figure 2 we used the next generation matrix 2122 to derive a formula for the control reproduction number when control measures are in force as follows2rcc1qiic11qas0 we used the markov chain monte carlo mcmc method to fit the model and adopted an adaptive metropolishastings m-h algorithm to carry out the mcmc procedure the algorithm is run for 100000 iterations with a burn-in of the first 70000 iterations and the geweke convergence diagnostic method is employed to assess convergence of chains we employed the likelihood-based method or generation interval-informed method of white and pagano 23 using the following formula3lrc pnt1texpttntnt1
where trcj1kpjntj k is the maximum value of the serial interval chosen as k6 here and x is the gamma function nn0n1   nt where nj denotes the total number of cases on day j and t is the last day of observations pj is the probability function for the generation interval on day j we assume that the generation interval follows a gamma distribution with mean e and variance v since the generation interval of the 2019-ncov is undetermined we investigated the sensitivity of rc to different e values ranging from 2 to 8 days given in table 2  the population of wuhan is around 11081000 inhabitants 18 hence we set s011081000 as of 10 january 2020 two patients had been recovered and were subsequently discharged from the hospital leading to r02 and 739 individuals were quarantined leading to sq0739 we set h01 corresponding to the reported confirmed case on 10 january 2020 the quarantined individuals were isolated for 14 days thus 114 according to the who 24 the incubation period of 2019-ncov is about 7 days hence 17 likelihood-based estimation of rc during the outbreak in wuhan gives a mean value of 639 with mean and variance of generation time of 6 and 2 days on the basis of a revised data series datarev1 the reproduction number based on likelihood-based estimation ranges from 166 to 10 and it follows from table 2 that rc is sensitive to changes in mean generation intervals fitting to the other revised data series datarev2 gives a mean value of 632 with mean and variance of generation time of 6 and 2 days note that the estimates of rc based on the two time series agree well and consequently both revised data series can be used to fit the proposed dynamics transmission model in this study we chose the estimations based on datarev1 as the comparison reference to verify and validate our model-based estimation thus in the following sections of the manuscript we will use the revised dataset datarev1 to fit the proposed model  by fitting the model without considering asymptomatic infections to the data of hospital notification for the confirmed 2019-ncov cases datarev1 we estimated the mean control reproductive number rc to be 647 95 ci 571723 whereas other parameter estimations are reported in table 1 note that the mean estimations of rc based on the likelihood method are within the 95 confidence interval of the model-based estimates table 2  using the estimated parameter values we predicted the trend of the 2019-ncov infection under the current intervention before 22 january 2020 the number of infected individuals it is expected to peak on around 10 march 2020 with a peak size of 163105 infectious individuals  to examine the possible impact of enhanced interventions on disease infections we plotted the number of infected individuals it and the predicted cumulative number of reported cases with varying quarantine rate q and contact rate c this analysis shows that reducing the contact rate persistently decreases the peak value but may either delay or bring forward the peak as shown in figure 3 and table 3  in more detail our analysis shows that increasing quarantine rate q by 10 or 20 times will bring forward the peak by 65 or 9 days and lead to a reduction of the peak value in terms of the number of infected individuals by 87 or 93 this indicates that enhancing quarantine and isolation following contact tracing and reducing the contact rate can significantly lower the peak and reduce the cumulative number of predicted reported cases figure 4 considering the spreading of the virus figure 5 and in order to examine the impact of the travel restriction on the infection in other cities such as beijing we initially calculated the daily number of exposed individuals imported from wuhan to beijing denoted by imet  according to our model we get the exposed fraction as of 22 january 2020 approximately 40000 persons from wuhan to beijing via trains around 37000 and flights around 3000 25 then we haveimet  40000  etn4
with 40 individuals being imported exposed individuals as of 22 january 2020 however there could potentially exist an ascertainment bias in reported case data since cases may have been larger than 40 individuals but have not been reported or reported with a delay in time we find that with travel restriction no imported exposed individuals to beijing the number of infected individuals in seven days will decrease by 9114 in beijing compared with the scenario of no travel restriction while given no travel restriction the number of infected individuals in seven days will decrease by 8884 only if we increase the quarantine rate by 100 thousand times as shown in figure 6a this means that the effect of a travel restriction in wuhan on the 2019-ncov infection in beijing is almost equivalent to increasing quarantine by a 100 thousand baseline value which is a rate that can hardly be achieved in any public health setting it follows from figure 6b that with travel restriction the number of cumulative individuals in seven days will significantly decrease by 7570 in beijing compared with the scenario of no travel restriction  based on the 2019-ncov cases data until 22 january 2020 we have estimated the basic reproduction numbers using different methods likelihood-based and model-based approaches the mean control reproduction number was estimated to be as high as 647 95 ci 571723 in comparison with the values of the sars epidemics r0  491 in beijing china in 2003 26 and mers in jeddah r0  3567 and riyadh r0  2028 kingdom of saudi arabia in 2014 27  our value is higher than other published estimates for instance reference 28 such a high reproduction number is consistent with the opinion that the virus has gone through at least threefour generations of transmission in the period covered by this study 24 note that our estimation is based on a dataset collected during a period of intensive social contacts before the chinese new year 25 january 2020 there were lots of annual summing-up meetings andor parties with higher than usual close contacts leading to a higher likelihood of infection transmission than that of the earlier periods covered by other studies furthermore we noted that more recently published studies based on datasets during periods comparable with ours reported similar findings in terms of a high basic reproduction number for instance reference 29 where authors using an exponential growth method computed a basic reproduction number of 611 95 ci 451816 assuming no changes in reporting rate and with a serial interval of 84  38 days variability in the estimation of the basic reproduction number is also a well-known methodological issue and standardized methods both for calculating and reporting it are still lacking 30 during the initial phases of an epidemics outbreak only small datasetstime-points can be used some crucial information may be missing and the quality accuracy and reliability of data improves over time in these situations estimations are highly dependent on the specific datasets utilized and revisingupdating such datasets could influence the results we note that several key clinical parameters could be inferred from relevant clinical data based on sero-epidemiological surveys and the possibility of spreading the infection from asymptomatic cases was only reported recently 31 our finding of a high reproduction number implies the potential of a very serious epidemic unless rather swift public health interventions are implemented 3233 during the season when the social contacts is the highest  note that the serial interval is an essential factor affecting the accuracy of the likelihood function estimation according to the current report the incubation period of wuhan patients with coronavirus pneumonia is about 2 to 15 days we then assume that the serial interval follows the gamma distribution with varying mean and variance which allows us to examine the influence on the reproduction number with the distribution of serial interval with mean 6 days and variance 2 days the likelihood-based estimation of the reproduction number is consistent with the model-based estimation it shows that longer serial intervals induce greater reproduction numbers and hence more new infections which further confirms that the epidemic may be more serious than what has been reported until now 15  based on the reported data we have estimated that the number of people who were identified through contact tracing and quarantined was 5897 as of 22 january 2020 in comparison with the total population size of wuhan the effort of close contact tracing and quarantine was insufficient and appears to have a limited impact in terms of reducing the number of infected cases andor slowing down the epidemic the contour plot of rc  1 gives the threshold values of contact rate and quarantine rate for a city to avoid an outbreak this high threshold rate of quarantine puts an extremely high requirement for the citys public health infrastructure and its citizens adherence to personal protective and public health interventions including a reduction of transmission-effective contacts separation and restriction during the quarantine  such a high level of quarantine rate and reduction of contact is possible only when the number of imported cases from the epicenter is minimal speaking in terms of the value of the travel restriction a strict travel restriction to the city of wuhan is expensive and resource-consuming imposing a substantial challenge to the decision- and policy-makers and the citys resilience moreover such a measure could only delay the transmission of the infectious disorder  in conclusion our simulations show that the appropriate duration of this travel restriction depends on a combination of effective quarantine and reduction of contact within the city considering the latest events the lock-down of wuhan on 23 january 2020 the adoption of the travel restriction strategy by other regions and provinces the introduction of new detection technologies etc the present model needs to be revised in that the basic reproduction number estimated here is no longer suitable for predicting future epidemic trends table 4 this will be the aim of a forthcoming article coronaviruses occasionally lead to major outbreaks with documented reproduction numbers ranging from 20 to 49 currently a fourth large-scale outbreak is occurring and spreading out from wuhan hubei province china to neighboring provinces and other countries there is a dearth of epidemiological data about the emerging coronavirus which would be of crucial importance to design and implement timely ad hoc effective public health interventions such as contact tracing quarantine and travel restrictions in this study we adopted a deterministic model to shed light on the transmission dynamics of the novel coronavirus and assess the impact of public health interventions on infection we found that the basic reproduction number could be as high as 647 95 ci 571723 which seems consistent with the special period prior to the spring festival when contacts were higher than usual and with the opinion that the virus has gone through at least threefour generations it is worth mentioning that our model made a very good prediction of the confirmed cases from 23 to 29 january 2020 as shown in table 4 particularly the predicted confirmed cases should be 7723 as of 29 january 2020 which is very close to the real number of cases of 7711 furthermore according to our model the outbreak under the most restrictive measures is expected to peak within two weeks since 23 january 2020 with a significant low peak value our investigation has major practical implications for public health decision- and policy-makers the rather high reproduction number suggests that the outbreak may be more serious than what has been reported so far given the particular season of increasing social contacts warranting effective strict public health measures aimed to mitigate the burden generated by the spreading of the new virus   viet duong jiebo luo phu pham tongyu yang yu wang  recently the pandemic of the novel coronavirus disease-2019 has presented governments with ultimate challenges in the united states the country with the highest confirmed covid-19 infection cases a nationwide social distancing protocol has been implemented by the president for the first time in a hundred years since the 1918 flu pandemic the us population is mandated to stay in their households and avoid public contact as a result the majority of public venues and services have ceased their operations following the closure of the university of washington on march 7th more than a thousand colleges and universities in the united states have cancelled in-person classes and campus activities impacting millions of students this paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining peoples opinions on social media we discover several topics embedded in a large number of covid-19 tweets that represent the most central issues related to the pandemic which are of great concerns for both college students and the general public moreover we find significant differences between these two groups of twitter users with respect to the sentiments they expressed towards the covid-19 issues to our best knowledge this is the first social media-based study which focuses on the college student communitys demographics and responses to prevalent social issues during a major crisis  first detected in wuhan china on december 31th 2019 covid-19 or the coronavirus outbreak grew rapidly in scale and severity and was officially declared as a pandemic on march 11th 2020 1  as of april 13th the world health organization who reported 1812734 confirmed cases of covid-19 worldwide including 113675 deaths 2  due to the novelty and intractability of the virus the global community particularly the elderly and those with underlying medical problems 3  are at a high risk for serious health and safety hazard however we suspect that the younger and physically healthier population is just as susceptible though in a different way to covid-19 in order to control the spread of the outbreak non-pharmaceutical interventions and preventive measures such as social-distancing and self-isolation have been implemented worldwide out of utmost necessity which has led to the large-scale shutdown of public gathering places as members of an active working and learning society people who dedicate most of their daily hours at workplaces and educational institutions are highly vulnerable to the impacts of the closure of these facilities this is especially true for college students the response to the covid-19 pandemic has brought a sudden disruption in the operations of schools colleges and universities influencing more than 17 billion students in 192 countries 4  located at the epicenter of the pandemic with 579005 confirmed cases including 22252 deaths 5  the us educational system which is one of the largest in the world has taken the biggest hit beginning with the university of washington which closed its campus on march 7th 2020 and moved classes online for its 50000 students many colleges have also immediately responded to this outbreak by cancelling all on-campus activities such as workshops conferences and sports and relocating their in-person classrooms to online platforms as of april 14th 2020 more than 124000 us public and private schools have closed due to the virus affecting at least 551 million students 6  this transition has introduced multiple challenges for students the foremost concern is related to how the government and education system handle the pandemic crisis with the study-from-home approach past surveys suggested that students experience severe limitation on particular subjects that benefit from physical interaction with the materials and tend to lose the pacing mechanism of scheduled lectures thus have a higher chance of dropping out than those in traditional settings 1  2  more recently as the covid-19 pandemic is unfolding sahu 3 hinted at other issues related to the closure of schools apart from online learning such as international students travel and students mental health this motivates us to provide a more comprehensive study on the student demographics regarding their primary subjects of concern and how they are expressed particularly amidst the covid-19 crisis in this study we attempt to explore the responses to the covid-19 pandemic by twitter users with the focus on the college students also we highlight our findings regarding the college student demographics by characterizing the outstanding differences in their behaviors from the general public such insights can be vital for educators and policymakers to measure the effectiveness of their on-going efforts in the global fight against covid-19 and the protection of our younger population in addition we train classification models to identify the demographics of users who posted tweets associated with covid-19 as well as extracting the sentiments they inherently expressed in their posts the models can be used in social media platforms to investigate central social problems with respect to both their universality and degree of impact and to draw the communitys attention towards high-priority targets for addressing such problems our main contributions are several folds 1 we approach the social issues related to covid-19 across different demographics using social media by collecting data from twitter 2 we deploy topic modeling methods on a novel dataset to highlight the topical patterns of the ongoing social media discussions during a major crisis regarding two different user demographics on twitter and 3 our implementation of state-of-the-art transformer models for natural language inference achieves new state-of-the-art performance for the twitter sentiment classification task which allows meaningful insights on social media behaviors to be discovered reliably our study draws knowledge from the body of research on characterizing the demographics of social media users along the dimensions such as gender 4  5  age 6  7  and social class 7  methods on inferring the twitter user demographics were typically reliant on mining fine-grained linguistic patterns from the users twitter biography short self-descriptive text and posts which are proven highly precise for certain attributes when properly constructed 5  8  these approaches have also been deployed with relatively strong performance for the college student demographics 9  10  due to the recent advance in neural networks for sequence and image classification wang et al 11 were able to leverage a multimodal multi-attibute and multilingual approach to achieve the state-of-the-art accuracy on gender age and organization entity classification building upon the discoveries of previous works we design and evaluate our own college student user classification method this enables us to identify the two pools of users college students and general public among college followers on twitter for the subsequent comparative analysis research on sentiment analysis for twitter textual data which tackles the problem of analyzing the messages posted on twitter in terms of the sentiments they express has also been performed twitter is a very challenging domain for sentiment analysis mainly due to the length limitation of texts 12  the majority of past approaches employed a traditional machine learning methods such as logistic regression svm mlp etc trained on lexicon features and sentiment-specific word embeddings vector representations of words 13  14  more recent approaches typically opted for sequence learning models trained to learn relevant embeddings for classification from large pretrained word embeddings particularly the glove 15 embeddings for twitter data best performing models of this breed include cliche 2017 16 and baziotis et al 2017 17  which shared the first place for sentiment analysis on twitter task 4a 18  at the international workshop on semantic evaluation 2017 semeval-2017 the novelty in our approach to twitter sentiment analysis involves the implementation of state-of-the-art transformer methods such as bert 19 and roberta 20  whose outstanding sentiment classification prowess remains untested for twitter data in the literature in this study we limit the user population to those who follow the official twitter accounts of colleges in the us news 2020 ranking of top 200 national universities relevant users identified as english speakers were collected using the tweepy api 7  since the lists of the followers of the colleges in consideration might overlap simply collecting tweets from these users could create major data redundancy and time complexity problems therefore we extract unique users from the combined results as well as their personal information and profile images to obtain a dataset of 12407254 unique users this set of users is relatively large and 1641582 of of these users have twitter protected accounts which means we are not allowed to collect tweets twitter posts from them thus we randomly sample 100000 users from the unprotected pool to represent the population of college followers for the subsequent tweet collection and text analysis tweets were collected using the tweepy api we retrieved a total of 1873022 tweets from the 100000 user samples posted within the timeframe between january 20th when the first covid-19 case was confirmed in the us and march 20th of 2020 to cover a two-month period when nationwide social distancing protocol and school closure were attracting mass concerns we then extracted tweets related to covid-19 with a list of keywords consisted of corona corona coronavirus covid-19 covid19 coronavirus covid 19 chinese virus and chinesevirus as a result we obtain 73787 unique covid-19 related tweets pertaining to 12776 users whom in this study we will address as affected users in addition tweets that are not related to covid-19 of the 12776 affected users are kept for the student inference task we develop a text preprocessing pipeline similar to that of baziotis et al 17 to ensure that our text dataset is to a high degree lexically comparable to natural language and include covid-19 domain-specific word knowledge from a novel dataset this is done by performing sentiment-aware tokenization spell correction word normalization segmentation for splitting hashtags and token annotation they implemented a tokenizer with the sentiwordnet corpus 21  which is capable of avoiding splitting expressions or words that should be kept intact as one token and identify most emoticons emojis expressions such as dates currencies acronyms censored words eg st etc in addition we perform spelling correction on the extracted tokens by composing a dictionary for the most commonly seen abbreviations censored words and elongated words for emphasis eg reallyyy the viterbi algorithm is used for word segmentation with word statistics unigrams and bigrams computed from a recently published twitter dataset of 50 million english tweets related to covid-19 22  to obtain the most probable segmentation posteriors moreover all texts are lower-cased while urls emails and mentioned usernames are annotated with common designated tags and removed to retain the natural language elements from the text data the processed tweets are then annotated by the standford corenlp english annotator 23  which uses syntactic constituency and dependency tree parsing to extract the appropriate part-of-speech pos tags and lemmas the basedictionary forms of words from the tweet tokens we consider age gender and organization entity to be highly descriptive attributes to first obtain a general view of our user samples according to national center for education statistics nces 8  as of fall 2017 562 of enrolled students aged between 19 and 29 years old 201 are under 18 and 566 of them were female these student demographic statistics are projected by nces to remain consistent through 2020 also organizational twitter accounts apparently should not be targeted for student inference because college students are individuals these attributes are extracted using the m3 multilingual multimodal multi-attribute deep learning system for inferring the demographics of users from four sources of information from twitter profiles users name first and last name in natural language screen name twitter username biography short self-descriptive text and profile image 11  we extract 1111 organization entities from 12776 affected users and disregard them from comparative analysis since they are not individuals also the gender and age attributes are used to verify our classification results although the m3 model is highly robust for gender 0918 macro-f1 and organization entity 0898 macro-f1 recognition without using tweets the distinguishing features for these attributes are widely available in the name profile image and description of twitter users which is not necessarily the case for the college student demographics since attributes from the users name and photo image are not necessarily indicative of college students we attempt to retrieve the students by matching the word student with the twitter biographies and found only 248 matches 222 of the nonorganizational users we also matched the keywords directly related to their degree status such as bs ms mba phd etc to their twitter biography and found 335 3 while college students are likely to mention their degree status rather than current occupation and many users mention the names of colleges in their biography these should not be deciding factors because they might just be college alumni or professors instead of actual students due to the fact that the users are already college followers we expect much higher percentages of college students thus the use of tweets for college student identification is critical to our study b heuristically identifying college students using tweets 1 gold-standard annotations we sample 2400 random users from the 11165 non-organizational affected users and includes their names profile images biographies and tweets from 120 to 3202020 this information is used by human annotators 9 to answer the prompt would you think this person is a college student with two response options yes or no 2 supervised classification we encode the standard bag of n-grams for 1 up to 4-grams representation of the users tweets which has been highly effective in text categorization and information retrieval 24  to use them as features for our classifiers to increase the generality of our bag of n-grams features we preprocess the tweets as described above and apply tf-idf vectorization a term re-weighting scheme that discounts the influence of common terms we train a random forest classifier and report the accuracy the percentage of correctly labeled users on 20 of the labeled samples 3 using heuristic to override the classifier regarding the self-distinguishing attributes of twitter users from tweets bergsma and van durme 5 discovered that users most frequently reveal their attributes in the possessive construction that is my x where x is an attribute quality or event that they possess in a linguistic sense as a matter of fact we found 306 tweets with the phrase my class among the 1156947 tweets from non-organizational users on the contrary phrases like i havehad a classes occur only 16 times therefore we extract this my x attribute type for the college student demographic as follows we first part-of-speech tag our data using the stanford corenlp tagger and then look for my x patterns where x is a sequence of tokens terminating in a noun to calculate the association between the attributes and the college student demographic we compute the pointwise mutual information 25 between each attribute a and student over the set of occurrences if p m i  0 the observed probability of a student and attribute co-occurring is greater than the probability of co-occurrence that we would expect if student and attribute a were independently distributed we employ two techniques for selecting distinctive attributes for college students 1 we rank the attributes by their pmi scores and use a threshold to select the top-ranked attributes 2 we manually filter the remaining set of attributes to select those that are judged to be discriminative including phrases closely associated with college students such as my zoom class my professor my dorm etc then we use a simple heuristic to use our identified self-distinguishing attributes in conjunction with a classifier trained on goldstandard annotations if the user has any self-distinguishing my-x attributes we assign the user to be a college student otherwise we trust the output of the classifier we apply this rule to bootstrap the knowledge learned by the classifier in conjunction with our domain-specific attributes to automatically label the unannotated users in the end we verify the performance of our heuristic on the same test set as the classifier 9 annotators are irb certified for social-behavioral-educational research 4 summary of results as previously discussed the random forest classifier trained on 1920 examples performs quite well with bag of n-grams features by correctly labeling 78 of the college students on the test set we experiment with our my-x attributes and set the pmi threshold to 05 and then manually filter out the irrelevant attributes applying our heuristics to override the classifier improve the accuracy further to 83 therefore we have firm grounds to utilize the combined classifier and my-x heuristics to label college students from the remaining users which account for an additional 2575 out of the total of 3460 college student users 31 of the non-organizational users looking at the age and gender distributions of the college students in our samples figure 2  the statistics are very consistent with real world data in the us as 538 of the college students we identified are female which is very close to the 567 female percentage predicted by nces for 2020 although age classification is a challenging task for the m3 model 0522 macro-f1 and even human 11  our results are still within a reasonable margin with ncess 2020 projection with 541 of the students in the 19-29 age group vs 567 and 286 under 18 vs 212 in order to understand the latent topics of the covid-19 tweets for college followers we utilize latent dirichlet allocation lda 26 to label universal topics demonstrated by the users to reduce the complexity of the lda analysis corpus only the lemmas of tokens with pos tags of type noun verb adjective and adverb are kept from the preprocessed tweets because they possess the most meaningful contents related to the topics we are looking to discover we not only look at individual tokens but also consider highly-correlated groups of two and three words thus bigrams and trigrams are computed and added to the corpus since certain terms frequently appear in those covid-19 tweets eg virus disease infection case test etc we transform our lda corpus using tf-idf vectorization we finetune our lda topic model and arrive at the optimal topic number of 55 and coherence score of 0373 we also implement t-sne dimensionality reduction technique 27 to transform the computed 3126-dimensional document-term topic posterior matrix into 2-dimensional data we then label the 6 most frequently discussed topics using the top 20 weighted topic keywords figure 3  evidently global news is the most popular topic among the tweets as the numbers of confirmed positive covid-19 cases and deaths are constantly increasing globally the presence of political discussions as well as the controversy related to the chinese origin of the virus is very strong due to the ongoing presidential election campaign in the us which gives solid evidence that the covid-19 pandemic is influencing our political picture the third and fourth most frequent topics involve social distancing and the closure of colleges regarding which of the topics attracts the most attention from the college community in comparison with the general public we find that college students tend to give more responses to covid-19 issues that particularly affect them in other words as most universities in the us announced the shutdown of on-campus activities and encouraged students to refrain from crowded travel and commute during march colleges students posted more tweets related to school closure 3204 and news from areas close to their living proximity 3356 in addition they were concerned about social distancing and controversies regarding the address of the covid-19 virus as chinese virus which are two very important topics that we will take a closer look in later analysis to expand the scope of our study from the topic modeling results we decide to dive deeper into the posts belonging to the each of the 6 most frequently discussed topics specifically for each topic we separate the college students and general population into two pools and apply the roberta model to classify and examine the sentiments they expressed also we use the same topic modeling techniques as described in section v to provide microscopic explanations to the sentiment results a transformer models for sentiment classification 1 roberta -robustly optimized bert pretraining bert bidirectional encoder representations from transformers 19  was designed to pretrain deep bidirectional representations of tokens from unlabeled text by jointly conditioning on both left and right context in all layers this was achieved by using a masked language model mlm whose pretraining objective is to predict the randomly masked tokens of the sequence input as a result the pretrained bert model can be finetuned with just one additional output layer to bring substantial performance gains for a wide range of language inference tasks including sentiment classification without extensive task-specific architecture modifications recently liu et al 20 provided a replication study of bert pretraining and discovered that bert was significantly undertrained yet it can still match the performance of every model published following its inception thus they presented additional insights on the design choices and training strategies of bert and introduced alternative bert-based models roberta that record state-of-the-art results on similar tasks they attributed their success to the use of a larger dataset for pretraining and better design choices for mlm they reported 0948 f1 score of their roberta base model for sst-2 a stanford sentiment treebank sst dataset with binary labels positive and negative for sentiment analysis task in comparison bert base only achieved 0928 f1 score therefore we choose roberta as the pretraining procedure for our twitter sentiment analysis model as well as comparing its performance with bert 2 training and evaluation we utilize the transformers 10 library by huggingface 28  which includes roberta base and bert base in pytorch 29  and implement the sentiment analysis models with an additional linear layer on top of the pretrained models outputs the adamw optimizer 30 is used to optimize the cross-entropy loss function we also use the fastai 11 api 31s deep learning wrapper for pytorch which allows us to split the models layers into groups in order to use discriminative finetuning and slanted triangular learning rates 32 for task-specific features ie learning word embeddings learning context embeddings and learning sentiment outputs we train and evaluate our models on the semeval-2017 task 4a dataset for twitter message sentiment classification on a 3-point scale negative neutral and positive table i  in the end both of our classifier models substantially outperform the top two performers of semeval-2017 table ii on the test dataset in particular the model with roberta pretraining achieves above 08 macro-f1 score which demonstrates its robustness for twitter sentiment classification and applicability for exploring the sentiments of our covid-19 tweets since we implement the roberta model to classify ternary sentiment labels the drop in performance compared to liu et al 20 is within expectation the confusion matrix for our roberta model is given in table iii   overall a very small percentage of positive sentiments are expressed among the covid-19 tweets lightest-colored blocks of figure 6  in addition more than one in five people of our user samples discussed covid-19 related issues in a negative light considering that 2281 out of a million of the us population are physically affected by covid-19 which is already dangerous the amount of negativity exhibited on twitter is very alarming as well evidently not only is the covid-19 pandemic a health and safety hazard it also has gloom-ridden impacts on our society moreover for the topic related to the chinese virus controversy there is an overwhelming number of negative responses we can see from figure 3 that racist is the 3rd most frequent keywords of this topic which suggests that many of twitter users associated calling coronavirus the chinese virus with racism 2 college students respond more negatively to covid-19 an important trend that outweighs the rest of our results is that there is a significantly higher percentage among the student population expressing negative sentiments towards the central issues of covid-19 especially on news related to the spread of the pandemic and social distancing this shows that our analysis of the data is consistent with the our speculation on the impacts of the covid-19 crisis on our younger population college students are likely to express negative feelings towards how social distancing and school closure are affecting their work and study environments moreover they tend to be subject to more negative emotions upon receiving news of the outbreak which might be due to the subsequent implications of these issues on those more related to their lives 3 negativity among college students through the topic modeling microscope we focus on examining the subtopics of school closing which is of high concern among college students and on social distancing and china controversy where highest gaps in the negative sentiments between college students and the general population are observed 145 and 138 absolute difference in percentage of negative tweets respectively in addition only negative and positive tweets are considered because they provide the most meaningful contexts associated with their sentiments in general nonneutral tweets on the social distancing and school closing topics express worrying emotions towards covid-19 and all the tweets revealing concerns on school closure are negative moreover many students exhibited aggression to the foreign community blaming them for the current disruptions in their lives as a result of social distancing college students also disclosed details of their online learning experience and mostly showed dislikes for remote learning 813 to reflect on the responses of college students on covid-19 in a more positive light it is encouraging that our college community remains aware and vocal on the racism problem related to the chinese virus controversy which sends a powerful message on the publics intolerance of racist behaviors on social media for the betterment of our society these findings shed new lights on an emerging direction of racism problems in the us during the covid-19 outbreak especially related to the east asian community in addition to the existing discussions in the literature that focus primarily on discrimination towards african american asian american and more recently muslim population 33 - 35  also in addition to addressing the uneasy feelings and barriers restricting the learning experience among students during the crisis the prevention of racism-charged hate speeches is an important task for educational institutions to protect their students vii conclusions and future work we have analyzed 73787 tweets from 12776 twitter college followers who posted tweets related the covid-19 pandemic in terms of the outstanding topics on several social issues we find significant differences in the sentiments expressed towards those topics between the users who are identified as colleges students and those of the general population college students tend to focus their discussions on topics closely surrounding their living environment such as school closure and local news although the percentages of positive covid-19 tweets are very low for both demographics college students are shown to be significantly more negative in addition microscopic examination of the positive and negative tweets reveals their overwhelmingly troubled feelings amidst the spread of covid-19 as well as unfavorable reactions to the disruption in their lives such as racism-charged aggression moreover we discover a shift in the target of racism during covid-19 towards the east asian community which the majority of college students and the general public are against since high accuracy is achieved in both of our demographic and sentiment classification models future studies may collect larger datasets to achieve better performance in addition this research mainly focuses on high-level attributes of tweets such as topic models and sentiments in understanding the characteristics of users who discussed social issues associated with covid-19 analysis on more fine-grained linguistic information such as emotion hate speech and racism detection can be performed to gain further insights on the more specific covid-19 related issues detailed in our study  computer vision for covid-19 control a survey anwaar ulhaq asim khan douglas gomes manoranjan paul  the covid-19 pandemic has triggered an urgent need to contribute to the fight against an immense threat to the human population computer vision as a subfield of artificial intelligence has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling covid-19 in response to this call computer vision researchers are putting their knowledge base at work to devise effective ways to counter covid-19 challenge and serve the global community new contributions are being shared with every passing day it motivated us to review the recent work collect information about available research resources and an indication of future research directions  into broad categories and provide comprehensive descriptions of representative methods in each category we aspire to give readers the ability to understand the baseline efforts and kickstart their work where others have left moreover we aim to identify new trends and ideas to build a more robust and well-planned strategy in this war of our times figure 1  a portrayal of current increase in research articles about coronavirus related research adapted from 1  our survey will include research papers in pre-print format due to time urgency imposed by this disease it is not an optimal approach due to the risk of lower quality and work without due validation many of the works have not been put into clinical trial as it is time-consuming nevertheless our intention here is to share ideas from a single platform while highlighting the computer vision community efforts we hope that our reader is aware of these contemporary challenges we follow a bottom-up approach to describing the research problems that need addressing first we start with disease diagnosis discuss disease prevention and control followed by treatment-related computer vision research work section 2 describes the overall taxonomy of computer vision research areas by classifying these efforts into three classes section 3 provides a detailed description of each research area relevant papers and a brief description of representative work section 4 describes available resources including research datasets their links deep learning models and codes section 5 provides the discussion and future work directions followed by concluding remarks and references the novel coronavirus sars-cov-2 previously known as 2019-ncov  is the seventh member of the coronaviridae family of viruses which are enveloped non-segmented positive-sense rna viruses 33  the mortality rate of covid-19 is less than that of the severe acute respiratory syndrome sars and middle east respiratory syndrome mers coronavirus diseases 10 for sars-cov and 37 for mers-cov however it is highly infectious and the number of cases has been increasing rapidly 62  the disease outbreak first reported in wuhan the hubei province of china 33  after several cases of pneumonia with unknown causes were reported on 31 december 2019 a novel coronavirus was discovered as the causative organism through deep sequencing analysis of samples of patients respiratory tract at chinese facilities on 7 january 2020 62  the outbreak was declared a public health emergency of international concern on 30 january 2020 on 11 february 2020 the world health organization whoannounced a name for the new coronavirus disease covid-19 it was officially being considered pandemic after 11 march announcement by who 75 in this section we describe the classification of computer vision techniques that try to counter the menace of covid-19 for better comprehensibility we have classified them into three key areas of research i diagnosis and prognosis ii disease prevention and control and iii disease treatment and management this taxonomy is shown in figure 2  in the following subsections we discuss the research fields the relevant papers and present a brief representative description of related works  an essential step in this fight is the reliable faster and affordable diagnostic process that can be readily accessible and available to the global community according to cambridge dictionary 80  diagnosis is the making of a judgment about the exact character of a disease or other problem esp after an examination or such a judgment and prognosis is a doctors judgment of the likely or expected development of a disease or of the chances of getting better currently reverse transcriptase quantitative polymerase chain reaction rt-qpcr tests are considered as the gold standard for diagnosing covid-19 71  during such a test small amounts of viral rna are extracted from a nasal swab amplified quantified virus detection is then performed using a fluorescent dye although accurate the test is time-consuming and manual which limits its availability in large scales some studies have also shown false-positive pcr testing 10  an alternative approach is the use of radiology examination that uses computed tomography ct imaging 3  a chest ct scan is a non-invasive test conducted to obtain a precise image of a patients chest it uses an enhanced form of x-ray technology providing more detailed images of the chest than a standard x-ray it produces images that include bones fats muscles and organs giving physicians a better view which is crucial when making accurate diagnoses there are two types of chest ct scan namely the high-resolution and spiral chest ct scan 77  the high-resolution chest ct scan provides more than a slice or image in a single rotation of the x-ray tube the spiral chest ct scan application involves a table that continuously moves through a tunnel-like hole while the x-ray tube follows a spiral path the advantage of the spiral ct is that it is capable of producing a three-dimensional image of the lungs images is shown in figure 3  as the identification of disease features is time-consuming even for expert radiologists computer vision can help by automating such a process to date various ct-scanning automated approaches have been proposed 3 85  to discuss the approach and performance of the computer vision ct-based disease diagnosis we have selected some recent representative works that provide an overview of their effectiveness it is worth noting that they have been presenting different performance metrics and using a diverse number of images and datasets these practices make their comparison very challenging some of the metrics include accuracy sensitivity specificity area under curve auc positive predictive value ppv negative predictive value npv and f1 score a quick elucidation on their definition can be useful the accuracy of a method determines how correct the values are predicted the precision determines the reproducibility of the measurement or how many of the predictions are correct recall shows how many of the correct results are discovered f1-score uses a combination of precision and recall to calculate a balanced average result the first class of work discussed here approaches diagnosis as a segmentation problem jun chen et al 11 has proposed a ct image dataset of 46096 images of both healthy and infected patients labeled by expert radiologists it was collected from 106 patients admitted with 51 confirmed covid-19 pneumonia and 55 control patients the work used deep learning models for segmentation only so that it could identify the infected area in ct images between healthy and infected patients it was based on unet semantic segmentation model 86  used to extract valid areas in the images it used 289 randomly selected ct images and tested it on other 600 randomly selected ct images the model achieved a per-patient sensitivity of 100 specificity of 9355 accuracy of 9524 ppv positive prediction value of 8462 and npvnegative prediction value of 100 in the retrospective dataset it resulted in a per-image sensitivity of 9434 specificity of 9916 accuracy of 9885 ppv of 8837 and npv of 9961 the trained model from this study was deployed at the renmin hospital of wuhan university wuhan hubei province china to accelerate the diagnosis of new covid-19 cases it was also open-sourced on the internet as to enable rapid review of new cases in other locations a cloud-based open-access artificial intelligence platform was constructed to provide support for detecting covid-19 pneumonia worldwide for this purpose a website has been made available to provide free access to the present model at http1214075149znyx-ncovindex the second type of work considered covid-19 as a binary classification problem lin li 40 proposed covnet to extract visual features from volumetric chest ct using transfer learning on the resnet50 lung segmentation was performed as a pre-processing task using the u-net model it used 4356 chest ct exams from 3322 patients from the dataset collected from 6 hospitals between august 2016 and february 2020 the sensitivity and specificity for covid-19 are 90 114 of 127 p-value0001 with 95 confidence interval ci of 95 ci 83 94 and 96 294 of 307 p-value0001 with 95 ci 93 98 respectively the model was also made available online for public use at httpsgithubcombkong999covnet the diagnosis problem was also approached as a 3-category classification task distinguishing healthy patients from those with other types of pneumonia and those with covid-19 song et al 40 used data from 88 patients diagnosed with the covid-19 101 patients infected with bacteria pneumonia and 86 healthy individuals it proposed dre-net relation extraction neural network based on resnet50 on which the feature pyramid network fpn 43 and the attention module were integrated to represent more fine-grained aspects of the images an online server is available for online diagnoses with ct images at httpbiomednsccgzcnserverncov2019 due to limited time available for annotations and labelling weakly-supervised deep learning-based approaches have also been developed using 3d ct volumes to detect covid-19 chuansheng zheng 85 proposed 3d deep convolutional neural network decovnet to detect covid-19 from ct volumes the weakly supervised deep learning model could accurately predict the covid-19 infectious probability in chest ct volumes without the need for annotating the lesions for training the ct images were segmented using a pre-trained unet it used 499 ct volumes for training collected from 13 december 2019 to 23 january 2020 and 131 ct volumes for testing collected from 24 january 2020 to 6 february 2020 the authors chose a probability threshold of 05 to classify covid-positive and covid-negative cases the algorithm obtained an accuracy of 0901 a positive predictive value of 0840 and a high negative predictive value of 0982 the developed deep learning model is available at httpsgithubcomsydney0zqcovid-19-detection  the drawback of using ct imaging is the need for high patient dose and enhanced cost 38  it makes digital chest x-ray radiography cxr as the imaging modality with the lower cost and wider availability for detecting chest pathology therefore automated diagnosis of covid-19 features in cxr will make it a highly useful diagnostic tool against the disease digital x-ray imagery computer-aided diagnosis is used for different diseases including osteoporosis 55  cancer 4 and cardiac disease 66  however as it is really hard to distinguish soft tissue with a poor contrast in x-ray imagery contrast enhancement is used as pre-processing step 36 21  lung segmentation of chest x-rays is a crucial and important step in order to identify lung nodules and various segmentation approaches are proposed in the literature 54 9 19 24  cxr examinations have shown consolidation in covid-19 infected patients in one study at hong kong 48  three different patients had daily cxr two of them showed progression in the lung consolidation over 3-4 days further cxr examinations show improvement over the subsequent two days the third patient showed no significant changes over an 8-day period however a similar study showed that the ground glass opacities in the right lower lobe periphery on the ct are not visible on the chest radiograph which was taken 1 hour apart from the first study however cxr is still recommended alongside ct for better radiological analysis various cxr-related automated approaches have been proposed the following section discusses the most salient work while table 2 presents a more systematic presentation of such methods to date many deep learning-based computer vision models for x-ray covid-19 were proposed one of the most significant development is the model covid-net 23 proposed by darwin ai canada in this work human-driven principled network design prototyping is combined with machine-driven design exploration to produce a network architecture for the detection of covid-19 cases from chest x-ray the first stage of the human-machine collaborative design strategy is based on residual architecture design principles the dataset used to train and evaluate covid-net is referred to as covidx 23 and comprise a total of 16756 chest radiography images across 13645 patient cases the proposed model achieved 924 accuracy 80 sensitivity for covid-19 diagnosis the initial network design prototype makes one of three classes a no infection normal b non-covid19 infection viral and bacterial and c covid-19 viral infection the goal is to aid clinicians to better decide which treatment strategy to employ depending on the cause of infection since covid-19 and non-covid19 infections require different treatment plans in the second stage data along with human-specific design requirements act as a guide to a design exploration strategy to learn and identify the optimal macro-and microarchitecture designs to construct the final tailor-made deep neural network architecture the proposed covidnet network diagram is shown in fig 4 and available publicly at httpsgithubcomlindawanggcovid-net ezz el-din hemdan et al 31 proposed the covidx-net based on seven different architectures of dcnns namely vgg19 densenet201 82  inceptionv3 resnetv2 inceptionresnetv2 xception and mobilenetv2 58  these models were trained on covid-19 cases provided by dr joseph cohen and dr adrian rosebrock available at httpsgithubcomieee8023covid-chestxray-dataset 17  tthe best model combination resulted in f1scores of 089 and 091 for normal and covid-19 cases similarly asmaa abbas et al 2 proposed a decompose transfer and compose detrac approach for the classification of covid-19 chest x-ray images the authors applied cnn features of pre-trained models on imagenet and resnet to perform the diagnoses the dataset consisted of 80 samples of normal cxrs with 4020 x 4892 pixels from the japanese society of radiological technology jsrt cohen jp covid-19 image data collection available at httpsgithubcomieee8023covid-chestxray-dataset 17  this model achieved an accuracy of 9512 with a sensitivity of 9791 a specificity of 9187 and a precision of 9336 the code is available at httpsgithubcomasmaa4maydetraccovid19 uncertainty-aware covid-19 classification and referral model was introduced by tbiraja ghoshal et al 27 with the proposed dropweights based on bayesian convolutional neural networks bcnn in order for covid-19 detection to be meaningful two tyes of predictive uncertainty in deep learning were used on a subsequent work 20  one of it is epistemic or model uncertainty accounts for the model parameters uncertainty as it does not take all of the aspects of the data into account or the lack of training data the other is aleatoric uncertainty that accounts for noise inherent in the observations due to class overlap label noise homoscedastic and heteroscedastic noise which cannot be reduced even if more data were to be collected bayesian active learning by disagreement bald 32  these are 3 chest radiographs selected out of the daily chest radiographs acquired in this patient the consolidation in the right lower zone on day 0 persist into day 4 with new consolidative changes in the right midzone periphery and perihilar region this midzone change improves on the day 7 film image adapted from 48  is based on mutual information that maximises the information between model posterior and predictions density functions approximated as the difference between the entropy of the predictive distribution and the mean entropy of predictions across samples a bccn model was trained on 68 posterior-anterior pa x-ray images of lungs with covid-19 cases from dr joseph cohens github repository 17  augmented the dataset with kaggles chest x-ray images pneumonia from healthy patients it achieved 8839 accuracy on the available dataset this work additionally recommended visualisation of distinct features as an additional insight to point prediction for a more informed decision-making 1024 process it used the saliency maps produced by various state-of-the-art methods eg class activation map cam 59  guided backpropagation and guided gradient and gradients to show more distinct features in the csr images  who has provided some guidelines on infection prevention and control ipc strategies for use when infection with a novel coronavirus is suspected 49  major ipc strategies to limit transmission in health care settings include early recognition and source control applying standard precautions for all patients implementing additional empiric precautions like airborne precautions for suspected cases of covid-19 implementing administrative controls and using environmental and engineering controls computer vision applications are providing excellent support for the implementation of ipc strategies the use of masks or protective equipment to limit the virus spread was a strategy identified in the early stage of disease progression some countries china being the most prominent example implemented it as a control strategy computer vision-based systems greatly facilitated such implementation zhongyuan wang et al 74 proposed masked face recognition approach using a multigranularity masked face recognition model resulting in 95 accuracy on a masked face images dataset the data was made public for research and provide three types of masked face datasets including masked face detection dataset mfdd 83  real-world masked face recognition dataset rmfrd and simulated masked face recognition dataset smfrd 18  infrared thermography was also recommended as an early detection strategy for infected people especially in crowns like passengers on an airport a comprehensive review of medical applications of infrared thermography is provided by bb lahiri 74 39  including fever screening somboonkaew et al 63 proposed a mobile-platform for an automatic fever screening system based on infrared forehead temperature best practices for standardized performance and testing of infrared thermographs intended for fever screening are discussed by ghassemi et al 26  negishi t 47 proposed an infection screening system using thermography and ccd camera with good stability and swiftness for non-contact vital-signs measurement by feature matching and music algorithm earlier for sard spread control chiu et al 15 proposed a computer vision systems help in fever screening which was used in earlier outbreaks of sars from 13 april to 12 may 2003 72327 patients and visitors passed through the only entrance allowed at tmu-wfh where a thermography station was in operation additional miscellaneous approaches for prevention and control are also worth noting an example is pandemic drones using remote sensing and digital imagery which were recommended for identifying infected people al-naji et al 5 have used such a system for remote life sign monitoring in disaster management in the past a similar application is to use vision-guided robot control for 3d object recognition and manipulation moreover 3d modelling and printers are helping to maintain the supply of healthcare equipment in this troubled time joshua m pearce 52 discusses reprap-class 3-d printers and open-source microcontrollers the applications are relevant since mass distributed manufacturing of ventilators has the potential to overcome medical supply shortages lastly germ scanning is an important step against combating covid-19 edouard a hay 30 have proposed a convolutional neural network for germ scanning such as the identification of bacteria light-sheet microscopy image data with more than 90 accuracy over a period of one month hundred and five patients or visitors were detected to have a thermographic fever detection edouard a hay 30 convolutional neural networks for identification of bacteria github repository httpsgithubcomrplab bacterial-identification light sheet microscopy image data over 90 accuracy to date there is no specific treatment for disease caused by the covid-19 virus however many of the symptoms can be treated and therefore treatment will depend on the patients clinical condition clinical management practices can be improved with practices like classifying patients based on the severity of the disease and providing them with immediate medical care due to the multidisciplinary nature of computer vision it has the potential to support various teams that are currently working on creating vaccination for the disease as well as clinical management an essential part of the fight against the virus is clinical management which can be done by identifying patients that are critically ill so that they get immediate medical attention or ventilator support a disease progression score is recommended to classify different types of infected patients in 29  it is called corona score and is calculated by measurements of infected areas and the severity of disease from ct images the corona score measures the progression of patients over time and it is computed by a volumetric summation of the network-activation maps graeme maclaren 45 supports that radiological evidence can also be an important tool to distinguish critical ill patients yunlu wang 73 used depth camera and deep learning as abnormal respiratory patterns classifier that may contribute to large-scale screening of people infected with the virus accurately and unobtrusively respiratory simulation model rsm is first proposed to fill the gap between the large amount of training data and scarce real-world data they proposed gru neural network with bidirectional and attentional mechanisms bi-at-gru to classify six clinically significant respiratory patterns eupnea tachypnea bradypnea biots cheyne-stokes and central-apnea to identify critically ill patients the proposed model can classify the respiratory patterns with accuracy precision recall and f1 of 945 944 951 and 948 respectively demo videos of this method working in situations of one subject and two subjects can be accessed online httpsdoiorg106084m9figshare11493666v1 the cov spike s glycoprotein is a key target for vaccines therapeutic antibodies and diagnostics that can guide future decisions the virus binds to host cells through its trimeric spike glycoprotein using biophysical assays the daniel wrapp et al 76 showed that this protein binds at least ten times more tightly than the corresponding spike protein of severe acute respiratory syndrome sars-cov to their common host cell receptor these studies provide valuable information to guide the development of medical countermeasures for 2019-ncov quantitative structure-activity relationship qsar analysis has perspectives on drug discovery and toxicology 53  it employs structural quantum chemical and physicochemical features calculated from molecular geometry as explanatory variables predicting physiological activity deep feature representation learning can be used for qsar analysis by incorporating 360images of molecular conformations into deep learning yoshihiro uesaw 68 proposed qsar quantitative structure-activity relationship analysis using deep learning based on a novel molecular image input technique such techniques can be used for drug discovery and can pave the way for vaccine development figure 8   corona score that is calculated by measurements of infected areas and severity of disease from ct images it can be used for identifying patients that are critical ill so that they get immediate medical attention image adapted from 29  it is a combination of data provided by many parties the radiological society of north america rsna others involved in the rsna pneumonia detection challenge dr joseph paul cohen and the team at mila involved in the covid-19 image data collection project for making data available to the global community  chestx-ray8 72 -the chest x-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases a tremendous number of x-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals picture archiving and communication systems pacs available at httpsnihccappboxcomvchestxray-nihcc other images  mfdd dataset can be used to train an accurate masked face detection model which serves for the subsequent masked face recognition task rmfrd dataset includes 5000 pictures of 525 people wearing masks and 90000 images of the same 525 subjects without masks to the best of our knowledge this is currently the worlds largest real-world masked face dataset smfrd is a simulated masked face data set covering 500000 face images of 10000 subjects these datasets are available at httpsgithubcomx-zhangyangreal-world-masked-face-dataset  thermal images datasets -there is no dataset of thermals for high fever screening however a fully annotated thermal face database and its application for thermal facial expression recognition were proposed by marcin kopaczka 37  information on further ideas of related data that can be figured out by using such systems is available at httpswwwflircomaudiscoverpublic-safetythermal-imaging-for-detecting-elevated-bodytemperature in this article we presented an extensive survey of computer vision efforts and methods to combat the covid-19 pandemic challenge and also gave a brief review of the representative work to date we divide the described methods into three categories based on their role in disease control computed tomography ct scans x-ray imagery and prevention and control we provide detailed summaries of preliminary representative work including available resources to facilitate further research and development we hope that in this first survey on computer vision methods for covid-19 control with extensive bibliography content one can find give valuable insight into this domain and encourage new research however this work can be considered only as an early review since many computer vision approaches are being proposed and tested to control covid-19 pandemic at the current time we believe that such efforts will be having a far-reaching impact with positive results to periods during the outbreak and post the covid-19 pandemic  deep sentiment classification and topic discovery on novel coronavirus or covid-19 online discussions nlp using lstm recurrent neural network approach hamed jelodar yongli wang rita orji  internet forums and public social media such as online healthcare forums provide a convenient channel for users peoplepatients concerned about health issues to discuss and share information with each other in late december 2019 an outbreak of a novel coronavirus infection from which results in the disease named covid-19 was reported and due to the rapid spread of the virus in other parts of the world the world health organization declared a state of emergency in this paper we used automated extraction of covid-19-related discussions from social media and a natural language process nlp method based on topic modeling to uncover various issues related to covid-19 from public opinions moreover we also investigate how to use lstm recurrent neural network for sentiment classification of covid-19 comments our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding covid-19 and to guide related decision-making 2 hamed jelodar 1 et al  online forums such as reddit enable healthcare service providers to collect peoplepatient experience data these forums are valuable sources of peoples opinions which can be examined for knowledge discovery and user behaviour analysis in a typical sub-reddit forum a user can use keywords and apply search tools to identify relevant questionsanswers or comments sent in by other reddit users moreover a registered user can create a topic or post a new question to start discussions with other community members in answering the questions users reflect and share their views and experiences in these online forums people may express their positive and negative comments or share questions problems and needs related to health issues by analysing these comments we can identify valuable recommendations for improving health-services and understanding the problems of users in late december 2019 the outbreak of a novel coronavirus causing covid-19 was reported 1  due to the rapid spread of the virus the world health organization declared a state of emergency in this paper we focused on analysing covid-19related comments to detect sentiment and semantic ideas relating to covid-19 based on the public opinions of people on reddit specifically we used automated extraction of covid-19-related discussions from social media and a natural language process nlp method based on topic modeling to uncover various issues related to covid-19 from public opinions the main contributions of this paper are as follows -we present a systematic framework based on nlp that is capable of extracting meaningful topics from covid-19-related comments on reddit -we propose a deep learning model based on long short-term memory lstm for sentiment classification of covid-19-related comments which produces better results compared with several other well-known machine-learning methods -we detect and uncover meaningful topics that are being discussed on covid-19-related issues on reddit as primary research -we calculate the polarity of the covid-19 comments related to sentiment and opinion analysis from 10 sub-reddits our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding covid-19 and to guide related decision-making overall the paper is structured as follows first we provide a brief introduction to online healthcare forums discussion of covid-19-related issues and some similar works is provided in section 2 in section 3 we describe the data pre-processing methods adopted in our research and the nlp and deeplearning methods applied to the covid-19 comments database next we present the results and discussion finally we conclude and discuss future works based on nlp approaches for analysing the online community in relation to the topic of covid-19 machine and deep-learning approaches based on sentiment and semantic analysis are popular methods of analysing text-content in online health forums many researchers have used these methods on social media such as twitter reddit 2 - 7  and health information websites 8  9  for example halder and colleagues 10 focused on exploring linguistic changes to analyse the emotional status of a user over time they utilized a recurrent neural network rnn to investigate user-content in a huge dataset from the mental-health online forums of healthboardscom mcroy and colleagues 11 investigated ways to automate identification of the information needs of breast cancer survivors based on user-posts of online health forums chakravorti and colleagues 12 extracted topics based on various health issues discussed in online forums by evaluating user posts of several subreddits eg rdepression ranxiety from 2012 to 2018 vandam and colleagues 13 presented a classification approach for identifying clinic-related posts in online health communities for that dataset the authors collected 9576 thread-initiating posts from webmd which is a health information website the covid-19-related comments from an online healthcare-oriented group can be considered potentially useful for extracting meaningful topics to better understand the opinions and highlight discussions of peopleusers and improve health strategies although there are similar works regarding various health issues in online forums to the best of our knowledge this is the first study to utilize nlp methods to evaluate covid-19-related comments from sub-reddit forums we propose utilizing the nlp technique based on topic modeling algorithms to automatically extract meaningful topics and design a deep-learning model based on lstm rnn for sentiment classification on covid-19 comments and to understand the positive or negative opinions of people as they relate to covid-19 issues to inform relevant decision-making this section clarifies the methods used to investigate the main contributions to this study which proposes the use of an unsupervised topic model with a collaborative deep-learning model based on lstn rnn to analyse covid-19-related comments from sub-reddits the developed framework shown in fig 2  uses sentiment and semantic analysis for mining and opinion analysis of covid-19-related comments reddit is an american social media a discussion website for various topics that includes web content ratings in this social media users are able to post questions and comments and to respond to each other regarding different subjects such as covid-19 the posts are organised by subjects created by online users called subreddits which cover a variety of topics like news science healthcare video books fitness food and image-sharing this website is an ideal source for collecting healthrelated information about covid-19-related issues this paper focuses on covid-19-related comments of 10 sub-reddits based on an existing dataset as the first step in producing this model one of the most important steps in pre-processing covid-19-related comments is removing useless wordsdata which are defined as stop-words in nlp from pure text moreover we also decreased the dimensionality of the features space by eliminating stop-words for example the most common words in the text comments are words that are usually meaningless and do not effectively influence the output such as articles conjunctions pronouns and linking verbs some examples include am is are they the these i that and them text-document modeling in nlp is a practical technique that represents an individual document and the set of text-documents based on terms appearing in the textdocuments topic modeling based on latent dirichlet allocation lda 14 is one type of document modelling approach as a third step we utilized topic modeling based on an lda topic model and gibbs sampling 15 for semantic extraction and latent topic discovery of covid-19-related comments covid-19 comments however can depend on various subjects that are discussed by reddit users in this step we can detect and discover these meaningful subjects or topics therefore based on the lda model we considered a collection of documents such as covid-19related comments and words as topics k where the discrete topic distributions are drawn from a symmetric dirichlet distribution the probability of observed data step 3 semantic processing step 4 deep learning and comment classification step 1 covid- 19 comments defining and applying stopwords  determined  parameters of topic dirichlet prior and also considered parameters of word dirichlet prior as  m is the number of text-documents and n is the vocabulary size moreover  was determined for the corpus-level topic distributions with a pair of dirichlet multinomials   was also determined for the topic-word distributions with a pair of dirichlet multinomials in addition the document-level variables were defined as  d  which may be sampled for each document the wordlevel variables z dn  w d n  were sampled in each text-document for each word 14  algorithm 1 pre-processing and removing the noise to prepare the input data word-probability under the topic of sampling --or the word distribution for topic k among covid-19-related comments 4   dirichlet 5 end for 6 for each covid-19-related comments d  1     d do 7 the topic distribution for document m 8 d  dirichlet 9 for per word in covid-19-related content-document d do 10 sampling the distribution of topics in the covid-19-related comments-documents to obtain the topic of the word z d  mul 11 word-sampling undert the topic w d  mul 12 end for 13 end for algorithm 2 describes a general process as part of our framework for extracting latent topics and semantic mining the input data consists of the number of covid-19-related comments as the context of the document line 1 processes the pure-data to eliminate noise and stop-words based on algorithm 1 lines 2-5 compute the probability of the word distribution from topic ki lines 6-11 compute the probability of the topic distribution from the covid-19-content-document m i as highlighted in equation 1 the variables  m  w n are computed for document-level and word-level of the framework in more detail the lda handles topics as multinomial distributions in documents and words as a probabilistic mixture of a pre-determined number from latent topics lines 1-3 of algorithm 3 show the semantic mining to extract the latent topics we then used a sorting function to determine the recommended highlighted topics because the gibbs sampling method is used in this step the time requested for model inference can be specified as the sum of the time for inferring lda therefore the time complexity for lda is on k where n denotes the total size of the corpus covid-19-related comments and k is the topic number  deep neural networks have been successfully employed for different types of machinelearning tasks such as nlp-based methods utilizing sentiment aspects for deep classification 16 - 21  deep neural networks are able to model high-level abstractions and to decrease the dimensions by utilizing multiple processing layers based on complex structures or to be combined with non-linear transformations rnns are popular models with demonstrated importance and strength in most nlp works 22 - 24  the purpose of rnns is to use consecutive information and the output is augmented by storing previous calculations in fact rnns are equipped with a memory function that saves formerly calculated information basic rnns however have some challenges due to gradient vanishing or exploding and they are unable to learn long-term dependencies lstm 25  26 units have the benefit of being able to avoid this challenge by adjusting the information in a cell state using 3 different gates the formula for each lstm cell can be formalized as the forget  f t  input i t  and output o t  gates for each lstm cell are determined by these 3 equations eqs 2-4 respectively in an lstm layer the forget gate determines which previous information from the cell state is forgotten the input gate controls or determines the new information that is saved in the memory cell the output gate controls or determines the amount of information in the internal memory cell to be exposed the cell-memoryinput block equations are in which c i is the cell state z t is the hidden output and x t is an input vector w and b are the weight matrix and the bias term respectively  is sigmoid and  is tanh is element-wise multiplication as the last step of this framework an lstm model was utilised to assess the covid-19-related comments of online users who posted on reddit in order to recognize the emotionsentiment elicited from these comments we designed two lstmlayers and for pre-trained embeddings considered the glove-50 dimension 1  which were trained over a large corpus of covid-19-related comments  figure 3  the processed text from the covid-19-related comments however is changed to vectors with a fixed dimension by converting pre-trained embeddings moreover covid-19 comments can also be described as a characters-sequence with its corresponding dimension creating a matrix 27  in this section we provide a detailed description of the data collection and experimental results followed by a comprehensive discussion of the results we assessed 563079 covid-19-related comments from reddit the dataset was collected between january 20 2020 and march 19 2020 the full dataset is available at kaggle website 2  we used mallet 3 to implement the inference and capture the lda topic model to retrieve latent topics we used the python library keras 4 to implement our deep-learning model according to table 1 and 2 and figures 4-8  the following observations were made topics 85 and 18 had a similar concept in peopleinfection topic 85 included words referring to people such as people virus day bad stop news worse sick spread and family this topic is the first ranked topic discovered from the generated latent topics in which most users express their opinion and comment on this issue based on table 1 and figure 5 a in this topic the terms people and virus were the most highlighted words with word-weights of 01295 and 00301 respectively also we can see the importance of the term family from this topic in addition topic 18 contains the telling words virus people symptoms infection cases disease pneumonia coronavirus and treatment other revealing words in topic 18 included people infection and treatment these terms initially suggest a set of user comments about treatment issues moreover the sentiment analysis of the terms suggest that negative words were more highlighted than positive words topic 63 also addresses healthcare and hospital issues with the most frequent term being hospital words such as hospital medical healthcare patients care and city were included the terms hospital medical and healthcare were the most highlighted words with word-weights of 00561 00282 and 00278 respectively other words worth mentioning that were seen for this topic were person patient staff workers and emergency topic 63 was assigned as medical staff issues topic 4 included words relating to money such as pay money companies insurance paid free cost tax years and employees moreover the sentiment analysis of the terms suggested that negative words were more highlighted than positive words topic 30 covers users comments concerning issues related to feelings and hopes and highlight words such as good hope feel house safe hard months fine live and friend moreover sentiment analysis of terms suggested that positive words were more highlighted than negative words positive words such as good hope safe fine kind and friend thus pertain to the phenomenon of positive feelings for topic 93 we can see that there was a clear focus on people age and covid issues with the top words being covid young risk fever immune age sick cough life cold elderly and older the terms covid young and risk were the most highlighted words with wordweights of 00299 00222 and 00218 respectively and this topic had negative polarity topic 48 also addresses covid-19 testing issues and contains words like people testing government country tested test infected home covid and pandemic based on the results the terms people and testing were the most highlighted words with word weights of 00447 and 00337 respectively moreover the opinion words based on sentiment analysis scored high in negative polarity for topic 17 the top terms of this topic were coronavirus quarantine stupid happening shit watch and dangerous thus pertaining to the phenomenon quarantine issues the terms coronavirus and quarantine were the most highlighted words with word-weights of 00353 and 00346 respectively sentiment analysis is a practical technique in nlp for opinion mining that can be used to classify textcomments based on word polarities 28 - 30  this technique has many applications in various disciplines such as opinion mining in online healthcare communities 31 - 33  we obtained the sentiment of the covid-19-related comments using the sentistrength algorithm 34 - 36  therefore with all covid-19-related comments tagged with sentiment scores we calculated the average sentiment of the entire dataset along with comments mentioning only 10 covid-19 sub-reddits the main objective of this analysis was to identify the overall sentiment figure 9 shows the sentiment of all comments in the database along with the average sentiment of comments containing the terms covid-19 for each of the polar comments in our labelled dataset we assigned negative and positive scores utilizing sentistrength and employed the various scores directly as rules for building inference about the polaritysentiment of the covid-19 comments based on sentistrength we determined that a comment was positive if the positive sentiment score was greater than the negative sentiment score and also considered a similar rule for determining a positive sentiment for example a score of 5 and -4 indicates positive polarity and a score of 4 and -6 indicates negative polarity moreover if the sentiment scores were equal such as -1 and 1 4 and -4 we determined that the comment was neutral to prepare the dataset to automatically classify the sentiment of the covid-19 comments for all of the data we labelled each of the comments as very positive positive very negative negative and neutral based on the sentiment score obtained using the sentistrength method the training set had 338666 covid-19-related comments and the testing set had 112888 comments in this experiment we evaluated the proposed lstm-model and also supervised machine-learning methods using the support vector machine senti-ml1 naive bayes senti-ml2 logistic regression senti-ml3 k nearest neighbors senti-ml4 techniques figure 4 shows the accuracy of the best model for classifying a covid-19 comment as either a very positive positive very negative negative or neutral sentiment our approach based on the lstm model which classified all covid-19 comments in the majority class achieved 8115 accuracy which was higher than that of traditional machinelearning algorithms we believe that the sentiment and semantic techniques can provide meaningful results with an overview of how userspeople feel about the disaster analysing social media comments on platforms such as reddit could provide meaningful information for understanding peoples opinions which might be difficult to achieve through traditional techniques such as manual methods the text content on reddit has been analysed in various studies 37 - 39  to the best of our knowledge this is the first study to analyse comments by considering semantic and sentiment aspects of covid-related comments from reddit for online health communities overall we extended the analysis to check whether we could find a dependency of semantic aspects of user-comments for different issues on covid-19-related topics in this case we considered an existing dataset that included 563079 comments from 10 sub-reddits we found and detected meaningful latent topics of terms about covid-19 comments related to various issues thus user comments proved to be a valuable source of information as shown in tables 1 and 2 and figures 4-8  a variety of different visualisations was used to interpret the generated lda results as mentioned lda is a probabilistic model that when applied to documents hypothesises that each document from a collection has been generated as a mixture of this research was limited to english-language text which was considered a selection criterion therefore the results do not reflect comments made in other languages in addition this study was limited to comments retrieved from january 20 2020 and march 19 2020 therefore the gap between the period in which the research was being completed and the time-frame of our study may have somewhat affected the timeliness of our results overall the study suggests that the systematic framework by combining nlp and deep-learning methods based on topic modelling and an lstm model enabled us to generate some valuable information from covid-19-related comments these kinds of statistical contributions can be useful for determining the positive and negative actions of an online community and to collect user opinions to help researchers and clinicians better understand the behaviour of people in a critical situation regarding future work we plan to evaluate other social media such as twitter using hybrid fuzzy deep-learning techniques 40 - 41 that can be used in the future for sentiment level classification as a novel method of retrieving meaningful latent topics from public comments to our knowledge this is the first study to analyse the association between covid-19 commentssentiment and semantic topics on reddit the main goal of this paper however was to show a novel application for nlp based on an lstm model to detect meaningful latent-topics and sentiment-comment-classification on covid-19related issues from healthcare forums such as sub-reddits we believe that the results of this paper will aid in understanding the concerns and needs of people with respect to covid-19-related issues moreover our findings may aid in improving practical strategies for public health services and interventions related to covid-19  deep sentiment classification and topic discovery on novel coronavirus or covid-19 online discussions nlp using lstm recurrent neural network approach hamed jelodar yongli wang rita orji  huang hucheng hucheng huang  internet forums and public social media such as online healthcare forums provide a convenient channel for users peoplepatients concerned about health issues to discuss and share information with each other in late december 2019 an outbreak of a novel coronavirus infection from which results in the disease named covid-19 was reported and due to the rapid spread of the virus in other parts of the world the world health organization declared a state of emergency in this paper we used automated extraction of covid-19-related discussions from social media and a natural language process nlp method based on topic modeling to uncover various issues related to covid-19 from public opinions moreover we also investigate how to use lstm recurrent neural network for sentiment classification of covid-19 comments our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding covid-19 and to guide related decision-making 2 hamed jelodar 1 et al  online forums such as reddit enable healthcare service providers to collect peoplepatient experience data these forums are valuable sources of peoples opinions which can be examined for knowledge discovery and user behaviour analysis in a typical sub-reddit forum a user can use keywords and apply search tools to identify relevant questionsanswers or comments sent in by other reddit users moreover a registered user can create a topic or post a new question to start discussions with other community members in answering the questions users reflect and share their views and experiences in these online forums people may express their positive and negative comments or share questions problems and needs related to health issues by analysing these comments we can identify valuable recommendations for improving health-services and understanding the problems of users in late december 2019 the outbreak of a novel coronavirus causing covid-19 was reported 1  due to the rapid spread of the virus the world health organization declared a state of emergency in this paper we focused on analysing covid-19related comments to detect sentiment and semantic ideas relating to covid-19 based on the public opinions of people on reddit specifically we used automated extraction of covid-19-related discussions from social media and a natural language process nlp method based on topic modeling to uncover various issues related to covid-19 from public opinions the main contributions of this paper are as follows -we present a systematic framework based on nlp that is capable of extracting meaningful topics from covid-19-related comments on reddit -we propose a deep learning model based on long short-term memory lstm for sentiment classification of covid-19-related comments which produces better results compared with several other well-known machine-learning methods -we detect and uncover meaningful topics that are being discussed on covid-19-related issues on reddit as primary research -we calculate the polarity of the covid-19 comments related to sentiment and opinion analysis from 10 sub-reddits our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding covid-19 and to guide related decision-making overall the paper is structured as follows first we provide a brief introduction to online healthcare forums discussion of covid-19-related issues and some similar works is provided in section 2 in section 3 we describe the data pre-processing methods adopted in our research and the nlp and deeplearning methods applied to the covid-19 comments database next we present the results and discussion finally we conclude and discuss future works based on nlp approaches for analysing the online community in relation to the topic of covid-19 machine and deep-learning approaches based on sentiment and semantic analysis are popular methods of analysing text-content in online health forums many researchers have used these methods on social media such as twitter reddit 2 - 7  and health information websites 8  9  for example halder and colleagues 10 focused on exploring linguistic changes to analyse the emotional status of a user over time they utilized a recurrent neural network rnn to investigate user-content in a huge dataset from the mental-health online forums of healthboardscom mcroy and colleagues 11 investigated ways to automate identification of the information needs of breast cancer survivors based on user-posts of online health forums chakravorti and colleagues 12 extracted topics based on various health issues discussed in online forums by evaluating user posts of several subreddits eg rdepression ranxiety from 2012 to 2018 vandam and colleagues 13 presented a classification approach for identifying clinic-related posts in online health communities for that dataset the authors collected 9576 thread-initiating posts from webmd which is a health information website the covid-19-related comments from an online healthcare-oriented group can be considered potentially useful for extracting meaningful topics to better understand the opinions and highlight discussions of peopleusers and improve health strategies although there are similar works regarding various health issues in online forums to the best of our knowledge this is the first study to utilize nlp methods to evaluate covid-19-related comments from sub-reddit forums we propose utilizing the nlp technique based on topic modeling algorithms to automatically extract meaningful topics and design a deep-learning model based on lstm rnn for sentiment classi-fication on covid-19 comments and to understand the positive or negative opinions of people as they relate to covid-19 issues to inform relevant decision-making this section clarifies the methods used to investigate the main contributions to this study which proposes the use of an unsupervised topic model with a collaborative deep-learning model based on lstn rnn to analyse covid-19-related comments from sub-reddits the developed framework shown in fig 2  uses sentiment and semantic analysis for mining and opinion analysis of covid-19-related comments reddit is an american social media a discussion website for various topics that includes web content ratings in this social media users are able to post questions and comments and to respond to each other regarding different subjects such as covid-19 the posts are organised by subjects created by online users called subreddits which cover a variety of topics like news science healthcare video books fitness food and image-sharing this website is an ideal source for collecting healthrelated information about covid-19-related issues this paper focuses on covid-19-related comments of 10 sub-reddits based on an existing dataset as the first step in producing this model one of the most important steps in pre-processing covid-19-related comments is removing useless wordsdata which are defined as stop-words in nlp from pure text moreover we also decreased the dimensionality of the features space by eliminating stop-words for example the most common words in the text comments are words that are usually meaningless and do not effectively influence the output such as articles conjunctions pronouns and linking verbs some examples include am is are they the these i that and them text-document modeling in nlp is a practical technique that represents an individual document and the set of text-documents based on terms appearing in the textdocuments topic modeling based on latent dirichlet allocation lda 14 is one type of document modelling approach as a third step we utilized topic modeling based on an lda topic model and gibbs sampling 15 for semantic extraction and latent topic discovery of covid-19-related comments covid-19 comments however can depend on various subjects that are discussed by reddit users in this step we can detect and discover these meaningful subjects or topics therefore based step 3 semantic processing step 4 deep learning and comment classification step 1 covid- 19 comments defining and applying stopwords on the lda model we considered a collection of documents such as covid-19related comments and words as topics k where the discrete topic distributions are drawn from a symmetric dirichlet distribution the probability of observed data d was computed and obtained from every covid-19-related comment in a corpus using the following equation determined  parameters of topic dirichlet prior and also considered parameters of word dirichlet prior as  m is the number of text-documents and n is the vocabulary size moreover  was determined for the corpus-level topic distributions with a pair of dirichlet multinomials   was also determined for the topic-word distributions with a pair of dirichlet multinomials in addition the document-level variables were defined as  d  which may be sampled for each document the wordlevel variables z dn  w d n  were sampled in each text-document for each word 14  algorithm 1 pre-processing and removing the noise to prepare the input data word-probability under the topic of sampling --or the word distribution for topic k among covid-19-related comments 4   dirichlet 5 end for 6 for each covid-19-related comments d  1     d do 7 the topic distribution for document m 8 d  dirichlet 9 for per word in covid-19-related content-document d do 10 sampling the distribution of topics in the covid-19-related comments-documents to obtain the topic of the word word-sampling undert the topic w d  mul 12 end for 13 end for algorithm 2 describes a general process as part of our framework for extracting latent topics and semantic mining the input data consists of the number of covid-19-related comments as the context of the document line 1 processes the pure-data to eliminate noise and stop-words based on algorithm 1 lines 2-5 compute the probability of the word distribution from topic ki lines 6-11 compute the probability of the topic distribution from the covid-19-content-document m i as highlighted in equation 1 the variables  m  w n are computed for document-level and word-level of the framework in more detail the lda handles topics as multinomial distributions in documents and words as a probabilistic mixture of a pre-determined number from latent topics lines 1-3 of algorithm 3 show the semantic mining to extract the latent topics we then used a sorting function to determine the recommended highlighted topics because the gibbs sampling method is used in this step the time requested for model inference can be specified as the sum of the time for inferring lda therefore the time complexity for lda is on k where n denotes the total size of the corpus covid-19-related comments and k is the topic number  deep neural networks have been successfully employed for different types of machinelearning tasks such as nlp-based methods utilizing sentiment aspects for deep classification 16 - 21  deep neural networks are able to model high-level abstractions and to decrease the dimensions by utilizing multiple processing layers based on complex structures or to be combined with non-linear transformations rnns are popular models with demonstrated importance and strength in most nlp works 22 - 24  the purpose of rnns is to use consecutive information and the output is augmented by storing previous calculations in fact rnns are equipped with a memory function that saves formerly calculated information basic rnns however have some challenges due to gradient vanishing or exploding and they are unable to learn long-term dependencies lstm 25  26 units have the benefit of being able to avoid this challenge by adjusting the information in a cell state using 3 different gates the formula for each lstm cell can be formalized as the forget  f t  input i t  and output o t  gates for each lstm cell are determined by these 3 equations eqs 2-4 respectively in an lstm layer the forget gate determines which previous information from the cell state is forgotten the input gate controls or determines the new information that is saved in the memory cell the output gate controls or determines the amount of information in the internal memory cell to be exposed the cell-memoryinput block equations are in which c i is the cell state z t is the hidden output and x t is an input vector w and b are the weight matrix and the bias term respectively  is sigmoid and  is tanh is element-wise multiplication as the last step of this framework an lstm model was utilised to assess the covid-19-related comments of online users who posted on reddit in order to recognize the emotionsentiment elicited from these comments we designed two lstmlayers and for pre-trained embeddings considered the glove-50 dimension 1  which were trained over a large corpus of covid-19-related comments figure 3  the processed text from the covid-19-related comments however is changed to vectors with a fixed dimension by converting pre-trained embeddings moreover covid-19 comments can also be described as a characters-sequence with its corresponding dimension creating a matrix 27  in this section we provide a detailed description of the data collection and experimental results followed by a comprehensive discussion of the results we assessed 563079 covid-19-related comments from reddit the dataset was collected between january 20 2020 and march 19 2020 the full dataset is available at kaggle website 2  we used mallet 3 to implement the inference and capture the lda topic model to retrieve latent topics we used the python library keras 4 to implement our deep-learning model according to table 1 and 2 and figures 4-8  the following observations were made topics 85 and 18 had a similar concept in peopleinfection topic 85 included words referring to people such as people virus day bad stop news worse sick spread and family this topic is the first ranked topic discovered from the generated latent topics in which most users express their opinion and comment on this issue based on table 1 and figure 5 a in this topic the terms people and virus were the most highlighted words with word-weights of 01295 and 00301 respectively also we can see the importance of the term family from this topic in addition topic 18 contains the telling words virus people symptoms infection cases disease pneumonia coronavirus and treatment other revealing words in topic 18 included people infection and treatment these terms initially suggest a set of user comments about treatment issues moreover the sentiment analysis of the terms suggest that negative words were more highlighted than positive words topic 63 also addresses healthcare and hospital issues with the most frequent term being hospital words such as hospital medical healthcare patients care and city were included the terms hospital medical and healthcare were the most highlighted words with word-weights of 00561 00282 and 00278 respectively other words worth mentioning that were seen for this topic were person patient staff workers and emergency topic 63 was assigned as medical staff issues topic 4 included words relating to money such as pay money companies insurance paid free cost tax years and employees moreover the sentiment analysis of the terms suggested that negative words were more highlighted than positive words topic 30 covers users comments concerning issues related to feelings and hopes and highlight words such as good hope feel house safe hard months fine live and friend moreover sentiment analysis of terms suggested that positive words were more highlighted than negative words positive words such as good hope safe fine kind and friend thus pertain to the phenomenon of positive feelings for topic 93 we can see that there was a clear focus on people age and covid issues with the top words being covid young risk fever immune age sick cough life cold elderly and older the terms covid young and risk were the most highlighted words with word-weights of 00299 00222 and 00218 respectively and this topic had negative polarity topic 48 also addresses covid-19 testing issues and contains words like people testing government country tested test infected home covid and pandemic based on the results the terms people and testing were the most highlighted words with word weights of 00447 and 00337 respectively moreover the opinion words based on sentiment analysis scored high in negative polarity for topic 17 the top terms of this topic were coronavirus quarantine stupid happening shit watch and dangerous thus pertaining to the phenomenon quarantine issues the terms coronavirus and quarantine were the most highlighted words with word-weights of 00353 and 00346 respectively sentiment analysis is a practical technique in nlp for opinion mining that can be used to classify textcomments based on word polarities 28 - 30  this technique has many applications in various disciplines such as opinion mining in online healthcare communities 31 - 33  we obtained the sentiment of the covid-19-related comments using the sentistrength algorithm 34 - 36  therefore with all covid-19-related comments tagged with sentiment scores we calculated the average sentiment of the entire dataset along with comments mentioning only 10 covid-19 sub-reddits the main objective of this analysis was to identify the overall sentiment figure 9 shows the sentiment of all comments in the database along with the average sentiment of comments containing the terms covid-19 for each of the polar comments in our labelled dataset we assigned negative and positive scores utilizing sentistrength and employed the various scores directly as rules for building inference about the polaritysentiment of the covid-19 comments based on sentistrength we determined that a comment was positive if the positive sentiment score was greater than the negative sentiment score and also considered a similar rule for determining a positive sentiment for example a score of 5 and -4 indicates positive polarity and a score of 4 and -6 indicates negative polarity moreover if the sentiment scores were equal such as -1 and 1 4 and -4 we determined that the comment was neutral to prepare the dataset to automatically classify the sentiment of the covid-19 comments for all of the data we labelled each of the comments as very positive positive very negative negative and neutral based on the sentiment score obtained using the sentistrength method the training set had 338666 covid-19-related comments and the testing set had 112888 comments in this experiment we evaluated the proposed lstm-model and also supervised machine-learning methods using the support vector machine senti-ml1 naive bayes senti-ml2 logistic regression senti-ml3 k nearest neighbors senti-ml4 techniques figure 4 shows the accuracy of the best model for classifying a covid-19 comment as either a very positive positive very negative negative or neutral sentiment our approach based on the lstm model which classified all covid-19 comments in the majority class achieved 8115 accuracy which was higher than that of traditional machinelearning algorithms we believe that the sentiment and semantic techniques can provide meaningful results with an overview of how userspeople feel about the disaster analysing social media comments on platforms such as reddit could provide meaningful information for understanding peoples opinions which might be difficult to achieve through traditional techniques such as manual methods the text content on reddit has been analysed in various studies 37 - 39  to the best of our knowledge this is the first study to analyse comments by considering semantic and sentiment aspects of covid-related comments from reddit for online health communities overall we extended the analysis to check whether we could find a dependency of semantic aspects of user-comments for different issues on covid-19-related topics in this case we considered an existing dataset that included 563079 comments from 10 sub-reddits we found and detected meaningful latent topics of terms about covid-19 comments related to various issues thus user comments proved to be a valuable source of information as shown in tables 1 and 2 and figures 4-8  a variety of different visualisations was used to interpret the generated lda results as mentioned lda is a probabilistic model that when applied to documents hypothesises that each document from a collection has been generated as a mixture of this research was limited to english-language text which was considered a selection criterion therefore the results do not reflect comments made in other languages in addition this study was limited to comments retrieved from january 20 2020 and march 19 2020 therefore the gap between the period in which the research was being completed and the time-frame of our study may have somewhat affected the timeliness of our results overall the study suggests that the systematic framework by combining nlp and deep-learning methods based on topic modelling and an lstm model enabled us to generate some valuable information from covid-19-related comments these kinds of statistical contributions can be useful for determining the positive and negative actions of an online community and to collect user opinions to help researchers and clinicians better understand the behaviour of people in a critical situation regarding future work we plan to evaluate other social media such as twitter using hybrid fuzzy deep-learning techniques 40 - 41 that can be used in the future for sentiment level classification as a novel method of retrieving meaningful latent topics from public comments to our knowledge this is the first study to analyse the association between covid-19 commentssentiment and semantic topics on reddit the main goal of this paper however was to show a novel application for nlp based on an lstm model to detect meaningful latent-topics and sentiment-comment-classification on covid-19related issues from healthcare forums such as sub-reddits we believe that the results of this paper will aid in understanding the concerns and needs of people with respect to covid-19-related issues moreover our findings may aid in improving practical strategies for public health services and interventions related to covid-19  artificial intelligence-powered search tools and resources in the fight against covid-19 larry kricka j sergei polevikov jason park y paolo fortina sergio bernardini daniel satchkov valentin kolesov maxim grishkov   the covid-19 pandemic has created unprecedented challenges for the medical and clinical diagnostic community the fight against covid-19 is being supported by a number of databases and artificial intelligence ai-based initiatives aimed at assessing dissemination of the disease 1 aiding in detection and diagnosis minimizing the spread of the disease and facilitating and accelerating research globally 2-7 prominent among these initiatives are the covid-19 open research dataset cord-19 8-10 and databases curated by the cdc 1112 nlm 13 and the who 14 ai-powered tools such as those from wellai 1516 and the allen institute for ai scisight 17-19 and contact tracing based on mobile communication technology 2021 the cord-19 dataset has resulted from a partnership between the semantic scholar team at the allen institute for ai and leading research groups chan zuckerberg initiative georgetown universitys center for security and emerging technology microsoft research the kaggle ai platform google and the national library of medicinenational institutes of health in coordination with the white house office of science and technology policy publications in the collection are sourced from pubmed central the biorxiv and medrxiv preprint servers and the who covid-19 database cord-19 is freely available downloadable and it is updated weekly the collection currently contains over 128000 publications with over 59000 full text as of 26 may 2020 on the disease covid-19 and the virus sars-cov-2 and related coronaviruses it is part of a call to action to the ai community to develop ai techniques in order to generate new insights to assist in the fight against covid-19 9 this call to action has been informed by a series of tasks described in the form of a series of questions that are listed in table 1 22 wellai has developed a machine learning ml search and analytics tool based on four neural networks and incorporating the complete list of nih medical categories unified medical language system umls semantic types for interrogation of the cord-19 dataset and this is available at httpswellaihealthcovid 16 it is now widely agreed that ml has significant applications in the physical and biological sciences 28 in the wellai covid-19 application a subset of ml -- ie neural networks  is being used neural networks facilitate discovery of highly complex and nonlinear relationships between sets of variables without having to search for a closed form mathematical solution neural networks can contain tens of thousands to millions of variables and this is the basis of their power the complexity of relationships neural networks can uncover is difficult to fathom but is enabled by an ever-increasing computing power somewhat surprisingly one of the biggest trends of the past 10 years is the increasing scientific role of neural network models of a language at first glance it seems counterintuitive that something so qualitative and subjective as language plays a role in learning about physical or biological sciences which by their nature strive for precision however nlp is set to play a major role in scientific learning over the coming decades because arguably the biggest problem for scientists today is an ever-growing body of data which defies any traditional tools of comprehension 29 for example the cord-19 dataset already contains 128000 articles digesting such a vast amount of information quickly can only be done by the nlp methods and can extend beyond capturing known knowledge and reveal new information and hidden connections 27 the wellai covid-19 application uses nlp neural networks to learn from the cord-19 dataset in order to summarize existing knowledge it can also be used to make discoveries in an unsupervised manner this application is based on unsupervised learning 19 20 but its main goal is to enable a researcher to generate ideas for the next set of concepts that are relevant to the discovery the umls concepts are used as variables in the model and these concepts provide a vast terminology crucially they deal with synonymy and by including all of the synonyms the number of umls concepts increased to 4224512 only 60892 concepts are used in the wellai covid-19 model grouped into 69 categories or umls semantic types broader wellai models are based on 25 million medical articles and use millions of concepts these concepts are a helpful starting point however they had to be altered for wellai models because they are somewhat outdated specifically when it comes to the terminology surrounding the novel coronavirus the altered concepts were applied to the cord-19 dataset this whole process was not trivial because application of concepts requires context different words can mean different things in different contexts complex ml models sensitive to the context of an article needed to be developed a series of wellai neural network models have been utilized to learn relationships between medical concepts relationships of any single concept to a set of concepts along with probabilities strength of the relationship is routine however it is more difficult to work with a group of concepts as inputs especially if the number of variables is not constant a researcher may use any number of concepts as a starting point of their research and a model was developed that can accept any number of concepts as inputs and update predicted related concepts along with actionable probabilities at a practical level searching the cord-19 data-set using the wellai tool begins with the results of the initial analysis based on covid-19 and sars coronavirus as the preloaded concepts and this produces a list of 69 concept categories each concept category has an associated list of concepts ranked according to their significance in relation to covid-19 based on log probability or negative log likelihood loss 30 of the strength of the concept relationship to covid-19 according to the wellai neural networks for clinical diagnostics there are several relevant major concept categories in the list including diagnostic procedure laboratory procedure laboratory or test result associated with each major concept category is a list of related concepts each linked to relevant publications read articles the search can be refined by adding any of the concepts to the selected concepts list a rerun of the search find by selected concepts option produces the new lists of concepts that are most related to the new list of selected concepts figure 1 underlying this ai-powered tool is a network of servers that make the searching quick and seemingly effortless significantly most of the questions in table 1 could be answered by the wellai covid-19 tool by entering a concept eg transmission mode or looking at the relevant concept category eg gene or genome for virus genetics and virus origin question scisight is an ai-powered visualization tool for exploring associations between concepts appearing in the cord-19 dataset and visualizing the emerging literature network around covid-19 17-1931 it is available at httpsscisightappsallenaiorg 17 scisight is based on scibert a pretrained language model trained on a large corpus of scientific publications to provide improved performance in natural language processing 32 initially scisight provides four different search options namely two scientific concepts that are important to the study of the virus proteinsgenescells and diseaseschemicals and a network of science search and a faceted search the user can explore associations between either of two preselected scientific concepts  proteinsgenescells or diseaseschemicals in the cord-19 dataset as follows selection of one of the preselected concepts displays the try list below the search box and this lists salient keywords with high relevance to sars-cov-2 there is also a graphical display of the network of associations between the preselected scientific concept and the top related terms mined from the dataset the thickness of the edges signifies that terms are co-mentioned more often in close proximity to each other in publications in the database clicking on an edge reveals the list of linked full text papers and hovering over a term reveals co-mentioned terms this is illustrated in figure 2 for the associations between the preselected concept diseaseschemicals and the key words virus infection selected from the try list alternatively one of two preselected scientific concepts can be chosen and a search term entered this generates and displays a list of autocompleted search suggestions selecting one of these suggestions again displays the network of top associations in the dataset a network of science search option allows the user to visualize research groups and their ties in the context of covid-19 searches can be by topics affiliation or authors or by the seven preloaded topics in the try list multiple combinations of topics affiliation or authors can be selected results are shown as a network of boxes that are color coded from high to low relevance each box shows top authors top affiliations and top topics in a group and the color-coded links between boxes reveal shared authors or topics selection of a box provides a list of publications relating to the contents of that particular box also results are ranked within each topic category eg author by means of a shaded bar another search option is faceted search this reveals how authors and topics interact over time in the context of covid-19 searches can be made by selecting combinations of author co-author characteristic intervention outcome journal license or source andor by selecting one of seven preloaded topics in the try list multiple combinations of topics affiliation or authors can be selected results are ranked within each topic category eg author by means of a shaded bar and a list of relevant publications and a graphic shows the number of papers per year population-wide datasets are now emerging that show the response of society to covid-19 the data includes commonly used terms in internet search engines satellite mapping data of human activity and the emerging interactive data from digital contact tracing contact tracing is an essential monitoring process for combating the spread of an infectious disease 19-21 it comprises three basic steps 1 contact identification 2 contact listing and 3 contact follow-up - and it forms one part of the test trace and quarantine mantra conventionally contact tracing is a manual process relying on finding individuals who have tested positive and then interviewing those individuals to identify all individuals who need to be quarantined the widespread availability of mobile communication technology eg smartphones is providing new ways of enabling contact tracing by using bluetooth to track nearby phones keep logs of those contacts and to warn people about others with whom they have been in contact in the digital age contact tracing can be passively achieved and integrated with diagnostic testing results on an individual level the actions can be bi-directional an individual can test positive and then initiate a cascade of notifications of all recent contacts alternatively an individual can be notified that they were in bluetooth proximity to an anonymous person who has tested positive public health authorities empowered with digital tracing can quickly identify positive contacts with a minimal workforce in the us apple and google are collaborating on tracking technology for ios and android smartphones 33 elsewhere in the world an example of a contact tracing app is trace together which has been deployed in singapore 3435 if a person is found to be positive for covid-19 then the app uses a smartphones bluetooth network to notify every participating trace together user that person was within 2 meters of for more than 30 minutes in china the alipay health code on the alipay app dictates freedom of travel based on three categories green for unrestricted travel yellow for a seven-day quarantine and red for a two-week quarantine 36 in south korea people receive location-based emergency text messages from the government to inform them if they have been in the vicinity of a confirmed case of covid-19 37 in italy the app immuni 3839 combines a personal clinical diary and contact tracing anonymous identification codes are generated by the users app rather than a central server in order to improve privacy by placing identification on the individual users device the contact tracing information is separate from identification the app complies with the european model outlined by the pepp-pt pan european privacy-preserving proximity tracing consortium 40 it is delivered for free and on a voluntary basis there has been resistance to app-based monitoring 39 but the italian government expect 60-70 of people will download the app in the uk a contact tracing app nhs covid-19 is currently being trialed in a limited geographical area with a population of 140000 41 this app registers duration and distance between devices and the data is fed into a centralized system where a risk algorithm estimates infection risk and triggers notifications other examples of pandemic data infrastructures include the google tool covid near you to identify patterns and hot spots by location zip-code 42 covid trace 43 that warns of exposure to covid-19 by comparing your locations over the previous 3 weeks against the time and locations of reported exposures coronapp which provides localized real-time data about covid-19 based on the geographic location of their smartphones 44 and a hashtag tracking tool for the evolution of covid hashtags on twitter 628 million tweets about covid-19 45 twitter is also being used to understand the impact of covid-19 eg psychological impact 46 one significant concern over digital contact tracing has been ethical issues eg privacy and the consequent impact on the rate of adoption of the apps 4748 some technology developers are focused on developing tracing apps that ensure privacy protection 49 currently in response to covid-19 clinical laboratories and the ivd industry are grappling with test development test validation fast-track clearance eg emergency use authorization 50 availability of analyzers tests and related supplies and testing capacity for both molecular tests for sars-cov-2 and tests for igmigg antibodies against this virus 5152 once these issues have been resolved the next major hurdle will be contact tracing to reduce the risk of future outbreaks ai-powered tools will be valuable to identify trends and associations between digital contact tracing tests and outbreaks of disease easily accessible ai-powered tools and databases are valuable in all types of research but especially so in the context of the urgent diagnostic and therapeutic challenges presented by the covid-19 pandemic it is hoped that the new ai-powered search tools will accelerate research and development in covid-19 as the world strives to develop efficient and timely testing and effective therapies to combat this disastrous pandemic another important part of our fight against covid-19 will be efficient digital contact tracing enabled by mobile communication technology linked with massively scaled-up testing as outlined in the recent roadmap to pandemic resilience 53  modeling the spread of covid-19 infection using a multilayer perceptron zlatan car sandi baressi egota nikola aneli ivan lorencin vedran mrzljak   coronavirus disease code-named covid-19 is an infectious disease caused by a virus a member of the betacoronavirus family named severe acute respiratory syndrome coronavirus 2 sars-cov-2 previously referred to as 2019 novel coronavirus 2019-ncov 1 2 it is thought that the virus outbreak has animal origins and it was first transmitted to humans in wuhan province china in novemberdecember 2019 35 at present no approved vaccines or specific antivirals are available for covid-19 6 7 previous sars pandemic in 2002 and 2003 was controlled and finally stopped by conventional control measures including travel restrictions and patient isolation currently these measures are applied in almost all countries with the covid-19 outbreak however their effectiveness depends on how rigorous they are 8 9 it follows that the methods enabling reliable prediction of spreading of covid-19 would be of great benefit in persuading public opinion why it is crucial to adhere to these measures in the past decade 10 11 modeling viral diseases such as covid-19 is extremely important in determining their possible future impact modeling the spread and the effect of such a disease can be supremely important in understanding its impact 12 while traditional statistical modeling can offer precise models 13 artificial intelligence ai techniques could be the key to finding high-quality predictive models 14 in this paper the authors present a machine learning solution a multilayer perceptron mlp artificial neural network ann 15 to model the spread of the disease which predicts the maximal number of people who contracted the disease per location in each time unit maximal number of people who recovered per location in each time unit and maximal number of deaths per location in each time unit mlp has been selected for its simplicity in comparison to other ai algorithms due to authors wishing to test the possibility of modeling using comparatively simple methods due to shorter training time associated with such methods because the quick generation of results is important when modeling diseases due to the as-fast-as-possible requirement for models with good enough regression performance modeling can be done on existing data using statistical analyses but when it comes to extremely complex models statistical analysis can fail to comprehend the intricacies contained in the analyzed data 16 more complex algorithms namely ai algorithms and especially machine learning algorithms can be used to learn not just the general trend but the intricacies of the data which results in higher quality models produced 10 ai algorithms have become increasingly applicable in various branches of science and industry ie medicine 17 for the classification of various diseases as well as creating regression models for estimation and prediction models obtained by machine learning techniques adjust their parameters to fit their predictions to existing data no matter what it contains by doing this the models take into account interinfluences of various input parameters that might not have been taken into consideration if traditional modeling methods were used 11 this ability to take into account hard to observe intricacies stored inside data should lend itself well when used in an attempt of regressing a complex model such as spread of covid-19 currently existing models of covid-19 spread have relatively poor results 18 or have made predictions which were proven to not correlate to real data 19 20 in the research presented the aim was to achieve an accurate regression model through the utilization of an ai algorithm using the data that existed during the time in which this research was performed this was done in order to demonstrate the possibility of using ai algorithms in early modeling of infective disease such as covid-19 spread the aim of the model is to observe all the collected data together instead of separating it into localities as that mode of observation could allow a machine learning method to achieve a better global model of viral spread mlp algorithm is trained using a novel coronavirus covid-19 cases 21 by john hopkins csse at the time of this research being performed the dataset contained 20706 data points and was split into the training 7515530 data points and testing 255176 data points sets the hyperparameters of the mlp are determined using a grid search algorithm the robustness of the different models is tested using k-fold cross-validation algorithm achieved results are then evaluated using the r2 metric a detailed look upon the techniques used has been given in materials and methods dataset used in this research is obtained from a publicly available repository operated by the johns hopkins university center for systems science and engineering jhu csse and supported by esri living atlas team and the johns hopkins university applied physics lab jhu apl 21 it contains the data for the coronavirus patients which describe the number of patients in a certain location defined by the name of location latitude and longitude for each day since the start of the covid-19 infections 22nd of january 2020 until 12th of march 2020 dataset is split into three groupsinfected recovered and deceased at the time of this research being performed the dataset contained the data for 406 locations and 51 days the geographical distribution of data contained in the dataset is given in figure 2 which shows the geographical distribution of infected patients at various points in time dataset as published is organized as time-series datashowing the spread of disease in various locations over time the data collected at the time of this research being performed was insufficient to attempt a time-series ai modeling to train the mlp the dataset is rearranged to create a set of inputs and outputs for each number of cases the latitude and longitude of the location as well as the date of data collection is added the date is converted into the number of days since the first entry into the dataset in this way each data point contains information about the number of patients contracted recovered or dead at a given location at a given day since the first noted case latitude longitude and the number of days since the first case are used as input data with the output data being the number of patients in each group in this manner the time-series dataset is rearranged in a manner that makes it appropriate to train a regressive mlp finally the dataset consisting of a total of 20706 data points is randomly split into five equal parts or so-called folds each of these parts is used as a testing set with the remaining parts used as a training set this means that training for each architecture is repeated 5 times with an 8020 16565 randomly selected data points for training and 4141 data points for testing set training-testing distribution multilayer perceptron mlp is a type of a fully connected feed-forward artificial neural network ann consisting of neurons arranged in layers 11 at least three layers make up mlp an input layer an output layer and one or more hidden layers the output layer consists of a single neuron the value of which is the output of the mlp annin the presented research this is the predicted number of patients the input layer consists of the neurons in the same number as the dataset inputs 22 mlps used in this research will as such have 3 neurons in the input layerone for each of the input data points latitude longitude days since infection the reason for selecting mlp as the method used in this research was the ease of implementation of such methods mlp is also known to provide high-quality models while keeping the training time relatively low compared to more complex methods mlp is based on calculating the values of neurons in a current layer as the activated summation of weighted outputs of neurons in a previous layer connected to the neuron 22 23 activation refers to the sums of weighted inputs being used as inputs to the so-called activation function which maps the input to the output either directly identity activation within certain limits sigmoid or tanh or maps it while removing unwanted values eg relu which removes negative values and maps positive ones directly 24 the weights of the neuron connections are initially random but then adjusted through the backward propagation process in which the error for a forward propagated of the mlp results gets back-propagated through and weights are adjusted proportionally to the error 25 due to the fact that mlp regressor can only regress a single value if the problem consists of multiple output values a modular model consisting of multiple models must be used while similarities are possible between models training the models completely separately means that all the architectures will be tested giving a higher chance to finding a better prediction model for each goal in the research presented three separate mlps are trainedone for each of the goalsinfected recovered and deceased patients to confirm the validity of the results the cross-validation process has been performed the cross-validation method used in this research is the k-fold algorithm 22 26 during this process the dataset is split into k subsets in presented case k  5 then each of them is used as a testing set while the remaining k  1 subsets are used as a training dataset 27 the result is then presented as the average of achieved scores with standard deviation noted the solution has been implemented using python 38 programming language using scikit-learn library 28 scikit-learn has been selected due to ease of use as well as the fact that it contains the implementation of most of the methods used in this research 29 activestate activepython implementation of python and needed libraries has been used 30 training has been performed using a high-performance computer hpcbura supercomputer to train the models 16 hpc nodes each containing 48 logical cpus 24 physical cores on intel xeon e5 with 64 gb of ram per each node 31resulting in total of 768 logical cpus used the operating system used is red hat enterprise linux with kernel version 3100-957 hyperparameters are values which define the architecture of the ann model correct values of hyperparameters are crucial in achieving a quality model to determine the best hyperparameter combination the grid search algorithm has been used the grid search algorithm takes a set of possible parameters for each of the adjusted hyperparameters then each possible combination of hyperparameters is determined 32 each of the combinations is used to train the mlp to avoid the possibility of poor solutions due to the initial random setting of the weights each set of hyperparameters is used for training three times each of the achieved models is then evaluated the hyperparameters adjusted in performed research are 28 29
solverthe algorithm used for recalculating the weights of the mlp during back propagation process in traininginitial learning rate value of learning rate at the beginning of trainingadjustment of learning ratethe way the learning rate will change during the training and if it will be adjusted depending on the current value of cost function or notnumber of hidden layers and neuronsdefined as tuple in which each integer defines a single hidden layer and the integer value defines the number of neurons in that layeractivation functionfunction used to transform the input values of the neuron to the output value of the neuron andregularization parameter l2parameter which limits the influence of input parameters to avoid the ann being trained with a bias towards a single input value which has a high correlation to the output larger the parameter more is the influence lowered possible hyperparameter values are given in table 1 every obtained model is evaluated using the coefficient of determination r2 the coefficient of determination defines how well is the variance which exists in the real data explained with the predicted data the real output data the actual number of patients is contained in the vector y while the predicted data obtained from the trained model is set into the vector y with that the coefficient of determination r2 can be determined as the coefficient between the residual variance and total variance 33
1r21sresidualstotal1i0myiyi2i0myi1mi0myi2with m being the number of evaluated samples length of vectors y and y r2 is defined in the range r2 01 with the value of 00 meaning that none of the variances in real data is explained in the predicted data and the value of 10 being the best possible value meaning all of the variances is explained in the predicted data due to cross-validation being used each architecture is trained 5 timeson differing data to present the results of cross-validation the average of r2 scores is calculated r215i15ri2 to show the variance between the scores on different folds the standard deviation of the r2 scores is also presented t15rt2r25 best models achieved show a high-quality regression with r2 scores of 098599 for the confirmed patient model 097941 for the recovered patient model and 099429 for the deceased patient model the best models achieved for all three goals number of infections recoveries and deaths have a same basic ann architecture these architectures consist of four hidden layers and 16 total hidden neurons distributed equally among layers4 neurons each best models for all three outputs also use the relu activation function and the lbfgs solver the best model for confirmed cases has a constant learning rate of 01 and has a regularization parameter of 00001 for the recovered cases mlp uses a constant learning rate of 05 and a regularization parameter of 0001 the model for predicting the number of deceased patients uses the adaptive learning rate of 001 with the regularization parameter set at 01 the hyperparameters of the best models are listed in table 2 
figure 3 shows the comparison of real data to data obtained from the model real data sorted by days as well as trends for all three modeled cases are shown in subfigures subfigures a c and e demonstrate the comparison of real data sorted by date for various locations and the data predicted by model each bar presents a number of patients in a given group per location for easier viewing the maximum of each daily count is plotted as the envelope of the plotted data in b d and f these envelopes show an approximation of maximal disease spread per patient group for both real data and modeled data which shows that the modeled data follows the collected data closely  table 3 shows the cross-validation results achieved for the best models shown in table 2 training time using 5-fold cross-validation on the system used and described in the materials and methods section is shown in table 4 taking into account 5376 training items and training repeated 5 times due to cross-validation for a total of 26880 models trained this means that the average model training time is 0088 minutes or 526 seconds results show that a similar architecture can be used for all three models suggesting a similar trend between all three goals the use of the relu activation function is not unexpected as it eliminates the negative values it is logical it is going to lend itself well to a model which predicts only positive values learning rates differ between models both the models for infected and recovered use a relatively high constant learning rate while the deceased model uses a significantly lower learning rate but adapts over iterations the regularization parameter is relatively low for the model of infections but raises for the recovered and deceased modelspointing to the fact that there is a higher influence of certain input parameters on the output of those models which needed to be suppressed models show poor tracking of sudden and unexpected changes such as the sudden jump in infections around day 22 still the model demonstrates good tracking of overall model change giving good predictions even after such unexpected leapsif given time to adjust due to the largest number of cases being located in china the model is largely fitted to that data future changes in the maximum number of infected deceased or recovered patients should be included in the model to further test its robustness cross-validation performed shown across the solution space shows a drop in r2 scores the model for deceased patients shows the lowest drop in scoring used the model of confirmed cases shows a more significant drop from 0986 to 094 but these results are still acceptable the highest drop is shown in the model of recovered patients where r2 score drops from 097941 to 0781 showing the low robustness of the model for recovered patients the architectures of the models that show the best results remain the same when cross-validation is applied the aim of this research which was to generate a model of coronavirus disease spread on a global level using machine learning methods was achieved the created models show a high fidelity to existing data with the exception of the model for recovered patients in comparison to already designed models presented models show a higher accuracy as well as tracking of deaths and recoveries additionally the presented model is created using a simpler ai algorithm and uses a comparatively simple architecture which has performance benefits in terms of computational time and resources 22 results demonstrate a clear ability to mathematically model a spread of an infective disease using ai on a relatively limited dataset meaning that comparatively long periods of data collection are not strictly necessary to achieve a good model with ai algorithms obtained results point towards the ability to use such algorithms to model similar phenomena in the future the achieved models show that it is possible to acquire a quality model of novel viral infections using ai methods with geographical and time data as inputs in this research high accuracy models have been achieved for all regression goals achieved results prove the fact that ai models can be used in modeling problems such as the spread and effect of infectious diseases this means that the application of ai methods should be attempted in modeling the present and future spread of infective diseases in an attempt to predict the impact of such infections on humankind model fitting to largely the chinese patient population shows that using the number of patients per country is not necessarily a good metric to use as a training goalfurther research should be invested in testing how different types of metrics eg percentage of disease in population affect model quality the code and models achieved can be found at a public repository made available by the authors 34 authors are also planning on the implementation of achieved models inside an easy to use and widely accessible web-interface future work should apply other methods in an attempt to find even better models or models that are simpler to use or more transparent than ones observed with mlp comparison of models for different infective diseases would be interesting more data being acquired should enable the use of other techniques such as recurrent neural networks to be applied on the analyses of infection models using time-series data  an opendata portal to share covid-19 drug repurposing data in real time kyle brimacombe r tongan zhao richard eastman t xin hu ke wang mark backus bolormaa baljinnyam catherine chen z lu chen tara eicher marc ferrer ying fu kirill gorshkov hui guo quinlin hanson m zina itkin stephen kales c carleen klumpp-thomas emily lee m sam michael tim mierzwa andrew patt manisha pradhan alex renn paul shinn jonathan shrimp h amit viraktamath kelli wilson m miao xu alexey zakharov v wei zhu wei zheng anton simeonov ewy math a donald lo c matthew hall d min shen   the pandemic of the atypical pneumonia coronavirus disease covid-19 caused by a novel beta coronavirus sars-cov-2 has significantly impacted global society from both an economic and public health standpoint as a result biomedical scientists around the world  from academic and government laboratories to biotechnology companies and pharmaceutical corporations  have mobilized to understand the disease to develop therapeutic interventions to mitigate its impact and to develop protective vaccines this rapid research response has resulted in the initiation of repurposing clinical trials for a range of agents including three that have received emergency use authorizations euas from the federal drug agency fda in the united states remdesivir1 chloroquine and hydroxychloroquine these drugs and others that have entered clinical trials were prioritized based on clinical observations or a contemporary understanding of sars-cov-2 biology in addition numerous drug repurposing efforts have been undertaken screening both approved and experimental agents210 yet many published reports solely focus on active hits and do not disclose the majority usually 95 of tested compounds that were inactive  information that is critical for understanding and validating disease and drug mechanism-of-action and for nominating repurposed and novel clinical lead candidates rapid and open sharing of complete screening datasets including negative results will greatly accelerate the research and discovery process essential to the covid-19 pandemic response to address this the national center for advancing translational sciences ncats has developed an online open science data portal for its covid-19 drug repurposing campaign  named opendata  with the goal of making data across a range of sars-cov-2 related assays available in real-time figure 1 this approach allows researchers rapid access to drug repurposing datasets that can support subsequent mechanistic study of compounds that perturb viral infectivity11 in this manner open data sharing can facilitate important insight and associated publications towards the development of interventions against covid-19 the opendata dashboard first shared publicly on may 25 2020 makes quantitative high-throughput screening hts data and detailed protocol information available for every assay screened the goal is to provide clinicians and researchers with a user-friendly tool that allows direct comparison of compounds across multiple assays with all primary concentration-response data made freely available through direct download httpsopendatancatsnihgovcovid19 the assays that have been developed to date cover a wide spectrum of the sars-cov-2 life cycle figure 2 including both viral and human host targets grouped into the following five categories based on different mechanisms of experimental design viral entry viral replication in vitro infectivity live virus infectivity and counter-screens which could flag false positives due to assay interference artifacts or cytotoxic effects these assays encompass protein-based assays such as the sars-cov-2 spike protein-ace2 interaction and viral enzyme activity assays in addition to cell-based pseudotyped particle entry and live virus cytopathic effect assays as additional assays are validated and screened this list will be expanded and updated importantly all assay documentation including assay overview methodology and detailed assay protocols are available and readily retrievable on the opendata site to facilitate adoption and ensure that other laboratories can replicate these assays table 1 in total over 10000 compounds are being tested in full concentration-response ranges from across multiple annotated small molecule libraries including 1 the ncats pharmaceutical collection npc a library of 2678 compounds approved for use by the food and drug administration and related agencies in other countries1213 2 a collection of 739 anti-infective compounds with potential anti-coronavirus activity that have been reported in the literature as repurposing candidates and 3 an annotatedbioactive collection of diverse small molecules with known mechanisms of action including as small molecule probes and experimental therapeutics designed to modulate a wide range of targets such as natural products epigenetic-associated compounds or compounds developed for oncology indications1415 together these libraries yield 9958 compound responses 8624 of which represent structurally unique compounds of which 1820 21 are approved for use in humans with an additional 989 11 having entered human clinical evaluation phase 13 trials importantly of these unique compounds 5224 60 have at least one annotated target providing insight into potential mechanism-of-action underlying active compounds that perturb the sars-cov-2 life cycle in some manner a key goal of the opendata portal is to allow users to view and inspect the concentration-response data for all compounds screened not limited to only active compounds these complete datasets including full concentration-response information and compound annotations can be directly downloaded providing extensive information for further analysis and data mining in addition a user-friendly heatmap visualization has been implemented to allow direct and convenient comparison of compound activity across the panel of all assays tested including orthogonal and counter assays figure 3 currently the opendata portal can be queried by a search term eg drug name primary mechanism-of-action gene symbol and ordered by any assay data field for increased interpretability a high-level overview of compound activity across multiple different assays is displayed by heatmap with sort functions based on curve class efficacy and potency in the heatmap darker colors indicate compounds that are more potent and efficacious ie high-quality actives lighter colors indicate less potent and efficacious compounds low-quality actives beige represents inactive compounds and slashed boxes denote compoundsassays not yet tested in addition to the ncats covid-19 drug repurposing screening data described above two regularly updated external resource sections have also been incorporated into the opendata portal a collection of publications focusing on small molecule drug repurposing screens and a list of publicly available multiomics datasets generated to evaluate aberrant biochemical pathways or identify putative biomarkers for covid-19 in summary the opendata portal described herein has been designed to share ncats sars-cov-2 complete datasets openly and without restriction and importantly in real-time given the time urgency of covid-19 pandemic response in addition we have constructed the portal such that hts datasets from studies published by other investigators can be integrated into the portal to allow a global view of covid-19 drug repurposing efforts and enable comparison of screening hits across multiple centers we envisage this site could act as a central portal and a unique resource to compare complete repurposing data generated in both the public and private sectorsthe authors welcome contributions of external hts data we hope that this platform will help research scientists clinical investigators and public health officials to facilitate open data sharing and analysis to expedite the development of sars-cov-2 interventions and to prioritize promising compounds and repurposed drugs for further development in treating covid-19  transmission dynamics and evolutionary history of 2019ncov xingguang li wei wang xiaofang zhao junjie zai qiang zhao yi li antoine chaillon   on 30 january 2020 the world health organization who declared the current outbreak of the novel coronavirus 2019ncov which was first detected in the chinese city of wuhan on 31 december 2019 a public health emergency of international concernan alarm it reserves for events that pose a risk to multiple countries and which requires a coordinated international response previous studies have confirmed that this virus can spread from person to person after identifying clusters of cases among families as well as transmission from patients to healthcare workers
1
 
2
 as of 3 february 2020 there have been 20 438 cases of 2019ncov confirmed in mainland china including 2788 serious 425 deaths and 632 discharged as well as 15 in hong kong 8 in macao and 10 in taiwan more than 150 cases had also been confirmed in at least 18 other countries on four continents in epidemiological studies the basic reproductive number r
0 is defined as the possible number of infection cases generated from a single infected person at a particular time point during an outbreak and is often used to describe transmission dynamics over the course of a disease epidemic on the basis of earlier research the initial r
0 was estimated to be 22 95 confidence interval 14 to 39 among the first 425 patients with 2019ncovinduced pneumonia
2
 consistent with the preliminary estimate of 14 to 25 presented by the who during their international health regulations emergency committee meeting on the 2019ncov outbreak it is possible that subsequent control measures such as the strict travel restrictions in wuhan and china as well as overseas may change or reduce the r
0 value over the course of the virus outbreak of note the coronaviridae family not only includes 2019ncov but also severe acute respiratory syndrome coronavirus sarscov middle east respiratory syndrome coronavirus merscov and the common cold viruses in immunocompetent individuals eg 229e oc43 nl63 and hku1
3
 the sarscov pathogen was responsible for the 20022003 outbreak of sars in guangdong province china which resulted in more than 8000 cases and 774 deaths in 37 countries worldwide
4
 
5
 
6
 the merscov pathogen was responsible for the 2012 outbreak of mers which resulted in 2494 cases and 858 deaths in 27 countries worldwide
7
 
8
 notably both sarscov and merscov are zoonotic in origin with prior studies revealing bats to be the animal host source
9
 
10
 
11
 
12
 and masked palm civets
13
 
14
 
15
 and camels
16
 
17
 to be the intermediate animal hosts between bats and humans of the two diseases respectively recent research has also reported that the 2019ncov virus is 96 identical at the genome level to a previously detected bat coronavirus which belongs to a sarsrelated coronavirus species ie sarscov
18
 like sarscov merscov and many other coronaviruses 2019ncov likely originated in bats but it remains unclear whether an intermediary animal host was involved before the virus jumped to humans as reported in earlier research however although bats could be the original host of 2019ncov the virus may have initially been transmitted to an intermediate animal host sold at the wuhan huanan seafood wholesale market thus facilitating the emergence of 2019ncov in humans
19

 in the present study we investigated the time origin and genetic diversity of 2019ncov in humans based on 32 genomes of virus strains sampled from china thailand and the usa with known sampling dates between 24 december 2019 and 23 january 2020 we conducted a comprehensive genetic analysis of four 2019ncov genome sequence datasets ie dataset14 dataset24 dataset30 and dataset32 and elucidated the transmission dynamics and evolutionary history of the virus outbreak in china thailand and the usa these analyses should extend our understanding of the origins and dynamics of the 2019ncov outbreak in china and elsewhere as of 28 january 2020 33 genomes of 2019ncov obtained from humans have been released on gisaid httpgisaidorg
20
 the betacovwuhanipbcamswh022019 epiisl403931 sample shows evidence of sequencing artifacts due to the appearance of clustered spurious singlenucleotide polymorphisms and thus was excluded in this study the final dataset dataset32 included 32 genomes of 2019ncov from china n  25 thailand n  2 and usa n  5 with sampling dates between 24 december 2019 and 23 january 2020 of the 25 samples collected from china 14 were from wuhan hubei province 6 were from shenzhen guangdong province 2 were from zhuhai guangdong province 2 were from hangzhou zhejiang province and 1 was from taiwan table s1 the sampling dates of betacovshenzhenhkusz0052020 and betacovshenzhenhkusz0022020 were known to the nearest month january 2020 for this dataset the 2019ncov genomes were aligned using mafft v7222
21
 and then manually curated using bioedit v725
22
 in addition we subsampled three other datasets that is dataset14 collected between 24 december 2019 and 1 january 2020 comprising 14 genomes from wuhan hubei province china dataset24 collected between 24 december 2019 and 18 january 2020 comprising 24 genomes from china and thailand and dataset30 collected between 24 december 2019 and 23 january 2020 comprising 30 genomes from china thailand and usa to assess the recombination for the full dataset ie dataset32 we employed the pairwise homoplasy index phi test to measure the similarity between closely linked sites using splitstree v4151
23
 the bestfit nucleotide substitution model for dataset32 was identified according to the akaike information criterion aic smallsample corrected aic aicc bayesian information criterion bic and performancebased decision theory dt method with 3 24 candidate models or 11 88 candidate models substitution schemes in jmodeltest v2110
24
 to evaluate the phylogenetic signals of the datasets we performed likelihoodmapping analysis
25
 using treepuzzle v53
26
 with 35 000 to 80 000 randomly chosen quartets for the four datasets maximumlikelihood ml phylogenies were reconstructed using the hasegawakishinoyano hky nucleotide substitution model in phyml v31
27
 bootstrap support values were calculated with 1000 replicates and trees were midpoint rooted regression analyses were used to determine the correlations among sampling dates and roottotip genetic divergences of the four ml phylogenies using tempest v15
28

 to reconstruct the evolutionary history of 2019ncov bayesian inference through a markov chain monte carlo mcmc framework was implemented in beast v184
29
 with the beagle library program v212
30
 used to improve computation for each dataset we employed hky as well as a constant size coalescent tree prior and strict molecular clock model to estimate the time to a most recent common ancestor tmrca we then used two schemes to set the time scale prior for each dataset that is constrained evolutionary rate method with a lognormal prior mean  10  103 substitutions per site per year 95 bayesian credible interval bci 1854  1044  103 substitutions per site per year placed on the evolutionary rate parameter based on previous studies
31
 
32
 
33
 and the tipdating method for which the evolutionary rate for each dataset was also estimated to ensure adequate mixing of model parameters mcmc chains were run for 100 million steps with sampling every 10 000 steps from the posterior distribution convergence was evaluated by calculating the effective sample sizes of the parameters using tracer v171
34
 all parameters had an effective sample size of more than 200 indicative of sufficient sampling trees were summarized as maximum clade credibility mcc trees using treeannotator v184 after discarding the first 10 as burnin and then visualized in figtree v144 httptreebioedacuksoftwarefigtree the hiv transmission cluster engine httpwwwhivtraceorg
35
 was employed to infer transmission network clusters for the full dataset ie dataset32 all pairwise distances were calculated and a putative linkage between each pair of genomes was considered whenever their divergence was less than equal to 00001 001 or less than equal to 000001 0001 substitutionssite tn93 substitution model multiple linkages were then combined into putative transmission clusters clusters comprised of only two linked nodes were identified as dyads this approach detects transmission clusters in which the clustering strains are genetically similar implying a direct or indirect epidemiological connection dataset32 included 32 genomes of 2019ncov strains sampled from china wuhan n  14 shenzhen n  6 zhuhai n  2 hangzhou n  2 taiwan n  1 thailand n  2 and usa n  5 with sampling dates between 24 december 2019 and 23 january 2020 table s1 the samples were primarily from china 78125 and wuhan 4375 the chinese city identified as the region of the original 2019ncov outbreak for dataset32 the hky model provided the best fit across the four different methods ie aic aicc bic and dt and two different substitution schemes ie 24 and 88 candidate models and was thus used in subsequent likelihoodmapping and phylogenetic analyses for the four datasets the phi test of dataset32 did not find statistically significant evidence for recombination p  10 likelihoodmapping analysis of dataset14 revealed that 100 of the quartets were distributed in the center of the triangle indicating a strong starlike topology signal reflecting a novel virus which may be due to exponential epidemic spread figure 1a likewise 919 818 and 747 of the quartets from dataset24 dataset30 and dataset32 respectively were distributed in the center of the triangle indicating relatively more phylogenetic signals as additional sequences were analyzed over time figure 1bd ml phylogenetic analysis of the four datasets also showed starlike topologies in accordance with the likelihoodmapping results figure 2 roottotip regression analyses between genetic divergence and sampling date using the bestfitting root showed that dataset14 had a relatively strong positive temporal signal r
2  2967 correlation coefficient  5446 figure 3a in contrast dataset24 had a minor negative temporal signal r
2  44428  102 correlation coefficient  2108 figure 3b whereas dataset30 and dataset32 both had minor positive temporal signals r
2  12155  102 correlation coefficient  1102 and r
2  11506  102 correlation coefficient  1073 figure 3cd on the basis of bayesian timescaled phylogenetic analysis using the constrained evolutionary rate method with a lognormal prior mean  10  103 substitutions per site per year 95 bci 1854  1044  103 substitutions per site per year placed on the evolutionary rate parameter we estimated the tmrca dates for 2019ncov from the four datasets that is 1 november 2019 95 bci 21 july 2019 and 29 december 2019 10 november 2019 95 bci 16 july 2019 and 16 january 2020 21 october 2019 95 bci 20 may 2019 and 19 january 2020 and 15 october 2019 95 bci 2 may 2019 and 17 january 2020 for dataset14 dataset24 dataset30 and dataset32 respectively table 1 furthermore based on bayesian timescaled phylogenetic analysis using the tipdating method we also estimated the tmrca dates and evolutionary rates from dataset30 and dataset32 with resulting showing 6 december 2019 95 bci 16 november 2019 and 22 december 2019 and 6 december 2019 95 bci 16 november 2019 and 21 december 2019 respectively and 17926  103 substitutions per site per year 95 bci 7216  10430558  103 and 18266  103 substitutions per site per year 95 bci 75813  10430883  103 respectively table 1 due to poor convergence in the mcmc chains we did not obtain the tmrca date and evolutionary rate from dataset14 and dataset24 the estimates of the mcc phylogenetic relationships among the 2019ncov genomes from the bayesian coalescent framework using the constrained evolutionary rate method with a lognormal prior mean  10  103 substitutions per site per year 95 bci 1854  1044  103 substitutions per site per year placed on the evolutionary rate parameter and using the tipdating method are displayed in figures 4 and 5 respectively as shown three phylogenetic clusters number of sequences 26 posterior probability 9910 were identified that is guangdong20sf0282020 and guangdong20sf0402020 from zhuhai guangdong province china reported from a family cluster infection usaca22020 and taiwan22020 from usa and taiwan guangdong20sf0122020 guangdong20sf0132020 guangdong20sf0252020 shenzhenhkusz0022020 shenzhenhkusz0052020 and usaaz12020 from shenzhen guangdong province china and usa which included five genomes guangdong20sf0122020 guangdong20sf0132020 guangdong20sf0252020 shenzhenhkusz0022020 and shenzhenhkusz0052020 reported from a family cluster infection we considered individuals as genetically linked when the genetic distance between 2019ncov strains was less than 001 substitutionssite this allowed us to identify a single large transmission cluster that included 30 of 32 9375 genomes thus suggesting low genetic divergence for dataset32 figure 6a we also considered individuals as genetically linked when the genetic distance between 2019ncov strains was less than 0001 substitutionssite this allowed us to identify three transmission clusters that included 15 of 32 46875 genomes for dataset32 figure 6b clusters ranged in size from two to nine genomes two clusters which contained two guangdong20sf0282020 and guangdong20sf0402020 and four genomes guangdong20sf0122020 guangdong20sf0132020 guangdong20sf0252020 and shenzhenhkusz0022020 respectively included individuals sampled exclusively from zhuhai and shenzhen respectively the largest cluster of nine genomes included five sampled from wuhan wuhanhu12019 wuhanivdchb012019 wuhanwiv042019 wuhanwiv062019 and wuhanipbcamswh042019 one sampled from hangzhou zhejiangwz022020 two sampled from thailand nonthaburi612020 and nonthaburi742020 and one sampled from usa usail12020 on the basis of dataset32 which included 32 genomes of 2019ncov strains sampled from china wuhan n  14 shenzhen n  6 zhuhai n  2 hangzhou n  2 taiwan n  1 thailand n  2 and usa n  5 with sampling dates between 24 december 2019 and 23 january 2020 and subsampled dataset14 dataset24 and dataset30 which included 14 24 and 30 2019ncov strain genomes respectively our likelihoodmapping analysis confirmed additional treelike signals from 0 to 82 182 and 254 over time thus indicating increasing genetic divergence of 2019ncov in human hosts figure 1 of note the strong starlike signal 100 of quartets were distributed in the center of the triangle from dataset14 at the beginning of the virus outbreak suggests that 2019ncov initially exhibited low genetic divergence with recent and rapid humantohuman transmission this result is consistent with the ml phylogenetic analyses which showed polytomy topology from dataset14 figure 2a the genetic divergence from dataset32 and dataset30 was higher than that for dataset14 but still demonstrated minor temporal signals figure 3 using the constrained evolutionary rate method the mean tmrca dates for 2019ncov based on the four datasets ranged from 15 october to 10 november 2019 when using a lognormal prior mean  10  103 substitutions per site per year 95 bci 1854  1044  103 substitutions per site per year placed on the evolutionary rate parameter table 1 this is considered reasonable given the limited genetic divergence and strong starlike signals and is also consistent with our previous study
36
 using the tipdating method the mean tmrca date and evolutionary rate for 2019ncov based on the dataset30 and dataset32 ranged from 16 november to 22 december 2019 and from 17926  103 to 18266  103 substitutions per site per year respectively table 1 the tmrca estimated by the tipdating method was relatively narrower than that determined by the constrained evolutionary rate method we identified three phylogenetic clusters with posterior probabilities between 99 and 10 using bayesian inference figures 4 and 5 we also identified three transmission clusters when the genetic distance between the 2019ncov strains was less than 0001 substitutionssite figure 6 intriguingly only one cluster guangdong20sf0282020 and guangdong20sf0402020 from zhuhai was identified by both phylogenetic and networkbased methods this is a good example showing the differences between phylogenetic posterior probability or bootstrap value and networkbased genetic distance methods however our conclusions should be considered preliminary and explained with caution due to the limited number of 2019ncov genome sequences presented in this study the first genome sequence of 2019ncov was made public in early january 2020 with several dozentaken from various peoplenow available the genome sequences of 2019ncov have already led to diagnostic tests as well as efforts to study its dispersal and evolution as the outbreak continues we will require multiple genome sequences of samples over the course of the outbreak and from different locations to determine how the virus evolves we also need to gain a better understanding of the viruss biology especially compared to findings from previous studies on the sars and mers viruses for instance 2019ncov can kill cultured human cells entering them via the same molecular receptor as sarscov
18
 therefore it is essential that we isolate share and study virus samples both in china and elsewhere to identify animals that exhibit similar infection to humans for drug and vaccine testing to better understand virus transmission eg airborne or close contact and to develop blood tests for viral antibodies currently 2019ncov has primarily caused severe illness and death in older people particularly those with preexisting conditions such as diabetes and heart disease although this virus does not typically infect or kill young and healthy individuals a 36yearold wuhan man with no known preexisting health conditions has been the youngest victim reported so far in situations where a virus jumps from one animal host to another specieswhich is probably how this coronavirus initially infected humansmost mutations are detrimental to or have no effect on the virus and selection pressure may improve survival in the new host therefore we predict that one or more mutations may be selected and sustained during the 2019ncov outbreak as the virus adapts to human hosts and possibly reduces its virulence as reported in the previous study
37
 however we are uncertain whether this will influence its transmissibility in conclusion our results emphasize the importance of likelihoodmapping transmission network and phylogenetic analyses in providing insights into the time origin genetic diversity and transmission dynamics of 2019ncov improving the linkage between patient records and genome sequence data would also allow largescale studies to be undertaken such research could directly influence public health in terms of prevention efforts introduced to reduce virus transmission in realtime the authors declare that there are no conflict of interests xl conceived and designed the study and drafted the manuscript xl and ac analyzed the data xl ww xz jz qz yl and ac interpreted the data and provided critical comments all authors reviewed and approved the final manuscript  word frequency and sentiment analysis of twitter messages during coronavirus pandemic a preprint nikhil rajput kumar bhavya grover ahuja vipin kumar rathi   the coronavirus pandemic has taken the world by storm as also the social media as the awareness about the ailment increased so did messages videos and posts acknowledging its presence the social networking site twitter demonstrated similar effect with the number of posts related to coronavirus showing an unprecedented growth in a very short span of time this paper presents a statistical analysis of the twitter messages related to this disease posted since january 2020 two types of empirical studies have been performed the first is on word frequency and the second on sentiments of the individual tweet messages inspection of the word frequency is useful in characterizing the patterns or trends in the words used on the site this would also reflect on the psychology of the twitter users at this critical juncture unigram bigram and trigram frequencies have been modeled by power law distribution the results have been validated by sum of square error sse r 2 and root mean square error rmse high values of r 2 and low values of sse and rmse lay the grounds for the goodness of fit of this model sentiment analysis has been conducted to understand the general attitudes of the twitter users at this time both tweets by general public and who were part of the corpus the results showed that the majority of the tweets had a positive polarity and only about 15 were negative  catastrophic global circumstances have a pronounced effect on the lives of human beings across the world the ramifications of such a scenario are experienced in diverse and multiplicative ways spanning routine tasks media and news reports detrimental physical and mental health and also routine conversations a similar footprint has been left by the global pandemic coronavirus particularly since february 2020 the outbreak has not only created havoc in the economic conditions physical health working conditions and manufacturing sector to name a few but has also created a niche in the minds of the people worldwide it has had serious repercussions on the psychological state of the humans that is most evident now one of the best possible mechanisms of capturing human emotions is to analyze the content they post on social media websites like twitter and facebook not to be surprised social media is ablaze with myriad content on coronavirus reflecting facts fears numbers and the overall thoughts dominating the peoples minds at this time this paper is an effort towards analysis of the present textual content posted by people on social media from a statistical perspective two techniques have been deployed to undertake statistical interpretation of text messages posted on twitter first being word frequency analysis and second sentiment analysis a well known and profoundly researched as well as used statistical tool for quantitative linguistics is word frequency analysis determining word frequencies in any document gives a strong idea about the patterns of word used and the sentimental content of the text the analysis can be carried out in computational as well as statistical settings an investigation of the probability distribution of word frequencies extracted from the twitter text messages posted by different users during the coronavirus outbreak in 2020 has been presented power law has been shown to capture patterns in the text analysis 1 2  sentiment analysis is a technique to gauge the sentimental content of a writing it can help understand attitudes in a text related to a particular subject sentiment analysis is a highly intriguing field of research that can assist in inferring the emotional content of a text sentiment analysis has been performed on two datasets one pertaining to tweets by world health organization who and the other tweets with 1000 retweets the sentimental quotient from the tweets has been deduced by computing the positive and negative polarities from the messages the paper has been structured as follows the next section presents a brief overview of some work in the area of word frequency analysis and emergence of power law section 3 details the analysis of the twitter data section 4 provides a discussion on the obtained results section 5 provides a discussion on the scope of sentiment analysis and outlines the methodology of sentiment analysis adopted in this paper section 6 presents the results of the sentiment analysis performed on the two datasets mentioned above the final section concludes the paper  several researchers have devised statistical and mathematical techniques to analyze literary artifacts a substantially significant approach among these is inferring the pattern of frequency distributions of the words in the text 3  zipfs law is mostly immanent in word frequency distributions 4 5  the law essentially proclaims that for the words vector x the word frequency distribution  varies as an inverse power of x some other distributions that are prevalent include zipf-mandelbrot 6  lognormal 7 8  and gauss-poisson 7  such studies have been conducted for several languages such as chinese 9  japanese 10  hindi 11 and many others 3  not only single word frequencies but also multi-word frequencies have been exhaustively explored one of the examples is 12 wherein bigram and trigram frequencies and versatilities were analyzed and 577 different bigrams and 6140 different trigrams were reported a well known distribution is the power law this non-normal distribution has been a subject of immense interest in the academic community due to its unique nature the rightly skewed distribution is mathematically represented as follows f where a is a constant and b is the scaling or exponential parameter power law has been deployed in various studies in 13  the authors explicitly focus on the presence of power law in social networks and use this property to create a degree threshold-based similarity measure that can help in link prediction in an attempt to model the self similar computer network traffic the authors claim that during fragmentation of the data into ethernet frames leads the power spectrum of the departure process mimics power law similar to that of observed traffic they also state that the power law was independent of the input process 14  it was shown in 15 that internet topologies also can be modelled by power law while investigating the presence of power laws in information retrieval data the authors showed that query frequency and 5 out of 24 term frequency distributions could be best fit by a power law 16  power law has found immense applications in different domains in this paper we intend to use it to model the word frequencies of twitter messages posted during this time in this section we present the details of the analysis performed on the data obtained pertaining to twitter messages from january 2020 upto now that is the time since the news of the coronavirus outbreak in china was spread across nations the word frequency data corresponding to the twitter messages has been taken from 17  the data source indicates that during march 11th to march 30th there were over 4 million tweets a day with the surge in the awareness also the data prominently captures the tweets in english spanish and french languages a total of four datasets have been used to carry out the study the first is the data on twitter id evolution reflecting on number of tweets and the other three are unigram bigram and trigram frequencies of words in the following subsections analysis of each has been undertaken first we consider the data corresponding to the number of twitter ids tweeting about coronavirus at a particular time fig 1 depicts the pattern of the twitter id evolution a couple of peaks can be observed in its evolution in the months of february and march three forms of tokens of words have been considered for the study viz unigram bigram and trigram these represent the frequencies of one word two words together and finally three words coupled the dataset provides the top 1000 unigrams top 1000 bigrams and the top 1000 trigrams fig 2 gives the visualization of the word cloud for unigrams it can be seen that coronavirus was the most frequent word fig 3 4  and 5 depict plots for the rank or index vs frequency distributions for unigram bigram and trigram respectively the graphical visualization well demonstrates that the nature or the pattern of the data follows power law the power law distribution can be seen to closely fit the data the exponents in case of unigram and bigrams are -1273 and -1375 respectively while for trigram it gives a smaller value of -05266 the computed parameters are reported in table 4  also heavy tails are observed specifically in the case of unigrams and bigrams a good fit by power law connotes a difference in the behaviour of tweet messages when compared to literary documents like novels and poems which are replicated by zipfs law with exponents close to 1  the quality of fit of the data using power law distribution has been evaluated using three goodness of fit metrics sse r 2 and rmse the value obtained for the three datasets with the three forms of token of words has been shown in table 4  we obtain a high value of r 2 in all the three cases unigram 09172 bigram 08718 and trigram 09461 also the values of sse and rmse obtained in all the three cases is quite low this confirms that power law provides a good model for the frequency data of the tweet messages sentiment analysis is a fast growing field due to its capacity to interpret emotional quotient of a textit has often been defined as a computational study that relates to peoples attitudes emotions and opinions towards a subject 18  the key intent of sentiment analysis is to gauge opinions identify hidden sentiments and finally to classify their polarity into positive negative or neutral sentiment analysis has been immensely used in customer reviews 19  news and 20  and stock market 21 to name a few several methods have been deployed for sentiment analysis including support vector machines 22  naive bayes 23  and artificial neural networks 24  there have also been several papers that have provided algorithms for sentiment analysis on twitter posts 25  26  27  28  in the present work we use the python built-in package textblob 29 to perform sentiment analysis of tweets pertaining to the coronavirus outbreak the analysis has been conducted on two datasets one corresponds to the tweets made by who and the other is the one that contains tweets that have been retweeted more than 1000 times the polarity values of individual tweets have been computed the interpretation of these values is as follows polarity  0 implies positive polarity  0 implies negative polarity  0 implies neutral the polarities range between -1 to 1 these polarity values have then been plotted in a histogram to highlight the overall sentiment ie more positivity or negativity the plots have been given in fig 6 7 8 and 9  table presents the results of the percentage of positive negative and neutral tweets based on the polarities in the dataset the following section outlines an analysis of the results obtained in this section we provide a detailed discussion of the results obtained from the sentiment analysis of the two datasets fig 6 corresponds to the histogram of sentiment polarities of tweets on coronavirus by general public it can be seen that majority of the tweets have a neutral sentiment followed by positive the same can be inferred from table 6 that shows that around 54 tweets are neutral 29 positive and a mere 15 is negative fig 7 corresponds to the histogram of sentiment polarities of tweets on coronavirus by who it can be seen that majority of the tweets have a neutral and positive sentiment table 6 that shows that around 60 tweets are positive 24 neutral and a mere 15 is negative this shows how who is trying to retain the positive spirit through its social media accounts fig 8 and 9 represent the histograms produced by removing the neutral tweets it readily reiterates that the positive emotions in the tweets are higher than negative ones this is a great sign that indicates that the humans are still focusing more on the positive and sharing the same light through their messages  the paper is an effort towards deriving statistical characterization of tweet messages posted during the coronavirus outbreak the spread of the disease has created an environment of threat risks and uncertainty among the population globally this response can be gauged from the immensely high number of tweet messages corresponding to the outbreak in a short duration of 2-3 months in the present work data related to tweets made since january 2020 have been analyzed with two major perspectives one understanding the word frequency pattern and the second sentiment analysis the study has resulted in the following observations the number of twitter ids tweeting about coronavius has increased rapidly with several peaks during february and march an empirical analysis of words in the messages was made by determining their frequencies of occurrence the most frequent words were coronavirus covid19 and wuhan unigram bigram and trigram frequencies the top 1000 were modeled it was seen that all of them followed the rightly skewed power law distribution with quite heavy tails in the first two cases the exponential parameters for the same were determined to be -1273 for unigram -1375 for bigram and -05266 for trigram the plots for the same have been depicted the goodness of fit for the model was determined using sse r 2 and rmse the results were found to be satisfactorily good the model can be used to determine the pattern of the words used during this time the sentiment analysis was performed on tweet messages by general public and who the polarities of the individual messages were determined and a histogram of the same was plotted it could be observed that most of the messages belonged to the neutral and positive categories with who messages 60 of the messages were found to be positive and with general messages 29 were found to be positive and 54 neutral in both the cases just about 15 messages were of negative sentiment the results obtained are a great reflection of the sentiments expressed worldwide during this pandemic   a transcriptional regulatory atlas of coronavirus infection of human cells scott ochsner a neil mckenna j   infection by coronaviruses cov represents a major current global public health concern signaling within and between airway epithelial and immune cells in response to infections by cov and other viruses is coordinated by a complex network of signaling pathway nodes these include chemokine and cytokine-activated receptors signaling enzymes and transcription factors and the genomic targets encoding their downstream effectors takeda et al 2003 stark et al 1998 darnell et al 1994 we recently described the signaling pathways project spp ochsner et al 2019 an integrated omics knowledgebase designed to assist bench researchers in leveraging publically archived transcriptomic and chip-seq datasets to generate research hypotheses a unique aspect of spp is its collection of consensus regulatory signatures or consensomes which rank genes based on the frequency of their significant differential expression across transcriptomic experiments mapped to a specific signaling pathway node or node family by surveying across multiple independent datasets we have shown that consensomes recapitulate pathway node-genomic target regulatory relationships to a high confidence level ochsner et al 2019 placing the transcriptional events associated with human cov infection in context with those associated with other signaling paradigms has the potential to catalyze the development of novel therapeutic approaches the cov research community has been active in generating and archiving transcriptomic datasets documenting the transcriptional response of human cells to infection by the three major cov species namely middle east respiratory syndrome coronavirus mers and severe acute respiratory syndrome coronaviruses 1 sars1 and 2 sars2 dediego et al 2011 josset et al 2013 sims et al 2013 yoshikawa et al 2010 to date however the field has lacked a resource that fully capitalizes on these datasets by firstly using them to rank human genes according to their transcriptional response to cov infection and secondly contextualizing these transcriptional responses by integrating them with omics data points relevant to host cellular signaling pathways here as a service to the research community to catalyze the development of novel cov therapeutics we generated consensomes for infection of human cells by mers sars1 and sars2 covs we then analyzed the extent to which high confidence transcriptional targets for these viruses intersected with genes with elevated rankings in transcriptomic and chip-seq consensomes for cellular signaling pathway nodes integration of the cov consensomes with the existing universes of spp transcriptomic and chip-seq data points in a series of use cases illuminates previously uncharacterized intersections between cov infection and human cellular signaling pathways the cov infection consensome and its underlying datasets provide researchers with a unique and freely accessible framework within which to generate and pressure test hypotheses around human cellular signaling pathways impacted by cov infection reflecting their well-documented roles in the response to viral infection we observed appreciable significant overlap between all tc95s and those for the toll-like tlr totura et al 2015 interferon ifnr hensley et al 2004 and tumor necrosis factor tnfr w wang et al 2007 receptor families fig 2 interestingly these signatures were particularly highly enriched in the sars2 tc95  potentially reflecting a particularly strong antiviral response to infection by sars2 tc95 overlaps for receptor systems with previously uncharacterized connections to cov infection including epidermal growth factor receptors glucocorticoid receptor and notch receptor signaling are consistent with the known roles of these receptor systems in the context of other viral infections hayward 2004 ito et al 2011 ng et al 2013 ostler et al 2019 zheng et al 2014 the relatively strong enrichment for xenobiotic receptors reflects work describing a role for pregnane x receptor in innate immunity s wang et al 2014 and points to a potential role for members of this family in the response to cov infection in general chip-seq enrichments for transcription factors and other nodes were more specific for individual cov infection tc95s compare fig 2 with figs 3 4 and 5 this is likely due to the fact that chip-seq consensomes are based on direct promoter binding by a specific node antigen whereas transcriptomic consensomes encompass both direct and indirect targets of specific receptor and enzyme node families not unexpectedly  and speaking again to validation of the consensomes - the strongest and most significant cov tc95 overlaps were observed for cc95s for known transcription factor mediators of the transcriptional response to cov infection including members of the nfb ludwig  planz 2008 poppe et al 2017 ruckle et al 2012 irf chiang  liu 2018 and stat blaszczyk et al 2016 frieman et al 2007 garcia-sastre et al 1998 transcription factor families consistent with its known role in the regulation of interferon-stimulated genes hasan et al 2013 we also observed appreciable overlap between the cc95 for tfeb and the all cov tc95 moreover the strong overlap between the gtf2btfiib cc95 and all viral tc95s reflects previous evidence identifying gtf2b as a target for orthomyxovirus haas et al 2018 herpes simplex virus gelev et al 2014 and hepatitis b virus haviv et al 1998 compared to the roles of receptors and transcription factors in the response to viral infection the roles of signaling enzymes are less well illuminated  indeed in the context of cov infection they are entirely unstudied through their regulation of cell cycle transitions cyclin-dependent kinases play important roles in the orchestration of dna replication and cell division processes that are critical in the viral life cycle cdk6 which has been suggested to be a critical g1 phase kinase bellail et al 2014 grossel  hinds 2006 has been shown to be targeted by a number of viral infections including kaposis sarcoma-associated herpesvirus kaldis et al 2001 and hiv-1 pauls et al 2014 consistent with this common role across distinct viral infections we observed robust overlap between the cdk95 tc95 fig 2 and the cdk6 cc95 fig 4 and those of all viral tc95s as with the tlrs ifnrs and tnfrs which are known to signal through cdk6 cingoz  goff 2018 handschick et al 2014 hennessy et al 2011 overlap with the cdk6 cc95 was particularly strong in the case of the sars2 tc95 fig 4 ccnt2 is a member of the highly conserved family cyclin family and along with cdk9 is a member of the viral-targeted p-tefb complex zaborowska et al 2016 reflecting a potential general role in viral infection appreciable overlap was observed between the ccnt2 cc95 and all viral tc95s fig 4 finally in the context of enzymes dna topoisomerase top1 has been shown to be required for efficient replication of simian virus 40 wobbe et al 1987 and ebola takahashi et al 2013 viruses the prominent overlap between its cc95 and those of sars2 and iav fig 4 suggest that it may play a similar role in facilitating the replication of these viruses we have coined the term co-nodes as a convenient catch-all for cellular factors that are not members of the three principal signaling pathway node categories receptors enzymes and transcription factors ochsner et al 2019 the breadth of functions encompassed by members of this category reflects the diverse mechanisms employed both by viruses to infect and propagate in cells as well as by hosts to mount an efficient immune response consistent with its characterized role in the recruitment of p-tefb by iv-1 tat protein schulze-gahmen et al 2013 we observed consistently strong enrichment of the aff4 cc95 in all viral tc95s fig 5 the targeting of cnot3 for degradation in response to adenoviral infection chalabi hagkarim et al 2018 is reflected in the significant overlap between its cc95 and the viral tc95s particularly in the case of the sars2 fig 5 by way of additional independent validation of the cov and iav consensomes gene set enrichment analysis subramanian et al 2005 reflected significant overlap between the cov and iav tc95s and a variety of viral infection and inflammatory transcription factor target gene sets supplementary information section 8 we previously showed figs 2 and 3 that the overlap of the cov tc95 genes was most robust among consensomes for members of the ifnr tlr and tnf receptor families fig 2 and the nfkb rela irf and stat transcription factor families fig 5 to investigate this further we ranked genes in the all cov consensome by their aggregate 80th percentile rankings across these consensomes supplementary information section 2 column v transcriptomic consensomes for ifnrs section 9 tlrs section 10 and tnfrs section 11 are provided in supplementary information as are links to the chip-atlas lists used to generate the chip-seq consensomes section 12 this redundancy ranking elevates genes with known critical roles in the response to viral infection that are acutely responsive to a spectrum of inflammatory signaling nodes such as ncoa7 doyle et al 2018 stat chapgier et al 2009 and tap1 gruter et al 1998 interestingly genes such as psmb9 csrnp1 and mrpl24 have all cov discovery rates that are comparable to or exceed those of many of the classic viral response isgs fig 1 but are either largely or completely uncharacterized in the context of viral infection this use case the reflects the potential of consensome-driven data mining to illuminate novel and potentially therapeutically relevant transcription pathway effectors in the response to cov infection although a lack of clinical data has so far prevented a definitive evaluation of the connection between pregnancy and susceptibility to sars2 infection in covid-19 pregnancy has been previously associated with the incidence of viral infectious diseases particularly respiratory infections sappenfield et al 2013 siston et al 2010 we were interested therefore to see consistent overlap between the progesterone receptor pgr tc95 and all the viral tc95s with the enrichment being particularly evident in the case of the sars2 tc95 fig 2 to investigate the specific nature of the crosstalk implied by this overlap in the context of the airway epithelium we first identified a set of 16 genes that were in both the all cov and pgr tc95s we then retrieved two spp experiments involving treatment of a549 airway epithelial cells with the pgr full antagonist ru486 ru alone or in combination with the gr agonist dexamethasone dex as shown in figure 6 there was nearly unanimous correlation in the direction of regulation of all 16 genes in response to cov infection and pgr loss of function these data indicate that antagonism between pgr and ifnr signaling in the airway epithelium may predispose pregnant women to infection by sars2 although telomerase activation has been well characterized in the context of cell immortalization by human tumor virus infection gewin et al 2004 klingelhutz et al 1996 yang et al 2004 no connection has previously been made between cov or iav infection and telomerase we were therefore intrigued to observe robust overlap between all viral tc95s and that of the telomerase catalytic subunit tert in support of this finding nfkb signaling has been shown to induce expression yin et al 2000 and nuclear translocation akiyama et al 2003 of tert and direct co-regulation by telomerase of nfkb-dependent transcription has been linked to chronic inflammation ghosh et al 2012 inspecting the all cov consensome underlying data points data not shown we found that the tert gene was not transcriptionally induced in response to infection by any of the covs indicating that the overlap between its tc95 and those of the covs might occur in response to an upstream regulatory signal if functional interactions between tert and inflammatory nodes did indeed take place in response to cov infection we anticipated that this would be reflected in close agreement regarding the direction of differential expression of cov infection-regulated genes in response to perturbation of tert on the one hand and on the other to stimulation of the classic viral response ifnrs to test this hypothesis we took the same set of 20 isgs referred to previously fig 1 and compared their direction of regulation across all experiments underlying the all cov tert and ifnr consensomes for reference the tert consensome section 13 and its underlying data points section 14 are provided in the supplementary information with respect to the ifnr and tert data points we observed a nearly universal alignment in the direction of regulation of all genes tested with those in the cov infection experiments fig 6 with agreement in the direction of regulation across 99 of the underlying probesets we should note that of the 1859 p  005 cov infection isg data points we observed repression rather than induction in response to cov infection in 303 15 which may be attributable to the impact of differences in cell type cell cycle stage or virus incubation time across the independent experiments our results are consistent with a model in which activation of telomerase is a component of the human innate immune response to viral infection epithelial to mesenchymal transition emt is the process by which epithelial cells lose their polarity and adhesive properties and acquire the migratory and invasive characteristics of mesenchymal stem cells lamouille et al 2014 emt is known to contribute to pulmonary fibrosis hill et al 2019 and acute interstitial pneumonia h li et al 2014 both of which have been reported in connection with sars2 infection in covid-19 adair  ledermann 2020 p zhou et al 2020 moreover emt is widely accepted as a core component of the process by which renal tubular cells transform into mesenchymal cells during the development of fibrosis in kidney disease y liu 2006 a signature comorbidity of sars2 infection durvasula et al 2020 of the 834 cc95s analyzed overlap p  005 was specific to the sars2 cc95 for only five snai2slug sox2 gata6 ctbp1 and prmt1 strikingly a literature search indicated that these five nodes were connected by documented roles in in the promotion of emt avasarala et al 2015 herreros-villanueva et al 2013 martinelli et al 2017 nieto 2002 sahu et al 2017 in addition to these nodes whose c95 genes were exclusively enriched p  005 in the sars2 c95 genes we identified several other emt-linked nodes whose cc95 genes were preferentially enriched p  005 in the sars2 tc95 genes figs 35 including the homeodomain transcription factor six2 c-a wang et al 2014 smad4 siraj et al 2019 and the co-nodes pygo2 chi et al 2019 ski tecalco-cruz et al 2018 brd7 t liu et al 2017 and stag2 nie et al 2019 to investigate this further we computed overlap between the individual viral tc99s and a list of 335 genes manually curated from the research literature as signature emt markers supplementary information section 15 zhao et al 2015 consistent with the node consensome enrichment analysis we observed significant enrichment of members of this gene set within the sars2 cc99 but not those of the all cov sars1 mers or iav consensomes supplementary information section 16 one possible explanation for this was the fact that the sars2 consensome was comprised of airway epithelial cell lines whereas the sars1 and mers consensomes included non-epithelial cell biosamples supplementary information section 1 to exclude this possibility therefore we next calculated airway epithelial cell-specific consensomes for sars1 and mers and computed overlap of their tc95s against the 864 pathway nodenode family cc95s  tc95s we found that significant overlap with the cov tc95s remained specific to sars2 data not shown confirming that significant overlap with the emt node signature was specific to the sars2 tc99 we next applied consensome redundancy analysis see use case 1 to isolate a set of sars2 regulated genes cpv p  005 that were high confidence targets ie in the cc80 for at least two of the emt nodes snai2 sox2 gata6 ctbp1 and prmt1 supplementary information section 4 column m a literature survey showed that 13 of these 21 genes had a documented connection to emt supplementary information section 5 column n figure 9 compares the percentile rankings for these genes across the three cov infection consensomes and the iav consensome although some emt genes such as cxcl2 and irf9 had elevated rankings across all four consensomes the collective emt gene signature had a significantly higher mean percentile value in the sars2 consensome than in each of the three others fig 9 although emt has been associated with infection by transmissible gastroenteritis virus xia et al 2017 this is to our knowledge the first evidence connecting cov infection and specifically sars2 infection to emt interestingly several members of the group of sars2-induced emt genes have been associated with signature pulmonary comorbidities of cov infection including adar diaz-pina et al 2018 cldn1 vukmirovic et al 2017 and sod2 gao et al 2008 of note in the context of these data is the fact that signaling through two sars2 cellular receptors ace2at2 and cd147basigin has been linked to emt in the context of organ fibrosis kato et al 2011 ruster  wolf 2011 c wang et al 2018 moreover overlap between of the cov tc95s and the tert cc95 was particularly robust in the case of sars2 a finding of potential relevance to the fact that telomerase has been implicated in emt z liu et al 2013 collectively our data indicate that emt warrants further investigation as a sars2-specific pathological mechanism having validated the all cov consensome we next wished to make it freely available to the research community for re-use in the characterization of signaling events associated with cov infection firstly the viral infection datasets were curated accordingly to our previously described protocol ochsner et al 2019 made available for browsing in the spp dataset listing httpswwwsignalingpathwaysorgdatasetsindexjsf as with other spp datasets and per fair data best practice cov infection datasets were associated with detailed descriptions assigned a digital object identifier and linked to the associated article to place the dataset in its original experimental context loading the cov datasets into the spp also automatically made the underlying data points discoverable by the spp query tool ominer ochsner et al 2019 these reports represent a summary of the current state of transcriptomic and chip-seq knowledge on the regulatory relationship of a given gene with upstream regulatory pathway nodes or in clinical and model experiments the full value of the integration of the cov consensome with the existing spp framework can perhaps be best appreciated in the context of the one click links to ominer regulation reports from the individual cov datasets these reports provide researchers with a wealth of contextual data on signaling pathways impacted by cov infection in the context of a specific gene table 2 shows links to regulation reports for the top twenty ranked genes in the all cov consensome the order of sections in the reports is receptors enzymes transcription factors co-nodes clinical and models the last section including data points from the cov infection model experiments that form the basis of this study an effective research community response to the impact of covs on human health demands systematic exploration of the transcriptional interface between viral infection and human cell signaling systems it also demands routine access to existing datasets that is unhindered either by paywalls or by lack of the informatics training required to manipulate archived datasets in their unprocessed state moreover the substantial logistical obstacles to bsl3 certification only emphasizes the need for fullest possible access to and re-usability of existing cov infection datasets to focus and refine hypotheses prior to carrying out in vivo cov infection experiments to this end we generated a set of cov infection consensomes that rank human genes by the reproducibility of their significant differential expression in response to infection of human cells by covs we then computed the cov consensomes against high confidence transcriptional signatures for a broad range of cellular signaling pathway nodes affording investigators with a broad range of signaling interests an entrez into the study of cov infection of human cells the five use cases described here represent illustrative examples of the types of analyses that users are empowered to carry out in the cov infection knowledgebase to democratize access to the cov consensome and its 3000000 underlying data points by the broadest possible audience we have integrated them into the spp database to create a cell signaling knowledgebase for cov infection incorporation of the cov data points into spp in this manner places them in the context of millions more existing spp data points documenting transcriptional regulatory relationships between pathway nodes and their genomic targets in doing so we afford users a unique appreciation of the cellular signaling pathway nodes whose gain or loss of function in response to cov infection gives rise to these transcriptional patterns the human cov and iav consensomes and their underlying datasets are living resources on spp that will be updated and versioned with appropriate datasets this will be particularly important in the case of sars2 as datasets involving infection of human cells with this virus are necessarily currently limited in number this will allow for hardening of observations that are intriguing but whose significance is currently unclear such as the overlap of the cov tc95s with the tert tc95 as well as the enrichment of emt genes among those with elevated rankings in the sars2 consensome we welcome feedback and suggestions from the research community for the future development of the spp cov infection consensomes differential expression values were calculated for each gene in each experiment using the limma analysis package from bioconductor then committed to the consensome analysis pipeline as previously described ochsner et al 2019 briefly the consensome algorithm surveys each experiment across all datasets and ranks genes according to the frequency with which they are significantly differentially expressed for each transcript we counted the number of experiments where the significance for differential expression was 005 and then generated the binomial probability referred to as the consensome p-value cpv of observing that many or more nominally significant experiments out of the number of experiments in which the transcript was assayed given a true probability of 005 a more detailed description of the transcriptomic consensome algorithm is available ochsner et al 2019 the consensomes and underlying datasets were loaded into an oracle 13c database and made available on the spp user interface as previously described ochsner et al 2019 gene overlap analysis was performed using the bioconductor geneoverlap analysis package httpswwwrdocumentationorgpackagesgeneoverlapversions180topicsgeneoverlap  implemented in r briefly given a whole set i of ids and two sets a  i and b  i and s  a  b geneoverlap calculates the significance of obtaining s the problem is formulated as a hypergeometric distribution or a contingency table which is solved by fishers exact test the universe for the overlap was set at a recent estimate of the total number of coding genes in the human genome 21500 pertea et al 2018 paired two sample t-test for comparing the mean percentile ranking of emt genes in the mers sars1 sars2 and iav consensomes was performed in prism at 12 degrees of freedom the procedure for generating transcriptomic consensomes has been previously described ochsner et al 2019 to generate the chip-seq consensomes we first retrieved processed gene lists from chip-atlas which rank human genes based upon their average macs2 occupancy across all publically archived datasets in which a given transcription factor enzyme or co-node is the ip antigen of the three stringency levels available 10 5 and 1 kb from the transcription start site we selected the most stringent 1 kb according to spp convention ochsner et al 2019 we then mapped the ip antigen to its pathway node category and class and the experimental cell line to its appropriate biosample physiological system and organ we then organized the ranked lists into percentiles to generate the node chip-seq consensome the spp knowledgebase is a gene-centric java enterprise edition 6 web-based application around which other gene mrna protein and bsm data from external databases such as ncbi are collected after undergoing semiautomated processing and biocuration as described above the data and annotations are stored in spps oracle 13c database restful web services exposing spp data which are served to responsively designed views in the user interface were created using a flat ui toolkit with a combination of javascript d3js ajax html5 and css3 javaserver faces and primefaces are the primary technologies behind the user interface spp has been optimized for firefox 24 chrome 30 safari 519 and internet explorer 9 with validations performed in browserstack and load testing in loaduiweb xml describing each dataset and experiment is generated and submitted to crossref to mint dois ochsner et al 2019 the entire set of data points used to generate the cov consensome has been uploaded in an r file to figshare and a link included for reviewer access the entire set of metadata for these data points is available in supplementary information section 1 consensome data points are in supplementary information sections 26 spp is freely accessible at httpswwwsignalingpathwaysorg programmatic access to all underlying data points and their associated metadata are supported by a restful api at httpswwwsignalingpathwaysorgdocs all spp datasets are biocurated versions of publically archived datasets are formatted according to the recommendations of the force11 joint declaration on data citation principles 74 and are made available under a creative commons cc 30 by license the original datasets are available are linked to from the corresponding spp datasets using the original repository accession identifiers these identifiers are for transcriptomic datasets the gene expression omnibus geo series gse and for cistromicchip-seq datasets the ncbi sequence read archive sra study identifier srp dois for the consensomes and datasets are pending the full spp source code is available in the spp github account under a creative commons cc 30 by license at httpsgithubcomsignaling-pathways-projectominer  role of intelligent computing in covid-19 prognosis a state-of-the-art review swapna hanumanthu rekha   throughout history several infectious diseases have alleged the lives of many people and induced critical situations that have taken a long time to overcome the situation the terms epidemic and pandemic have been used to describe the disease that emerges over a definite period of time 1 during a particular course of time the existence of more cases of illness or other health situations than normal in a given area is defined as epidemics 2 on the other hand pandemics are outbreaks of the infectious disease that can enormously increase the morbidity and mortality over a vast geographical area due to the factors such as raise of worldwide travel urbanization changes in usage of land and misusing of the natural environment the occurrence of the pandemics has increased from the past century 3 in the past the outbreak of smallpox has killed of nearly 500 million world population in the last 100 years of its survival 4 due to the outbreak of spanish influenza in 1918 an estimate of 17 to 100 million deaths occurred 5 from the last 20 years several pandemics have been reported such as acute respiratory syndrome coronavirus sars-cov in 2002 to 2003 h1n1 influenza in 2009 and the middle east respiratory syndrome coronavirus mers-cov in 2012 since december 2019 the novel outbreak of coronavirus has infected more than thousand and killed above hundreds of individuals within the first few days in wuhan city of hubei province in south china in the 21st century the pandemics such as sars-cov has infected 8096 individuals causing 774 deaths and mers-cov has infected 2494 individuals causing 858 deaths while the sars-cov-2 has infected more than 348 million individuals causing 248144 deaths across 213 countries as on may 3 2020 these evidential facts state that the transmission ratio of sars-cov-2 is greater than other pandemics a list of some dangerous pandemics happened over time is listed in table 1
 due to the rapid increase of patients at the time of outbreak it becomes extremely hard for the radiologist to complete the diagnostic process within constrained accessible time 6 the analysis of medical images such as x-rays computer tomography and scanners plays a crucial role to overcome the limitations of diagnostic process within constrained accessible time now-a-days machine learning and deep learning techniques helps the physicians in the accurate prediction of imaging modalities in pneumonia ml is a wing of artificial intelligence that has the ability to acquire relationships from the data without defining them a priori 7 due to the availability of large number of intelligent tools for the analysis collection and storage of large volume of data machine learning techniques have been extensively utilized in the clinical diagnosis machine learning approaches can be efficiently used in applications of healthcare such as disease identification diagnosis of disease discovery and manufacturing of drug analysis of medical images collection of crowd sourced data research and clinical trials management of smart health records prediction of outbreak etc some recent studies show the usage of machine learning techniques in the time series forecasting of ebola outbreak 8 in order to select the better performing classifier for forecasting ebola casualties experiments were conducted on ten different classifiers further results demonstrate that the proposed model achieves 9095  accuracy 539  mae and 4241  rmse value even though machine learning approaches have rapidly used in the diagnosis of outbreaks these approaches still have some limitations such as complete utilization of biomedical data temporal dependency owing to high-dimensionality sparsity heterogeneity and irregularity 9 10 11 on the other hand due to the deep architectural design the deep learning models are the best accurate models for handling medical datasets such as classification of brain abnormalities classification of different types of cancer classification of pathogenic bacteria and segmentation of biomedical images 12 13 14 15 16 several studies show that dl models are adopted in the diagnosis and classification of pneumonia and other diseases on radiography a deep learning model build on mask-rcnn has been utilized for the detection and localization of pneumonia in chest x-ray images 17 in order to perform pixel-wise segmentation the model makes use of global and local features the robustness of the system is achieved through the modification of training process and post processing step further results show that the model outperforms in the identification of pneumonia in chest x-ray images to improve the performance of prediction a bioinspired meta-heuristic optimization algorithm has been presented by martinez-alvarez et al 18 in this approach to prevent the researches from initializing with arbitrary values the input parameters are initialized with the disease statistics also this approach has the ability to stop after certain number of iterations without setting this value further to explore wider search space in less number of iterations a parallel multi-virus approach has been proposed finally it has been integrated with dl models for finding the optimal parameters in the training phase deep learning prototypes have been widely used in the prediction and forecasting of outbreak over machine learning models because of its features such as greater performance feature extraction without human intervention and identification and not making the use of engineering advantage in training phase the major objective of this paper is to provide a review of different machine learning approaches used in the prediction classification and forecasting of covid-19 first we describe the origin of sars-cov-2 virus its transmission rate comparison of sars-cov-2 with other pandemics in the history and its impact on the global health then the analysis is broaden by describing the advantages of computing approaches such as statistical and mathematical models ml and dl approaches in the prediction of covid-19 along with its applications further an analysis number of articles published in different computing approaches by different countries till date impact of the nature of data in the prediction of covid-19 have been presented as the researchers and technocrats are the main targets of this review we highlighted some of the challenges in the ongoing research of different computing approaches at the end of the paper the remaining section of the paper is systematized as follows section 2 describes the origin of covid-19 and its impact the application of statistical ml and dl models in the diagnosis and prognosis of covid-19 has been portrayed in section 3 section 4 illustrates the critical investigation on the analysed data types of covid research growth in publication of ml approaches for covid comparative analysis on the type of methods etc few challenges in ml related to covid-19 have been focused in section 5 section 6 outlines the conclusion with a brief discussion on covid-19 impact in real life and novel research directions in the 21st century human corona viruses such as sars-coronavirus sars-cov and mers-coronavirus mers-cov that have emerged from the animal reservoirs caused global epidemic with distressing morbidity and mortality these human corona viruses belong to the subfamily of coronavirinae that is a part of coronaviridae family it was named as corona because of the presence of spike like structure on the outer surface of the virus under electron microscope its rna is a single stranded with a diameter of 80-120 nm and nucleic material range varying from 26 to 32 kbs in length 19 these are basically divided into four types of genera named as alpha  beta  gamma  and delta  - and -cov usually infects mammals while - and -cov tends to affect birds among the six susceptible human viruses hcov-229e and hcov-nl63 of -covs and hcov-hku1 and hcov-oc43 of -covs shows low pathogenicity and moderate respiratory symptoms as common cold the other two familiar -covs such as sars-cov and mers-cov exhibit acute and malignant respiratory diseases 20 the figure 1
shows the transmission process of corona viruses from animal sources to human the people in wuhan city of hubei province in south china were reported in local hospitals with an unidentified pneumonia on december 2019 21 initially these cases were related to huanan seafood wholesale market which is famous for variety of live species all these cases have clinical characteristics similar to those of viral pneumonia such as dry cough fever dyspnea and lung infiltrates on imaging after the analysis of samples collected from the throat swab the authorities from centers for disease control cdc announced the unidentified pneumonia as novel coronavirus pneumonia ncp on 7th january 2020 22 later it was named as severe acute respiratory syndrome coronavirus 2 sars-cov-2 by the international committee on taxonomy of viruses 23 24 and the disease was renamed as covid-19 by the who on 11th february 2020 25 the covid-19 generated by sars-cov-2 is a -coronavirus the sequence analysis of sars-cov-2 matches with the typical structure of coronaviruses as depicted in figure 2
 the structure of sar-cov consists of 14 binding remnants that collaborate precisely with human angiotensin-converting enzyme 2 as eight binding residues of the sars-cov has sustained in the sars-cov-2 the structure of sars-cov-2 genome contributes 795 similarity to sars-cov 27 in addition both bat cov and human sars-cov-2 share the identical ancestor as the genome sequencing of covid-19 shows 962  identity to bat cov ratg13 28 due to the genetic recombination occurrence at s protein in the rbd area of sars-cov-2 the sars-cov-2 has greater transmission ability than sars-cov 29 after the spread of covid-19 to 18 countries through human-to human transmission the who announced the epidemic as public health emergency of international concern pheic on 30th january 2020 in addition critical situation was created when the first case not imported from china was registered in the united states on 26th february 2020 the who declared the covid-19 as pandemic on 11th march 2020 when it imposes serious hazard to public health as the number of cases outside the china has raised 13 times and the number of countries distressed by covid-19 has increased by three times since the last two months the number of covid-19 cases registered has crossed all the previous records of the viral disease due to its rapid spread it is considered to be the most dangerous disease till date according to the who the sars-cov-2 has infected about 348 million people and caused 248144 deaths across 213 countries of the world as on may 3 2020 among the countries the usa has reported about 1188122 positive cases and 68595 deaths as on may 3 2020 and stood in first place in both positive cases as well as in death rate similarly other countries like spain italy uk france germany russia turkey iran brazil canada belgium peru and netherland are placed as top countries with more than 50000 cases after the pandemic of covid-19 outside the mainland of china the number of deaths per day due to covid1-9 pandemic is also gradually increasing from the starting day of transmission to till date fig 3 fig 4
 predicts the top countries in the world having more than 50000 and number of deaths per day as on may 3 2020 random forest algorithm rf is one of the most promising and recognized classifier that uses multiple trees to train and predict data samples this approach has been extensively used in the fields of chemometrics and bioinformatics 32 33 because of its praiseworthy characteristics random forest has been used in resolving issues of the ncovid-19 infection for precise and rapid recognition of covid-19 a tool based on random forest algorithm to extract 11 key blood indices from clinical available blood samples was suggested by wu et al 34 in this study random forest algorithm is used as a discrimination tool to explore patients with covid-19 symptoms the proposed method achieved better outcome in the prediction of covid-19 with accuracy of 09795 for the cross-validation set and 09697 for the test set further the tools also achieved better performance in terms of sensitivity specificity and overall accuracy of 09512 09697 and 09595 respectively on an external validation set moreover it achieved an accuracy of 09167 in a detailed clinical estimate of 24 samples collected from infected covid-19 patients after multiple verifications the proposed approach has been emerged as a precise tool for the recognition of covid-19 infection to predict the hospital stay of patients infected with novel coronavirus a model based on linear regression and random forest has been suggested by qi et al 35 the proposed model based on 6 second-order characteristics was refined on features obtained from pneumonia lesions in training and inter-validation datasets further the predictive efficiency has been evaluated using lung-lobe and patients-level test dataset from the conclusions it is observed that model was efficient in segregating short and long-term stay of patients in hospital infected with coronavirus infection moreover linear regression model exhibited a sensitivity and specificity of 10 and 089 while a sensitivity and specificity of 075 and 10 has been exhibited by the random forest model the following table 2
depicts the usage of random forest approach in the prognosis of sars-cov-2 infection as svm is used as a powerful tool for data regression and classification it has superior performance in many real world applications such as medical image analysis over other machine learning approaches because of its better performance it has been used in the classification and analysis of covid-19 to predict the threat of positive covid-19 diagnosis different machine learning approaches such as neural networks random forests gradient boosting trees logistic regression and support vector machines for training the sample data was suggested by batista et al 41 the performance of the different machine learning approaches was trained on arbitrary sample data of 70 of patients and was tested on 30 of new not seen data form the results it concludes that the support vector machine algorithm outperforms with auc sensitivity specificity and brier score of 085 068 085 and 016 respectively when compared with other machine learning algorithms for the diagnosis of covid-19 infected patients form the chest x-ray images a machine learning model developed on multi-level thresholding and svm has been suggested by hassanien et al 42 furthermore the results depict that the proposed model achieves better performance with an average sensitivity of 9576 specificity of 997 and accuracy of 9748 respectively table 3
represents the usage of machine learning approaches in the recognition and diagnosis of covid-19 besides random forest and support vector machine other approaches of machine learning approaches such as linear and logistic regression xgboost k-means neural network have also been used in the clinical and public health approach many research works are being performed and table 4
represents the usage levels of these approaches in solving some of the conflicts of covid-19 convolutional neural network has proven to be one of the most successful algorithms in the analysis of medical image with high accuracy previously the identification of the nature of pulmonary nodules in ct images prediction of pneumonia in x-ray images labeling of polyps automatically at the time of colonoscopic videos have been done using convolutional neural networks 58 59 60 the authentication features for identifying covid-19 in medical images are bilateral distribution of patchy shadows and ground glass opacity 61 abbas et al 62 have developed a decompose transfer and compose detrac model based on convolutional neural network to categorize the covid-19 chest x-ray images using the class decomposition mechanism the class boundaries are investigated that helps the model to accord with any anomalies in the image dataset further the results show that an accuracy of 9512 was attained by detrac in the recognition of covid-19 x-ray images from other normal and pneumonia cases to recognize covid-19 patient from chest x-ray images distinct convolutional neural network frameworks namely resnet50 inceptionv3 and inception-resnetv2 have been proposed by narin et al 63 further insufficient data and training problem can be overwhelmed by applying deep transfer learning technique using imagenet from the results it can be observed that highest classification performance with 98 accuracy can be attained by the resnet50 model over the other two models to interpret and predict the number of positive cases a covid-19 forecasting model build on convolutional neural network cnn was suggested by the huang et al 64 the main focus of the study was to consider the cities with large number of positive cases in china the overall competence of different algorithms was compared using mean absolute error mae and root mean square error rmse the outcomes indicate that cnn achieves greatest prediction efficacy when collated with other approaches of deep learning such as lstm gru and mlp furthermore the actual usage and feasibility of the proposed model in forecasting the total registered cases were also documented in their study to automatically recognize the ncov-2019 positive cases from chest x-ray images mukherjee et al 65 have proposed tailored shallow convolutional neural network architecture the architecture was designed with few parameters for validating 130 covid-19 positive x-ray images and 5-fold cross validation was used to avoid possible bias in the experimental tests moreover the proposed method achieved an accuracy sensitivity and auc of 9692 0942 and 09869 respectively which is dominative over other compared methods a multitask deep learning model has been proposed by amyar et al 66 to perform the automatic screening and segmentation covid-19 chest ct images for reconstruction and segmentation one encoder and two decoders along with multi-layer perceptron has been used in the architecture for the classification purpose then the model has been evaluated with a dataset of 1044 patients which includes 449 patients suffering with covid-19 100 normal cases 98 patients with lung cancer and 397 cases of different types of pathology moreover results indicate that the model obtains better performance over the other image segmentation and classification techniques the application of cnn in the classification and diagnosis of covid-19 has been depicted in table 5
 long short-term memory is a type of the recurrent neural network that can store knowledge of previous states and can be trained for work that needs memory lstm is one of the efficient models for the prediction of time series sequential data 81 as past data is retained in the hidden states lstm approach can perform more accurate predictions of the output a novel multivariate spatiotemporal model has been proposed by jana et al 82 this model uses ensemble of convolutional lstm to make accurate forecast of the dynamics of covid-19 transmission in large geographic region for converting the available spatial features into set of 2d images a data preparation method is used further the proposed model is trained using usa and italy data from the findings it can be observed that the model achieves 557 and 03 mape for usa and italy respectively to predict the number of covid-19 cases in india a data-driven estimation approach based on lstm and curve fitting has been suggested by et al 83 this approach was also used to estimate the effective of social distancing measures on the spread of the pandemic further the findings show the accuracy of proposed approach in predicting the number of positive and recovered cases in india table 6
shows the applicability of lstm in resolving the issues of covid-19 pandemic in addition to cnn and lstm approaches several other deep learning approaches such as generative adversarial networks and autoencoder have also been used in the analysis and prediction of covid-19 pandemic generative adversarial networks gans are algorithmic architectures that consist of two neural networks called generator and discriminator to generate new synthetic instances of data from real ones that have been never observed before gans have been widely used in image generation video generation and voice generation an autoencoder is a sort of artificial neural network used to grasp efficient data coding in an unsupervised way the following table 7
represents the usage of other deep learning approaches in the prediction and diagnosis of novel coronavirus disease from the past few pandemics mathematical and statistical models have been successfully used in the estimation of human loss and also in the prediction of total number of deaths until a specific period or end of the pandemic as the mathematical and statistical approaches shows better performance researchers have also used the same approaches in the estimation of spread rate and death count of the current pandemics to foresee the window period for testing positive negative results as well as false negative result a multivariate model has been developed by xu et al 99 the clinical characteristics that are responsible for false negative outcomes of sars-cov2 nucleic acid identification are determined using these models zhong et al 100 have used the simple mathematical models for the early prediction of novel coronavirus disease in china the prediction models estimated that the total number of positive cases may reach to 76000 to 230000 in late february to early march it is also estimated that from early may to late june the number of registered cases will rapidly decrease to predict the number of deaths in china boltzmanns function based analysis has been proposed by gao et al 101 the prediction model gives better prediction accuracy in the assessment of severity of situation the impact of primary health conditions such as cardiac disease diabetes and age of the patient in the prediction of death rate has been presented by banerjee et al 102 azad et al 103 have used holts and autoregressive integrated moving average arima model in the temporary forecasting of covid-19 infected patients in india from 30 january to 21 april 2020 the following table 8
shows some other analysis done in the prediction of covid-19 using mathematical and statistical approaches it has been evident from the system review that machine learning approaches also have been efficiently used in the prediction and diagnosis of covid-19 among the available machine learning approaches mostly support vector and random forest have been used in the recognition of novel covid-19 outbreak as support vector machine is one of the best classifier algorithms with minimum error rate and maximum accuracy it gives better prediction results a machine learning model suggested by barstugan et al 46 have been efficiently used in the classification of medical images using svm for classification and grey level co-occurrence matrix glcm for feature extraction the proposed model achieves 9968 classification accuracy as random forest uses multiple tress to identify the samples and robust to noise it has been extensively utilized in the classification of medical images moreover random forest approach is suitable for multiclass problem whereas svm is suitable for two-class problem a random forest model proposed by tang et al 38 have been successfully utilized in the severity assessment of coronavirus infected patients using 30 quantitative features the proposed model achieves 0933 true positive rate 074 true negative rate and 0875 accuracy table 9
represents the analysis of performance metrics of various machine learning algorithms in the diagnosis of covid-19 infection it has been evident from the systematic analysis that most of the machine learning approaches trained on small datasets moreover the limitations specified in the table provide scope for the researchers to develop more accurate prediction and forecasting models in the future due to the advantages of deep learning approaches over machine learning approaches such as excellent performance feature extraction without human intervention and handling complex and multimodal more number of researches has been carried in the diagnosis of covid-19 infection using deep learning approaches from the systematic review it is observed that cnn is one of the most commonly used deep learning approaches for the prediction of pandemic from the medical images over other approaches due to its ability to extract features automatically with a deep learning algorithm proposed by wang et al 113 have achieved an overall accuracy of 731 on external validation this is due to the limitations such as large number of variable objects and presence of only one radiologist in outlining the roi area it has been also observed that the implementation of exact architecture may not yield in better solutions a new modified cnn for categorizing x-ray images proposed by rahimzadeh et al 72 improves the overall performance by extracting multiple features using xception and resnet50v2 networks the model has been applied on 11302 images and conclusions show that the proposed method achieves an overall average accuracy of 9956 for covid-19 cases an iteratively pruned model proposed by rajaraman et al 68 improves the classification performance by combining distinct ensemble schemes the empirical results show that the proposed model outperforms by achieving 9901 accuracy and 09972 of area under curve value a resnet based framework proposed by farooq et al 114 have achieved an overall accuracy of 962 with augmentation hence the performance can be enhanced by considering some aspects such as presence of expert knowledge about the task to be solved additional augmentation and preprocessing steps optimization of hyperparameters and so on in the implementation the following table 10
depicts the performance analysis of some deep learning approaches that may enable the researchers to select an appropriate deep learning approach and architecture for resolving conflicts in covid-19 pandemic as well as further scope to improve the overall performance of different approaches since the start of outbreak of covid-19 in december 2019 several researchers and modeling groups around the world have developed abundant number of prediction models 125 126 127 128 using mathematical and intelligent computing approaches to predict the trends of covid-19 in different regions of the world the list of covid-19 forecasting attempts all over the world using various statistical models is publicly accessible1
 the modeling results have forecasted information about trends in covid-19 around the world such as infection cases future deaths recovered cases hospitality needs impact of social distancing travelling restrictions and so on these models have shown a vast range of variations in predictions due to uncertainty of data it has been found that the design issues have also been observed in the most cited forecasting technique from the imhe 129 130 though the issues are resolved in later revised model the prediction errors still persist high one of the reason for these variations is only small amount of information is available at the beginning of the outbreak and lack of reliable data due to frequent segregation of data over different geographical regions another reason is most of the prediction models have forecasted the future results by considering data of confirmed cases of those who got infected with symptoms and tested at the hospitals these predictions have not taken into consideration the data of asymptomatic patients furthermore the factors affecting the positive and death rate such as age gender hypertension chronic diseases etc have not considered in some prediction models the selection of suitable model for performing epidemic study also influences the predictions of the model a simple mode is not realistic as it does not include more epidemiological information while the complex model is biologically authentic though the complex model includes more biological information it requires more parameters when compared to simple model the complex model also leads to larger degree of uncertainty if the increased parameters are unknown therefore to make more accurate predictions in the future more research has to be carried on improving the tools and models of the prediction on large biological information despite having lower fatality rate sars-cov-2 caused thrice the total of deaths when compared to the combined statistics of deaths caused by both mers and sars-cov as the symptoms of covid-19 are similar to common influenza it becomes difficult to detect the infection in addition covid-19 is much more contagious than influenza due to asymptomatic condition further the shortage of medical supply rapid spread and the non-availability of a vaccine or drug for treating covid-19 are the major reasons that attracted most of the researchers to carry vast research on covid-19 when compared to other pandemics figure 5
represents the articles contributed for the analysis from jan 13 2020 to may 3 2020 in the initial stage of disease less than 1 of articles have been published in the month of january around 6 of articles issued in the month of february 16 of articles reported in the month of march and 77 of the articles are published in the month of april so it can be concluded that up to 3rd may 2020 majority of the articles have been published in the month of april as the number of infected covid-19 cases increased world wise it is worthy to note that with the increase of covid-19 cases throughout the world researchers have shown incredible interest to decipher the problem of various aspects on covid-19 through intelligent ml and other computing approaches it is evident from the figure 6
 that majority of work on covid-19 predictiondiagnosis has been conducted using deep learning approaches 39 next 37 of research has been experimented using the machine learning approaches only 24 of the work has been done using mathematical and statistical models in the prediction of covid-19 from the figure it can be concluded that more appropriate prediction and diagnosis of covid-19 diseases can be performed using deep learning approaches more work has been contributed on deep learning approaches when compared to other approaches because of the features such as excellent performance ability to handle complex and multi-modal data feature extraction without human intervention and absence of engineering advantage in training phase articles from different sources such as lancet jama nejm elsevier oxford wiley nature bmj science and medrxiv have been considered for the systematic review it is evident from the figure 7
that majority of the articles have been published in bmj ie 176 next 171 articles have been published in lancet the articles from nature have contributed 153  the science has contributed 85 of the studies 76 of the studies have been contributed by journal of medical virology nejm jama clinical infectious diseases journal of infection travel medicine infectious diseases international journal of infectious diseases eurosurveillance emerging infectious diseases radiology viruses infection control hospital epidemiology emerging microbes infection journal of hospital infection have been contributed between 2 to 4 of the studies remaining journals like annals of internal medicine journal of the american academy of dermatology international journal of antimicrobial agents journal of clinical medicine journal of travel medicine journal of virology methods in molecular biology have contributed less than 1 of the studies the figure 8
displays the number of articles published country wise on the diagnosis and prognosis of covid-19 using statistical machine learning and deep learning approaches from the figure it can observed that majority of the articles ie 217 have been contributed by the researchers of china 181 articles have been contributed from the us researchers the researchers of uk have contributed 149 of the articles in the study 1316 of articles have been contributed by italy the countries india france republic of korea and japan have contributed 604 56 42 and 35 of the studies respectively 213 have been contributed by the researchers of the countries like egypt and australia the contributions from the researchers of turkey are 14 and nearly 106 of total research has been contributed by the researchers of canada germany and saudi arabia however the researchers of pakistan netherlands and brazil have contributed the 0711 of the studies next the researchers in the countries like iran spain iraq greece thailand and singapore have contributed 035 of the studies the forecasting of pandemics can be done with the information of registered number of covid-19 cases along with their geographical locations the most used dataset for the forecasting and prediction of covid-19 is collated by john hopkins university 131 this dataset contains information such as the daily positive cases total patients recovered per day and the death rates at a country as well as state level another data source kaggle also contains the daily number of covid-19 cases 132 this dataset is annotated with attributes such as patient demographics case reporting date and location when working with real datasets most of the researchers face class imbalance distribution issues another limitation is the variations in interventions population densities and demographics have a major impact on the prediction however many good researchers have suggested for the use of real clinical data under the supervision of doctors for further diagnosis of covid-19 other than online data the main problem with online data is the presence of large extent of missing values which may affect the proper analysis using any intelligent based methods the growth nature and spread of covid-19 can be predicted using the rich textual data available from various online sources the interpretation of the epidemic is quite complicated as most of the studies have taken into account only a few determinants assuming the affect of virus in terms of positive and death case everywhere in the globe could produce inaccurate predictions to make accurate predictions it is necessary for the researches to understand the spread at the local regional national as well as international levels to be accurate analysts would also need data from the medical authorities that distinguish between deaths caused by the coronavirus and deaths that would have happened due to anonymous disease in most of the studies this data is not known or not available the range to which people pursue the local governments quarantine policies or measures to prevent the spread of infection is also difficult to find these factors need to be considered to make accurate prediction of covid-19 using online data while experimenting with any ml based techniques the screening of medical images such as chest ct images or x-ray images is considered as an alternative solution to overcome the shortage of rt-pcr supply now-a-days most of the research for the classification and prediction of covid-19 has been carried on medical image data set as medical image analysis helps the physicians in the accurate prediction of imaging modalities in pneumonia from the literature review it has been observed there are still some limitations in the usage of medical image datasets the vast challenge in the medical diagnosis is the classification of medical images due to the limited accessibility of medical images the process of automated distinction is difficult in ct images as those share some common imagery characteristics among novel covid and other forms of pneumonia none of the studies reviewed by the authors accurately reported the high specificity of ct in distinguishing covid-19 from other pneumonia with identical ct findings thus restricting the usage of ct as a confirmatory diagnostic test the presence of mild or no ct findings in many early cases of infection highlights the difficulties of early detection moreover the ct scan tools are expensive and patients are exposed to more radiation even though chest x-ray images are cheaper and expose the patient to less radiation these images have higher false diagnosis rate from the figure 9
it has been observed that majority of work has been done on classification of covid-19 46 next 36 of work has been done on the prediction of covid-19 only 20 of the work has been done on the forecasting of covid-19from the figure it can be concluded that less work has been done on prediction and forecasting due to the lack of real world datasets and availability of less number of training medical images the implementation of predictive tools using deep learning and machine learning requires huge volume of data even though few datasets for medical images and textual analysis are publicly available these datasets are small when compared to the needs of the deep learning approaches the main reason for the scarcity of measured data is the frequent segregation of data over different geographical regions therefore aligning of the data sources is one of the key issues that need to be solved another limitation arises in the development of quality datasets as real time datasets contain poorly quantified biases as results of this poor outcomes will be produced if models are trained on unrepresentative data although transfer learning allow models to be specific with regional characteristics it is difficult to perform model selection due to the fast moving nature of the data therefore designing an analytical approach to overcome these limitations is one of the key challenges that need to be addressed most of the researchers and technocrats are also facing the problem of lack of real data this issue can be accomplished by creating more real world datasets with updated covid-19 data another issue that needs to be observed is the less involvement of medical community in most of the studies either few or no physician has involved in the assessment of medical images such as x-rays and ct images there exists a hidden risk in all scientific work as most of the methods in the study are based on statistical learning on quickly produced datasets the outcome of the research may have biases that may impact the policies taken by the government in controlling the spread of disease therefore the challenge is to find the uncertainty of conclusions produced in this research the correctness of the data can be ensured by providing reproducible conclusions this in turn creates the challenge of balancing the requirements against the urgency some data science approaches such as ultrasound scans and magnetic resonance imaging mri have limited exposure in combating covid-19 even though ultrasound scans have shown good performance as that of chest ct scans no studies have explored the usage of ultrasound scans in the prediction of covid-19 though some studies 133 have shown the efficient usage of mri in predicting covid-19 infections still the approach remained unexplored due to the scarcity of adequate training data therefore the challenge is to develop well-annotated dataset to make potential usage of new approaches in the prediction of covid-19 infection the usage of predictive tools in diagnosis of covid-19 imposes a problem in developing countries that have limited access to healthcare facilities therefore a key challenge is the development of tools that should be capable of deployment in economically underprivileged regions for example the development of mobile app for contact tracing should consider factors such as low cost limited resources accessibility to illiterates or disability people and support of multiple languages so that it can be effectively deployed in economically deprived regions most of the studies have carried out only by considering the characteristics of covid-19 and other pneumonia the outcome of these studies may not produce accurate results as they not considered other the impact of other factors such as age gender diabetes hypertension chronic liver and kidney disease and so on therefore to perform accurate predictions more research has to be carried on symptom based identification of covid-19 moreover apart from prediction and forecasting based models future research requires more attention on the classification problems on covid-19 through various symptoms for easy and quick diagnosis also most of the research is dedicated to top affected countries with this pandemic disease and further research may be enhanced for remaining mostly affected countries around the globe further accurate prediction in the number of deaths and infections with advance machine learning approach is of utmost importance in the present scenario as most of the machine learning models are highly accurate with large amount of data so it may be worthy to note that with the increasing no of data and datasets of majorly covid affected countries many highly accurate models will be developed as a leading solution to this outbreak the researchers are always active in addressing the emerging challenges that arises in different application domains in recent days covid-19 an infection caused by sars-cov2 is one of the most emerging research areas as it affected more than 3 million people in 213 countries within a short span of time therefore to empower the government and healthcare sector it is necessary to analyze various forecasting and prediction tools in this paper an overall comprehensive summary of ongoing work in the prediction of covid-19 infection using various intelligent approaches has been presented initially the origin dissemination and the affect of covid-19 on the public health has been discussed the major contribution of the study is the analysis of various prediction and forecasting models such as statistical machine learning and deep learning approaches and their applications in the control of the pandemic following this the analysis is broadened by making a critical investigation on the growth of studies carried on covid-19 in various journals by country and the performance analysis of the statistical machine learning and deep learning approaches finally at the end of the paper some of the challenges observed as a part of systematic review are highlighted that may further help the researchers and technocrats to develop more accurate prediction models in the prediction of covid-19 the sars-cov-2 has infected about 348 million people and caused 248144 deaths across 213 countries of the world as on may 3 2020 according to the who as the total of covid-19 cases registered has crossed all the previous records of the viral disease since last moths it is considered to be the most dangerous disease till date the whole world political social economic and financial structures have been disturbed because of the outbreak of pandemic the economies of the worlds topmost countries such as the us china uk germany france italy japan and many others are at the edge of destruction as 162 countries have moved into the lockdown to prevent the transmission of pandemic the business across the world is operating in fear of an impending collapse of global financial markets the sectors such supply chains trade transport tourism and the hotel industry have been damaged extremely because of the pandemic other sectors like apparel  textile building and construction sectors have been affected adversely due to the lack of labour supply and availability of raw material the other sector that is badly affected is the aviation sector as both international and domestic flights cancelled due to the implementation of lockdown in many countries even though the effect of lock down is less on essential goods retailer other retailers such as shops and malls have been highly impacted by the pandemic due to the pandemic the educational institutions have also been seriously affected and led to the shutdown of institutions which caused an interruption in the students learning activity as well as in internal and external assessment necessary for the qualification of the student even though several sectors have been affected there are some sectors such as digital and internet economy food based retail chemicals and pharma sectors that have seen growth during the pandemic lockdown the deep learning and machine learning approaches are useful in forecasting the impact of covid-19 on different sectors which may help the government in implementing proper policies to overcome the economic crisis from the systematic analysis it is evident that computing intelligent approaches such as ml dl mathematical and statistical approaches have been profitably used in the prediction and screening of covid-19 pandemic it is observed that svm rfk-means and linear regression of ml approaches have been mostly used for solving issues of covid-19 while in case of dl cnn and its variants are mostly utilized for predicting the pandemic even though computing approaches have been successfully used in the prediction and forecasting of covid-19 pandemic still there exist certain limitations such as limited availability of annotated medical images not taking into account predictive end events such as mortality or admission into critical care unit while forecasting not considering some features of medical images such as ggo crazy-paving pattern and bilateral involvement which are prerequisite in the diagnosis covid-19 training on small datasets and not coping with data irregularities need urgent focus to develop more accurate models it is also observed that every researchers and modeling groups all over the world are presently facing the issue of scarcity of data therefore real-world datasets with more epidemiological data need to be created for the development of more accurate prediction models moreover the accuracy of prediction tools can be enhanced by the usage of advanced computing intelligent approaches such as ensemble method like bagging stacking etc application of optimization techniques usage of artificial neural networks and higher order neural networks in the screening and prediction of covid-19 which is considered as further scope of research the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare that this manuscript has no conflict of interest with any other published source and has not been published previously partly or in full no data have been fabricated or manipulated to support our conclusions  the global spread of middle east respiratory syndrome an analysis fusing traditional epidemiological tracing and molecular phylodynamics jae min eleonora cella massimo ciccozzi antonello pelosi marco salemi mattia prosperi   mers-cov is the second coronavirus after severe acute respiratory syndrome sars-cov with the potential to cause a pandemic 3 characterized by its corona or crown shape it is a single-stranded rna virus with approximately 30000 nucleotides in its genome while current literature mostly concur that camels are important zoonotic reservoirs for mers-cov there has been some evidence that bats might be primary viral reservoirs and that mers-cov will jump to a different host such as dromedary camels and subsequently humans when opportunities arise similar transmission dynamic has been observed with sars-cov where palm civets acted as intermediary host between bats and humans 4 5 however coronaviruses isolated from bats are more genetically distant from human mers-cov than those isolated from camels which have shown very high similarities to humans 6 virus transmission from camels is thought to be connected to consuming camel milk or urine working with camels andor handling camel products 7 secondary transmissions are largely associated with hospitals or close contacts of mers cases as shown with the family cluster in the united kingdom uk relatives or people living in close contact with an infected patient are susceptible to the pathogen even outside of hospital settings 8 while the exact methods of mers-cov transmissions are unknown respiratory droplet and aerosol transmissions are cited as most probable but there is no conclusive evidence on how close a person has to be for exposure and what protection is most suitable 3 9 for example the korean ministry of health and welfare kmoh classified close contacts as those who were within 2 meters of mers-cov infected patient or in contact with respiratory droplets without personal protective equipment yet the extent to which of those close contacts were infected is unknown 10 it has been hypothesized that camels can transmit a higher dose of viruses to humans while the quantity is lower between humans and that mers-cov has not fully adapted for human-to-human transmission 11 12 food oral-fecal and fomite transmissions are also possible transmission routes since the virus has been detected in camel milk patient fecal sample and hospital surfaces 1316 patients with mers have a wide range of symptoms from being completely asymptomatic to suffering from severe respiratory illnesses fever cough chills and myalgia are some of the most commonly reported symptoms in mild cases but respiratory distress kidney failure and septic shock have been reported in acute cases 17 18 there are neither vaccines nor specific medications against mers-cov so treatments are usually palliative in nature 17 19 more than a third of those infected with mers-cov die 1 for comparative perspective case fatality was one in ten for the sars pandemic of 2003 20 research is yet to be done on the relationship between symptoms and transmissibility given that clinical procedures for acute patients can generate aerosolized viral particles patients with severe respiratory distress would be more likely to transmit the virus compared to asymptomatic patients but transmissibility of airborne mers-cov is unknown 12 in addition research in transmission is essential with regards to superspreaders who are sources for large number of cases for healthcare associated outbreaks 21 the uncertainty of pathogen transmission lack of vaccine against mers-cov and deficit in mers specific treatments make public health interventions challenging to design 22 with ease of international travel the possibility of mers spread is present in all nations notably countries without mers endemic are unfamiliar with the infectious agent that may be imported by travelers and are therefore particularly vulnerable in this paper we reviewed epidemiological contact tracing information from public health agencies and peer-reviewed literature in order to see geographic and temporal distribution of mers cases around the globe concurrently we used genetic sequences to infer transmission dynamics and inter-host evolution of mers-cov the combined analysis was used to present a phylodynamic picture detailing international zoonotic and healthcare associated transmissions at genetic and population levels these analyses can be used to understand pathogen spread and to implement public health measures to curb a pandemic 23 detailed review of current literature was conducted using the five-step model of khan et al 24 the literature searches on pubmed used following keywords middle east respiratory syndrome mers and mers cov the articles potential relevance to our topic was examined initially by article title then by abstract content included papers were case reports and articles on phylogenetics healthcare related outbreaks and epidemiology and we excluded papers on viral structure and model organism research relevant publications from national and international public health agencies were reviewed as well whos disease outbreak news and mers risk assessments spanning september 2012 to june 2016 were used as primary sources to create a map of global transmissions 1 the data were supplemented with the european centre for disease prevention and control ecdcs communicable disease threats reports and korean center for disease control and prevention kcdc and kmoh reports published online 25 26 number of cases in hospital settings and non-hospital settings were compared using mann-whitney u test in sas software v94 for windows 27 following the literature review forest plots of basic reproduction numbers were created using distillersr forest plot generator 28 mers-cov sequences isolated from humans and camels were downloaded from the genbank repository and are listed in additional file 1 29 combined open reading frames 1a and 1b orf1ab region was chosen specifically for the high number of sequences available and low phylogenetic noise in order to create the most informative phylodynamic analyses duplicate sequences from single patients were excluded to explore inter-host dynamics without intra-host evolution and only sequences with known date and place city state or country of collection were included based on the metadata associated with the genbank accession we created three datasets first with all cov sequences isolated from both humans and camels second dataset with solely sequences from humans and third only with sequence from camels the sequences were aligned using clustal x and manually edited using bioedit 30 31 then the best fitting nucleotide substitution model was chosen via hierarchical likelihood ratio test with modeltest implemented in paup40 32 33 three preliminary analyses were performed prior to bayesian analysis recombination phylogenetic signal and temporal signal tests first identification of recombinant strains was done using bootscanrecscan method in rdp4 with default window size and parameters 34 35 second phylogenetic signals in the datasets were investigated with tree-puzzle 36 it reported maximum likelihood for each new tree generated as a single dot on a triangle the distribution of these dots among the seven zones of the triangle indicated their phylogenetic signals if a dot was located in the central triangle its tree had unresolved phylogeny with star-like structure if a dot was located in the sides of the triangle the phylogeny was network like and only partially resolved lastly if the dot was positioned in one of the three corners the tree topology was considered fully resolved 37 third amount of evolutionary change over time or temporal signal from the sequences was examined using tempest 38 the three datasets were analyzed separately using default parameters in order to conduct bayesian phylogenetic analyses on mers-cov evolution three parameters for demographic growth model were tested 1 molecular clock 2 prior probability distribution and 3 marginal likelihood estimator first molecular clock was calibrated using dates of sample collection with the bayesian markov chain monte carlo mcmc method in beast v18 39 both strict and relaxed molecular clocks were tested but relaxed clock with underlying lognormal distribution was superior for the investigation of demographic growth of mers-cov orf1ab four independent mcmc runs were carried out each with one of the following coalescent prior probability distributions constant population size exponential growth bayesian gaussian markov random field gmrf skyride plot and bayesian skyline plot 4042 both parameters listed above were tested with path sampling and stepping stone marginal likelihood estimators 43 44 best fitting model was chosen by comparing the bayes factors 45 if log bayes factor was 1 and 3 there was weak evidence against the model with lower marginal likelihood 43 higher log values indicated stronger evidence such that values greater than 3 and 5 were considered to give strong and very strong evidence respectively for all three datasets the mcmc sampler was run for at least 50 million generations sampling every 5000 generations only parameter estimates with effective sample size ess greater than 250 were accepted to ensure proper mixing of the mcmc phylogeographic analyses were conducted also using bayesian-mcmc method in beast using 1 best fitting nucleotide substitution model chosen by modeltest 2 relaxed molecular clock with underlying lognormal rate distribution 3 gmrf skyride plot as demographic model and 4 evolutionary rate previously estimated with sample collection dates for all three datasets the mcmc chains were run for at least 100 million generations and sampled every 10000 steps using the standard continuous time markov chain over discrete sampling locations 31 cities and countries and bayesian stochastic search variable selection procedures to infer social network phylogeographic analyses were carried out for total and human datasets 46 for the camel subset continuous geographical trait analysis was used since spatial distribution of the mers-cov from camels was presumed not to be independent of their genetic phylogeny 47 maximum clade credibility trees which had the largest product of posterior clade probabilities were selected after 10 burn-in using tree annotator part of the beast package 39 calculations of posterior probability were used to establish statistical support for monophyletic clades we retrieved 196 mers-cov sequences from genbank 88 from camels and 108 from humans to analyze molecular evolution of the coronavirus over time additional file 1 all available orf1ab sequences as of march 16 2016 were included and their dates ranged from april 2012 to the latest deposited sequences of june 2015 these sequences were from china egypt france germany oman qatar saudi arabia korea thailand uae uk and us 48 main dataset with all sequences subset with only sequences from humans and third subset with sequences from camels were created sequences were aligned with clustal x and four were removed due to poor alignment genbank accession numbers kj361499 kj156916 kj156941 and kj156942 due to possibilities of recombinant mers-cov strains aligned sequences were tested for recombination via rdp4 4951 sequences 441sa15 and 451sa15 had similarities to two different strains 470sa15 and 355sa13 and were removed from the dataset since recombinant strains could convolute the evolutionary analysis results shown in additional file 2 figure s1 then the phylogenetic content of orf1ab region was investigated by likelihood mapping method to ensure that there were enough signals to compute phylogenies 36 the percentage of dots or noise level falling in the central triangle was 14 for all sequences 23 for human sequences only and 32 for camel sequences only additional file 2 figure s2a b and c respectively for all three datasets the three corners of the triangles summed to be greater than 95 and the central triangles had less than 30 noise so we expected fully resolved tree-like phylogenies 37 temporal signal of the sequences was also investigated temporal signal refers to how much genetic change has taken place between sampling times which is especially important in this case since samples were collected over time 38 in order to make molecular clock inferences genetic distances on phylogenetic trees have to be translated into temporal distances root-to-tip divergence plots are shown in additional file 2 figure s3a b and c no major deviations from the regression line were observed indicating that the genetic divergence of the sequence more or less align with what is expected given the sequence sampling date the regression plots for the main dataset and the human subset had r2 values of 075 and 063 respectively indicating a strong association while the camel subset had r2 of 042 which was weaker but still positive finally the datasets were analyzed for their evolutionary history over time and space using beast the tamura-nei 93 tn93 evolutionary model with gamma distribution g  005 was chosen by modeltest as best fit the gmrf skyride plot with stepping stone marginal likelihood estimator was selected as the best demographic model by beast phylogeographic trees of the main dataset is shown in fig 3 and the camel set is shown in fig 4 main dataset colored according to countries is available as additional file 2 figure s4 and human set trees are included as additional file 2 figures s5 and s6

 over 1300 mers cases or almost 80 of global incidence have been reported from saudi arabia 1 korea has the second highest number of cases n  185 and united arab emirates uae with 83 cases is third 1 in the past three years since discovery of mers-cov eight major healthcare associated outbreaks have been reported worldwide the ninth wave recently occurring in multiple cities of saudi arabia those described here are based on cases with documented nosocomial transmission and are indicated by blue colored clusters in fig 5 cases of mers patients traveling internationally are indicated by arrows representing the direction of their travel and probable zoonotic transmission cases are marked by brown camels in the same figure
 the first hospital related outbreak was in zarqa jordan where 2 cases were confirmed for mers-cov and 11 were declared probable cases retrospectively 52 at the time of this outbreak in april source of the respiratory illnesses was unknown but it was later determined to be identical to the novel coronavirus mers-cov identified in the following september 5254 in total nine cases were reported in 2012 and these early sequences 389jo12 can be seen at the top of the tree in fig 3 in january of 2013 uk reported an intra-familial cluster of 3 where the index case had traveled to pakistan and saudi arabia 8 the three cases 245gb13 9gb13 and 10gb13 in fig 3 formed a distinct cluster in our tree as expected since the viruses isolated would have great genetic similarity consistent with proximity of the dates and location of infection secondary transmission occurred in france where the cov was transmitted to a patient sharing a hospital room with a mers confirmed case who had traveled to uae 55 56 a major outbreak at several hospitals in the al-hasa region of saudi arabia occurred around april 2013 where epidemiological investigation linked 23 mers cases to dialysis and intensive care units 5759 these are indicated in orange near the middle of the phylogenetic tree fig 3 from june to august of the same year there was a community outbreak in hafr al-batin where 12 patients were infected 60 although there was a transmission to a healthcare worker the rest of the patients in this cluster were presumed to have gathered for the large regional camel market and therefore both zoonotic and human-to-human transmissions may have occurred simultaneously interestingly both the al-hasa and hafr-al-batin outbreaks cluster with sequences from riyadh which may indicate travel of persons and possibly mers-cov with them to and from this capital city in the following year from march to june of 2014 the biggest outbreak to date was reported from multiple countries saudi arabia uae iran and jordan 6164 sequences from patients during this time distinctly separated into three clusters jeddah cluster in green abu dhabi cluster red and riyadh cluster pink and they can be seen forming independent clades on the tree and concurrently there were at least 13 cases of exported mers in uae jordan philippines malaysia algeria egypt greece netherlands qatar united states us turkey and austria via travelers who visited saudi arabia as shown in fig 5 1 26 6567 in 2015 a traveler who visited the arabian peninsula was the index case in the largest outbreak of mers outside of the middle east affecting 185 patients and inciting national panic in south korea 2 19 68 nosocomial transmissions were reported from six hospitals especially prevalent in two major medical centers where 3 superspreaders have been linked to 166 cases 2 when total number of cases are compared nosocomial incidence is distinctly greater than incidence of community acquired or zoonotic infections size differences are statistically significant p  00001 and superspreading may be the culprit son of a korean mers patient traveled to china and tested positive there 313cn15 69 the cluster involving korean and chinese samples seem most closely related to saudi arabian strain 465sa15 which was isolated from hofuf in the middle east there was an outbreak in king abulaziz medical center in riyadh with 130 cases from june to august as well as multi-facility outbreaks in riyadh and madinah from september to november of 2015 1 26 70 in 2016 an outbreak in buraidah of saudi arabia occurred in february and march 1 26 and recently an outbreak in a riyadh hospital with 24 cases due to superspreading occurred mid-2016 71 a wide range of basic reproduction numbers r0 from 032 to 13 has been reported for mers and are summarized in fig 6 9 14 21 7279 most researchers agree that mers has a low potential to become a pandemic at this point in time but once the cov is introduced in a nosocomial setting the r0 can range from 2 to 67 and even from 7 to 193 75 78
 after two farm workers tested positive for the cov qatar and who carried out an investigation of camel related mers cases in october 2013 80 all 14 of their camels tested positive for mers-cov antibodies and 11 were positive for the virus itself the partial sequences of the mers-cov sampled from camels were found genetically similar to the human samples parallel case was observed in saudi arabia where researchers were able to pinpoint the specific camel that carried a 100 genetically identical virus as the patient 81 these sequences 11sa13 and 12sa13 can be seen clustering together at the top of fig 4 sequences from camels can be seen interspersed throughout the tree in fig 3 when the camel sequences are examined alone fig 4 it is easy to see that mers-cov from uae form distinct clusters on their own in red while those from saudi arabia seem to have multiple clades in our camel subset analysis the united arab emirates clade diverged from saudi arabian sequences early right after our estimated time to most recent common ancestor tmrca of all taxa march 2012 with 95 confidence intervals july 2011-november 2012 this clade also includes some al-hasa sequences al-hasa is the easternmost region of saudi arabia and therefore geographically closest to uae the intermixing of jeddah and riyadh sequences in the bottom half of the tree may be explained by the camel trades that occur between these two large cities we analyzed the data separately for sequences isolated from humans and camels in order to ascertain when the introduction of mers-cov occurred in humans for the main dataset with sequences from humans and camels tmrca was estimated to be september 2008 95 confidence interval september 2005-june 2011 of riyadh saudi arabia origin for the human subset tmrca for all taxa was march 2011 ci june 2009-november 2011 and for the camel subset tmrca was march 2012 ci july 2011-november 2012 as mentioned above there were three aims for this paper 1 to examine case incidence trends over time and geographic area using epidemiological information 2 to trace evolutionary history of the virus in circulation using genetic data and 3 to explore ways in which these two analyses can be combined to design public health interventions while we aimed to have a thorough and complete representation of all contact tracing conducted the data may not be exhaustive we chose pubmed as the source for peer-reviewed literature since it contains relevant articles from life science and medical journals and we believe the flexibility in article search and use of mesh terms are its strong points in addition the database and genbank are overseen by the same organization national library of medicine which allowed us to link sequence meta-data with literature however pubmed does not include dissertations and conference proceedings that may be relevant and may be biased against articles written in languages other than english non-heterogeneous sequencing of mers-cov is a limitation of this study some patients have been sampled and had their mers-cov sequenced multiple times while others were not reached duplicate sequences from single patients were excluded based on available meta-data since sampling bias can create apparent sinks that are not present in reality 82 better sampling and meta-data annotation would greatly aid further analysis in this regard saudi arabian sequences dominate the phylogeographic tree with greatest number of sequences and large number of probable ancestors figs 3 and 4 this was expected since overwhelming majority of mers cases have been reported there although saudi arabia had the largest amount of genetic sequences available which may skew the phylogenetic analysis it also had the most number of mers cases thus the number of sequences was relatively proportional to the number of cases for the country saudi arabia was the most frequent ancestor for many foreign exports and phylogenetic data indicated that the direction of probable transmission was always from mers endemic region of middle east to non-endemic regions such as europe and asia it has previously been posited that an area with great population such as central saudi arabia may be a hub of genetically diverse mers-covs introduced by passage of people and animals 83 for example the two cases exported to us were genetically dissimilar even though they were both healthcare workers returning from saudi arabia the 348us14 sequence clustered with the hospital outbreak of jeddah 2014 while the other us sequence 349us14 formed a distinct clade with riyadh 84 the jeddah sequences seem most genetically similar to mers-cov isolated from a camel in qatar 286qa14 even though jeddah is located in the west coast of saudi arabia and qatar borders the east the intermixing of geographic backgrounds and phylogenetic clustering seem consistent with the theory on presence of several circulating mers-covs in arabian peninsula due to movement of animals and people geographical distribution of camels and human mers-cov cases are found to be highly correlated 85 since majority of human cases have been concentrated in the middle east and camels in this region showed high seroprevalence of mers-cov antibodies there has been ongoing testing in other camel dwelling regions such as africa and asia 8689 seropositivity ranging from 143 to 100 have been found in countries with dromedaries and a nice summary covering these studies is provided by omrani et al 90 the later tmrca for the camels march 2012 relative to tmrca for cov isolated from humans march 2011 was unexpected since antibodies against mers-cov have been detected in camels from samples archived as early as 1980s but this may be due to dearth of early sequences from camels 87 88 91 earliest cov isolates from camels were sequenced in 2013 and the short sampling timeframe from 2013 to 2015 makes the root of the tree less than reliable 92 93 while antibodies have been isolated from archived samples the virus itself has not been successfully recovered these historical samples may have been isolated in the latter part of the infection or much later after virus has cleared from the system other studies have shown that the virus most likely circulated in camels first becoming genetically divergent even within a single country before jumping to human hosts and because of the wide prevalence of seropositivity along with different lineages of cov in camels more mers-cov infections from camels to humans are bound to occur in the future 49 87 nevertheless this hypothesis cannot be validated without testing for mers-cov antibodies or even mers-cov in historical human samples 17 seasonality of mers has been previously hypothesized from the high incidence of cases during spring and early summer months and it has been postulated that this may be correlated to the camel birthing pattern young newborn camels encountering mers-cov for the first time may get ill and transmit the virus to humans 13 94 95 however outbreaks in latter half of 2015 and early 2016 countered against the expected seasonality 96 infectiousness of mers-cov isolated from camels has been demonstrated by raj et al and shepherds and slaughterhouse workers have been shown to have 15 to 23 times higher proportion of individuals with mers antibodies than the general population 97 98 and zoonotic transmission is an important factor to take into account since evolutionary rates of the virus may be different in humans and camels 99 as mers-cov in camels seem to be much more prevalent and as shown here genetically diverse contacts with camels should be engaged with proper personal protection equipment and public awareness about mers-cov infection related to camels should be raised in surveys conducted regarding mers awareness of the saudi arabian public less than half 471 and 489 were aware that bats and camels could be primary sources of mers-cov 100 101 and there was an optimistic outlook on the fatality rate and treatment of the viral infection which may in turn be factors keeping the mild cases from seeking medical attention only half of the pilgrims surveyed by alqahtani et al were aware of mers and about quarter stated drinking camel milk or visiting a camel farm as possible activities to be pursued during hajj 102 in a 2013 study no mers-cov was detected amongst over 1000 pilgrims tested but 2 of those tested in 2014 were positive 103 104 the crowded living quarters during hajj can impair adequate infection control so health agencies have recommend visiting pilgrims to carefully wash hands consume hygienic foods and isolate themselves when ill 105 106 awareness among health care providers should be raised as well superspreading is commonly observed in hospital settings because of the sustained contact in close quarters medical treatments generating aerosolized virus and susceptibility of hospitalized patients 21 even in the endemic nation of saudi arabia the ministry of health identified issues such as ambiguity on mers case definition inadequate infection control and overcrowding to be sources of hospital outbreaks 107 similar concerns have been raised in korea as well doctor shopping and suboptimal infection control due to crowded hospital rooms seem to have propagated the spread nationwide 19 an interesting facet of this international analysis was the difference in outcomes for mers-cov infected patients depending on where they are located about 41 of patients in the middle eastern and african countries died 1 fatality in europe was 47 where about half of those infected died most likely due to severity of illness in cases transported there for treatment asian countries on the other hand have an atypical 203 case fatality we attempted to estimate the basic reproduction number via bayesian analysis but the mixing of human and camel sequences with multiple introductions of mers-cov from animals to humans was not possible to model however from the literature review many researchers seem to have reached consistent conclusion that the r0 is less than 1 even in endemic countries notable deviations arose only when mers-cov was introduced to a hospital setting then r0 increased by folds superspreading has been cited to be responsible for this trend since the exponential number of transmissions by a few can raise the r0 76 although fundamental and gravely essential part of epidemiological investigation contact tracing is costly labor intensive and prone to human error along with many cases where possible sources of transmissions are unknown zoonotic transmissions were at times conjectural based on patients occupation or recall of food consumption nevertheless we were able to illustrate known and probable transmission events starting from first known cases of mers to recent cases of 2016 in addition we have conducted bayesian phylogeographic analysis of mers-cov strains in both humans and camels which is the most up-to-date and comprehensive to our knowledge purely epidemiological data such as incidence reports and contact tracing can be prone to inaccuracies but can provide background information on individual cases and population level transmission genetic data circumvents human errors and presents quantitative information about an infectious agent but it is not fully informative without accompanying details on collection using phylodynamics to investigate evolutionary history of pathogen can add indispensable details to curb an outbreak such as identifying most closely related cases and predicting origin of the virus revealing additional details at molecular level when epidemiological tracing is inadequate as demonstrated with mers combining these two methods in a holistic approach is valuable for understanding pathogen history and transmission in order to implement effective public health measures through combination of epidemiological data and genetic analysis we present evolutionary history of mers-cov affecting middle east and beyond with focus on hospital outbreaks and zoonotic transmissions  continual bert continual learning for adaptive extractive summarization of covid-19 literature jong park won  the scientific community continues to publish an overwhelming amount of new research related to covid-19 on a daily basis leading to much literature without little to no attention to aid the community in understanding the rapidly flowing array of covid-19 literature we propose a novel bert architecture that provides a brief yet original summarization of lengthy papers the model continually learns on new data in online fashion while minimizing catastrophic forgetting thus fitting to the need of the community benchmark and manual examination of its performance show that the model provide a sound summary of new scientific literature 1   the rapid emergence of the novel coronavirus without much known history has engrossed the international scientific community resulting in an overwhelming amount of publications and data released on a daily basis the rate of publications has far exceeded the time-consuming peer-review process leaving many important information with little to no attention in an attempt to absorb and utilize the unprecedented amount of covid-19 scientific literature prominent journals have opened publications to the public and several platforms have prompted the data science community to aid in the process one of the notable platform has been covid-19 open research dataset cord-19 2 containing thousands of papers published on pubmed and multiple tasks to understand the papers recent progress on language processing has made possible the exploration of massive text corpus otherwise infeasible by manual work attention-based mechanisms vaswani et al 2017 and pre-trianed language representations such as bert devlin et al 2018  open gpt-2 radford et al 2019  xlnet  and elmo peters et al 2018  have achieved a great success in many language fields including sentence prediction and text summarization many language models are adopting a common practice of pretraining on a huge corpus mined from the web followed by a fine-tuning process targeted for specific tasks following this trend we focus on utilizing the popular bert architecture for text summarization task more specifically extractive summarization where important sentences are picked from the text verbatim this task fits the need of the scientific community to rapidly process and extract important information from the inundating number of covid-19 publications while adhering to their original text however as covid-19 papers are published on a daily basis many of them with time-sensitive or unseen content the model also needs to train in an online fashion without experiencing catastrophic forgetting to this end we propose continual bert a novel bert architecture built on existing techniques to learn and extract summaries from a continual stream of new tasks while retaining previously learned information heavily inspired by schwarz et al 2018  our architecture utilizes two separate bert models with layer-wise connections and deploys an alternating training process to minimize catastrophic forgetting it also stacks a small transformer encoder on top for extracting summary sentences from text to the real-world training scenarios progressive neural networks instantiates new neural networks with layer-wise connections for new tasks which mitigates catastrophic forgetting but renders inscalable progress  compress schwarz et al 2018 addresses the scalability by using two separate neural networks with layer-wise connections and online elastic weight consolidation  dynamically expandable networks lee et al 2017 takes a different approach by adaptively sizing the neurons in each layer of a network regularized with group sparse regularization scardapane et al 2016  albeit the rapid development on continual learning few works has focused on incorporating the process onto bert for language processing ernie 20 sun et al 2019 modifies the pretraining aspect of bert with a continual learning framework that learns on a broad spectrum of tasks in contrast our model modifies the fine-tuning process of bert for continual learning which enables leveraging any pre-trained models and focus more on the task-specific fine-tuning process extractive summarization summarization of text has two categories extractive and abstractive the former extracts sentences deemed as a summary while the latter constructs a unique summary that assimilates the extracted sentences in the field of extractive summarization summarunner nallapati et al 2016 uses a sequential model based on recurrent neural network latent zhang et al 2018 distingushes latent and activated variable sentences and extracts gold summaries the latter sentences to improve training neusum jointly learns and scores extracted sentences and bertsum liu 2019 stacks transformer layers on top of bert for sentence extraction although our architecture assimilates bertsum it differs in that it implements an additional component of continual learning for online tasks the base of continual bert is two identical bertbase models -12-layer 768-hidden and 12-headsinitialized with the pre-trained bert-base-uncased weights following progress  compress the first bert model is referred to as active column ac for its ability to train actively without restrictions while the second bert model is named knowl- edge base kb for its preservation of previously learned information the model trains using an alternative training scheme which is described on 32 during the latter stage of the training the model uses online elastic weight consolidation ewc in conjunction with knowledge distillation ewc calculates the bayesian posterior distribution of parameters using laplace approximation to calibrate gradient descent towards the overlapping learning region of both the previous and new tasks the modified online version of ewc addresses the quadratic cost of the original ewc by using a running sum of the diagonal fisher information matrix and the mean of online gaussian approximation the model also establishes layer-wise connections from kb to ac using custom adaptors to enable positive forward transfer of previous tasks inputs are calculated in parallel through kb and ac where kb captures the hidden states at each transformer layer and wires it to the one layer up in ac with calculations as follows where h i1 and h kb i1 are the hidden states from layer i-1 of ac and kb respectively w i  u i  v i learnable weights b i  c i biases  non-linear operation element-wise multiplication and  learnable vector  is initialized from a uniform distribution on the interval 0 1 for extracting summary sentences from literature we stack a two-layered transformer encoder on the top more formally a transformer encoder 2-layer 768-hidden and 12-heads computes the importance of each sentences in text with hidden states from the two bert system two identical bert models are initiated and training on new tasks involves layer-wise connection from knowledge base to active column the final output from active column is fed into a small two-layered transformer stack that outputs the extracted summary sentences the newly learned parameters are consolidated into the previously learned parameters training is divided into two stages compress and progress which are executed in the order listed compress during compress the first stage ac trains normally on a new task with an exception of incorporating layer-wise connections from kb on each layer this provides positive forward transfer from previously learned tasks to benefit training on the new task no other restrictions are imposed other than the loss function hence unrestricted the model aims to minimize the loss where y is the abstract sentence label progress after compress progress begins where knowledge from ac is distilled into kb in a teacher-student scheme with online ewc to preserve previous information and thus minimize catastrophic forgetting the model aims to minimize the ewc loss where  is the parameters learned on a new task f  k1 and   k1 the diagonal fisher and the mean of all previous learned parameters of the online ewc gaussian approximation and  hyperparameter to dictate the degradation of previously learned parameters in addition the model aims to reduce the knowledge distillation loss where x is the input  k x and  kb x prediction of ac after learning and kb e expectation over a task dataset in conclusion during the progress step the model minimizes the loss the alternating training procedure allows the model be online -continuously learning on new tasks online training is essential to actively updating covid-19 literature where new information not only relies on previous data but also are timecritical to the understanding of the status quo of the coronavirus since our tasks contain text that are similar in topic ie covid-19 we preserve the weights of ac throughout new tasks to benefit from previous unrestricted learning if the tasks differ in nature we recommend re-initializing ac with pre-trained bert weights to maximize the learning of the new task  we use pubmed articles from cord-19 as our dataset  we modify the dataset to include only articles with abstracts resulting in total of 42000 out of 57037 articles we use abstracts as the gold summary and train the model to extract up to 15 sentences that are most similar to the abstract to simulate an online training environment we divide the articles into tasks of 5000 articles ordered by ascending publication time the model initially learns on tasks with older articles and gradually transits through to the newest articles in an online fashion we prepare the dataset as a classification task for bert with each classification being a sentence as proposed by liu 2019  each sentence is padded with cls at the front to tag it as a classifiable entity and alternating word embedding scheme is used to distinguish adjacent sentences as different sentences the model learns to classify cls  which outputs indice for summary sentences the models weights are initialized with xavier initialization glorot and bengio 2010  for optimization we use adamw loshchilov and hutter 2019 with weight decay of 001  1 09  2 0999 and 1e  6 for learning scheduler we use linear learning schedule with learning rate of 5e  4 and 5 of each task approximately 300 articles for warm-up for knowledge distillation we use  of 20 and  ce 05 for ewc we use  of 15 and  099 other settings include weight decay of 001 batch size of 64 and 3 epoch for training ac and kb on each task after learning on nine tasks ordered in ascending publication time continual bert recorded a loss of 021 for compress consolidation stage and 215 for progress stage indicating a difficulty in calibrating to both the previous and new parameters rouge evaluation on the scisummnet is presented on 2 and manual evaluation on recent covid-19 literature is presented on  the manual summary evaluation which is a more realistic and sound technique compared to rogue shows that the model can produce a sound summary of extracted sentences spread throughout the literature this summary assimilate many sentences provided by the authors which further supports the models capability to learn well on new tasks in an online manner the online training ability of continual bert enables adaptive learning on new data flowing in a time-sequential manner especially fitting to the overwhelming amount of covid-19 literature published on a daily basis in contrast to the provided abstracts extractive summarization of those literature can provide not only a sound original summary of the article but also indications of where the interesting sentences and ideas lies within the text this feature can be handy with longer papers much of covid-19 literature as the readers can save significant amount of time while understanding the broad idea of the papers the scalable architecture of continual bert also enables continually learning over longer time and in more frequency to digest new research data and information faster the difficulty that continual bert experienced during the progress step can be justified with the fact that cord-19 contains publications dating back to the 20th century which present radically different information to the more modern publications this information disparity can be fixed by penalizing more for older publications through time threshold such as before the coronavirus pandemic we hope that the model provides a ground for other researchers to explore into the area of summarization for covid-19 and many other literature for future explorations we propose constructing a dynamic version of the model such as dynamically increasingdecreasing network neurons manual evaluation on a recent covid-19 publication published on july 1 2020 extracted summary is in lower-case since the pre-trained model is uncased bert-base-uncase structure of the full sars-cov-2 rna genome in infected cells lan et al 2020 28 pages abstract sars-cov-2 is a betacoronavirus with a singlestranded positive-sense 30-kilobase rna genome responsible for the ongoing covid-19 pandemic currently there are no antiviral drugs or vaccines with proven efficacy and development of these treatments are hampered by our limited understanding of the molecular and structural biology of the virus like many other rna viruses rna structures in coronaviruses regulate gene expression and are crucial for viral replication although genome and transcriptome data were recently reported there is to date little experimental data on predicted rna structures in sars-cov-2 and most putative regulatory sequences are uncharacterized here we report the secondary structure of the entire sars-cov-2 genome in infected cells at single nucleotide resolution using dimethyl sulfate mutational profiling with sequencing dms-mapseq our results reveal previously undescribed structures within critical regulatory elements such as the genomic transcription-regulating sequences trss contrary to previous studies our in-cell data show that the structure of the frameshift element which is a major drug target is drastically different from prevailing in vitro models the genomic structure detailed here lays the groundwork for coronavirus rna biology and will guide the design of sars-cov-2 rna-based therapeutics extracted summary sars-cov-2 is an enveloped virus belonging to the genus beta coronavirus which also includes sarscov the virus responsible forthe 2003 sars outbreak and middle east respiratory syndrome coronavirus merscov the virus responsible for the 2012 mers outbreak despite the devastating effects these viruses have had on public health and the economy currently no effective antivirals treatment or vaccines exist there is therefore an urgent need to understand their uniquerna biology and develop new therapeutics against this class of viruses coronaviruses covs have single -stranded and positive -sense genomes that are the largest of all known rna viruses 27 32 kb masters  2006  previous studies oncoronavirus structures have focused on several conserved regions that are important forviral replication for several of these regions such as the 5 utr the 3 utr  and the frameshift element fse structures have been predicted computationally with supportive experimental data from rnase probing and nuclear magnetic resonance nmr spectroscopy plant et al  2005 yang and leibowitz  2015   identifying highly influential travellers for spreading disease on a public transport system ahmad shoghri el jessica liebig raja jurdak lauren gardner salil kanhere s  the recent outbreak of a novel coronavirus and its rapid spread underlines the importance of understanding human mobility enclosed spaces such as public transport vehicles eg buses and trains offer a suitable environment for infections to spread widely and quickly investigating the movement patterns and the physical encounters of individuals on public transit systems is thus critical to understand the drivers of infectious disease outbreaks for instance previous work has explored the impact of recurring patterns inherent in human mobility on disease spread but has not considered other dimensions such as the distance travelled or the number of encounters here we consider multiple mobility dimensions simultaneously to uncover critical information for the design of effective intervention strategies we use one month of citywide smart card travel data collected in sydney australia to classify bus passengers along three dimensions namely the degree of exploration the distance travelled and the number of encounters additionally we simulate disease spread on the transport network and trace the infection paths we investigate in detail the transmissions between the classified groups while varying the infection probability and the suspension time of pathogens our results show that characterizing individuals along multiple dimensions simultaneously uncovers a complex infection interplay between the different groups of passengers that would remain hidden when considering only a single dimension we also identify groups that are more influential than others given specific disease characteristics which can guide containment and vaccination efforts  human mobility continues to play a vital role in spreading infectious diseases within a population 1 2  ongoing population growth and the high reliance of individuals on public transport services in highly populated cities provide a suitable platform for contagious diseases such as measles the recently emerged coronavirus and influenza to spread widely and rapidly 3 4 5  for example individuals travelling on a bus are in close enough proximity to infect each other and can carry the infection to distant locations across the public transport network 6 4  additionally some pathogens may remain in the environment eg a bus for a prolonged period and can infect susceptible individuals after the infectious person has left the area 1 7  furthermore transport services shorten distances and times and strongly connect different suburbs potentially exposing communities to a high infection risk 4  the risk of disease spread due to human movement is evident from the current novel coronavirus outbreak in china and internationally with chinese authorities shutting down public transportation within the affected area 5  the recent uptake of smart travel cards and the availability of this data have created an unprecedented proxy to elicit different travelling behaviours and to study their effects on disease spread 8 9  the analysis of such data is critical to understand the spreading dynamics of a disease and consequently to develop effective containment strategies 10  previous studies investigated several spreading dynamics of infectious diseases however to the best of our knowledge none of the studies has incorporated different aspects and dimensions of mobility behaviour simultaneously in this paper we study three aspects of mobility behaviour ie the degree of exploration the distance travelled and the number of encounters of passengers using the sydney bus network in the context of infectious disease spread by considering the three dimensions simultaneously we identify previously unknown mobility behaviours the high spatiotemporal resolution of the dataset allows us to construct a time resolved physical human contact network to simulate disease spread specifically we trace the infection flows between groups of passengers who display different mobility behaviours to investigate the change in the spreading dynamics in addition we investigate how changes in the infection probability and the time pathogens remain suspended in the environment affect the spreading of the disease this study identifies the most influential passenger groups in a disease spread scenario for different disease characteristics and types our simulation results identify four dominant transmission paths between the mobility groups that should become a focus of containment efforts in addition we find that highly connected passengers who regularly visit the same places have the highest spreading power when pathogens do not remain in the environment however with an increase in the suspension time of pathogens highly connected passengers who visit new locations become the most efficient spreaders an increase in the infection probability on the other hand amplifies the spreading power of all mobility groups especially for passengers who regularly visit the same places and travel short distances until reaching a saturation point at a probability of 05 the remainder of the paper is organised as follows we begin by discussing relevant previous work in section ii in section iii we present our framework for modelling infectious diseases on human contact networks we explain the approach for classifying the individuals based on their movement behaviour and introduce the dataset that we use for our case study in section iv we run extensive trace driven simulations to investigate the underlying interactions and disease transmission dynamics between the different groups of passengers furthermore we study the effect of changing the infection probability and the time pathogens remain in the environment on the transmission dynamics finally we identify the most influential mobility behaviours for various disease characteristics which can guide intervention strategies section v concludes the paper studies of epidemiology have long recognized that human mobility plays a key role in fostering severe disease epidemics that may result in high rates of morbidity and mortality 4  furthermore these studies have acknowledged the importance of identifying the most influential individuals as it can aid to predict outbreaks before their occurrence 10 11  health related datasets and detailed patient mobility profiles present informative data that may be used to reflect the status of a disease and its progression 8 12  however accessing such information is challenging due to privacy concerns and other related issues 10 12  in the absence of health related data previous work has studied alternative data sources an important body of research has explored the use of call detail records cdrs and data from the global positioning system gps to build epidemiological models and to study the spatial transmission of various diseases in a population at both city and country levels 13 20  in 20 cdr and gps datasets were exploited to extract two types of mobility behaviours the authors used the recurrent mobility and the total mobility characteristics to group individuals into returners and explorers returners are individuals who can be characterized by their most visited locations as these dominate their movement behaviour whereas explorers are individuals who often visit new places and cannot be characterized by their most frequently visited locations the statistical measure used to compute the total mobility of an individual is the total radius of gyration r g  defined as 20  where l is the set of all visited locations by the individual r i is the coordinates of the visited location i n i is the individuals visitation frequency of location i r cm is the centre of mass of all visited locations and n is the total number of visited locations the authors also defined the k-radius of gyration denoted r g k  which is similar to the overall mobility formula with the difference that the set of locations l is reduced to the k most visited locations the value of r g k represents the recurrent mobility of the individual the correlation between the recurrent and the total mobility values distinguishes between the two mobility patterns namely returners and explorers if the recurrent mobility of an individual dominates the total mobility that is r g k  r g 2 the individual is classified as a returner otherwise the individual is an explorer the authors of 20 found that explorers have more impact on disease spread than returners their experiments consisted of 10000 individuals chosen randomly from a pool of 46000 individuals to study the impact of each mobility behaviour on the spreading different proportions of returners and explorers were used the extent of disease spread is computed through the global invasion diffusion threshold r   this experimental setup presents three main limitations first changing the proportion of the mobility groups alters the topology and the characteristics of the network being studied second the contact links connect geographical areas rather then actual human physical encounters third their study of spreading power was performed on a static network in which if a link existed at any point in time that link is considered present during the entire period of study these limitations make the experiments theoretical as they study a snapshot of a possibly unrealistic contact network several other limitations emerge when cdr and gps datasets are used in the context of disease spread 17 21  most importantly these datasets lack accurate localization of the individuals due to the distant positioning of cellular towers and poor satellite signals 22  hence these datasets do not guarantee the existence of real physical encounters between the individuals 22  in addition individuals who are tracked via gps may be driving a car and hence are not in physical contact with other individuals 22  recent studies of epidemiology showed an increasing interest in dynamic networks that guarantee the existence of real physical human contact when studying disease spread 23  a well suited source of data to study the spreading dynamics of diseases in dense cities are public transit records 4  several studies have confirmed the presence of a risk factor between the use of bus transportation services and the spreading of many airborne diseases such as tuberculosis measles and influenza 24 4  the authors of 24 stated that bus routes are veins connecting even the most diverse of populations and showed that individuals who reported regular use of buses are more likely to be infected by tuberculosis in fact the congregated and enclosed setting of buses presents a suitable environment for any contagious respiratory disease to spread widely the infectious pathogens can easily be transmitted onward among passengers through coughing and sneezing 4  in addition natural and artificial air flow can move suspended pathogens through space this makes all individuals in an enclosed space like a bus susceptible in our previous work 25 we confirmed the existence of explorers and returners in the public bus transit dataset of sydney australia furthermore through extensive simulations we showed that explorers are generally more influential in spreading a disease through the network in comparison to returners also long distance travellers were found to be more influential than short distance travellers however when only long distance travellers are considered returners showed a greater propensity in spreading the disease over explorers the work proved the presence of a deeper and more complex interplay between various mobility aspects when it comes to spreading a disease on a public transport system in our previous work we did not consider the connectivity aspect of the individuals which holds critical information in contact based spreading scenarios further our simulations only considered direct encounters between passengers and assumed an infection probability of 1 while disease transmission is possible through direct encounters ie the infected and the susceptible individual are present in the same place at the same time pathogens can remain in the environment for an extended period of time 1 7  therefore an infectious person can infect susceptible individuals without a direct encounter contact networks where only direct encounters are considered are commonly called spst same place same time networks networks that in addition to direct contacts capture indirect encounters caused by suspended pathogen are called spdt same place different time networks previous studies have shown that considering the suspension time of pathogens changes the underlying topology of the contact network and alters the spreading dynamics of contagious diseases significantly 1 7  this paper addresses the limitations of our previous work through the addition of the connectivity dimension and by considering different suspension times of particles and infection probabilities we identify groups of passengers that have a high potential to spread a disease through a public bus network although there are several studies that recognized the importance of human mobility data to identify the most influential individuals in a network none of the studies tried to use a comprehensive mobility dataset to extract patterns along different movement aspects simultaneously and study the detailed interaction between the different patterns in particular we consider the passengers total mobility recurrent mobility and connectivity the impact of each group on the spreading will be evaluated as all the infectious activities occurring in the background of the simulations are traced to understand the disease spread dynamics on a public transport network we construct spst and spdt contact networks from the smart card data and run a susceptible-infected-recovered s-i-r disease spread model on top at the beginning of the simulation all bus passengers except a given number of randomly chosen seed nodes are susceptible the seed nodes are infectious and able to transmit the disease to susceptible individuals when a susceptible individual encounters an infectious individual or in the case of spdt networks comes in contact with pathogens that remain in the environment the susceptible individual moves to the infectious state with a given probability the individual remains infectious for a given period of time before recovering from the disease once in the recovered state the individual is no longer susceptible and remains in the recovered state until the end of the simulation figure 1 exemplifies the s-i-r disease spread simulation on the bus network we demonstrate how the spreading dynamics are affected by changing two key parameters namely the probability of infection denoted  and the suspension time of pathogens denoted d t  the case d t  0 corresponds to an spst disease spread scenario and hence a susceptible passenger will be infected only if both individuals meet on the same bus at the same time when d t  0 the infectious particles remain on the bus for an additional time d t  allowing the infectious passenger to infect susceptible individuals after disembarking while simulating the empirical movements of individuals we track all encounters and infection transmissions at every encounter the identification number of the two passengers in contact are recorded similarly when an infection is transmitted the identification numbers of the infectious and the susceptible individuals are recorded to understand how different mobility behaviours influence the transmission paths of the simulated disease we classify the bus passengers into different mobility groups we modified the opportunistic network environment one simulator 26 to carry out our trace driven simulations and spread a disease on a large scale real-world transport network before running the disease simulations on the constructed networks we cluster the bus passengers into different groups based on their mobility behaviour to do so we simultaneously consider the degree of exploration the distance travelled and the number of encounters during the period of study first we plot passengers mobility profiles in three-dimensional space with the x-axis corresponding to the passengers total radius of gyration the y-axis corresponding to the k-radius of gyration ie the recurrent mobility and the z-axis corresponding to the number of encounters next we cluster the individuals into two groups along each dimension the degree of exploration is divided into returners and explorers the distance travelled into short distances and long distances and the number of encounters into low connected and highly connected individuals classifying our passengers along the three dimensions results in 2 3  8 different types of movement behaviours in order to identify each of the groups we normalize the values of the three dimensions between 01 and use the approaches detailed in the following subsections 1 returners and explorers to split the population based on the degree of exploration we project all the points onto the xy-plane and use the bisector method to differentiate between returners and explorers when plotting the passengers total mobility and recurrent mobility values on the cartesian plane points along the x-axis correspond to explorers as their recurrent mobility does not dominate their total mobility and points along the y  x line correspond to returners whose total mobility can be well represented by their recurrent mobility as r g k  r g  our clustering approach results in 354 explorers and 647 returners 2 short distances and long distances to cluster the passengers based on their travelled distance we project the points onto the x-axis and apply a standard k-means clustering algorithm 27 with k2 this results in two groups namely passengers who travel short distances and have a relatively low radius of gyration 87 and passengers who travel long distances and have a relatively high radius of gyration 13 3 low connected and highly connected individuals in order to cluster the passengers based on their degree centrality ie the number of encounters we use a similar approach as in the previous section we project the points onto the z-axis and apply the standard k-means clustering algorithm 27 with k2 which splits the population in low connected passengers 393 and highly connected passengers 607 specifically we differentiate between passengers who encounter a high number of other passengers and those who experience fewer encounters with other passengers during the month of study the public transport dataset consists of 20295908 trips made by 2 million bus users the dataset is recorded in the greater sydney area of new south wales australia during the month of april in 2017 each trip record records the following information the passengers smart card identification the bus number in use and the time and location the passenger entered and exited the bus sydney for a short time or travellers who lost or damaged their card as infrequent travellers cannot be classified accurately due to the lack of sufficient data records we remove these passengers from our analysis in order to explore how a threshold on the number of trips affects the total number of passengers included in our analysis we plot in fig 3 the population size against varying threshold values between 1 and the maximum number of trips observed in the data the population size drops rapidly with the increase of the threshold especially at low values this is due to the high number of passengers who use the bus only occasionally see fig 2  for our analysis we set the threshold to 15 trips per month that is individuals who travelled less than 15 times with the bus during april are excluded from the analysis the threshold of 15 trips is chosen so that the passengers have travelled at least on half of the days of the month the final dataset has 36013436 records belonging to 424290 bus passengers in fig 4 we compare some key topological aspects of the original and the resulting networks figure 4 a shows the distribution of connected component sizes for both networks we notice that the two networks have similar structures with a giant component and several smaller components there are fewer components of size one in the resulting network which can be attributed to the fact that the original network contained many infrequent travellers who were isolated from the rest of the network this observation also applies to the isolated components consisting of less than eleven individuals figure 4 b shows the degree distribution of the original and the resulting networks the degree of a passenger is the total number of direct encounters experienced during the month of april we notice that the degree distributions of the original and the resulting networks increase linearly until reaching maximum values of 9726 and 5521 respectively then both distributions drop exponentially the drop in the frequencies of the resulting network is due to the removal of passengers who travelled less than 15 trips the difference between the two distributions is especially clear at low degree values as passengers with few number of trips are less likely to have higher number of contacts in this section we present the identified mobility patterns and discuss the results of our disease spread simulations the different groups resulting from our classification and clustering tasks are visualized in fig 5  all subfigures display the same plot from a different angle each point in fig 5 corresponds to one individual in the dataset and its coordinates represent the values of the three-dimensional movement behaviour of the corresponding passenger in the coming figures and tables we refer to the groups using the notation degree of exploration connectivity distance travelled the pie chart in fig 6 summarizes the percentage of each of the eight classified groups of passengers in the network we notice that highly connected returners who travel short distances constitute the major portion 368 of the population this group of individuals are regular commuters who tend to use public transportation to commute between home and work during peak hours and rarely explore or visit other places during the month 209 of public transport users are classified as low connected returners who travel short distances we believe that these individuals regularly travel to specific locations that are less crowded or during off-peak hours for example people who go to shopping malls in the afternoon on the other hand explorers are individuals who in addition to their regular commute visit other places for example going to malls to shop or going to touristic attractions for leisure the following subsections present and discuss our simulation results for every experiment we randomly choose 500 individuals who are infectious at the beginning of the simulation and can transmit the disease to susceptible individuals individuals remain infectious for five days which is the average infectious period for influenza 28  before recovering every experiment is simulated 100 times and the results averaged  in our first experiment we set the infection probability  to 1 and the pathogen suspension time d t to 0 corresponding to an spst disease spread scenario this experiment serves as a baseline for comparison to other parameter settings that are explored in further simulations in table i  we present the total number of encounters and the total number of infections that were transmitted and received by every mobility group in addition we compute the average number of encounters the average number of transmissions and the average number of infections received per individual for each group dividing the total number of infections caused by a given group by its population size results in the average number of infections that one individual from that group causes during the simulation period this average value of infections is not constrained to a specific target group but to all groups in the network similarly dividing the total number of infections received by a given group by its population size results in the average number of infections that one individual of that group receives during the simulation period interestingly the averages of received infections per individual is nearly the same across all the groups with a value just divide individuals into explorers and returners but to distinguish them further along other dimensions such as the distance travelled and the connectivity as their spreading abilities differ in order to visualise the disease transmission dynamics between the groups we use a chord diagram see fig 7  the diagram shows cumulative disease flows between the different groups the eight different groups are represented by circle segments with each group being associated to a unique colour for example the red segment corresponds to highly connected explorers who travel long distances see label a in fig  7  the links indicate the volume of disease transmissions between any two groups and are assigned the same colour as the source group the thickness of each link is proportional to the average number of people that one individual from the source group infects in the target group for example the link labeled b shows the volume of disease flow transmitted from the group of highly connected explorers who travel long distances red segment to the group of highly connected returners who travel short distances blue segment links that start and end at the same segment represent disease transmissions between individuals of the same mobility group for scaling purposes we multiply all average number of infections caused per individual by 1000 and show the resulting values in the chord diagram the diagram in fig 7 clearly identifies four dominant infection paths these occur amongst highly connected returners who travel long distances cyan segment with individuals causing on average 052 infections within their own mobility group and highly connected returners who travel short distances blue segment with individuals causing on average 057 infections within their own mobility group highly connected explorers who travel long distances red segment infect on average 05 highly connected returners who travel short distances blue segment during the simulation period highly connected explorers who travel short distances orange segment infect on average 047 highly connected returners who travel short distances blue segment furthermore low connected returners who travel short distances form a group that is prone to receive infections but less likely to infect individuals from other groups see the incoming non-pink links that occupy the majority 77 of the pink segment in fig 7 and table i  on the other hand highly connected explorers who travel long distances have caused the greatest number of infections per individual on average however this group is less likely to get infected in comparison to other groups see red segment in fig 7  we observe that the disease transmissions from highly connected explorers who travel long distances dominate this groups activity as the red outgoing links going to all other groups constitute the majority of the segment with more than 90 that is even a low number of infected individuals of this group would be sufficient to infect other groups and spread the disease through the entire network highly connected explorers who travel long distances infect 17 individuals on average during the simulation period highly connected returners who travel short distances receive a high number of infections and mostly infect individuals within their own group see blue segment in fig 7  this behaviour is expected as this group consists of regular commuters who display consisted movement behaviour highly connected returners who travel long distances see cyan segment in fig  7  display a similar behaviour of mostly infecting individuals within their own group highly connected explorers who travel the mobility groups are represented by differently coloured circle segments eg the red segment labelled a corresponds highly connected explorers travelling long distances links represent disease flow between mobility groups and are coloured by the source group the link with label b shows the average number of infections that highly connected explorers who travel long distances transmit to highly connected returners who travel short distances c refers to the start of the link coloured by the receiving group d refers to the end of the link coloured by the transmitting group e refers to the relative infection transmissions receptions and overall total for each segment long distances spread infections to all other groups see red segment in fig 7  although the average number of encounters is lower than groups who infect specific target groups our results highlight important interactions between the eight identified groups and shed light on disease spread dynamics that should be given more attention while monitoring a disease and applying prevention measures to understand how different disease types and characteristics change the spreading dynamics between the eight groups we perform two additional experiments in this experiment we run the simulations with different suspension times of pathogens ie d t  15 30 60 and 120 minutes while keeping   1 for each value of d t we construct a matrix that shows the difference in the average number of infections caused and received by each mobility group in comparison to experiment 1   1 d t  0 positive values refer to a gain in disease transmissions whereas negative values indicate a loss the rows of the matrix correspond to the groups that cause the infections and the columns correspond to the groups that receive the infections figures 8a and 8 b show the matrices for a suspension time of 15 minutes and 30 minutes respectively the matrix in fig  8 a shows that the average number of infections caused per individual increased or remained the same for the four groups of explorers top four rows of the matrix the spreading potential of the four groups of returners bottom four rows of the matrix generally decreased as the suspension time is increased to 30 minutes we observe further increases in the average number of infections caused by explorers and further decreases for returners see fig 8 b we highlight that the increase in d t weakens the spread of infections within self-loops the two groups of highly connected returners the loss in the infection power of returners coincides with an increase in infections caused by explorers especially for highly connected explorers who travel long distances since almost every individual of the population is infected at the end of the simulation period we conclude that an increase in the time that pathogens remain in the environment favours the infection power of explorers that is explorers are even more influential in an spdt disease spread scenario we only show the results for d t  15 and 30 as no change in the behaviour was seen for d t  60 and 120 the values for explorers keep increasing and those of returners decrease in the third experiment we vary the infection probability  while setting the pathogen suspension time d t  0 the considered probabilities are 005 01 015 025 05 075 and 1 to understand the effect of the infection probability on the disease spread we construct matrices that show the differences in the average number of infections caused per individual between each two consecutive values of  the average number of infections caused per individual increases rapidly for all groups with an increase of  from 005 to 025 this result is visualised in fig 9 with all matrix elements being positive when  is increased from 025 to 05 we see only a slight increase in the spreading power of all groups further increases of  to 075 and 1 do not result in significant changes in spreading powers figure 9 shows that all individuals who travel short distances experienced the most increase in the number of received infections whether they are low or highly connected this pattern can be seen through the dark coloured columns of the short distances groups the observation is due to these groups constituting the highest percentages in the network allowing them to have the highest total number of encounters see fig  6 and table i  in addition the increase in the probability of infection strengthens the self-loops of the groups infections within the same group especially those of the short distance returners we conclude that increasing the infection probability favours the spreading power all mobility groups the increase of spreading power is relative to the interaction between each pair of mobility groups for increasing infection probabilities each element in the matrix increases until ultimately reaching the values presented in section iv-b in which the probability is set to 1 this is the first study to identify mobility patterns along three dimensions simultaneously namely the degree of exploration the distance travelled and the number of encounters we found previously unknown mobility patterns that were thoroughly investigated to understand the spreading dynamics of contagious diseases on a city wide public transport system we ran extensive disease spread simulations with varying values for the infection probability and the suspension time of pathogens our results show that characterizing individuals along multiple dimensions simultaneously uncovers a complex infection interplay between the different groups of travellers furthermore the infection probability and the suspension time of pathogens play different roles in the spread highly connected passengers who regularly return to the same places play the most important role in the spreading when pathogens do not remain in the environment however with an increase in the suspension time of pathogens highly connected passengers who visit new locations are the most influential unlike the suspension time increasing the infection probability does not affect particular mobility groups but increases the infection power of all groups especially for returners who travel short distances our simulation experiments are abstractions of the real-world and flexible to adapt to different contexts we presented a framework that can be applied to model any disease that is spread through a physical contact network our findings are especially beneficial to advise health authorities on the design of more efficient intervention and containment strategies depending on the characteristics of the emerging diseases we plan to open-source the modified simulator in order to be used broadly for similar types of datasets and scenarios  twitter discussions and emotions about covid-19 pandemic a machine learning approach jia xue junxiang chen ran hu chen chen chengda zheng xiaoqian liu tingshao zhu   eight million people have been confirmed positive of covid-19 across 110 countries as of mid-june 2020 and the death toll has reached close to 435000 1  the widespread utilization of social media such as twitter accelerates the process of exchanging information and expressing opinions about public events and health crises 2 3 4 5  covid-19 is one of the trending topics on twitter since january 2020 and has continued to be discussed to date since quarantine measures have been implemented across most countries eg the shelter-in-place order in the united states people have been increasingly relying on different social media platforms to receive news and express opinions twitter data are valuable in revealing public discussions and sentiments to interesting topics and real-time news updates in global pandemics such as h1n1 and ebola 6 7 8 9  chew and eysenbachs study shows that twitter can be used for real-time infodemiology studies a source of opinions for health authorities to respond to public concerns 6  in the current covid-19 pandemic many government officials worldwide are using twitter as one of the main communication channels to regularly share policy updates and news related to covid-19 to the general public 10  since the covid-19 outbreak a growing number of studies have collected twitter data to understand the public responses to and discussions around covid-19 11 12 13 14 15 16  for instance abd-alrazaq and colleagues adopt topic modeling and sentiment analysis to understand main discussion themes and sentiments around covid-19 using tweets collected between february 2 and march 15 2020 11  budhwani and sun compare tweets discussions before and after march 16 2020 when president trump tweeted the chinese virus and find a significant increase use of chinese virus in peoples tweets across many states in the united states 14  mackey and colleagues analyze about 3465 tweets collected between march 2 and 20 2020 using a topic model to explore users self-reported experiences with covid-19 and related symptoms 16  ahmed and colleagues conduct social network analysis and content analysis of the collected tweets between march 27 and april 4 2020  to understand what may have driven the misinformation that linked 5g towers in the united kingdom to the covid-19 pandemic 12  as conversations on twitter continue to take place and evolve it is worth continuing to use tweets as a source of data to track and understand what the salient topics discussed on twitter in response to the covid-19 pandemic and track their changes in different time periods are to expand the literature on public reactions to the covid-19 the present study aims to examine the public discourses and emotions about the covid-19 pandemic by analyzing more than 4 million tweets collected between march 7 and april 21 2020 we used a purposive sampling approach to collect covid-19 related tweets published between march 7 and april 21 2020 our twitter data mining approach followed the pipeline displayed in figure 1  data preparation included three steps 1 sampling 2 data collection and 3 preprocessing the raw data the data analysis stage included unsupervised machine learning sentiment analysis and qualitative method the unit of analysis was each message-level tweet unsupervised learning is one approach in machine learning and used to examine data for patterns and derives a probabilistic clustering based on the text data we chose unsupervised learning because it is commonly used when existing studies have little observations or insights of the unstructured text data 17  since a qualitative approach has challenges analyzing large-scale twitter data unsupervised learning allows us to conduct exploratory analyses of large text data in social science research in the present study we first employed an unsupervised machine learning approach to identify salient latent topics using the topics we used a qualitative approach to develop themes further as a qualitative approach allows a deeper dive into the data such as through manual coding and inductively developing themes based on the latent topics generated by machine learning algorithms we used a list of covid-19 related hashtags as search terms to fetch tweets such as coronavirus 2019ncov covid19 coronaoutbreak and quarantine appendix figure 1  we used python to clean the raw data the process was as follows 18  1 we removed the hashtag symbol users and urls from the tweets in the dataset 2 we removed non-english characters non-ascii characters because the present study focuses on tweets in english 3 we removed special characters punctuations and stop-words listed in 19 from the dataset as they do not contribute to the semantic meanings of the messages latent dirichlet allocation lda 20 is one of the widely used unsupervised machine learning approaches allowing researchers to analyze unstructured text data eg twitter messages based on the data itself the algorithm produces frequently mentioned pairs of words the pairs of words co-occur together and the latent topics and their distributions over topics in the document 21  existing studies have indicated the feasibility of using lda in identifying the patterns and themes of the tweets texts related to covid-19 11 22  to triangulate and contextualize findings from the lda model we employed a qualitative approach to develop themes further specifically using braun and clarkes 23 six steps of thematic analysis 1 getting familiar with the keyword data 2 generating initial codes 3 searching for themes 4 reviewing potential themes 5 defining themes and 6 reporting since the thematic approach relies on human interpretation a process that can be significantly influenced by the personal understanding of the topics and a variety of bias we had two team members conduct the first five steps independently then the two members reviewed all identified themes and resolved disagreements together finally we finalized themes corresponding to each one of the 13 topics we used sentiment analysis a natural language processing nlp approach to classify the main sentiments of a given twitter message such as fear and joy 24  in the study we used the nrc emotion lexicon which consisted of eight primary emotions anger anticipation fear surprise sadness joy disgust and trust 25  we followed the four steps to calculate the emotion index for each twitter message including 1 removing articles pronouns eg and the or to 2 applying a stemmer by removing the predefined list of prefixes and suffixes eg running after stemming becomes run 26  and 3 calculating the emotion index we only kept one emotion with the maximum matching counts if one sentence has multiple emotions and 4 calculating the scores for each eight-emotion type we discussed the four steps in detail in the previous study 18  a total of four million n4196020 tweets consisted of our final dataset after pre-processing all raw data we identified the most popular tweeted bigrams pairs of words related to covid-19 bigrams captured two concessive words regardless of the grammar structure and semantic meaning and may not be self-explanatory 27  including covid 19 stay home social distancing new cases dont know confirmed cases home order new york tested positive death toll and stay safe popular unigrams included virus lockdown quarantine people new home like stay dont and cases we presented the most popular unigrams and bigrams related to covid-19 in table 1 and visualized them using the word clouds in figure 3 and figure 4  our approach latent dirichlet allocation lda produced frequently co-occurred pairs of words related to covid-19 and organized these co-occurring words into different topics lda allowed researchers to manually define the number of topics eg ten topics twenty topics that we would like to generate consistent with the previous studies we used the coherence model -gensim 28 to calculate the most appropriate number of topics based on the specific data itself for this dataset the number of topics n13 returned by lda had the highest coherence score as well as the smallest topic number for example the numbers of topics n19 or n20 had higher coherence scores than the number of topics n13 but they represented larger topic numbers shown in table 2  for example topic 3 had the highest distribution 887 among all 13 common latent topics the bigrams were associated with topic 3 including tested positive coronavirus outbreak new york shelter place and mental health these pairs of words frequently co-occurred together and therefore the lda model assigned them to the same topic  we presented the results of the sentiment analysis for each of the thirteen latent topics in table 3  we also ran a one-tailed z test to examine if each of the eight emotions is statistically significantly different across topics the p-value smaller than 01 was set as a threshold for significance for example statistical significance for anticipation in topic 2 indicated that it was very likely p  001 that the anticipation emotion is more prevalently expressed in topic 2 217 than all other topics 1320 2090 330 1510 420 330 220 1330 5 1240 2380 260 1430 430 350 210 1340 6 1310 2250 240 1340 460 350 300 1280 7 1250 2190 250 1700 370 320 330 1310 8 1380 2070 240 1650 380 310 240 1210 9 1250 2070 280 1550 790 340 240 1230 10 1460 1740 300 1880 330 330 190 1130 11 1180 2060 250 1550 600 370 270 1190 12 1250 2140 280 1790 420 330 260 1420 13 13 30 2080 260 1480 430 420 310 1150 notes the sum of the percentage under each topic is not equal to 100 the rests are either neutral or other emotions p-value from z-test  p 001  the qualitative content analysis approach allowed users to categorize these topics into different distinct themes two team members discussed these bigrams and generated tweets samples in each topic and then categorized the identified 13 topics into 5 different themes to protect the privacy and anonymity of the twitter users we did not present any user-related information such as users twitter handles or other identifying information therefore sample tweets shown in table   4 were excerpts drawn from original tweets worldwide it is now 182726 new zealand prime minster jacinda ardern says the government will partially relax its lockdown in a week as a decline in  we organized thirteen topics into five themes including public health measures to slow the spread of covid-19 eg facemasks test kits vaccine social stigma associated with covid-19 eg chinese virus wuhan virus coronavirus news cases and deaths eg new cases deaths covid-19 in the united states eg new york protests task force and coronavirus cases in the rest of the world eg uk global issue for example theme public health measures to slow the spread of covid-19 included relevant topics of facemasks quarantine test kits lockdown safety a need for the vaccine and uss shelter-in-place in addition home quarantine self-quarantine were two of the most commonly co-occurred words under topic quarantine in this study we address the problem of public discussions and emotions using covid-19 related messages on twitter twitter users discuss five main themes related to covid-19 between march 7 to april 21 2020 topic modeling of the tweets is effective in providing insights about coronavirus topics and concerns on twitter the results show several essential points first the public is using a variety of terms referring to covid-19 including virus covid 19 coronavirus corona virus in addition coronavirus has been referred to as the china virus that can create stigma and harm efforts to address the covid-19 outbreak 14  second discussions about the pandemic in new york are salient and its associated public sentiments are anger third public discussions about the chinese communist party ppc and the spread of the virus emerge as a new topic which is not identified in a previous study 18  suggesting the connection between the covid-19 and politics is increasing to be circulating on twitter as the situation evolves fourth public sentiment on the spread of coronavirus is anticipation for the potential measures that can be taken and followed by a mixed feeling of trust anger and fear results suggest that the public is not surprised by the rapid spread of growth fifth the public reveals a significant feeling of fear when they discuss the coronavirus crisis and deaths lastly results show that trust for the authorities no longer remain as a prominent emotion when twitter users discuss covid-19 which is different than an early study 18 showing that people hold trust for public health authorities based on a sample of tweets posted from january 20 to march 7 2020 our findings are consistent with previous studies using social media data to assess the public health response and sentiments for covid-19 and suggest that public attention has been focusing on the following topics since january 2020 including 1 the confirmed cases and death rates 11 18 30  2 preventive measures 11 18 30  3 health authorities and government policies 10 18  4 an outbreak in new york 18  5 covid-19 stigma by referencing the coronavirus as the chinese virus 11 14  6 negative psychological reactions eg fear or mental health consequences 11 31 32 compared with the study examining public discussions and concerns for covid-19 using twitter data from january 20 to march 7 2020 we find that several salient topics are no longer popular  first future research could further explore public trust and confidence in existing measures and policies which is essential compared to prior work our study shows that twitter users reveal a feeling of joy when talking about herd immunity we also find sentiments of fear and anticipation related to topics of quarantine and shelter-in-place in addition future studies could evaluate how government officials eg president trump and international organizations eg who deliver and convey messages to the public and its impact on the public opinions and sentiments third future studies could examine the spread of anti-chineseasian sentiments social media and how people use social media platforms to resist and challenge covid-19 stigma fourth our findings do not show that misinformation during the covid-19 pandemic is a prominent theme an existing study shows that 25 n153 of sampled tweets contained misinformation 35  they also find that the term covid-19 has lower rates of misinformation than that associated with the terms of 2019ncov and corona future research is suggested in the investigation of misinformation and how it expands on social media finally the study finds that trust is no longer prominent when people tweet about confirmed cases and deaths instead fear has replaced trust to be the dominant emotion future research is suggested to examine the changes in trust over time the study shows that twitter data and machine learning approaches can be leveraged for infodemiology study by studying the evolving public discussions and sentiments during the covid-19 pandemic our findings facilitate an understanding of public discussions and concerns about covid-19 pandemic among twitter users between march 7 and april 21 2020 several topics were consistently dominant on twitter such as the confirmed cases and death rates preventive measures health authorities and government policies covid-19 stigma and negative psychological reactions eg fear as the situation evolves rapidly new salient topics emerge accordingly anticipation for the potential measures that can be taken to stop the spread of covid-19 is still significantly prevalent across all topics however twitter users reveal fear when tweeting about covid-19 new cases or death rather than trust 18  real-time monitoring and assessment of the twitter discussion and concerns can be promising for public health emergency responses and planning hearing and reacting to real concerns from the public can enhance trust between the healthcare systems and the public as well as prepare for a future public health emergency  automated chest ct image segmentation of covid-19 lung infection based on 3d u-net dominik mller iaki soto rey  frank kramer  the coronavirus disease 2019 covid-19 affects billions of lives around the world and has a significant impact on public healthcare due to rising skepticism towards the sensitivity of rt-pcr as screening method medical imaging like computed tomography offers great potential as alternative for this reason automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring however publicly available covid-19 imaging data is limited which leads to overfitting of traditional approaches to address this problem we propose an innovative automated segmentation pipeline for covid-19 infected regions which is able to handle small datasets by utilization as variant databases our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation for further reduction of the overfitting risk we implemented a standard 3d u-net architecture instead of new or computational complex neural network architectures through a 5-fold cross-validation on 20 ct scans of covid-19 patients we were able to develop a highly accurate as well as robust segmentation model for lungs and covid-19 infected regions without overfitting on the limited data our method achieved dice similarity coefficients of 0956 for lungs and 0761 for infection we demonstrated that the proposed method outperforms related approaches advances the state-of-the-art for covid-19 segmentation and improves medical image analysis with limited data the code and model are available under the following link httpsgithubcomfrankkramer-labcovid19miscnn  the ongoing coronavirus pandemic has currently 4 th of may 2020 spread to 187 countries in the world 1  the world health organization who declared the outbreak as a public health emergency of international concern on the 30 th of january 2020 and as a pandemic on the 11 th of march 2020 2 3  because of the rapid spread of severe respiratory syndrome coronavirus 2 sars-cov-2 billions of lives around the world were changed a sars-cov-2 infection can lead to a severe pneumonia with potentially fatal outcome 3 4 5  until now there are 3531618 confirmed cases in total resulting in 248097 deaths 1  so far there is neither an effective treatment for the infection nor is there an effective prevention against it such as a vaccination 3 4 6 7  additionally the rapid increase of confirmed cases and the resulting estimated basic reproduction numbers show that sars-cov-2 is highly contagious 4 6 8  therefore fast detection and isolation of infected persons are crucial in order to limit the spread of the virus the who named this new disease coronavirus disease 2019 short form covid-19 the reverse transcription polymerase chain reaction rt-pcr was established as the standard approach for covid-19 screening 2 4 6  rt-pcr is able to detect the viral rna in specimens obtained by nasopharyngeal swab oropharyngeal swab bronchoalveolar lavage or tracheal aspirate 2 4 6 7  however a variety of recent studies indicate that rt-pcr testing suffers from a low sensitivity approximately around 71 whereby repeated testing is needed for accurate diagnosis 9 10  furthermore rt-pcr screening is time-consuming and has increasing availability limitations due to shortage of required material 10  an alternative solution to rt-pcr for covid-19 screening is medical imaging like x-ray or computed tomography ct the medical imaging technology has made significant progress in recent years and is now a commonly used method for diagnosis as well for quantification assessment of numerous diseases 11 12 13  particularly chest ct screening has emerged as a routine diagnostic tool for pneumonia therefore chest ct imaging has also been strongly recommended for covid-19 diagnosis and follow-up 9  in addition ct imaging is playing an important role in covid-19 quantification assessment as well as disease monitoring covid-19 infected areas are distinguishable on ct images by groundglass opacity ggo in the early infection stage and by pulmonary consolidation in the late infection stage 6 9 14  in comparison to rt-pcr several studies showed that ct is more sensitive and effective for covid-19 screening and that chest ct imaging is more sensitive for covid-19 testing even without the occurrence of clinical symptoms 9 10 12 14  notably a large clinical study with 1014 patients in wuhan china determined that chest ct analysis can achieve 097 sensitivity 025 specificity and 068 accuracy for covid-19 detection 9  still evaluation of medical images is a manual tedious and time-consuming process performed by radiologists even though increasing ct scan resolution and number of slices resulted in higher sensitivity and accuracy these improvements also increased the workload additionally annotations of medical images are often highly influenced by clinical experience 15 16  a solution for these challenges could be clinical decision support systems based on automated medical image analysis in recent years artificial intelligence has seen a rapid growth with deep learning models whereas image segmentation is a popular sub-field 11 17 18  the aim of medical image segmentation mis is the automated identification and labeling of regions of interest roi eg organs like lungs or medical abnormalities like cancer and lesions in recent studies medical image segmentation models based on neural networks proved powerful prediction capabilities and achieved similar results as radiologists regarding the performance 11 19  it would be a helpful tool to implement such an automatic segmentation for covid-19 infected regions as clinical decision support for physicians by automatic highlighting abnormal features and rois image segmentation is able to aid radiologists in diagnosis disease course monitoring reduction of timeconsuming inspection processes and improvement of accuracy 11 12 20  nevertheless training accurate and robust models requires sufficient annotated medical imaging data because manual annotation is labor-intensive timeconsuming and requires experienced radiologists it is common that publicly available data is limited 11 12 16  this lack of data often results in an overfitting of the traditional data-hungry models especially for covid-19 large enough medical imaging datasets are currently unavailable 12 16  in this work we push towards creating an accurate and state-of-the-art mis pipeline for covid-19 lung infection segmentation which is capable of being trained on small datasets consisting of 3d ct volumes in order to avoid overfitting we exploit extensive on-the-fly data augmentation as well as diverse preprocessing methods in order to further reduce the risk of overfitting we implement the standard u-net architecture instead of other more computational complex variants like the residual architecture of the u-net furthermore we use a 5-fold cross-validation for reliable performance evaluation since the breakthrough of convolutional neural network cnn architectures for computer vision neural networks became one of the most accurate and popular machine learning algorithm for automated medical image analysis 11 17 21  two of the major tasks in this field are classification and segmentation whereas medical image classification aims to label a complete image to predefined classes eg to a diagnosis medical image segmentation aims to label each pixel in order to identify rois eg organs or medical abnormalities popular deep learning architectures which achieved performance equivalent to humans are inception-v3 resnet as well as densenet for classification and vb-net u-net and various variants of the u-net for segmentation 12 22 23 24  in reaction to the rapid spread of the coronavirus many scientists quickly reacted and developed various approaches based on deep learning to contribute to the efforts against covid-19 furthermore the scientific community focused their efforts on the development of models for covid-19 classification because x-ray and ct images of infected patients could be collected without further required annotations 12 20  these classification algorithms can be categorized through their objectives 1 classification of covid-19 from non-covid-19 healthy patients which resulted into models achieving a sensitivity of 941 specificity of 955 and auc of 0979 jin et al 25  2 classification of covid-19 from other pneumonia which resulted in models achieving a sensitivity of 90 specificity of 96 and auc of 096 li et al 26  3 severity assessment of covid-19 which resulted in a model achieving a true positive rate of 933 true negative rate of 745 and accuracy of 875 tang et al 27  in the last weeks clinicians started to publish covid-19 ct images with annotated rois which allowed the training of segmentation models automated segmentation is highly desired as covid-19 application 12 28  the segmentation of lung lung lobes and lung infection provides accurate quantification data for progression assessment in follow-up comprehensive prediction of severity in the enrollment and visualization of lesion distribution using percentage of infection poi 12  still the limited amount of annotated imaging data causes a challenging task for detecting the variety of shapes textures and localizations of lesions or nodules nonetheless multiple approaches try to solve these problems with different methods the most popular network models for covid-19 segmentation are variants of the u-net which achieved reasonable performance on sufficiently sized 2d datasets 5 12 29 30 31 32 33  in order to compensate limited dataset sizes more attention has been drawn to semisupervised learning pipelines 12 34  these methods optimize a supervised training on labeled data along with an unsupervised training on unlabeled data another approach is the development of special neural network architectures for handling limited dataset sizes frequently attention mechanism are built into the classic u-net architecture like the inf-net from fan et al 34 or the miniseg from qiu et al 35  particularly worth mentioning is the development of a benchmark model with a 3d u-net from ma et al because the authors also provide high reproducibility through a publicly available dataset 16 36  the implemented medical image segmentation pipeline can be summarized in the following core steps and is illustrated in figure this pipeline was based on miscnn 37  which is an inhouse developed open-source framework to setup complete medical image segmentation pipelines with convolutional neural networks and deep learning models on top of tensorflowkeras 38  miscnn supports extensive preprocessing data augmentation state-of-the-art deep learning models and diverse evaluation techniques in this study we used the public dataset from ma et al which consists of 20 annotated covid-19 chest ct volumes 16 36  at the time of this paper this dataset is the only publicly available 3d volume set with annotated covid-19 infection segmentation 16  the ct scans were collected from the coronacases initiative and radiopaedia and were licensed under cc by-nc-sa each ct volume was first labeled by junior annotators then refined by two radiologists with 5 years of experience and afterwards the annotations verified by senior radiologists with more than 10 years of experience 16  despite the fact that the sample size is rather small the annotation process led to an excellent high-quality dataset the volumes had a resolution of 512x512 coronacases initiative or 630x630 radiopaedia with a number of slices of about 176 by mean 200 by median the ct images were labeled into four classes background lung left lung right and covid-19 infection in our pipeline we performed a 5-fold crossvalidation on the dataset this resulted in five fitting and inference runs with each time 16 samples as training dataset and 4 samples for prediction we decided not to follow the convention of splitting the dataset into training validation and testing sets due to the limited dataset size and because we do not configure any hyper parameters afterwards on basis of validationtesting results in order to simplify the pattern finding and fitting process for the model we applied several preprocessing methods on the dataset we exploited the hounsfield units hu scale by clipping the pixel intensity values of the images to -1250 as minimum and 250 as maximum because we were interested in infected regions 50 to 100 hu and lung regions -1000 to -700 hu it was only possible to apply the clipping approach on the coronacases initiative cts because the radiopaedia volumes were already normalized to a grayscale range between 0 and 255 varying signal intensity ranges of images can drastically influence the fitting process and the resulting performance of segmentation models 39  for achieving dynamic signal intensity range consistency it is recommended to scale and standardize imaging data therefore we normalized the coronacases initiative ct volumes likewise to grayscale range afterwards all samples were standardized via z-score medical imaging volumes have commonly inhomogeneous voxel spacings the interpretation of diverse voxel spacings is a challenging task for deep neural networks therefore it is possible to drastically reduce complexity by resampling volumes in an imaging dataset to homogeneous voxel spacing which is also called target spacing resampling voxel spacings also directly resizes the volume shape and determines the contextual information which the neural network model is able to capture as a result the target spacing has a huge impact on the final model performance we decided to resample all ct volumes to a target spacing of 158x158x270 resulting in a median volume shape of 267x254x104  the aim of data augmentation is to create more data of reasonable variations of the desired pattern and thus artificially increase the number of training images in order to compensate the small dataset size we performed extensive data augmentation by using the batchgenerators interface within miscnn the batchgenerators package is an api for state-of-the-art data augmentation on medical images from the division of medical image computing at the german cancer research center dkfz 40  we implemented three types of augmentations spatial augmentation by mirroring elastic deformations rotations and scaling color augmentations by brightness contrast and gamma alterations noise augmentations by adding gaussian noise we performed the data augmentation onthe-fly on each image before it was forwarded into the neural network model furthermore each augmentation method had a random probability of 15 to be applied on the current image with random intensity or parameters eg random angle for rotation through this technique the probability that the model encounters the exact same image twice during the training process decreases significantly in image analysis there are two popular methods the analysis of full images or patchwise by slicing the volume into smaller cuboid patches 11  we selected the patchwise approach in order to exploit random cropping for the fitting process through random forwarding only a single cropped patch from the image to the fitting process another type of data augmentation is induced and the risk of overfitting additionally decreased furthermore full image analysis requires unnecessary resolution reduction of the 3d volumes in order to handle the enormous gpu memory requirements by slicing the volumes into patches with a shape of 160x160x80 we were able to utilize highresolution data for inference the volumes were sliced into patches according to a grid between the patches we introduced an overlap of half the patch size 80x80x40 to increase prediction performance after the inference of each patch they were reassembled into the original volume shape whereas overlapping regions were averaged the complete batch generation process including the patch cropping and data augmentation for training was implemented as on-the-fly this means that batches are created during the fitting process instead beforehand this allowed the creation of novel and unique images by the data augmentation in each iteration for training we used a batch size of 2 the neural network architecture and its hyper parameters are one of the key parts in a medical image segmentation pipeline in this work we implemented the standard 3d u-net as architecture in order to avoid unnecessary parameter increase by more complex architectures like the residual variant of the 3d u-net 23 41 42  upsampling was achieved via transposed convolution and downsampling via maximum pooling the architecture used 32 feature maps at its highest resolution and 512 at its lowest all convolutions were applied with a kernel size of 3x3x3 in a stride of 1x1x1 except for up-and downsampling convolutions which were applied with a kernel size of 2x2x2 in a stride of 2x2x2 in medical image segmentation it is common that semantic annotation includes a strong bias in class distribution towards the background class our dataset revealed a class distribution of 89 for background 9 for lungs and 1 for infection in order to compensate this class bias we utilized the sum of the tversky index 43 and the categorical cross-entropy as loss function for model fitting 1  we implemented a multi-class adaptation for the tversky index 2  which is an asymmetric similarity index to measure the overlap of the segmented region with the ground truth it allows for flexibility in balancing the false positive rate fp and false negative fn rate the crossentropy 3 is a commonly used loss function in machine learning and calculates the total entropy between the predicted and true distribution the multi-class adaptation for multiple categories categorical cross-entropy is represented through the sum of the binary cross-entropy for each class c whereas yoc is the binary indicator whether the class label c is the correct classification for observation o the variable poc is the predicted probability that observation o is of class c for model fitting an adam optimization was used with the initial weight decay of 1e-3 44  we utilized a dynamic learning rate which reduced the learning rate by a factor of 01 in case the training loss did not decrease for 15 epochs the minimal learning rate was set to 1e-5 in order to further reduce the risk of overfitting we exploited the early stopping technique for training in which the training process stopped without a fitting loss decrease after 100 epochs the neural network model was trained for a maximum of 1000 epochs instead of the common epoch definition as a single iteration over the dataset we defined an epoch as the iteration over 150 training batches this allowed for an improved fitting process for randomly generated batches in which the dataset acts as a variation database according to our available gpu vram we selected a batch size of 2 during the fitting process we computed the segmentation performance for each epoch on randomly cropped and data augmented patches from the validation dataset this allowed for an evaluation of the overfitting on the training data after the training we used three widely popular evaluation metrics in the community for medical image analysis to do the inference performance measurement in order to measure the segmentation overlap between prediction and ground truth the dice similarity coefficient defined in 4 is the most widespread metric in computer vision in contrast the sensitivity 5 and specificity 6 are one of the most popular metrics in medical fields all metrics are based on the confusion matrix where tp fp tn and fn represent the true positive false positive true negative and false negative rate respectively we calculated the evaluation metrics for each fold in the cross-validation and thus for all samples in our dataset the two lung classes lung left and lung right were averaged by mean into a single class lungs during the evaluation in order to ensure full reproducibility and to create a base for further research the complete code of this project including extensive documentation is available in the following git repository httpsgithubcomfrankkramer-labcovid19miscnn the sequential training of the complete cross-validation on a single gpu took around 130 hours all folds did not require the entire 1000 epochs for training and instead were early stopped after an average of 312 epochs through validation monitoring during the training no overfitting was observed the training and validation loss function revealed no significant distinction from each other which can be seen in figure 4  during the fitting the performance settled down at a loss of around 0383 which is a generalized dsc average of all class-wise dscs of around 0919 because of this robust training process without any signs of overfitting we concluded that fitting on randomly generated patches via extensive data augmentation and random cropping from a variant database is highly efficient for limited imaging data after the training the inference revealed a strong segmentation performance for lungs and covid-19 infected regions which is illustrated in figure 6  overall the cross-validation models achieved a dsc of around 0956 for lung and 0761 for covid-19 infection segmentation furthermore the models achieved a sensitivity and specificity of 0956 and 0998 for lungs as well as 0730 and 0999 for infection respectively more details on the inference performance is listed in table 1 from a medical perspective detection of covid-19 infection is a challenging task and one of the reasons for the weaker segmentation accuracy in contrast to the lung segmentation the reason for this is the variety of ggo and pulmonary consolidation morphology nevertheless our medical image segmentation pipeline allowed fitting a model which is able to segment covid-19 infection with state-of-the-art accuracy that is comparable to models trained on large datasets for further evaluation we compared our pipeline to other available covid-19 segmentation approaches based on ct scans the authors ma et al  who also provided the dataset we used for our analysis implemented a 3d u-net approach as a baseline for benchmarking 16  they were able to achieve a dsc of 070355 and 06078 for lungs and covid-19 infection respectively with our model we were able to outperform this baseline it is important to mention that we trained with a cross-validation distribution of 80 training and 20 testing whereas they used the inverted distribution 20 training and 80 testing another approach from yan et al developed a novel neural network architecture covid-segnet specifically designed for covid-19 infection segmentation with limited data 29  the authors tested their architecture on a limited dataset consisting of ten covid-19 cases from brainlab co ltd germany and were able to achieve a dsc of 0987 and 0726 for lungs and infection respectively hence covid-segnet as well as our approach achieved similar results this raises the question if it is possible to further increase our performance by switching from the standard u-net of our pipeline to an architecture specifically designed for covid-19 infection segmentation like covid-segnet further approaches with the aim to utilize specifically designed architectures were inf-net fan et al and miniseg qiu et al 34 35  both were trained on 2d ct scans and achieved for covid-19 infection segmentation dscs of 0764 and 0773 respectively although diverse datasets were used for training which leads to incomparability of the results it is highly impressive that they achieved similar performance as approaches based on 3d imaging data the 3d transformation of these architectures and the integration into our pipeline would be an interesting experiment to evaluate improvement possibilities however it is important to note that the majority of current segmentation approaches in research are not suited for clinical usage the bias of current models is that they are only trained with covid-19 related images therefore it is not certain how good the models can differentiate between covid-19 lesions and other pneumonia or entirely unrelated medical conditions like cancer furthermore identical to covid-19 classification the models reveal huge differences depending on which dataset they were trained on segmentation models purely based on covid-19 scans are often not able to segment accurately in the presence of other medical conditions 16  additionally there is a high potential for false positive segmentation of pneumonia lesions that are not caused by covid-19 this demonstrates that these models could be biased and are not suitable for covid-19 screening nevertheless current infection segmentation models are this offers the opportunity for quantitative assessment and disease monitoring as applications in clinical studies despite that our model and those of others which are based on limited data are capable for accurate segmentation it is essential to discuss their robustness currently there are no large as well as annotated imaging datasets available for covid-19 segmentation 16  existing small datasets may have incomplete and inaccurate labels which results in challenging handicaps for models more imaging data with more variance different covid-19 states other pneumonia etc need to be collected annotated and published for researchers similar to ma et al community accepted benchmark datasets have to be established in order to fully ensure robustness as well as comparability of models 16 36  in this paper we developed and evaluated an approach for automated segmentation of covid-19 infected regions in ct volumes our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation thus it is possible to handle limited dataset sizes which act as variant database instead of novel and complex neural network architectures we utilized the standard 3d u-net we proved that our medical image segmentation pipeline is able to successfully train accurate and robust models without overfitting on limited data furthermore we were able to outperform current state-of-the-art semantic segmentation approaches for lungs and covid-19 infected regions our work has great potential to be applied as a clinical decision support system for covid-19 quantitative assessment and disease monitoring in a clinical environment nevertheless further research is needed on covid-19 semantic segmentation in clinical studies for evaluating clinical performance and robustness we want to thank bernhard bauer and fabian rabe for sharing their gpu hardware nvidia quadro p6000 with us which was used for this work we also want to thank dennis klonnek jana glckler johann frei florian auer peter parys zaynab hammoud and edmund mller for their useful comments  the authors declare no conflicts of interest  naist covid multilingual covid-19 twitter and weibo dataset zhiwei gao shuntaro yada shoko wakamiya eiji aramaki   the outbreak of the coronavirus disease 2019 was observed at the end of 2019 in wuhan hubei province china since january 2020 it has rapidly spread worldwide on march 11 2020 the world health organization who announced that covid-19 can be characterized as a pandemic the virus causing covid-19 severe acute respiratory syndrome coronavirus-2 sars-cov-2 has infected more than 12 million people worldwide and 60000 people have lost their lives 2 who highly recommends main-taining social distancing measures and several countries with severe epidemics are further requesting citizens to stay home in this scenario online social media such as twitter weibo and instagram are playing an important role in sharing information and perception about covid-19 social media is recognized as one of the valuable resource of data that can lead to prediction of various phenomena related to an event for example lampos and cristianini 2010 showed that microblog data facilitated better public-health surveillance such as the prediction of the number of patients suffering from influenza to encourage and support the social media studies on covid-19 it is crucial to make relevant datasets available to the public here we publish a multilingual dataset that contains over 20 million microblogs related to covid-19 in english japanese and chinese from twitter and weibo since january 20 2020 until march 24 2020 chen et al 2020 and lopez et al 2020 have already released multilingual datasets collected from twitter given that china is the very first country to have faced a covid-19 outbreak we further collected microblogs about covid-19 from weibo one of the most popular social media in china similar to twitter the remainder of the paper is organized as described follows in section 2 we elaborate on the method of data collection in section 3 we provide a quantitative analysis of the dataset such as the character count per microblog and the microblog count per day in section 4 we present the daily word cloud images created from microblogs of each language as an example of text-mining analysis finally in section 5 we present the conclusion with our future work  to collect the microblogs related to covid-19 we adopted keyword-based search for english and japanese we collected microblogs related to covid-19 from twitter while we obtained chinese microblogs from weibo we employed twitter search api 3 for tweets a web crawler was applied to retrieve weibo posts we developed three sets of query keywords as shown in table 1 according to the stages of covid-19 spread corresponding to these sets our dataset can be divided into three phases phase 1 january 20 to february 23 2020 in combination with the term wuhan we used the keywords pneumonia and coronavirus in english and their translations in japanese and chinese we included the chinese city name wuhan as the primary keyword because wuhan   in japanese and  in chinese observed the earliest outbreak with the maximum number of confirmed cases note that in the said period the official disease name covid-19 was yet to be defined phase 2 february 24 to 29 2020 who assigned the official name covid-19 on february 11 we added it to the keywords in combination with wuhan although this resulted in a smaller number of retrieval because all the microblogs included wuhan phase 3 march 1-24 2020 to obtain more data we relaxed search con- ditions by querying each set of keywords separately as shown in table 2  we have collected over 16 million microblogs in english 9 million in japanese and 180 thousand in chinese during january 20 to march 24 2020 to collect twitter and weibo posts we have adopted a uniform daily timing to collect microblogs from 000 to 2359 jst of the previous day to ensure the uniqueness of the data for twitter we filtered out all retweets by adding the -filterretweets operator for weibo we searched for original microblogs only note that we have collected smaller amounts of the data from weibo than twitter because anti-crawling mechanism in weibo limits our web crawler to access only the first 50 pages of the search content we released the first version of the dataset on github at httpsgithubcomsociocomcovi d19dataset following the terms of service of twitter and weibo we mainly published microblog ids instead of exposing original text and metadata the dataset consists of the lists of microblog ids with two fields of metadata their timestamps and the query keywords mentioned in the microblogs among our search queries this helps make subsets suitable for subsequent applications and tasks since a weibos microblog is uniquely determined by the combination of user id and microblog id we share the corresponding user id and microblog id for each microblog in the form of user idmicroblog id we provide basic statistics of our dataset in terms of its quantitative volume first we show the number of characters in microblogs next we plot the number of microblogs per time series while microblogs contain multimodal data eg images and movies their core content is text we report the number of characters to quantify the total amount of our dataset table 3 shows the sum mean and standard deviation of the number of characters for each language in our dataset we removed urls and punctuations from each microblog to expose the amount of characters that constituted the essential content in figure 1a  a sudden and dramatic increase in the number of english microblogs can be observed on january 28 2020 according to the news that particular day saw a discussion on the death toll in mainland china reaching 100 5 on the same day japan also observed a sharp rise in the relevant microblogs as shown in figure 1c  this was a result of many users tweeting extensively about the three newly confirmed cases in japan which included people who had not been to wuhan 6 subsequently there was a substantial increase in the english microblogs on february 25 2020 as shown in figure 1a  on that day there were reports that trump privately vents over his teams response to coronavirus -even though he says that the virus is under control 7 leading to many microblogs against trump on twitter in march as figure 1b shows the number of microblogs in major english-speaking countries showed an upward trend as the number of the confirmed cases increased and the largest number of microblogs exceeded 9 million a day meanwhile in japan the number of daily confirmed cases was relatively small as shown in figure 1 d therefore we assumed that japanese twitter users are not as interested in covid-19 as in the major english-speaking countries in particular there was a decline in the number of microblogs from march 12 to march 15 2020 march 12 2020 was the olympic flame lighting ceremony and the torch relay for the tokyo 2020 olympics 8 therefore we speculate that this sudden decrease was caused by a shift in attention from covid-19 to the torch relay for many japanese users with regard to the chinese microblogs the trends of the numbers are shown in figures 1e and 1f these do not fully reflect the quantitative trends of the confirmed cases owing to the limited amount of the microblogs we could collect on a daily basis in addition to the quantitative analysis we show an example of qualitative analysis based on our dataset as an initial attempt we adopted a word cloud which is an electronic image that shows words used in a particular piece of electronic texts or series of texts 9 in word clouds term frequency for each word in a corpus is proportional to its font size which enables us to grasp the top-a the number of english microblogs and the daily confirmed cases in major english-speaking countries in january and february b the number of english microblogs and the daily confirmed cases in major english-speaking countries in march c the number of japanese microblogs and the daily confirmed cases in japan in january and february d the number of japanese microblogs and the daily confirmed cases in japan in march e the number of chinese microblogs and the daily confirmed cases in china in january and february ics of the corpus visually daily word cloud images of our dataset for each language are available at httpsaoinaistjp2020-covidwo rdcloud henceforth we provide brief interpre-tations of these word clouds to demonstrate a possible text-mining approach that can be applied to our dataset in figure 2  note that we removed stop words followed by tokenization in our word clouds for the chinese and japanese tokenization we used jieba 10 and mecab 11  respectively we also filtered out the search keywords in each microblog to reduce the disturbance of these keywords in the image a us citizen who lived in wuhan passed away because of covid-19 in wuhan on february 8 2020 12 this was the first casualty of a us citizen the word cloud of this day shown in figure 2a  contains the related words eg american us citizen and die figure 2 b is the word cloud on march 16 2020 in which social distancing an important phrase to fight against the epidemic appears notably we can also notice that another socially important phrase stay home has an increased in size in our word cloud series from march 20 2020 the first local transmission of covid-19 inside japan was reported on january 28 2020 as described in section 32 figure 2c shows the word cloud on that day it reflects the fact that the infected patient lived in nara prefecture and drove a sightseeing-tour bus that carried travelers from wuhan we can observe the relevant keywords such as   nara   bus and   drive on march 24 2020 japan and international olympic committee ioc officially agreed to postpone the planned 2020 tokyo olympics until 2021 13 a notable change in japanese word cloud series can be found as the novel appearance of the words  olympics and  postponing in that days figure ie figure 2d  we can also notice that a youtube video became viral in japanese twitter from around january 29 to february 6 2020 by observing the corresponding word clouds the video was originally made by a wuhan citizen and subtitled in japanese later by another youtuber 14 which tells 10 httpsgithubcomfxsjyjieba 11 httpstaku910githubiomecab 12 february 8 2020 cnbc httpscnbcx2r4uy z1 13 march 24 2020 the washington post httpswa post2uyxeng 14 january 29 2020 youtube httpsyoutubem cfn5eh5ove the situation of wuhan in lockdown in addition to the word youtube the corresponding word clouds contain the tokens of the video title ie  hypocenter  video and   japanese translation figure 2e shows the word cloud on january 20 2020 and also shows that the term    zhong nanshan has a larger weight it was on january 20 that dr zhong indicated the existence of human-to-human transmission of covid-19 15 that triggered extensive discussion on weibo figure 2f shows the word cloud on march 10 2020 and the word     mobile cabin hospital was more conspicuous according to chinas national health commission all of wuhans mobile cabin hospitals were closed on march 10 16 the mobile cabin hospitals which were instrumental in preventing the spread of the epidemic also had attracted much attention we published a multilingual dataset of microblogs related to covid-19 collected by relevant query keywords at httpsgithubcomsociocomco vid19dataset the dataset covered english and japanese tweets from twitter and chinese posts from weibo the present version of the dataset april 20 2020 encompassed microblogs from january 20 to march 24 2020 we then showed one of the possible utilization of our dataset through the daily microblog count analysis as an example of the quantitative analyses and the word cloud-based analysis as an example of the qualitative analyses the results of the analyses are summarized as follows for china which is the first country to have faced a full-blown outbreak of covid-19 we can observe from social media that people took the situation and prevention seriously as the number of confirmed cases in china decreased the trend in social media shifted toward the concern for the global situation in the uk and the us the main english-speaking countries initially there was less social media interests owing to fewer confirmed cases the subsequent outbreaks sprung the discussion about covid-19 on social media including the promotion of precautionary measures and recommendations to keep social distancing measures meanwhile japan showed relatively sluggish growth however on march 24 2020 the announcement of the postponement of the 2020 olympic games in tokyo along with a relatively rapid growth of confirmed cases was reflected in the increased social media activity this was accompanied by microblogs expressing concerns about the epidemic and dissatisfaction with government measures we believe that this dataset can be analyzed further in many ways such as sentiment-based analysis 17  comparison with web search queries moving logs 1819  etc various combinations of data can enable deeper analyses of social media communication furthermore our dataset would contribute to extract useful clinical information  high expression of ace2 receptor of 2019-ncov on the epithelial cells of oral mucosa hao xu liang zhong jiaxin deng jiakuan peng hongxia dan xin zeng taiwen li qianming chen   since december 2019 an increasing number of patients with pneumonia occurred in wuhan hubei province china which attracted much attention not only within china but across the world12 the novel pneumonia was named as corona virus disease 19 covid-19 by world health organization who httpswwwwhointdocsdefault-sourcecoronavirusesituation-reports20200211-sitrep-22-ncovpdfsfvrsnfb6d49b12 the common symptoms of covid-19 at illness onset were fever fatigue dry cough myalgia and dyspnea3 in addition some patients might suffer from headache dizziness abdominal pain diarrhea nausea and vomiting3 onset of disease may lead to progressive respiratory failure due to alveolar damage and even death4 scientists then isolated a novel coronavirus from human airway epithelial cells which was named 2019-ncov5 lu et al6 found that 2019-ncov was closer to bat-sl-covzc45 and bat-sl-covzxc21 at the whole-genome level and the external subdomain of the 2019-ncov receptor-binding domain rbd was more similar to that of severe acute respiratory syndrome sars coronavirus sars-cov study of zhou et al4 indicated that the angiotensin-converting enzyme ii ace2 is likely the cell receptor of 2019-ncov which were also the receptor for sars-cov and hcov-nl6378 zhou et al4 also proved that 2019-ncov does not use other coronavirus receptors aminopeptidase n and dipeptidyl peptidase 4 the study of xu et al9 found that the rbd domain of the 2019-ncov s-protein supports strong interaction with human ace2 molecules these findings suggest that the ace2 plays an important role in cellular entry thus ace2-expressing cells may act as target cells and are susceptible to 2019-ncov infection10 the expression and distribution of the ace2 in human body may indicate the potential infection routes of 2019-ncov through the developed single-cell rna sequencing scrna-seq technique and single-cell transcriptomes based on the public database researchers analyzed the ace2 rna expression profile at single-cell resolution high ace2 expression was identified in type ii alveolar cells at2 of lung1012 esophagus upper and stratified epithelial cells absorptive enterocytes from ileum and colon12 cholangiocytes13 myocardial cells kidney proximal tubule cells and bladder urothelial cells10 these findings indicated that those organs with high ace2-expressing cells should be considered as potential high risk for 2019-ncov infection10 in order to investigated the potential routes of 2019-ncov infection on the mucosa of oral cavity we explored whether the ace2 is expressed and the ace2-expressing cell composition and proportion in oral cavity based on the public bulk rna-seq profiles from two public databases and single-cell transcriptomes from an independent data generated in-house the result showed that the ace2 could be expressed in the oral cavity and was highly enriched in epithelial cells moreover among different oral sites ace2 expression was higher in tongue than buccal and gingival tissues these findings indicate that the mucosa of oral cavity may be a potentially high risk route of 2019-ncov infection na-seq profile data of 13 organs including 695 para-carcinoma normal tissues as control from public tcga were obtained for our analysis and fig 1a showed that ace2 could be expressed in various organs the mean expression of different organs could be found in table 1 according to the mean expression of ace2 the mucosa of oral cavity could express ace2 and the results were validated by the data of normal tissues from the fantom5 cage dataset fig 1b to investigate the ace2 expression on mucosa of oral cavity we looked into the ace2 expression in different oral sites according to the site information provided by the tcga among the 32 adjacent normal tissues 13 tissues located in the oral tongue 2 tissues located in the base of tongue 3 tissues located in the floor of mouse and 14 tissues did not definite the site and were just put into the category of oral cavity the mean expression distribution of different sites was shown in fig 1c when we combined the base of tongue floor of mouth and oral cavity as other sites and compared them with oral tongue we found the obvious tendency that the mean expression of ace2 was higher in oral tongue 13 tissues than others 19 tissues fig 1d while may due to the limitation of the sample size the p value was not significant p  0062 single cell rna-seq was utilized for four oral tissues and the data was analyzed to confirm the above results and assess the cell type-specific expression of ace2 after the data preprocessing shown in section materials and methods 22 969 cells were acquired and 7 cell types were identified fig 2a including epithelial cells marker genes including sfn krt6a and krt10 fibroblasts marker genes including fap pdpn col1a2 dcn col3a1 col6a1 t cells marker genes including cd2 cd3d cd3e and cd3g macrophages marker genes including cd163 csf1r cd68 and fcgr2a mast cells marker genes including cma1 ms4a2 tpsab1 tpsb2 b cells marker genes including slamf7 fcrl5 and cd79a and endothelial cells marker genes including pecam1 vwf and eng the heatmap of main cell markers across the cell types can be found in fig 2b according to fig 2c d we confirmed the ace2 was expressed in oral tissues 052 ace2-positive cells and higher in oral tongue than buccal and gingival tissues 9586 ace2-positive cells located in oral tongue figure 2e shows that the ace2-positive cells could be found in oral tissues including epithelial cells 119 ace2-positive cells t cells 05 b cells 05 and fibroblast 05 and the ace2 was highly enriched in epithelial cells of which 9338 ace2-positive cells belong to epithelial cells fig 2f the above results indicated that the ace2 could be expressed on the epithelial cells of the oral mucosa and highly enriched in tongue epithelial cells in the last two decades coronavirus has caused two large-scale pandemics sars in 2002 and the middle east respiratory syndrome mers in 201214 in december 2019 a novel coronavirus 2019-ncov induced an outbreak of pneumonia in wuhan china restated the risk of coronaviruses posed to public health15 the infection routes and pathogenesis of 2019-ncov are not fully understood by far and the study of 2019-ncov host cell receptor ace2 could be valuable for the prevention and treatment of the covid-19 in this study the analysis of public bulk-seq rna datasets showed that the mucosa of oral cavity could express the ace2 and was higher in tongue than other oral sites the results of this study were consistent with the study of zou et al10 in general many organs with higher expression of ace2 than lung such as intestine heart and kidney according to the study of zhao et al11 the ace2 expression in lung is concentrated in a small population of type ii alveolar cells at2 that may cause the relatively low ace2 expression of lung in bulk-seq rna datasets analysis even though the result of zou et al indicated that the respiratory tract should also be considered as a vulnerable target to 2019-ncov infection10 the results of our single cell rna-seq profiles validated the ace2 expression in oral cavity and the level of ace2 expression in oral tissues was higher in tongue than buccal or gingival tissues furthermore we have also demonstrated that the ace2-positive cells were enriched in epithelial cells which was also reported by previous study101216 these findings indicated that oral cavity could be regarded as potentially high risk for 2019-ncov infectious susceptibility interestingly we found that the ace2 also expressed in lymphocytes within oral mucosa and similar results were found in various organs of the digestive system and in lungs1112 whether those facts have reminded the 2019-ncov attacks the lymphocytes and leads to the severe illness of patients needs more in vitro and in vivo evidence and validations though the proportion of ace2-positive lymphocytes is quite small previous studies have investigated the ace2 mrna and protein expression in various tissues by bulk samples1718 however the distribution of ace2 through bulk data could not indicate the cell type-specific expression of ace2 recently developed single-cell rna-sequencing technology enabled the generation of vast amounts of the transcriptomic data at cellular resolution19 the ace2 expression profile in various organs tissues and cell types provides the bioinformatics evidence for the potential infection routes of 2019-ncov which might also be associated with presented symptoms although studies have reported multiple symptoms of hospitalized patients with 2019-ncov infection320 some cases at home might be asymptomatic it is worth noting that a previous study showed that 99 of the patients had no clinical manifestation of oral human papillomavirus hpv but hpv dna was detected in 81 of oral mucosa samples and anti-hpv iga was detected in the saliva of 44 of the patients21 likewise although 2019-ncov infection hardly presented oral symptoms the ace2 expression in the oral cavity indicated that the oral infection route of 2019-ncov cannot be excluded moreover a latest pilot experiment showed that 4 out of 62 stool specimens tested positive to 2019-ncov and another four patients in a separate cohort who tested positive to rectal swabs had the 2019-ncov being detected in the gastrointestinal tract saliva or urine20 thus our results support that in addition to the respiratory droplets and direct contact fecaloral transmission might also be the route of transmission of 2019-ncov our results are mainly based on public datasets and single cell rna-sequencing data of in-house oral tissues with minimal diseased lesion which from our previous project found no significant expression difference among the common epithelial markers in our past study and other previous study2224 it is warrant that further histological methods are used to confirm our results and enhance the persuasion of the conclusion the ace2-expressing cells in oral tissues especially in epithelial cells of tongue might provide possible routes of entry for the 2019-ncov which indicate oral cavity might be a potential risk route of 2019-ncov infection those preliminary findings have explained the basic mechanism that the oral cavity is a potentially high risk for 2019-ncov infectious susceptibility and provide a piece of evidence for the future prevention strategy in clinical practice as well as daily life bulk rna-seq data of para-carcinoma normal tissues which were taken as control tissues in the studies were downloaded from the cancer genome atlas tcga httpswwwcancergovtcga and 695 para-carcinoma normal tissues distributed in different organs were obtained for this study which included intestine 51 tissues kidney 129 tissues stomach 35 tissues bile duct 9 tissues liver 50 tissues oral cavity 32 tissues lung 110 tissues thyroid 59 tissues esophagus 11 tissues bladder 19 tissues breast 113 tissues uterus 25 tissues and prostate 52 tissues the rna-seq data were batch effects normalized and log2-transformed for the subsequent analysis violin plot was used to show the distribution of ace2 expression among different organs t test was performed to compare the ace2 expression between two different groups shown in boxplot besides bulk rna-seq data of normal tissues were downloaded from functional annotation of the mammalian genome cap analysis of gene expression fantom5 cage dataset25 as only two samples of this dataset which owns 60 samples in total located in the tongue we just downloaded 14 organ types to validate the ace2 expression in oral cavity with bar plot including colon ovary breast cerebellum epididymis esophagus gallbladder heart muscle kidney liver lung pancreas prostate and tongue all analyses were performed in r r version 360 and significant level was set as 005 due to our previous project about oral potential malignant disorders four tissues of oral mucosa were obtained from patients after informed consent and ethical approval from west china hospital of stomatology sichuan university these oral tissues had been sent for single cell rna sequence the four were taken from three patients with an average age of 50 which were all diagnosed as hyperkeratosis without dysplasia by pathologists just showing an increase in cell number in the spinous layer andor in the basalparabasal cell layers without cellular atypia and its genetic profiles would be much more closed to normal tissue than malignant tissue2224 two tissues were from independent lesions of one patients dorsum linguae the other two tissues came from buccal and gingival sites fresh biopsy tissues were washed twice by d-pbs hyclone and collected into prechilled macs tissue storage solution miltenyi single-cell suspensions were generated from biopsy tissues using whole skin dissociation kit human miltenyi manufacturer guidelines and filtered by 70 m macs smartstrainers miltenyi carryover red blood cells were lysed by red blood cell lysis buffer abcam and dead cells were removed by easysep dead cell removal annexin v kit stemcell finally cell pellets were re-suspended in d-pbs to generate single-cell gel beads-in-emulsion gems the 10 chromium platform was used to capture and barcode cells cells were partitioned into gems along with gel beads coated with oligonucleotides and cdnas with both barcodes were amplified and a library was constructed using the 10 genomics chromium single cell kit v3 chemistry for each sample the resulting libraries were sequenced on an illumina novaseq 6000 system the fastq files were analyzed with the cell ranger software suite version 31 10 genomics the seurat version 30 was applied to read the gene-barcode matrix of four tissues to control quality we removed cells with 200 genes and 500 umi counts and as well as the cells with mitochondrial content higher than 5 besides the genes detected in 3 cells were filtered out the sctransform wrapper in seurat was applied to normalize the data and remove confounding sources of variation the integratedata was used for integrated the seurat objects from four tissues the uniform manifold approximation and projection umap was used for dimensionality reduction and clustering the cells cell types were assigned based on their canonical markers umap plots heatmap and violin plots were generated with seurat in r  early epidemiological analysis of the coronavirus disease 2019 outbreak based on crowdsourced data a population-level observational study kaiyuan sun jenny chen ccile viboud   as the outbreak of coronavirus disease 2019 covid-19 is rapidly expanding in china and beyond with the potential to become a worldwide pandemic1 real-time analyses of epidemiological data are needed to increase situational awareness and inform interventions2 previously real-time analyses have shed light on the transmissibility severity and natural history of an emerging pathogen in the first few weeks of an outbreak such as with severe acute respiratory syndrome sars the 2009 influenza pandemic and ebola3 4 5 6 analyses of detailed line lists of patients are particularly useful to infer key epidemiological parameters such as the incubation and infectious periods and delays between infection and detection isolation and reporting of cases3 4 however official individual patient data rarely become publicly available early on in an outbreak when the information is most needed building on our previous experience collating news reports to monitor transmission of ebola virus7 here we present an effort to compile individual patient information and subnational epidemic curves on covid-19 from a variety of online resources data were made publicly available in real time and were used by the infectious disease modelling community to generate and compare epidemiological estimates relevant to interventions we describe the data generation process and provide an early analysis of age patterns of covid-19 case counts across china and internationally and delays between symptom onset admissions to hospital and reporting for cases reported until jan 31 2020 
research in context
evidence before this study
an outbreak of coronavirus disease 2019 covid-19 was recognised in early january 2020 in wuhan city hubei province china the new virus is thought to have originated from an animal-to-human spillover event linked to seafood and live-animal markets the infection has spread locally in wuhan and elsewhere in china despite strict intervention measures implemented in the region where the infection originated on jan 23 2020 more than 500 patients infected with covid-19 outside of mainland china have been reported between jan 1 and feb 14 2020 although laboratory testing for covid-19 quickly ramped up in china and elsewhere information on individual patients remains scarce and official datasets have not been made publicly available patient-level information is important to estimate key time-to-delay events such as the incubation period and interval between symptom onset and visit to a hospital analyse the age profile of infected patients reconstruct epidemic curves by onset dates and infer transmission parameters we searched pubmed for publications between jan 1 1990 and feb 6 2020 using combinations of the following terms coronavirus or 2019-ncov and line list or case description or patient data and digital surveillance or social media or crowd-sourced data the search retrieved one relevant study on middle east respiratory syndrome coronavirus that mentioned flutrackers in their discussion a website that aggregates epidemiological information on emerging pathogens however flutrackers does not report individual-level data on covid-19
added value of this study
to our knowledge this is the first study that uses crowdsourced data from social media sources to monitor the covid-19 outbreak we searched dxycn a chinese health-care-oriented social network that broadcasts information from local and national health authorities to reconstruct patient-level information on covid-19 in china we also queried international media sources and national health agency websites to collate data on international exportations of covid-19 we describe the demographic characteristics delays between symptom onset seeking care at a hospital or clinic and reporting for 507 patients infected with covid-19 reported until jan 31 2020 the overall cumulative progression of the outbreak is consistent between our line list and an official report published by the chinese national health authorities on jan 28 2020 the estimated incubation period in our data aligns with that of previous work our dataset was made available in the public domain on jan 21 2020
implications of all the available evidence
crowdsourced line-list data can be reconstructed from social media data especially when a central resource is available to curate relevant information public access to line lists is important so that several teams with different expertise can provide their own insights and interpretations of the data especially in the early phase of an outbreak when little information is available publicly available line lists can also increase transparency the main issue with the quality of patient-level data obtained during health emergencies is the potential lack of information from locations overwhelmed by the outbreak in this case hubei province and other provinces with weaker health infrastructures future studies based on larger samples of patients with covid-19 could explore in more detail the transmission dynamics of the outbreak in different locations the effectiveness of interventions and the demographic factors driving transmission
 in this population-level observational study we used crowdsourced reports from dxycn a social network for chinese physicians health-care professionals pharmacies and health-care facilities established in 2000 this online platform is providing real-time coverage of the covid-19 outbreak in china obtained by collating and curating reports from news media government television and national and provincial health agencies the information reported includes time-stamped cumulative counts of covid-19 infections outbreak maps and real-time streaming of health authority announcements in chinese directly or through state media8 every report is linked to an online source which can be accessed for more detailed information on individual cases these are publicly available de-identified patient data reported directly by public health authorities or by state media no patient consent was needed and no ethics approval was required we closely monitored updates on dxycn between jan 20 2020 and jan 31 2020 to extract key information on individual patients in near real-time and reports of daily case counts for individual-level patient data we used descriptions from the original source in chinese to retrieve age sex province of identification travel history reporting date dates of symptom onset and seeking care at a hospital or clinic and discharge status when available individual-level patient data were formatted into a line-list database for further quantitative analysis individual-level patient data were entered from dxycn by a native chinese speaker ks who also generated an english summary for each patient entries were checked by a second person jc since dxycn primarily provides information on patients reported in china we also compiled additional information on internationally exported cases of covid-19 we obtained data for 21 countries outside of mainland china australia cambodia canada france germany hong kong india italy japan malaysia nepal russia singapore south korea sri lanka taiwan thailand united arab emirates the uk the usa and vietnam we gathered and cross-checked data for infected patients outside of china using several sources including global news media kyodo news straits times and cnn official press releases from each countrys ministry of health and disease control agencies in addition to detailed information on individual patients we reconstructed the daily progression of reported patients in each province of china from jan 13 until jan 31 2020 we used the daily outbreak situation reports communicated by provincial health authorities covered by state television and media and posted on dxycn all patients in our databases had a laboratory confirmed sars coronavirus 2 sars-cov-2 infection our covid-19 database was made publicly available as a google sheet disseminated via twitter on jan 21 2020 and posted on the website of northeastern university boston ma usa on jan 24 2020 where it is updated in real time data used in this analysis frozen at jan 31 2020 are available online as a spreadsheet we assessed the age distribution of all patients with covid-19 by discharge status we adjusted the age profile of chinese patients by the population of china we used 2016 population estimates from the institute for health metrics and evaluation9 to calculate the relative risk rr of infection with covid-19 by age group to calculate the rr we followed the method used by lemaitre and colleagues10 to explore the age profile of influenza where rr for age group i is defined as 
rriciiciniini
 where c
i is the number of cases in age group i and n
i is the population size of age group i to estimate trends in the strength of case detection and interventions we analysed delays between symptom onset and visit to a health-care provider at a hospital or clinic and from seeking care at a hospital or clinic to reporting by time period and location we considered the period before and after jan 18 2020 when media attention and awareness of the outbreak became more pronounced11 we used non-parametric tests to assess differences in delays between seeking care at a hospital or clinic and reporting between locations wilcoxon test to compare two locations and kruskallwallis test to compare three or more locations we estimated the duration of the incubation period on the basis of our line list data we analysed a subset of patients returning from wuhan who had spent less than a week in wuhan to ensure a narrowly defined exposure window the incubation period was estimated as the midpoint between the time spent in wuhan and the date of symptom onset we did all analyses in r version 353 we considered p values of less than 005 to be significant the funder had no role in study design data compilation data analysis data interpretation or writing of the report all authors had access to the data and had final responsibility for the decision to submit for publication our line list comprised 507 patients reported from jan 13 to jan 31 2020 including 364 72 from mainland china and 143 28 from outside of china table
 our sample captured 52 of 9826 covid-19 cases reported by who on jan 31 2020 the sex ratio was skewed towards males in mainland china five of 30 provinces were represented with 133 26 patients reported by beijing 87 17 by shaanxi 41 8 by hubei capital city is wuhan 19 4 by tianjin and 22 4 by yunnan of 435 patients with known relation to wuhan city most reported a travel history to the city 135 30 or were residents of the city 152 30 while 80 16 had no direct relation to the city 122 24 patients all reported in beijing had no information about their recent history with wuhan the age distribution of covid-19 cases was skewed towards older age groups with a median age of 45 years iqr 3356 for patients who were alive or who had an unknown outcome at the time of reporting figure 1
 the median age of patients who had died at the time of reporting was 70 years iqr 6581 few patients 13 3 were younger than 15 years adjustment for the age demographics of china confirmed a deficit of infections among children with a rr below 05 in patients younger than 15 years figure 1 the rr measure indicated a sharp increase in the likelihood of reported covid-19 among people aged 30 years and older a timeline of cases in our crowdsourced patient line list is shown by date of onset in figure 2
 indicating an acceleration of reported cases by jan 13 2020 the outbreak progression based on the crowdsourced patient line list was consistent with the timeline published by china center for disease control and prevention cdc on jan 28 202012 which is based on a more comprehensive database of more than 6000 patients with covid-19 since jan 23 2020 the cumulative number of cases has slowed down in the crowdsourced and china cdc curves figure 2 which probably reflects the delay between disease onset and reporting the median reporting delay was 5 days iqr 38 in our data province-level epidemic curves are shown by reporting date in figure 3
 as of jan 31 2020 16 52 of 30 provinces in mainland china had reported more than 100 confirmed cases the apparent rapid growth of newly reported cases between jan 18 and jan 31 2020 in several provinces outside of hubei province is consistent with sustained local transmission across the study period the median delay between symptom onset and seeking care at a hospital or clinic was 2 days iqr 05 days in mainland china figure 4
 this delay decreased from 5 days before jan 18 2020 to 2 days thereafter wilcoxon test p00009 some provinces such as tianjin and yunnan had shorter delays data by province not shown while the early cases from hubei province were characterised by longer delays in seeking care median 0 days iqr 01 the median delay between seeking care at a hospital or clinic and reporting was 2 days iqr 25 days in mainland china and decreased from 9 days before jan 18 2020 to 2 days thereafter wilcoxon test p00001 figure 4 similarly to delays in seeking care at a hospital or clinic reporting was quickest in tianjin and yunnan median 1 day iqr 01 and slowest in hubei province median 12 days iqr 716 the median delay between symptom onset and seeking care at a hospital or clinic was 1 day iqr 03 for international travellers and shorter than for patients in hubei province or the rest of mainland china kruskalwallis test p00001 figure 4 even in the period after jan 18 2020 when awareness of the outbreak increased a shorter delay between symptom onset and seeking care at a hospital or clinic was seen for international patients than for those in mainland china wilcoxon test p00001 for international cases the delay between seeking care at a hospital or clinic and reporting was 2 days iqr 14 also shorter than for mainland china wilcoxon test p00001 figure 4 on the basis of 33 patients with a travel history to wuhan we estimated the median incubation period for covid-19 to be 45 days iqr 3055 appendix p 2 information from patient line lists is crucial but difficult to obtain at the beginning of an outbreak here we have shown that careful compilation of crowdsourced reports curated by a long-standing chinese medical social network provides a valuable picture of the outbreak of covid-19 in real time the outbreak timeline is consistent with aggregated case counts provided by health authorities for comparison china cdc published the first epidemic curve by symptom onset on jan 28 202012 line lists provide unique information on the delays between symptom onset and detection by the health-care system reporting delays and travel histories this information cannot be extracted from aggregated case counts published by official sources line list data can help assess the effectiveness of interventions and the potential for widespread transmission beyond the initial foci of infection in particular shorter delays between symptom onset and admission to hospital or seeking care in a hospital or clinic accelerate detection and isolation of cases effectively shortening the infectious period a useful feature of our crowdsourced database was the availability of travel histories for patients returning from wuhan which along with dates of symptom onset allowed for estimation of the incubation period here and in related work13 14 a narrow window of exposure could be defined for a subset of patients who had a short stay in wuhan at a time when the epidemic was still localised to wuhan several teams have used our dataset and datasets from others to estimate a mean incubation period for covid-19 to be 56 days 95 ci 21113 14 15 16 our own estimate median 45 days iqr 3055 is consistent with previous work that used other modelling approaches13 14 15 16 the incubation period is a useful parameter to guide isolation and contact tracing based on existing data the disease status of a contact should be known with near certainty after a period of observation of 14 days13 availability of a public dataset enables independent estimation of important epidemiological parameters by several teams allowing for confirmation and cross-checking at a time when information can be conflicting and noisy an interesting finding in our data relates to the age distribution of patients we found a heavy skew of infection towards older age groups with substantially fewer children infected this pattern could indicate age-related differences in susceptibility to infection severe outcomes or behaviour however a substantial portion of the patients in our database are travellers a population that is usually predominantly adults although does not exclude children furthermore because patient data in our dataset were captured by the health system they are biased towards the more severe spectrum of the disease especially for patients from mainland china clinical reports have shown that severity of covid-19 is associated with the presence of chronic conditions16 17 which are more frequent in older age groups nevertheless we would also expect children younger than 5 years to be at risk of severe outcomes and to be reported to the health-care system as is seen for other respiratory infections18
 biological differences could have a role in shaping these age profiles a detailed analysis of one of the early covid-19 clusters by chan and colleagues19 revealed symptomatic infections in five adult members of the same household while a child in the same household aged 10 years was infected but remained asymptomatic potentially indicating biological differences in the risk of clinical disease driven by age previous immunity from infection with a related coronavirus has been speculated to potentially protect children from sars20 21 and so might also have a role in covid-19 in any case if the age distribution of cases reported here was to be confirmed and the epidemic were to progress globally we would expect an increase in respiratory mortality concentrated among people aged 30 years and older this mortality pattern would be substantially different from the profile of the 2009 influenza pandemic for which excess mortality was concentrated in those younger than 65 years21
 in our dataset we saw a rapid increase in the number of people infected with covid-19 in several provinces of china consistent with local transmission outside of hubei province as of jan 31 2020 province-level epidemic curves are only available by date of reporting rather than date of symptom onset which usually inflates recent case counts if detection has increased furthermore province-level data include both returning travellers from hubei province ie importations and locally acquired cases which also usually inflate the apparent risk of local transmission notably other lines of evidence suggest that local transmission is now well established outside of hubei province because travel increased just before the chinese new year on jan 25 2020 and before implementation of the travel ban in wuhan22 accordingly our own data include evidence of transmission clusters in non-travellers with for instance a second-generation transmission event reported in shaanxi on jan 21 2020 our study had several limitations one of which was the data we used although all provinces in mainland china provide aggregated information on infections and deaths individual-level patient descriptions are only available for a subset of provinces geographical coverage is heterogeneous in our line list and we have a notable deficit of cases from hubei province the foci of the covid-19 outbreak we expect that little patient-level information is shared on social media by province-level and city-level health authorities in wuhan and hubei province because health systems are overwhelmed for similar reasons provinces with a large total case count at the end of january 2020 or with a weaker health infrastructure were under-represented in our line list with the exception of beijing other limitations in our data include severity only patients who had severe enough symptoms to seek care were captured and changes in case definition a series of epidemiological criteria were required for covid-19 testing including travel history to wuhan within the past 2 weeks residence in wuhan within the past 2 weeks contact with individuals from wuhan with fever and respiratory symptoms within the past 2 weeks and being part of an established disease cluster some of these criteria eg relation to wuhan were relaxed over time appendix as a result we have an over-representation of travel-related cases in our database the reproduction number is an important quantity for outbreak control we refrained from estimating this parameter because reporting changes could bias estimates relying on epidemic growth rates furthermore our dataset captured cases all over china and does not reflect transmission patterns in any particular location a mean reproduction number of 2527 has previously been estimated on the basis of the volume of importations of international cases in the pre-intervention period in wuhan11
 we recognise that although our data source is useful and timely it should not replace official statistics manual compilation of detailed line lists from media sources is highly time consuming and is not sustainable when case counts reach several thousands here we provide detailed data on 507 patients when the official case count was over 9000 by jan 31 2020 representing a sample of approximately 5 of reported cases and a much smaller proportion of the full spectrum of covid-19 cases which include mild infections a crowdsourced system would not be expected to catch all cases especially if many cases are too mild to be captured by the health-care system digital surveillance or social media notably dxycn does not generate data outside of traditional surveillance systems but rather provides a channel of rapid communication between the public and health authorities in turn our approach has helped extract and repackage information from health authorities into an analytical format which was not available elsewhere at the time of writing efforts are underway to coordinate compilation of covid-19 data from online sources across several academic teams ultimately we expect that a line list of patients will be shared by government sources with the global community however data cleaning and access issues might take a prohibitively long time to resolve for the west african ebola outbreak a similarly coordinated effort to publish a line list took 2 years23 given the progression of the covid-19 outbreak such a long delay would be counterproductive overall the novelty of our approach was to rely on a unique source for social media and news reports in china which aggregated and curated relevant information this approach facilitated entry of robust and standard data on clinical and demographic information reassuringly dxycn maintains a special section dedicated to debunking fake news myths and rumours about the covid-19 outbreak looking to the future collection of patient data in the context of emergencies could include information on whether patients are identified through contact tracing or because they seek care on their own furthermore data interpretability could be improved by gathering more quantitative information on how case definitions are used in practice in conclusion crowdsourced epidemiological data can be useful to monitor emerging outbreaks such as covid-19 and as previously ebola virus7 these efforts can help generate and disseminate detailed information in the early stages of an outbreak when little other data are available enabling independent estimation of key parameters that affect interventions based on our small sample of patients with covid-19 we note an intriguing age distribution reminiscent of that of sars which warrants further epidemiological and serological studies we also report early signs that the response is strengthening in china on the basis of a decrease in case detection time and rapid management of travel-related infections that are identified internationally this is an early report of a rapidly evolving situation and the parameters discussed here could change quickly in the coming weeks we will continue to monitor the epidemiology of this outbreak using data from news reports and official sources 
for an example of an online source see httpsncovdxycnncovh5viewpneumonia
for the who situation report as of jan 31 2020 see httpswwwwhointdocsdefault-sourcecoronavirusesituation-reports20200131-sitrep-11-ncovpdfsfvrsnde7c0f74
for the laboratory for the modeling of biological  socio-technical systems website at northeastern university see httpswwwmobs-laborg2019ncovhtml
for the spreadsheet of patient-level data until jan 31 2020 see httpsdocsgooglecomspreadsheetsd1gb5cyg0fjutsqh3hll-c5a23zioxmwh5vebklfshzgedituspsharing
for dxy website see dxycn

 all data used in this report have been made publicly available on the laboratory for the modeling of biological  socio-technical systems website of northeastern university the available data include daily case counts of covid-19 by reporting date and chinese province and a de-identified line list of patients with covid-19 the line list includes geographical location country and province reporting date dates of symptom onset and seeking care at a hospital or clinic relation to wuhan discharge status when known an english summary of the case description from media sources and a link to the original source of data  clustering of covid-19 morbidity cases in germany d petrusevich a  the covid-19 coronavirus has spread almost all over the world though it has been reported recently that the epidemic declines in china in other countries it still hasnt achieved peak level the data analysis methods may help struggling against the disease the covid-19 tracking germany dataset has been handled in the research its daily refreshed dataset available at the kagglecom site it contains information on number of fallen ill people in germany the cases are grouped by federal land city age diapason and date the main goal of the research is to underline differences in morbidity registered in different lands of germany there have been published new suggestions about connection between coronavirus morbidity and bcg vaccination this question is also taken into account analysis based on the handled dataset is able to make only oblique conclusions because of lack of information differences in coronavirus morbidity in various regions and various age groups are highlighted the regions of germany are clustered into groups by gravity of recent situation  in this research the data analysis methods are used in order to detect special features in morbidity and mortality of the coronavirus disease in germany 1  nowadays data science and information technologies in common are used in a lot of tasks of various themes 2  today there are attempts to help medic staff understand special features of this disease the federal lands of germany are investigated because there are assumptions on the influence of bcg vaccines 3  according to this publication the vaccinated people have got light forms of the illness still there are debates on this question attempt to test this hypothesis has been done the bcg vaccine implementation has been cancelled in germany nowadays 4 but in the eastern germany this vaccine was implemented totally if this hypothesis is true there should be statistical differences in data of the former eastern germany lands and the western germany regions these features are investigated in the present paper also division of federal lands into clusters by severity of the coronavirus is done nowadays this analysis is processed in a lot of projects of publications 5 6  its necessary in order to understand special features in morbidity make conclusions about differences of immune system structure of infected in light and heavy forms differences in age population density in region number of deaths and infection cases are analyzed the covid-19 tracking germany dataset 1 contains information on disease cases that is daily refreshed there are columns state containing name of the federal land where the cases have occurred 7  county holding information on the city or other administrative element where they have taken mip engineering-2020 iop conf series materials science and engineering 862 2020 042037 iop publishing doi1010881757-899x8624042037 2 place age-group gender date containing date of the cases cases and deaths holding information on number of illness cases and death cases information is grouped by date federal land city age groups and gender according to 7 the federal lands list has been handled germany has got complicated history and administrative division some administrative elements have been united into groups saarland is the region transferred from france after the 2 nd world war this relatively small land by population has been combined with neighbouring rhineland-palatinate in the experiments there are cities that are still individual administrative elements in this research they are combined with surrounding federal lands bremen has been combined with lower saxony schleswig-holstein and hamburg have been united there are few cities with population more than 1 mln people but in berlin there are more than 3 mln citizens 7  this city is usually handled individually in the experiments illness cases are grouped by age into categories 0 -4 5 -14 15 -34 35 -59 60 years and older the date of the handled dataset is the 2 nd of april its refreshed daily version of this day is used in the experiments the last cases included into it are marked with the 1 st of april the first cases have been detected in bayern on the 28 th of january by the 2 nd of april the number of coronavirus cases in germany is 77477 the dataset 1 has been clustered manually by the illness spread rates and values of morbidity available in the data automatic methods of clustering including hierarchical divisive and agglomerative clustering 8 9 are implemented in the second part of the experiment groups of regions with different behaviour of illness spread and morbidity are constructed and analyzed the first parameter that should be mentioned is growth rate of illness cases by this value the lands of germany can be divided into three clusters the first group with low speed of growth mainly include the lands of former eastern germany the ratio of deathscases is less or equal to 1 the maximum growth per day isnt more than 200 person per day for the majority of these lands the first case of illness has been detected much later than in the other groups this cluster includes saxony saxony-anhalt mecklenburg-vorpommern brandenburg thuringia saarland the second cluster of intermediate growth rate of illness cases number the maximum growth doesnt achieve 1000 person per day but its higher than in the first cluster the ratio deathscases is approximately the same as in the first cluster it includes berlin rhineland-palatinate hesse schleswig-holstein here united with hamburg lower saxony here united with bremen the third cluster includes regions with high growth rate of illness cases quantity the ratio deathscases varies maximum speed of growth is more than 1000 person per day the first cases are detected much earlier than in the other clusters the deathscases is more than 1 and usually is twice more than in the first cluster there are north rhine-westphalia bavaria here the first cases in germany have been detected baden-wurttemberg examples of growth rates in each cluster are presented at the figures 1 -3 the x axis denotes days from the first case in the land one can notice that during the first month there havent been a lot of illness cases in bavaria figure 3 the same is also right for rhineland-palatinate region during the first ten days theres no quick growth the first case in saxony has been noticed on the 2 nd of march the first case in rhineland-palatinate has been noticed on the 28 th of february the first case of illness in bavaria has been noticed on the 28 th of january one can notice that growth rates have been at low level in the very beginning of the epidemic in all cases and they have started raising quickly approximately at 10 th -15 th of march the majority of death cases is in the group of 60-99 years by age statistical data that can be obtained from the dataset is presented in the table 1 the date when the first coronavirus case has been detected the date when 50 of overall level infection the 2 nd of april is the date of the analyzed dataset has been achieved are shown in the last columns they can demonstrate how fast the growth rates increase population density of regions is taken from the statistical data of 2018 year 7  one can notice that theres high quantity of illness cases in the lands with the maximal density of population though the density is lower in bavaria at the same time these are the lands that are close or have got borders with countries in which situation is very difficult netherlands and italy the highest growth rate is marked in bavaria the lands of the second cluster hesse schleswig-holstein and hamburg rhineland-palatinate and saarland lower saxony and bremen have got lower values of ratio deathscases berlin has got very high density and still situation there is better than in the third cluster lower saxony and bremen have got lower density but the ratio is high the lands of the first cluster have got low density of population though their ratio values are higher than ones in the second cluster almost in all lands 50 of infected people rate data of the 2 nd of april has been achieved on the 23 rd of march thus approximately during one week number of infected people doubled it means that the peak of morbidity is still in the future one can conclude that by absolute numbers the former eastern germany regions handle the coronavirus epidemic better than other regions at the same time looking at the relative parameters one can see that the situation is slightly better in schleswig-holstein here with hamburg and in hesse rhineland-palatinate saarland regions looking at figures 1 -3 one can see that the epidemic still doesnt achieve its peak values new research on bcg application 3 is devoted to statistical comparison between coronavirus infection rates its severity and rates of bcg vaccines implementation available statistics isnt enough to make thorough research and conclusions information in the handled dataset can be analyzed to make tests on this hypothesis nowadays the bcg overall implementation is cancelled in some european countries including germany 4  but in the former eastern germany this procedure was done thus one can try to find statistical traces of this notice there are two problems that cant be investigated observing ordinary open data the first one is expiration date of bcg vaccine it produces complex effect on organism and can be used to a set of illnesses not only against the tuberculosis but this effect weakens with time the individuals younger than some limit domain of age should go through coronavirus easier at the same time the deaths rate is higher for the group people of 60-99 years the vaccine has got limited effect according to this hypothesis 3  the second problem is lack of statistical data on people of former eastern germany a lot of them have migrated to the lands of the western germany and they participate in the statistics of these regions at the same time one have to propose that the majority of people older than 30 years living in the federal lands of former eastern germany are born there and have been vaccinated the first idea to be tested is the comparison of regions that were included into the eastern and western germany on the number of deaths this information is shown in the table 2 three age groups have been analyzed 15-34 years 35-59 years and older than 59 years quantity of illness cases deaths are presented in the first column in each pair the ratio deathscases is shown in the second column in each pair the first column is 0 almost for all regions young people dont die because of the coronavirus there are some death cases if an age is between 15 and 34 years old these cases should be handled individually but theres no open data one can suppose that there have been chronic health conditions 5 6  and at last percent of deceased people older than 59 years is in the interval 3 -7 one cannot say that the eastern germany regions have got lower ratio there are the highest percents in the lands bavaria baden-wurttemberg lower saxony with bremen north rhine-westfalia and saxony-anhalt though absolute numbers in this region are extremely low the hypothesis about the bcg influence isnt confirmed here but as it was mentioned above there isnt enough information in the dataset to test this idea at the same time its possible to check the density of the infected people the population density varies in different lands so it would be better not to divide number of cases on population of a land but to use population density values if the quantity of infected people increases this value also grows if the density grows the population grows but area is the same the ratio decreases results of this experiment are presented in the table 3 values in the last column of the table 3 dont depend on the population density or area of lands in this experiment the lands can also be separated into three clusters that have got much in common with the differentiation in the paragraph 31 the former eastern germany without brandenburg mecklenburg-vorpommern saxony-anhalt thuringia saxony and even berlin which should be handled separately has got ratio less than 10 the intermediate cluster contains brandenburg separated from the cluster of the former eastern germany and the following lands schleswig-holstein and hamburg rhineland-palatinate and saarland hesse north rhine-westfalia lower saxony and bremen the ratio is less than 30 in this group north rhine-westfalia has got the highest population density because of that the ratio isnt very high though absolute values can be interpreted as a disaster the regions with heavy coronavirus situation are bavaria and baden-wurttemberg  here x is an old value  is the transformed one is its mean value and is its standard deviation after this step standard deviation of all the values is 1 mean value is 0 agglomerative clustering method has been implemented to unite lands of germany into new groups clusters the euclidean metrics has been used to measure distance between classes and the ward method has shown the best results in order to combine them the agnes command is used the dendrogram is presented in figure 4  one can notice that regions with heavy situation are combined into one cluster bavaria north rhine-westfalia and baden-wurttemberg berlin is separated into its own cluster because of its high population in comparison to the other regions of germany saxony-anhalt and lower saxony with bremen are united into the third cluster theyve got close values of deaths  infected people ratio presented in the table 1 the lands of the fourth cluster are also close by the same parameter saxony hesse schleswig-holstein with hamburg and rhineland-palatinate with saarland have got deaths  infected people ratio close to 3 regions of mecklenburg-vorpommern brandenburg and thuringia are united into the fifth cluster it should be mentioned that these regions were parts of the former eastern germany other lands are included into the third and fourth clusters berlin is a separated cluster as it was mentioned above here one cant conclude that there are differences between former western and eastern germanies the agglomerative coefficient is about 87 the second clustering experiment handles dataset containing number of overall illness cases population density and result of their division the table 3  the divisive hierarchical clustering method the diana command is used has shown the best results the divisive coefficient is about 88 8 9  the first cluster contains all lands of the former eastern germany except berlin berlin is separated into an individual cluster the third cluster contains hesse schleswig-holstein with hamburg rhineland-palatinate with saarland and lower saxony with bremen the fourth cluster contains the regions with maximal quantity of infected people bavaria north rhine-westfalia and baden-wurttemberg thus its possible to conclude that the former eastern germany lands behave in a different way during the epidemic but the thorough analysis has to use individual depersonalized information on implementation of vaccines also other clustering techniques for example 9  are going to be implemented in further research the coronavirus in germany dataset 1 has been investigated in this paper the clustering of the german regions by severeness of the illness has been done there are regions with heavy situation bavaria north rhine-westfalia and baden-wurttemberg lands with low speed of spread mecklenburg-vorpommern brandenburg thuringia and regions with intermediate speed hesse schleswig-holstein and hamburg rhineland-palatinate and saarland lower saxony and bremen the automatic agglomerative and divisive clustering methods have got common results and there are some differences the cluster of regions with very high speed of infection spread appears in both experiments berlin is separated into an individual cluster because its very large city by population in also the hypothesis on bcg vaccine influence on the morbidity and mortality from the coronavirus 3 has been investigated the vaccine has been implemented to all citizens of the former eastern germany at the same time in the western germany the total vaccination hasnt been done the bcg overall implementation is cancelled in some european countries including germany the hypothesis could be tested on the data from germany thus there should be statistical differences in the morbidity in various regions of germany there are such features and in one of the clustering experiments all the lands of the former eastern germany have been included in one cluster of low speed of illness spread except berlin but to make confident conclusions one has to use medical data and information about people who moved between regions of former western and eastern parts of germany thus further analysis is required and datasets of bcg vaccine implementation is necessary to complete such research  recognizing flu-like symptoms from videos tuan thi hue li wang ning ye jian zhang sebastian maurer-stroh li cheng   while the recent swine flu pandemic was luckily less severe than initially thought there remains a constant threat of mutated or reassorted influenza strains that give rise to new outbreaks that could range from small local clusters 1 to seasonal epidemics 2 or even global pandemics 3 4 similarly history has also shown us that previously unknown pathogens such as the sars coronavirus could emerge and cause serious outbreaks 5 just in 2012 and 2013 there were 2 new outbreaks of different viruses with pandemic potential mers-cov 6 and h7n9 7 triggering increased surveillance alerts respiratory diseases often manifest themselves through similar flu-like symptoms and early detection of new outbreaks is of central importance in order to delay or prevent their escalation and wider spread however classical surveillance systems are mostly relying on time-delayed and costly virological tests requiring hospital or physician visits 810 one potential alternative is to detect typical flu-like symptom in human behaviors by automatically analyzing video footage from public areas such as airports bus stations which exploits the existing vision-based surveillance infrastructure in public venues this will provide a unique valuable source of information that is complementary to the existing public health monitoring network under this context we make a first attempt on the recognition of typical flu-like symptoms sneeze and cough actions and propose a novel discriminative approach which is further evaluated on a new sneeze-cough action dataset major contributions our first contribution is a new video action dataset1 dedicated towards the problem of flu-like symptoms detection that is of central importance in early surveillance of respiratory disease outbreaks a series of experiments are conducted with performance analysis that reveals some of the characteristics of this dataset our second contribution is two novel types of action matching kernels amks that are shown to perform competitively comparing to the state-of-the-art methods in particular we show that pyramid match kernel 11 and spatial pyramid matching 12 are both special cases of the proposed kernels the kernels are also closely connected to the recent developments in hough transform 13 14 related work current respiratory disease surveillance systems are known to lag significantly behind the onset of outbreaks 15 16 mostly due to their heavy reliance on virological and clinical data including physician visits very recently a web-based surveillance tool has been developed by google 17 which is made possible through search engines by taking advantage of the social health-seeking behavior of patients there are nonetheless concerns that there sometimes exists non-negligible bias in the detection results driven by disease publicity rather than the disease itself the work presented in this paper to our best knowledge is the first to examine this problem with the help of vision-based surveillance and analysis research on video action recognition and retrieval 18 has recently witnessed a dramatic increase mainly due to the vast demand to analyze and understand human actions from video footage of everyday life and from web hosts such as youtube myspace videos flickr and sciencestage established methods for modeling and analyzing human actions are often generative statistical approaches especially the markov models eg
19 20 recently the discriminative learning scheme has also been extended to allow structured predictions eg conditional random fields 21 they nevertheless often rely on learning with sophisticated parametric models similar to a number of recent works 2225 we also assume a human action can be sufficiently described by a set of local features in space-time a local feature typically comes with two aspects a descriptor vector and its space-time location as the number and locations of the local features are usually not fixed often a bag-of-words bow method is utilized to map the feature descriptors to a histogram vector in the space spanned by codewords as in 11 24 26 or hierarchical codewords as described in the pyramid match kernel 11 the bow representation has demonstrated impressive performance on image and action analysis tasks nevertheless it does not retain information regarding space-time layout of the local features on the other hand the spatial or space-time layout of local features has long been regarded as an important cue to infer the existence of a global object from local features the elegant hough transform 27 is originally devised to detect lines and circles an important generalization is developed by ballard 28 to detect objects of arbitrary shapes leibe et al in their seminal work 13 consider a probabilistic variant of the hough transform where the bow model is integrated into the voting space by means of conditional and posteriori probabilities this is further followed by 14 where a dedicated max-margin learning method is developed throughout these methods a crucial step is the construction of a voting space where all local features are made to vote for the existence and if so the location of the global object they belong to an interesting observation is that this voting space is employed by 14 in an implicit manner as clearly revealed from equations 12 and 13 of 14 the model or the parameter vector w is implicitly related to the voting space w is interpreted as weights for the activations of codewords where influence from the voting space is implicitly carried out via the activations a latent variant has also been used for object detection 29 recently there have been attempts to integrate the two sources of information bow and the space-time layout the spatial pyramid matching 12 a probabilistic variant of hough transform also called implicit shape model 13 and utilizing the skeleton structure of human body 30 are such examples in the next section we show that our amk explicitly incorporates the space-time layout and the bow model we will also show that the pyramid match kernel and the spatial pyramid matching are special cases of our proposed amks with proper feature extensions section sneeze-cough a public health surveillance dataset will describe our new sneeze-cough dataset in details and followed the experimental results in section results and discussion presented with such an action below we describe a set of kernel design recipes that are able to integrate both the bow representation and the space-time locations of local features unary extensions a unary extension partitions the volume into disjoint parts one such scheme is to partition into concentric layers as displayed in figure 1a by pooling the codeword assignments of these features in their bow representation one partition is characterized by a histogram of length k when k codewords are used a length ks vector is thus produced as a unary extension by concatenating over s partitions other partition schemes are also possible for example partitioning the volume into half in each dimension results in 222 blocks and is denoted as s8 we can further partition each block into smaller ones where each block has its own histogram interestingly this is the same three-layer spatial pyramid as depicted in figure 1 of spatial pyramid matching 12 the only difference is that here we consider a 3d space-time volume instead of a 2d image space by summing all the histograms over layers with proper weights and by using histogram intersection similarity measure we get back exactly the spatial pyramid matching 12 in fact the degenerate case of unary extensions by setting s to 1 returns the original bow model meanwhile by fixing s to 1 considering a bow model with hierarchical codewords and by using histogram intersection similarity measure the pyramid match kernel 11 is recovered binary extension different from the unary extensions a binary extension considers the interactions between a pair of features in space-time figure 1b provides such an example where similar to the concept of co-occurrence matrix a 3-dimensional array or 3-tensor is used to accumulate the counts of feature pairs using both volume and bow representations indexed by codewords codewords distance naively this leads to a vector of length kks by accumulating the quantized distance of each feature pair with s possible outcomes in practice it is further summarized into a more compact vector representation for a fixed distance a a k-dim vector is extracted from the diagonal elements b a k-dim vector is obtained by summing over all the off-diagonal elements row-wise for both cases the output vectors are normalized to sum to 1 as each case ends up giving a ks vector a concatenation of both finally leads to a vector representation of length 2ks from feature extensions to kernels it is straightforward to carry on and build a kernel from the extended vectors mentioned above in fact a kernel can be built by considering different feature extension by examining on a variety of similarity measures eg linear 2 histogram intersection radial basis function and by choosing from hierarchical vs standard codeword representations a family of kernels can thus be devised using the above recipes where the examples we illustrate in the paper comprise only a small fraction original pyramid match kernels in the original pyramid match kernel paper 11 a video action is represented as a set of local features descriptors excluding their space-time location information therefore an action of interest is represented as  this is followed by building hierarchical codewords using eg hierarchical k-means in each scale l0l-1 a histogram hl of codewords can be computed note the length of corresponding histogram decreases as we navigate to the upper layers of the hierarchy by concatenating these histograms the action p is then characterized as a feature vector p  h0phl-1p as in 11 the kernel function between two actions p and q is thus defined by
1 here wl is used to limit the contribution from a particular scale of histogram as inversely proportional to its scale wl  2-l nl is the partial increment from level l-1 to level l
2  denotes the histogram intersection
3 which can be equivalently written as
4 where id stores the codeword index of a local feature  is the indicator function and k is an index of the set of codewords amk type ii as illustrated in figure 2 instead of using histogram intersection of eq 3 we consider a matching function by modifying eq 4 to incorporate space-time locations of local features
5 where mlpq is a geometric measure of the feature pair and is computed as their affinity in the space-time volume
6 as shown in figure 2    refers to its quantized 3d location in the volume while dx dy and dt denote the number of quantization levels on each of the dimensions respectively it is easy to check that for the trivial case dx  dy  dt  1 eq 3 is recovered from eq 5 in other words the pyramid match kernel 11 can be regarded as a special case of amk type ii when no spatial and temporal constraints are enforced action matching kernels are mercer kernels it is easy to check that amks type i are mercer kernels ie the kernel matrix is positive semi-definite psd 31 32 as long as proper similarity measures such as 2 and histogram intersection are utilized an important property of amk type ii as defined by eqs 1 2 5 and 6 is that it is a mercer kernel this is clear from the fact that eq 6 is a psd as well as the fact that mercer kernels are closed under positive scaling and summation operations 31 and the weights wl are always positive endowed with a mercer kernel the induced convex optimization problem is guaranteed to produce a unique optimal solution using the support vector machine svm classifiers 31 in practice we use the binarymulticlass algorithms of libsvm 33 with customized kernels the action volume an action is naturally bounded in 3d space-time as eg illustrated in figure 1a bottom left in fact this is a property inherit in the problems regarding action recognition and retrieval in a typical recognition dataset such as kth 22 where there is only one person performing an action in a video the action is bounded by the size of the frames one possible scheme is to consider a local coordinate with its origin fixed to the center of these features and to explicitly examine all possible scales in a manner similarly to that of the hough voting space a simple scheme is instead considered in this paper where the action volume is determined by aligning the bounding boxes detected using a human detector 34 as a result its scale is also implicitly decided we describe here the new sneeze-cough video dataset that tailors to the specific challenges and characteristics of recognizing flu-like behavior symptoms in public areas note written consent on publication and use of the video data was obtained from each volunteer and the study was cleared by the bioinformatics institute ethics committee represented by the executive director this dataset contains 960 color video clips of imitated surveillance video settings collected from 20 human subjects 8 females and 12 males of 20 to 50 years old using a canon vixia hf20 camcorder a gallery of sample frames are displayed in figure 3 the data acquisition process is carried out in an indoor environment with semi-controlled lighting condition sun lights through windows are present in some of the videos and the camera is mounted on a tripod mimicking the relative height of a typical surveillance camera each clip contains one specific action performed by one subject in a particular view and pose video shots are normalized at 480  290 resolution with stream rate of 5 frame per second each lasts for around 15 seconds in addition to the two flu-like behaviors namely sneeze and cough six common background action types are also included drinking phone calling scratching head stretching arms wiping glasses and waving hands note we deliberately cover a spectrum of possible background action types that are relatively close to our actions of interest in addition each human subject performs each action six times under 2 different poses standing and walking and 3 different views roughly frontalleftright we also perform horizontal flip on each video to produce an additional video set of reflective views which results in a final set of 1920 videos throughout the experiments the following parameters are used for amk type i and ii the number of codewords is fixed to k  1024 by default 2 similarity measure is used for amk type i meanwhile amk type ii employs histogram intersection together with a hierarchical codewords of 4 levels these two different similarity measures are utilized here to showcase the flexibility of incorporating various similarity measures into the proposed amks libsvm 33 is used with the trade-off parameter c  10 to verify that the proposed amks are able to accommodate different local features two publicly available local feature detectors  descriptors are considered namely hoghof also called space time interest point 22 and cuboid 23 accuracy measures for kth we use the standard accuracy measure by averaging the diagonal values from the row-normalized confusion matrix for binary classification this becomes 2 which however is problematic for datasets with imbalanced class distributions such as sneeze-cough as 34 of the sneeze-cough examples belongs to background actions category and using the standard accuracy measure a rate of 75 is reached when a classifier is biased towards blindly assigning every example to background actions this leads to the utilization of precision and recall which are computed by  and  respectively we thus adopt a different accuracy measure of  for this binary classification task which can be regarded as a lower-bounding summary of the precision recall pair kth the kth dataset 22 contains 2391 video shots from 25 actors repeatedly performing 6 different action types under 4 different background contexts the actions include boxing handclapping handwaving jogging running and walking to facilitate direct comparison the same split scheme of 22 is adopted in our experiments where videos from 16 persons are used for training and the other 9 persons are retained for testing table 1 compares results of our proposed amks with reported works our implementation of the two baseline methods 882 for cuboid and 885 for hoghof is consistent with what has been reported in the literature 891
35 for cuboid and 918
35 for hoghof and the results of the proposed amk methods with best rate of 934 with cuboid and 942 with hoghof are competitive when comparing to the state-of-the-art approaches sneeze-cough for the sneeze-cough dataset we use 15 persons for training and retain the rest 5 persons for testing we would like to emphasize that this dataset is significantly different and is more challenging comparing to the kth dataset first the actions in this dataset except for hand-waving are usually of short time-span in contrast to actions such as walk or boxing that usually consist of a good number of repetitive action cycles second there exist large variations within the sneeze and cough actions over eg different genders ages and views this is further complicated by the fact that the background actions commonly seen in public areas such as phone calling scratching head are often very similar in appearance to flu-like symptoms meanwhile these 6 background actions by themselves are highly complex and exhibit large variations as well as indicated in the sample frames of figure 3 by experimenting with 8-class recognition tasks confusion matrices are obtained to facilitate our investigation into the inter- and cross- actions pattern of this new dataset figure 4 presents the confusion matrices obtained using baseline method bow 2 and the proposed amk type ii kernel when comparing the confusion matrices to the counterparts from kth dataset figure 4a vs figure 5a and figure 4b vs figure 5b it can be seen that the two baseline methods perform much worse on the sneeze-cough dataset than on the kth dataset this loss in accuracy suggests that the sneeze-cough dataset is much more challenging than the kth dataset meanwhile the actions of sneeze and cough seems to be more correlated with the subset of actions call drink scratch  rather than the rest ones of stretch wave wipe  this might due to the fact that for the action subset sneeze cough call drink scratch  hands are usually placed near the face while for stretch wave wipe  hands are often placed further away from the face the gain in accuracy by adopting the amk ii kernel is also evident when we compare the matrices c and a or d and b in figure 5 for the kth dataset as well as by comparing the same two pair of matrices in figure 4 for the sneeze-cough dataset for kth an improvement of around five points has been observed for the averaged diagonal elements as for sneeze-cough this improvement is around ten points these observations hold valid for both features our major interest in this dataset is to recognize flu-like symptoms sneeze and cough from the background actions therefore binary classification experiments are conducted to examine the performance of the proposed amk kernels and the results are listed in table 2 for each amk kernel type in addition to the precision recall pair the accuracy measure provides an easy-to-compare summary of its performance on average we can see that this dataset is rather challenging the best method merely reach an accuracy of around 444 which can be partially explained by the large variations within coughsneeze over various gendersagesviews as well as their similar space-time appearances to those of the background actions for both cuboid and hoghof local features we see significant improvement in accuracy by using specific amks compared to baseline methods take cuboid feature for example we observe that using the amk type i kernel with ub and s27 leads to an increase of accuracy by 125 points interestingly although the best results ie 434 vs 444 are similar for both local features they are in fact obtained from different type of amk kernels in this paper we develop a new family of kernels in the proposed approach that explicitly integrates the two important aspects of action local features space-time layout and bow representations meanwhile a new public health action dataset is introduced in this paper to facilitate the study of detecting typical flu-like symptoms in public areas this dataset is shown to be significantly different from and is more challenging than established datasets such as the kth dataset we demonstrate that our approach while achieving competitive performance on the well-studied kth dataset produces reasonable results for this unique and challenging sneeze-cough dataset for ongoing work we would extend the current approach to retrieve flu-like behavior symptoms from video archives which often contain multiple persons simultaneously performing a series of actions often in crowded environments in particular we plan to work with real surveillance datasets and test correlation of daily or weekly average sneezecough incidence with public health records of respiratory disease trends over time to show utility of the approach and if it is following or preceding reported peaks from hospital or doctor visit-based reporting systems we envision that this approach can also be useful for detection of a variety of emergency situations triggering respiratory symptoms such as fires gas leaks or chemical spills from accidents or even terrorist attacks 1 the dataset is made available at httpwebbiia-staredusgchengliflurecognitionhtm 2 tp tn fp and fn refer to true positive true negative false positive and false negative respectively  immune and metabolic signatures of covid-19 revealed by transcriptomics data reuse luiz gardinassi g camila souza o helioswilton sales-campos simone fonseca g   the outbreak of coronavirus disease 19 covid-19 first recognized in wuhan china rapidly became a pandemic of major impact not only on global public health but also on economy and social well-being 1 sars-cov-2 infection results in clinical outcomes ranging from asymptomatic status to severe disease and ultimately death 2 understanding of the molecular mechanisms underlying the pathology of covid-19 is required to design effective therapies and safe vaccines in this context current investigations have been devoted to biochemical characterization and cellular phenotyping in patients to development of animal models of covid-19 3 transcriptomics of peripheral blood cells has been a powerful tool to characterize human immune responses to diverse pathogens including respiratory viruses 46 gene expression profiling by different analytical platforms and sample types revealed that covid-19 patients exhibit i activation of humoral immunity hypercytokinemia apoptosis 7 and dynamic toll like receptor tlr signaling 8 in peripheral leukocytes ii induction of interferon stimulated genes isgs chemokines and inflammation in the lower respiratory tract 7 9 10 of importance the results and interpretation of these data were based on single-gene-level analyses in which significance of quantitative changes of each gene are calculated separately and they are latter submitted to pathway enrichment analysis however the statistical power and sensitivity to identify pathways or gene modules computational gene networks associated with disease phenotypes can be enhanced by the use of non-parametric rank-based tests such as the robust positional framework gene set enrichment analysis gsea 11 moreover interpretation of transcriptional changes during covid-19 has been primarily evaluated using canonical pathways that do not often reflect human responses therefore we propose alternative strategies to analyze and interpret transcriptomics data which provide novel insights into immune and metabolic responses during covid-19 datasets used in this study included public transcriptomes available at the genome sequence archive gsa or human gsa in national genomics data center beijing institute of genomics big chinese academy of sciences for rna-seq data related to sars-cov-2 infection cra002390 and hra000143 gene expression omnibus geo for rna-seq data related to sars-cov-2 infection gse147507 and microarray data related to sars-cov-1 infection gse1739 or influenza a virus iav infection gse34205 gse6269 gse29366 gse38900 gse20346 gse52428 gse40012 gse68310 gse61754 gse90732 and arrayexpress for nanostring ncounter data related to sars-cov-2 infection e-mtab-8871 deseq2-normalized counts were used for the rna-seq dataset cra002390 7 while raw read counts for the rna-seq datasets gse147507 9 or hra000143 10 were treated and normalized to log2 counts per million with edger package for r 12 normalized data was acquired for nanostring ncounter e-mtab-8871 8 normalized microarray datasets were acquired with omicc platform 13 detailed information about the datasets used in this study are described in table 1 data were analyzed with the positional framework gene set enrichment analysis gsea 11 using pre-ranked mode 1000 permutations and weighted enrichment statistics the blood transcriptional modules btms 24 and metabolic pathways annotated in the kyoto encyclopedia of genes and genomes kegg database 25 were used as gene sets to construct the network of btms from peripheral blood mononuclear cell pbmc transcriptomes genes were pre-ranked by the wald test statistics score calculated with deseq2 package comparing each gene in covid-19 patients and healthy controls as described 7 btms detected with a false discovery rate fdr adjusted p  0001 were then linked by the number of genes shared between two gene modules to perform the btm-driven meta-analysis between respiratory viruses gene lists from each dataset were pre-ranked by log2 fold change of experimental samples over healthy controls gene modules significantly associated with at least 50 of the datasets were selected by a nominal p  0001 for pbmcs and whole blood the datasets were not merged at the single-gene-level each dataset was composed by a different number of genes and samples and different types of samples table 1 the output of the gsea provides a normalized enrichment score nes for each btm associated with each dataset the nes was then compared between datasets selected at the determined cut-off p  0001 to enforce confidence in the enrichments we also retained only the btms that were associated with at least 50 of the datasets independently of infection sample type and regulation metabolic pathways from kegg database were selected by a fdr adjusted p  005 for pbmcs from covid-19 patients for balf datasets cra002390 and hra000143 genes were also pre-ranked by log2 fold change of experimental samples over healthy controls and used as input in pre-ranked gsea btms and kegg metabolic pathways were selected by relaxed significance nominal p  005 and consistent up- or downregulation in both datasets for lung biopsies gse147507 one sample from covid-19 patients shows a distinct read count profile and was considered an outlier as described 26 the remaining sample was used to perform single sample gsea in which genes were pre-ranked by log2 fold change of the experimental sample over healthy controls networks were visualized and generated with cytoscape v372 27 heat maps were generated with the package gplots for r and hierarchical clustering with the package amap for r using euclidian distance metric and ward linkage the bubble plots were generated with the package ggplot2 for r graphpad prisma v 8 was used to perform t-tests on nanostring ncounter data and generate bar plots to evaluate the robustness of our approach validate previous findings and obtain novel perspectives into immune responses to sars-cov-2 infection we constructed a modular transcriptional network of pbmcs from covid-19 patients genes were pre-ranked by the wald test statistics score calculated with deseq2 package 7 and used as input in pre-ranked gsea we interpreted the dynamics in gene expression of covid-19 patients using the alternative tool to conventional pathways the btms which were particularly devised to evaluate human immune responses 24 to ensure maximal confidence we applied a conservative statistical cutoff fdr adjusted p  0001 to select significant btms figure 1a the transcriptional network captured several cellular characteristics of sars-cov-2 infection in peripheral blood including t and nk cell figure 1d cytopenia 28 and upregulation of cell cycle or genes associated with plasma cells and immunoglobulins 7 in addition our approach also detected increased signals of monocytes figure 1b dendritic cells figure 1c and of the mitochondrial respiratory electron transport chain in sars-cov-2 infection figure 1a suggesting a critical role of metabolic pathways for the immune response of covid-19 patients to gather further insights on host responses to sars-cov-2 infection the modular transcriptional signature of covid-19 patients was compared to that of individuals infected with sars-cov-1 or iav for this we analyzed 11 additional public transcriptome datasets spanning over 600 samples from human pbmcs or whole blood gene lists from each dataset were pre-ranked by the log2 fold changes relative to healthy controls and used as input in pre-ranked gsea the statistical cutoff was established at nominal p  0001 whereas only bmts present in at least 50 of datasets are shown figure 2a independently of the cohort technology to quantify gene expression rna-seq or microarray and type of sample pbmcs or whole blood we observed a core transcriptional response that is comparable between infections caused by sars-cov-2 sars-cov-1 and iav this core response includes modules of cell cycle and proliferation monocytes and dendritic cells indeed the module m67 dendritic cells was upregulated in almost all datasets of interest sars-cov-1 and iav infections also induced significant reduction of peripheral t lymphocytes and nk cells datasets from iav infection induced activation of type i interferonantiviral responses or rig-1 like receptor signaling while only sars-cov-1 induced significant association to one module antiviral ifn signature data from a different cohort of patients and analytical platform also demonstrated that several genes involved in type i interferonantiviral responses were not significantly altered in whole blood of covid-19 patients figure 2b we also evaluated btms that were uniquely associated to the transcriptomes from covid-19 patients which showed enrichment in immune-related modules and heme biosynthesis figure 2c data indicates an upregulation of heme biosynthesis in pbmcs from covid-19 patients figure 2d because immune responses are tightly connected to metabolic programs 4 2931 we explored metabolic pathway enrichment with the kegg database in addition to porphyrin metabolism which shares significant proportion of genes with btm m222 heme biosynthesis ii our analysis confirmed the upregulation of glycolysis and gluconeogenesis 7 and detected other pathways such as tricarboxylic acid tca cycle oxidative phosphorylation tryptophan metabolism glycan degradation nucleotide metabolism and galactose metabolism figure 2e because the lung is the primary site of infection and failure of this organ is a severe complication of sars-cov-2 infection we also evaluated immune and metabolic signatures in the lower respiratory tract of covid-19 patients for that we performed a btm-driven meta-analysis of transcriptomes from samples of bronchioalveolar lavage fluid balf 7 using a relaxed statistical cutoff nominal p  005 there were nine significant btms and three kegg metabolic pathways that were consistently up or downregulated among both datasets figure 3a btms reflect upregulated networks of chemokines and neutrophils as well as reduced expression of genes related to dendritic cells monocytes and t cell activation we also found consistent upregulation of the modules related to chemokines figure 3b and neutrophils figure 3c in lung tissue data from one covid-19 patient few metabolic pathways were consistently regulated between the balf datasets including the upregulation of oxidative phosphorylation and downregulation of fructose and mannose metabolism and other glycan degradation figure 3a none of these metabolic pathways were significantly enriched on the sample of lung tissue here we used a robust modular transcriptomics approach that captured significant changes of cellular patterns in peripheral blood of covid-19 patients including t lymphopenia and reduced numbers of nk cells 28 several hypothesis have been formulated to explain the lymphopenia during covid-19 including t cell infection by sars-cov-2 32 or t cell exhaustion 33 in addition we identified upregulated expression of chemokines and neutrophils in the lung tissue and balf of covid-19 patients that support an immunopathological role for these granulocytes 34 these data are in line with findings by zhou et al 10 which also suggest higher proportion of neutrophils activated dendritic cells and activated mast cells via cell deconvolution of balf transcriptomes interestingly our data suggest increased proportion of monocytes and dendritic cells in the circulation but not in the balf using single-cell rna-seq some studies demonstrated that dendritic cells are indeed reduced in the balf 35 and there are significant phenotypical alterations of monocytes from covid-19 patients compared to healthy controls 36 we demonstrated that compared to sars-cov-1 or iav sars-cov-2 infection fails to induce significant type i interferon responses in pbmcs figure 2a or whole blood figure 2b which corroborates the low concentrations of type i interferon in the circulation of covid-19 patients 9 37 38 these findings contrast with induction of isg expression in both lung tissue 9 and balf 10 of covid-19 patients while recent studies indicate that type i and iii interferons negatively affect the lung epithelium during viral infections 39 40 the transcriptional response of peripheral leukocytes reflects the systemic adaptations to the inflammatory environment imposed by sars-cov-2 infection whereas type i interferon signaling in peripheral leukocytes might affect immunity in other organs such as the kidneys 41 importantly recent data suggest an improvement of patients with uncomplicated covid-19 treated with interferon-alpha2b 42 we expect that several factors will contribute to differences in transcriptional profiles of larger cohorts of covid-19 patients especially those bearing comorbidities associated with severe disease higher expression of angiotensin-converting enzyme 2 ace2 has been suggested as a potential mechanism of susceptibility of individuals with comorbidities associated with covid-19 43 however severe disease and death also occur after infection of otherwise healthy individuals indicating that a series of mechanisms account for the severity of covid-19 upregulated expression of genes that coordinate heme biosynthesis has been described in sepsis secondary to pneumonia and suggest a protective mechanism against oxidative stress 44 hypoxia also modulates the expression of genes coding for proteins that coordinate heme biosynthesis 45 we hypothesize that excessive heme accumulation could amplify pro-inflammatory cytokine production 46 47 or cause intravascular coagulation 48 and promote pathology during covid-19 strikingly we observed the modulation of several metabolic pathways in pbmcs and balf while oxidative phosphorylation was the only significant metabolic pathway overlapping in both compartments this suggests a critical role for mitochondrial activity during covid-19 many metabolites composing the pathways identified in the current study have been quantified via metabolomics of plasma or serum from covid-19 patients 49 50 mass spectrometry measurements revealed the modulation of pathways such as tca cycle and fructose and mannose metabolism 50 tryptophan metabolism glycolysis and gluconeogenesis and others 49 metabolomics analysis of human pbmcs infected with iav showed activation of tryptophan metabolism and glycolysis whereas glucose consumption via hexosamine biosynthesis underlies the cytokine storm promoted by iav infection 51 and could also affect covid-19 taken together this study demonstrates unappreciated inflammatory networks and metabolic pathways that are associated with covid-19 publicly available datasets were analyzed in this study this data can be found here genome sequence archive gsa or human gsa in national genomics data center beijing institute of genomics big chinese academy of sciences for rna-seq data related to sars-cov-2 infection cra002390 and hra000143 gene expression omnibus geo for rna-seq data related to sars-cov-2 infection gse147507 and microarray data related to sars-cov-1 infection gse1739 or influenza a virus iav infection gse34205 gse6269 gse29366 gse38900 gse20346 gse52428 gse40012 gse68310 gse61754 gse90732 and arrayexpress for nanostring ncounter data related to sars-cov-2 infection e-mtab-8871b lg selected the data performed data analysis interpreted the results and wrote the manuscript cs hs-c and sf interpreted the results and critically reviewed the manuscript all authors contributed to the article and approved the submitted version the authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest  is working from home the new norm an observational study based on a large geo-tagged covid-19 twitter dataset yunhe feng wenjun zhou  as the covid-19 pandemic swept over the world people discussed facts expressed opinions and shared sentiments on social media since the reaction to covid-19 in different locations may be tied to local cases government regulations healthcare resources and socioeconomic factors we curated a large geo-tagged twitter dataset and performed exploratory analysis by location specifically we collected 650563 unique geo-tagged tweets across the united states 50 states and washington dc covering the date range from january 25 to may 10 2020 tweet locations enabled us to conduct region-specific studies such as tweeting volumes and sentiment sometimes in response to local regulations and reported covid-19 cases during this period many people started working from home the gap between workdays and weekends in hourly tweet volumes inspired us to propose algorithms to estimate work engagement during the covid-19 crisis this paper also summarizes themes and topics of tweets in our dataset using both social media exclusive tools ie hashtags mentions and the latent dirichlet allocation model we welcome requests for data sharing and conversations for more insights dataset link httpcovid19researchsitegeo-taggedtwitterdatasets keywords work from home  stay-at-home order  lockdown  reopen  spatiotemporal analysis  twitter  covid-19  the covid-19 pandemic has had a widespread impact on peoples daily lives all over the globe according to local pandemic conditions countries worldwide adopted various containment policies to protect their residents and slow down the spread of covid-19 although countries like sweden and south korea did not lock down cities during the pandemic most of the other countries including china italy spain and india imposed long and stringent lockdowns to restrict gathering and social contact inside the same country different strategies and timelines were also set by regions and cities to flatten the curve and fight against the covid-19 crisis people expressed various opinions attitudes and emotions on the same covid-19 regulations due to local hospital resources economic statuses demographics and many other geographic factors therefore it is reasonable and necessary to consider the location information when investigating the public reactions to covid-19 however it is challenging to conduct such large-scale studies using traditional surveys and questionnaires first regulations and policies proposed and enforced in different regions are time-sensitive and changeable making it hard to determine when surveys to be conducted and which survey questions to be included for example california and tennessee implemented stay-at-home orders on different dates the initialized plannings and executive orders could also be tuned promptly such as extending lockdowns due to the fast-growing covid-19 confirmed cases traditional surveys are not flexible enough for such changes second it is time-consuming and expensive to recruit a large number arxiv200608581v1 cssi 15 jun 2020 of participants to take surveys because demographics especially geographical locations must be considered if a comparative spatial study is conducted it takes more time to recruit participants from multiple regions in this paper we built a large geo-tagged twitter dataset enabling fine-grained investigations of the public reactions to the covid-19 pandemic more than 170 million english covid-19 related tweets were harvested from jan 25 to may 10 2020 among which 650563 geo-tagged tweets posted within the united states were selected we took the us as an example to explore the public reactions in different regions because states in the us determined when how and what policies and regulations were imposed independently we first presented an overview of both daily and hourly tweet distributions then state-level and county-level geographic patterns of covid-19 tweets were illustrated we also proposed algorithms to evaluate work engagement by comparing tweeting behaviors on workdays and weekends in addition we extracted the involved popular topics using both social media exclusive tools ie hashtags and mentions and general topic models finally we analyzed public emotions using polarized words and facial emojis we summarized the contributions and findings of this paper as follows  a large geo-tagged covid-19 twitter dataset containing more than 650000 tweets collected from jan 25 to may 10 2020 in the united states was built and published at httpcovid19researchsitegeo-tagged twitterdatasets we listed tweet ids for all 50 states and washington dc respectively  we profiled geospatial distributions of covid-19 tweets at multiple location levels and reported the difference between states after normalizing tweet volumes based on covid-19 case and death numbers for example we found residents in oregon montana texas and california reacted more intensely to the confirmed cases and deaths than other states  we defined work engagement measurements based on the difference between workdays and weekends by hourly tweeting volumes  when studying work engagement patterns after lockdown and reopen we reporeted a few interesting findings for example the new york state showed lower work engagement than other states in the first week under stay-at-home orders the average hourly work engagement in the afternoon ie from 1300 to 1659 in the first week of reopening was much higher than the first week of staying at home  we also conducted a comprehensive social sentiment analysis via facial emojis to measure the general publics emotions on stay-at-home orders reopening the firsthundredththousandth confirmed cases and the firsthundredththousandth deaths we observed that negative moods dominated the public sentiment over these key covid-19 events which showed a similar pattern across states in this section we first provide an overview of tweet daily distributions demonstrating when covid-19 tweets became viral next the hourly distributions during different periods were illustrated we then proposed methods to measure work engagement by comparing the hourly tweeting frequencies on workdays and weekends we also studied the influence of covid-19 regulations such as stay-at-home orders and reopening on work engagement figure 1 shows the daily distribution of geo-tagged tweets within the top 10 states with the highest tweet volumes 1 we can see that daily tweet volumes generated by different states show similar trends in fact we tested statistical relationships regarding the daily volumes over time for all pairs of two arbitrary states and found strong linear correlations existed among 932 state pairs with a pearsons r  08 and p  0001 based on key dates we split the entire observation period into the following three phases  phase 1 from jan 25 to feb 24 31 days people mentioned little about covid-19 except for a small peak at the end of january  phase 2 from feb 25 to mar 14 19 days the number of covid-19 related tweets began to increase quickly on feb 25 us health officials warned the covid-19 community spread in america was coming 1  on march 13 the us declared the national emergency due to covid-19 2  -29  3-1  3-2  3-3  3-4  3-5  3-6  3-7  3-8  3-9  3-10  3-11  3-12  3-13  3-14  3-15  3-16  3-17  3-18  3-19  3-20  3-21  3-22  3-23  3-24  3-25  3-26  3-27  3-28  3-29  3-30  3- figure 1  the daily number of tweets from the top 10 states generating most tweets  phase 3 from mar 15 to may 10 57 days people began to adjust to the new normal caused by covid-19 such as working from home and city lockdowns for each tweet we converted the utc time zone to its local time zone 2 according to the state where it was posted the aggregated hourly tweeting distributions in different phases are shown in figure 2  the tweeting behaviors on workdays and weekends were studied separately because we wanted to figure out how the working status impacted on tweeting patterns we colored the tweeting frequency gaps during business hours 800-1659 as green if people tweeted more frequently on weekends than workdays otherwise the hourly gap is colored as red 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23 local hour 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23 local hour 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23 local hour 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23 local hour in phase 1 there existed a tweeting gap from 800 to 1659 between workdays and weekends the tweeting peak occurred at 1200-1259 on weekends but at 1700-1759 on workdays we think it may be explained by the fact that people engage at work during regular working hours and have little time to post tweets on workdays but they become free to express concerns on covid-19 on twitter after work the hourly distribution patterns changed in phase 2 when confirmed covid-19 cases increased quickly in the united states people posted covid-19 tweets more frequently during business hours than at the same time slots on weekends indicating covid-19 had drawn great attention of workers when they were working it is interesting to note that a green tweeting gap from 800 to 1659 reappeared in phase 3 when most people had worked from home these findings motivated us to take advantage of the tweeting frequencies on workdays and weekends to estimate work engagement in the covid-19 crisis see section 4 twitter users can tag tweets with general locations eg city neighborhood or exact gps locations in this section we utilized the two types of tweet locations to explore geographic patterns of covid-19 tweets at state and county levels we extracted the state information from both general and exact tweet locations and calculated tweet volume percentages for each state as shown in figure 3 a the most populated states ie california texas new york and florida contributed the most tweets in contrast less populated states such as wyoming montana north dakota and south dakota created the least tweets we measured the relationship between tweet volumes and populations for all states and found a strong linear correlation existed pearsons r  0977 and p  0001 then we normalized tweet volumes using state residential populations figure 3 b illustrates washington dc posted the highest volume of tweets by every 1000 residents followed by nevada new york california and maryland the rest states demonstrate similar patterns we think the top ranking of washington dc might be caused by its functionality serving as a political center where covid-19 news and policies were spawned unlike state populations we did not find strong correlations between tweet counts and cumulative confirmed covid-19 cases pearsons r  0544 and p  0001 or deaths pearsons r  0450 and p  0001 we further normalized tweet volumes based on covid-19 cumulative number of cases and deaths in each state figure 3 c and figure 3 d shows the average number of tweets generated by each covid-19 case and each death respectively note that hawaii and alaska not plotted in figure 3 c and figure 3 d ranked as the first and second in both scenarios residents in states like oregon montana texas and california reacted sensitively to both confirmed cases and deaths as these states dominated in figure 3 c and figure 3 d  we utilized gps locations to profile the geographic distribution of covid-19 tweets at the county level because general tweet locations might not contain county information in our collected geo-tagged tweets 395 of them contained gps locations we resorted to nominatim 3  a search engine for openstreetmap data to identify the counties where each tweet gps coordinate lay figure 4 a and figure 4 b visualize the raw gps coordinate and corresponding county distributions large cities in each state demonstrated a higher tweeting density than small ones in fact we found a strong correlations between gps-tagged tweet counts and county populations pearsons r  0871 and p  0001 but such correlations did not hold true for cumulative confirmed covid-19 cases pearsons r  0590 and p  0001 or deaths pearsons r  0497 and p  0001  in this section we first propose methods to measure hourly and daily work engagement then we investigate how stay-at-home orders and reopening influenced hourly and daily work engagement respectively note that we use the term of lockdown referring stay-at-home orders in this section the lockdown dates and reopening dates for each state are retrieved from wikipedia 4 and new york times 5 respectively we assumed that 1 people would tweet less frequently during working hours if they engaged more on their working tasks 2 if people spent no time on work tasks during business hours on workdays their tweeting behaviors kept the same as that on weekends especially when people were confined at home we think the two assumptions are intuitive and reasonable as both phase 1 and phase 3 showed the meaningful working-hour tweeting gaps in figure 2 b and figure 2 d we took the tweeting gap size as an indicator of work engagement more specifically let h j i denote the tweet volume at the i-th hour on the j-th day in a week for example h 2 8 meant the number of tweets posted from 800 to 859 on tuesdays we took monday as the first day in one week accordingly the total number of tweets on the j-th day in a week was represented by the total tweet volumes on workdays and weekends can be expressed as note that we estimated both hourly and daily work engagement by considering at least seven days because the data sparsity would lead to unreliable results if fewer days were involved the work engagement at i-th hour hi could be defined as the ratio of the normalized tweeting frequency at i-th hour on weekends over that on workdays and minus one as expressed in equation 1  where i  8 10 11 12 13 14 15 16 a larger positive hi indicates higher work engagement when hi equals 0 it means there exists no difference on work engagement at i-th hour between workdays and weekends a positive value of hi implies people are more engaged at work on workdays than weekends although it is rare a negative hi means people fail to focus more on their work on workdays than weekends we also defined the daily from monday to friday only work engagement by aggregating tweeting frequencies in regular working hours from 800 to 1659 the daily work engagement on j-th day was expressed as where j  1 2 3 4 5 and t j was the total tweet count on the j-th day similar to hourly engagement a larger positive dj means higher work engagement we chose the ten states that generated the most massive tweet volumes from 800 to 1659 on each day of the first week after stay-at-home orders were enforced to study the hourly and daily work engagement table 1 illustrates the hourly work engagement of the ten states except for california and new york all other eight states had positive average work engagement scores see the second last column in table 1  implying people worked more extensively on workdays than weekends georgia and maryland demonstrated relatively higher average work engagements  015 across all the ten states people focused more on work tasks at 1000 and 1300 than other hour slots and reached the lowest engagement score at 1100 see the second last row in table 1  table 2 shows the daily work engagement in the first week after stay-at-home orders were announced the average daily patterns for each state is very similar to the hourly ones for example both the daily and hourly average engagements in california and new york were negative based on average daily work engagement of the ten states we found people put themselves more in their work on thursday and friday than monday tuesday and wednesday see the second last row in table 2  besides the first stay-at-home week we reported the average hourly and daily work engagement for states in a more extended period ranging from five weeks ahead of and three weeks after stay-at-home orders were issued as shown in figure 5  hourly and daily work engagement patterns of the same state are very similar along the nine weeks states performed very differently one month before local lockdowns see x-axis-4 and x-axis-5 in figure 5 a and figure 5 b surprisingly most states achieved higher work engagement in the first two weeks of lockdowns see x-axislockdown and x-axis1 in figure 5 a and figure 5 b than before lockdowns  some states had started to reopen partially since the end of april we selected the states that were partially reopened before may 3 3 to investigate their hourly and daily work engagement in the first week of reopening as table 3 and table 4 show averaged hourly and daily work engagement of the nine states except alaska are positive people demonstrated much higher work engagement in the afternoon than in the morning see the last second row in table 3  figure 6 demonstrates the afternoon work engagement of reopening is much larger than its counterpart in the first week of lockdowns also the average work engagement of reopening on tuesday and friday improves a lot when comparing with lockdowns  in this section we summarized and revealed the themes people discussed on twitter social network exclusive tools ie hashtags and mentions and general text-based topic models were used to infer underlying tweet topics during the covid-19 pandemic  hashtags are widely used on social networks to categorize topics and increase engagement according to twitter hashtagged words that become very popular are often trending topics we found hashtags were extensively used in geo-tagged covid-19 tweets -each tweet contained 068 hashtags on average our dataset covered more than 86000 unique hashtags and 952 of them appeared less than 10 times covid-19 and its variations eg covid19 and coronavid19 were the most popular ones accounting for over 25 of all hashtags to make the visualization of hashtags more readable we did not plot covid-19 and its variations in figure 7  in other words figure 7 displays top hashtags starting from the second most popular one all hashtags were grouped into five categories namely covid-19 healthcare place politics and others people use mentions to get someones attention on social networks we found most of the frequent mentions were about politicians and news media as illustrated in figure 8  the mention of realdonaldtrump accounted for 45 of all mentions and was the most popular one to make figure 8 more readable the mention of realdonaldtrump was not plotted other national eg vp and jeobiden and regional eg nygovcuomo and gavinnewsom politicians were mentioned many times as news channels played a crucial role in broadcasting the updated covid-19 news and policies to the public it is not surprising to observe news media such as cnn foxnews nytimes and youtube are prevalent in figure 8  in addition the world health organization who the beer brand corona and elon musk elonmusk were among the top 40 mentions to further explore what people tweeted we adopted latent dirichlet allocation lda 6 to infer coherent topics from plain-text tweets we created a tweet corpus by treating each unique tweet as one document commonly used text preprocessing techniques such as tokenization lemmatization and removing stop words were then applied on each document to improve modeling performance next we performed the term frequency-inverse document frequency potus cnn vp nygovcuomo cdcgov speakerpelosi gop whitehouse joebiden who foxnews nytimes youtube msnbc senatemajldr gavinnewsom seanhannity govrondesantis senschumer maddow abc ingrahamangle cbsnews nycmayor nbcnews washingtonpost donaldjtrumpjr corona chriscuomo gopleader presssec berniesanders barackobama aoc tuckercarlson senategop govwhitmer chrislhayes lindseygrahamsc elonmusk tf-idf on the whole tweet corpus to assign higher weights to most import words finally the lda model was applied on the tf-idf corpus to extract latent topics we determined the optimal number of topics in lda using c v metric which was reported as the best coherence measure by combining normalized pointwise mutual information npmi and the cosine similarity 7  for each topic number we trained 500-pass lda models for ten times we found the average c v scores demonstrated an increasing trend as the topic number became larger but the increasing speed became relatively slow if more than ten topics were considered therefore we chose ten as the most suitable topic number in our study the ten topics and words in each topic are illustrated in table 5  we can see that topic 1 is mostly related to statistics of covid-19 such as deaths cases tests and rates in the topic of treatment healthcare related words eg mask patient hospital nurse medical and ppe are clustered together topic 3 is about politics as top keywords include trump president vote and democratic the emotion topic mainly consists of informal language expressing emotions topic 5 is related to impact on work businesses and schools we believe americans who are bilingual in spanish and english contributed to the topic of spanish topic 7 is calling for unity in the community in the topic of places many states eg florida and california and cities eg new york and san francisco are mentioned the last two topics are about praying and home activities when people followed stay-at-home orders these topics are very informative and well summarize the overall conversations on social media  in this section we conducted a comprehensive sentiment analysis from three aspects first the overall public emotions were investigated using polarized words and facial emojis then we studied how sentiment changed over time at the national and state levels during the covid-19 pandemic finally event-specific emotions were reported emotionally polarized words or sentences express either positive or negative emotions we leveraged textblob 8 to estimate the sentimental polarity of words and tweets for each word textblob offers a subjectivity score within the range 00 10 where 00 is most objective and 10 is most subjective and a polarity score within the range -10 10 where -01 is the most negative and 10 is the most positive we used the subjectivity threshold to filter out objective tweets and used the polarity threshold to determine the sentiment for example a subjectivity threshold of 05 would only select the tweets with a subjectivity score greater than 05 as the candidates for polarity checking a polarity threshold of 07 treated tweets with a polarity score greater than 07 as positive and those with a polarity less than -07 as negative figure 9 illustrates the ratio of the number of positive tweets over negative ones with different combinations of subjectivity and polarity thresholds positive and negative emotions evenly matched with each other when the ratio equaled one we can see that emotion patterns changes along with threshold settings specifically positive emotions dominated on twitter with small polarity and subjectivity thresholds however negative emotions became to overshadow the positive ones under large polarity and subjectivity thresholds figure 10 shows three examples of polarized word clouds where the ratio was greater than 1 subjectivity02 polarity07 equal to 1 subjectivity08 polarity02 and less than 1 subjectivity08 polarity07  besides polarized-text based sentiment analysis we took advantage of facial emojis to further study the public emotions facial emojis are suitable to measure tweet sentiments because they are ubiquitous on social media conveying diverse positive neutral and negative feelings we grouped the sub-categories of facial emojis suggested by the unicode consortium into positive neutral and negative categories specifically all face-smiling face-affection face-tongue face-hat emojis and were regarded as positive all face-neutral-skeptical face-glasses emojis and were grouped as neutral and all face-sleepy face-unwell face-concerned face-negative emojis were treated as negative a full list of our emoji emotion categories are available at httpcovid19researchsiteemoji-category we detected 4739 352 positive emojis 2438 181 neutral emojis and 6271 466 negative emojis in our dataset negative emojis accounted for almost half of all emoji usages table 6 illustrates top emojis by sentiment categories with their usage frequencies the most frequent emojis in the three categories were very representative as expected still was the most popular emojis in all categories which kept consistent with many other recent research findings 9 10  the thinking face emoji was the most widely used neutral facial emoji indicting people were puzzled on covid-19 surprisingly the face with medical mask emoji ranked higher than any other negative emojis the skull emoji appeared more frequently than any other positive and neutral emojis except  we think the sneezing face and the hot face emoji are very likely to be relative to suspected symptoms of covid-19  we used facial emojis to track the different types of public sentiment during the covid-19 pandemic figure 11 a shows the daily overall emotions aggregated by all states in phase 1 from jan 25 to feb 24 the publish emotions changed in large ranges due to the data sparsity in phase 2 from feb 25 to mar 14 positive and negative emotions overshadowed each other dynamically but demonstrated stable trends in phase 3 from mar 15 to may 10 negative sentiment dominated both positive and neutral emotions expressing the publics concerns on covid-19 we also investigated the daily positive neutral and negative sentiment of different states as presented in figure 11 b figure 11 c and figure 11  we studied the event-specific sentiment by aggregating tweets posted from different states when the same critical covid-19 events occurred we focused on the following eight events  the first the 100 th  and the 1000 th confirmed covid-19 cases  the first the 100 th  and the 1000 th confirmed covid-19 deaths  lockdown figure 12  we carried out the one-way multivariate analysis of variance manova and found the p-value was nearly 10 indicating there was no significant difference among these eight event-specific sentiments when the first case 100 th cases and first death were confirmed sentiment standard deviations were much larger than the rest events suggesting people in different states expressed varying and -29  3-1  3-2  3-3  3-4  3-5  3-6  3-7  3-8  3-9  3-10  3-11  3-12  3-13  3-14  3-15  3-16  3-17  3-18  3-19  3-20  3-21  3-22  3-23  3-24  3-25  3-26  3-27  3-28  3-29  3-30  3-  this paper presents a large public geo-tagged covid-19 twitter dataset containing 650563 unique geo-tagged covid-19 tweets posted in the united states from jan 25 to may 10 a small number of tweets were missing during the data collection period due to corrupted files and intermittent internet connectivity issues we compensated for the data gaps using the covid-19 dataset collected by chen et al 11  as different covid-19 keywords were used in 11 and our study to filter tweet streaming it did not compensate for the missing data perfectly however given the small proportion of missing data we do not expect the conclusions to change for more details about our dataset please refer to appendix a based on the geo-tagged dataset we investigated fine-grained public reactions during the covid-19 pandemic first we studied the daily tweeting patterns in different states and found most state pairs had a strong linear correlation the local time zones inferred from tweet locations make it possible to compare the hourly tweeting behaviors on workdays and weekends their different hourly patterns during 800 to 1700 inspired us to propose approaches to measure work engagement second we utilized tweet locations to explore geographic distributions of covid-19 tweets at state and county levels third we summarized and revealed the themes people discussed on twitter using both social network exclusive tools ie hashtags and mentions and general text-based topic models finally we reported comprehensive sentiment analytics including the overall public emotions how public feelings changed over time and the expressed emotions when specific events occurred hopefully this geo-tagged twitter dataset can facilitate more fine-grained covid-19 studies in the future in this section we first described how we collected twitter data and compensated for data gaps then we removed twitter bots to enhance data analytics at last we extracted the us geo-tagged covid-19 tweets from general tweets we utilized twitters streaming apis to crawl real-time tweets containing a set of coronavirus wuhan corona ncov keywords related to the novel coronavirus outbreak since january 25 2020 4 after the world health organization who announced the official name of covid-19 on february 11 2020 we added covid19 covid19 coronapocalypse coronavid19 covid19 covid-19 and covid into our keyword set we collected more than 170 million tweets generated by 27 million unique users from january 25 to may 10 2020 each tweet was formatted in a json file with named attributes and associated values we lost 385 tweets uniformly distributed among may 18 and apr 4 due to corrupted files and missed 88 hours of data because of intermittent internet connectivity issues in the entire data collection period more details about data gaps are available at http covid19researchsitegeo-taggedtwitterdatasetsknowndatagapscsv to compensate for these data gaps we sought for the covid-19 dataset maintained by chen et al 11 and downloaded 16459659 tweets figure 13  the daily number of tweets from the top 10 states generating most tweets one of the challenges when dealing with messy text like tweets is to remove noisy data generated by twitter bots inspired by the bot detection approach proposed in 12  we conceived the two types of twitter users as bots 1 those who posted more than 5000 covid-19 tweets more than 46 tweets on average per day during our data collection period 2 those who posted over 1000 covid-19 tweets in total and the top three frequent posting intervals covered at least their 90 tweets for the two types of bots we removed 317101 tweets created by 32 bots and 120932 tweets by 36 bots respectively a3 geo-tagged data in the us twitter allows users to optionally tag tweets with different precise geographic information indicating the real-time location of users when tweeting typical tweet locations can be either a box polygon of coordinates specifying general areas like cities and neighborhoods or an exact gps latitude and longitude coordinate we detected and examined the place attribute in collected tweet json files if the embedded countrycode was us and the extracted state was among the 50 states and washington dc in the united states we added the tweet into our geo-tagged dataset after removing retweets 650563 geo-tagged unique tweets from 246032 users in the united states were collected among them 38818 tweets 596 of our dataset were retrieved from the dataset proposed by chen et al 11  the monthly number of geo-tagged tweets in each state is shown in figure 14   we further analyzed the users in our dataset to demonstrate they crowdsourced the public figure 15 shows the user proportion versus the number of posted tweets we found only 0055 users tweeted more than one geo-tagged tweet per day on average generating 11844 tweets 182 of all tweets in our dataset to be specific 9671 users had no more than ten records in our dataset   open access institutional and news media tweet dataset for covid-19 social science research jingyuan yu  as covid-19 quickly became one of the most concerned global crisis the demand for data in academic research is also increasing currently there are several open access twitter datasets but none of them is dedicated to the institutional and news media twitter data collection to fill this blank we retrieved data from 69 institutionalnews media twitter accounts 17 of them were related to government and international organizations 52 of them were news media across north america europe and asia we believe our open access data can provide researchers more availability to conduct social science research  covid-19 was announced as pandemic by who on 11 mar 1  being the only pandemic in the past 10 years the last one was 2009 swing flu it was first detected as an unknown pneumonia in wuhan hubei china on april 1st 2020 john hopkins university coronavirus resource center 2  reported that the ongoing epidemic has infected 938373 people the death toll has already reached 47272 given the highly contagious nature of the virus as well as the significant mortality rate numerous governments have announced national lockdown the impact of covid-19 on world economy and politics is unprecedented in modern time on the past ebola epidemic crisis scholars found the importance of using twitter data to do social science research 3  4  many of them use this microblog data as social indicators to analyze the effect of epidemic outbreak on public concerns 5  health information needs and health seeking behavior 6  and public response to policy makers 7 etc current open access covid-19 twitter data were mainly collected by keywords such as coronavirus covid-19 etc 8  9  none of the them is dedicated to governmentnews media tweet collection given that our retrieval targets are policy makers and news source we believe our dataset can provide scholars more valuable data to conduct social science research in related fields such as crisis communication public relation etc we used twitter rest api to retrieve twitter data from march 12 2020 there are 8 collection categories gov tweet governments international organizations etc us news tweet uk news tweet spain news tweet germany news tweet france news tweet china news tweet and additional news tweet news source added later each of them contains various collection target twitter account name details see next section at the first time we collected the most recent 3200 tweets of every collection target before march 12 2020 we didnt set a time limit which implies that the date of the first tweet from each of the sources may vary and even be relatively old we update our dataset every week last update on april 2 2020 after removing duplicated data by matching tweet id we get the clean version data on the other hand due to the data collection strategy we made there may be tweets both related and unrelated to covid-19 which opens up new possibilities for academic analysis gov tweet category contains the following accounts table1  the dataset is available on github at the following address httpsgithubcomnarcisoyuinstitional-and-news-media-tweet-dataset-for-covid-19social-science-research our data was collected in compliance with twitters official developer agreement and policy 10  the dataset will be updated weekly interested researchers will need to agree upon the terms of usage dictated by the chosen license following twitter official policies we released and stored only tweet ids as far as we know two tools can be used to hydrate full information hydrator httpsgithubcomdocnowhydrator and twarc httpsgithubcomdocnowtwarc interested researchers shall follow the usage instructions of the fore-mentioned tools  modelling the covid-19 epidemics in brasil parametric identification and public health measures influence r cotta m c naveira-cotta p p magal  a siru-type epidemic model is proposed for the prediction of covid-19 spreading within brasil and analyse the influence of public health measures on simulating the control of this infectious disease since the reported cases are typically only a fraction of the total number of the symptomatic infectious individuals the model accounts for both reported and unreported cases also the model allows for the time variation of both the transmission rate and the fraction of asymptomatic infectious that become reported symptomatic individuals so as to reflect public health interventions towards its control along the course of the epidemic evolution an analytical exponential behaviour for the accumulated reported cases evolution is assumed at the onset of the epidemy for explicitly estimating initial conditions while a bayesian inference approach is adopted for parametric estimations employing the present direct problem model with the data from the known portion of the epidemics evolution represented by the time series for the reported cases of infected individuals the direct-inverse problem analysis is then employed with the actual data from china with the first half been employed for the parametric estimation and the second half for validation of the predictive capability of the proposed approach the full dataset for china is then employed in another parameter identification aimed at refining the values for the average times that asymptomatic infectious individuals and that symptomatic individuals remain infectious following this validation the available data on reported cases in brasil from february 15 th till march 29 th  2020 is used for estimating parameters and then predict the epidemy evolution under these conditions finally public health interventions are simulated aimed at diminishing the effects of the disease spreading by acting on both the transmission rate and the fraction of the total number of the symptomatic infectious individuals considering time variable exponential behaviours for these two parameters usually assumed constant in epidemic evolutions without intervention it is demonstrated that a combination of actions to affect both parameters can have a much faster and effective result in the control of the epidemy dynamics  medrxiv preprint the analysis of the impact of public health measures 1 which however did not consider in the modelling the presence of unreported infection cases which are in practice inherent to this process the present work is first based on the sir-type model proposed in 2  which deals with the epidemic outbreak in wuhan by introducing the unreported cases in the modelling and evaluating the consequences of public health interventions it was a direct application of previous developments 3 4 on the fundamental problem of parameter identification in mathematical epidemic models accounting for unreported cases this same modelling approach was more recently employed in the analysis of the epidemic outbreak in different countries  a new human coronavirus started spreading in wuhan china by the end of 2019 and turned into a pandemic disease called covid-19 as declared by the world health organization on march 11 th  2020 the affected countries and cities around the world have been reacting in different ways towards locally controlling the disease evolution these measures include general isolation through quarantine and massive testing for focused isolation with varying degrees of success so far as can be analysed from the limited data available naturally china offers the longest time series on reported infected cases and the resulting effects of combining different public health interventions as of march 26 th  2020 there were no reports in china of further internal contaminations and all the new cases are associated with infected individuals that reentered in the country despite the apparent success of the interventions in china each region or country might require a specific combination of measures due to demographic spatial distribution and age structure health system capabilities and social-economical characteristics in this sense it urges to have a mathematical model that would allow for the simulation of such possible interventions on the epidemic evolution within the following few weeks or months this article presents a collaborative research effort towards the construction of an epidemic evolution prediction tool which combines direct and inverse problem analysis and is both reliable and easy to implement and execute initially motivated by offering some insight into the control of covid-19 within brasil the classical susceptible-infectious-recovered sir model describes the transmission of diseases between susceptible and infective individuals and provides the basic framework for almost all later epidemic models at the onset of the coronavirus epidemy in china there were some initial studies for the prediction of its evolution and and france 5 6 7  besides identifying unreported cases this simple and robust model also allows for introducing a latency period and a time variable transmission rate which can simulate a public health orientation change such as in a general isolation measure in addition an analytical exponential behaviour is assumed for the accumulated reported cases evolution along a second phase just following the onset of the epidemy which upon fitting of the available data allows for the explicit analytical estimation of the transmission rate and the associated initial conditions required by the model here the sir-type model in 2 3 4 5 6 7 is implemented for the direct problem formulation of the covid-19 epidemic evolution adding a time variable parametrization for the fraction of asymptomatic infectious that become reported symptomatic individuals a very important parameter in the public health measure associated with massive testing and consequent focused isolation the same analytical identification procedure is maintained for the required initial conditions as obtained from the early stages exponential behaviour however a bayesian inference approach is here adopted for parametric estimation employing the markov chain monte carlo method with the metropolis-hastings sampling algorithm 8 9 10 11 12  at first the goal of the inverse problem analysis was estimating the parameters associated with the transmission rate and the fraction of asymptomatic infectious that become reported symptomatic individuals which can be quite different in the various regions and countries and also very according to the public health measures then in light of the success in this parametric identification an extended estimation was also employed which incorporates the average time the asymptomatic infectious are asymptomatic and the average time the infectious stay in the symptomatic condition due to the relative uncertainty on these parameters in the literature the proposed approach was then applied to the data from china first by taking just the first half of these data points in the estimation while using the second half to validate the model using the estimated parameters with just the first half of the epidemy evolution and second by employing the whole time series in the mcmc estimation procedure thus identifying parameters for the whole evolution period this second estimation was particularly aimed at refining the data for the average times that asymptomatic infectious individuals and that symptomatic individuals remain infectious upon validation of the approach through the data for china we have proceeded to the analysis of the epidemic dynamics in brasil after about 35 days of collected information on reported infected individuals first the available data was employed in the parametric estimation followed by the prediction of the epidemy evolution in brasil then we have explored the time variation of both the transmission rate and the fraction of asymptomatic infectious that become reported symptomatic individuals so as to reflect public health interventions in simulating possible government measures as described in what follows the implemented sir-type model 2 3 4 5 6 7 is given by the following initial value problem where with initial conditions  2t   the transmission rate t is also allowed to be a time variable function along the evolution process figure 1 below illustrates the infection process as a flow chart the time variable coefficients t and ft are given by these parametrized functions are particularly useful in interpreting the effects of public health interventions for instance the transmission rate t is particularly affected by a reduced circulation achieved through a general isolation or quarantine measure while the fraction ft of asymptomatic infectious that become reported thus isolated cases can be drastically increased by a massive testing measure with focused isolation in the above relations is the attenuation factor for the transmission rate n is the time in days for application of the public health intervention to change transmission rate is the argument of the ft variation between the limits  0   the first time variable function has been previously considered while the second one has been introduced in the present work so as to allow the examination of combined measures the cumulative number of reported cases at time t   which is the quantity offered by the actual available data and the a priori unknown cumulative number of unreported cases   are given by the daily number of reported cases from the model   can be obtained by computing the solution of the following equation with initial conditions inverse problem analysis is nowadays a common practice in various science and engineering contexts in which the groups involved with experimental data and numerical simulation synergistically collaborate so as to obtain the maximum information from the available data towards the best possible use of the modelling for the problem under study here as mentioned in the introduction we first review an analytical parametric identification described in more details in 4 5 6 7  that from the initial phases of the epidemic evolution allows to explicitly obtain the unknown initial conditions of the model while offering a reliable estimate for the transmission rate at the onset of the epidemy nevertheless even after these estimates a few other parameters in the model remain uncertain either due to the specific characteristics of the physical conditions or reaction to the epidemy in each specific region or due to lack of epidemiological information on the disease itself therefore an inverse problem analysis was undertaken aimed at estimating the main parameters involved in the model as summarized in table   1 below first for the dataset on the accumulated reported cases for china the focus is on the parametrized time variation of the transmission rate  0 and  and the fraction of asymptomatic infectious that become reported  0  in this case assumed constant followed by an effort to refine the information on the average times 1 and 1 through all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint a simultaneous estimation of the five parameters then employing the dataset for brasil the parametrized time variation of the transmission rate  0 and  and the fraction of asymptomatic infectious that become reported  0  initially assumed constant are estimated in addition due to the behaviour of the estimated crt curve in this case it is also attempted to estimate a possible time variation for the fraction of asymptomatic infectious that become reported   by parametrization of an abrupt variation that requires just the estimation of and  the statistical inversion approach here implemented falls within the bayesian statistical framework 8 9 10 11 12  in which probability distribution models for the measurements and the unknowns are constructed separately and explicitly as shall be briefly reviewed in what follows as explained in previous works employing this model 4 5 6 7  it is assumed that in the early phase of the epidemic the cumulative number of reported cases grows approximately exponentially according to the following functional form after fitting this function to the early stages of the epidemic evolution one may extract the information on the unknown initial conditions in the form 4 5 6 7  all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint in addition an excellent estimate for the initial transmission rate can be obtained from the same fitted function in the form also the the basic reproductive number for this initial phase model is estimated as the statistical approach for the solution of inverse problems here adopted employs  where m is the number of parameters for estimating p we assume that a vector of measured data is available y containing the measurements yi at time ti i  1  i bayes theorem can then be stated as 8 9  where  posterior p is the posterior probability density that is the conditional probability of the parameters p given the measurements y  prior p is the prior density that is the coded information about the parameters prior to the measurements  yp is the likelihood function which expresses the likelihood of different measurement outcomes involved allowing one to do bayesian inference even in rich and complex models the idea behind the metropolis-hasting sampling algorithm is illustrated below and these steps should be repeat until it is judged that a sufficiently representative sample has been generated start the chain with an initial value that usually comes from any prior information that you may have 2 randomly generate a proposed jump aiming that the chain will move around and efficiently explores the region of the parameter space the proposal distribution can take on many different forms in this work a gaussian random walk was employed implying that the proposed jumps will usually be near the current one 3 compute the probability of moving from the current value to the proposed one candidates moving to regions of higher probability will be definitely accepted candidates in regions of lower probability can be accepted only probabilistically if the proposed jump is rejected the current value is tally again for more details on theoretical aspects of the metropolis-hastings algorithm and mcmc methods and its application the reader should refer to 8 9 10 11 12  before proceeding to the analysis of the covid-19 epidemic evolution within brasil the major concern in the present contribution the need was felt in validating the proposed direct-inverse problem analysis approach in this sense due to the largest available dataset all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  on this pandemic we have chosen to use the information from china in terms of the accumulated confirmed infectious cases the data for china was extracted from 6  complemented by the most recent data from 13 up to march 25 th  2020 the exponential fit for the early phase of the china crt dataset provided the estimates of the three parameters 1  014936 2  037680 3  10 from which we have estimated 0  5046 the remaining data for the initial conditions 0 and 0  and the early stage transmission rate 0  are in fact recalculated from within the mcmc algorithm since the changing values of f will affect them according to eqs 7c-e the average times in the model were taken as 17 and 17 days and the isolation measures were taken at n29 days 6  first experimental data from china from the period of january 19 th up to february 17 th was employed in demonstrating the estimation of three parameters 0   and 0  assuming there is no significant time variation in the function ft   0 in the absence of more informative priors uniform distributions were employed for all three parameters under estimation table 1 presents the prior information and the initial guesses for the parameters if the initial guesses were used to predict the crt behavior an over-estimation of the accumulated reported infected individuals would occur especially in the long term as can be noticed in figure 1  confirming the need for a proper parameter estimation table 1 -prior distributions and initial guesses for the parameters to be estimated 0   and 0 china the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is table 2  it should be recalled that non-informative priors were adopted for the three parameters as presented in table 1  and except for the transmission rate when eq7e provides an excellent initial guess the remaining guesses were completely the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint arbitrary such as in the analysis for a less complete dataset as will be discussed in the next section although the present estimated parameters have led to a good prediction of the second half of the china epidemic evolution data there are still uncertainties associated with the average times here assumed both equal to 7 days according to 6  this choice was based on early observations of the infected asymptomatic and symptomatic patients in wuhan but more recent studies have been refining the information on the epidemic evolution and all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint the disease itself such as in 14 15 16 17  for this reason we have also implemented a statistical inverse analysis with the full dataset of china but now seeking the estimation of five parameters so as to simultaneously estimate the average times 1 and 1 both uniform and gaussian distributions were adopted for the two new parameters with initial guesses of 17 days and 17 days and n29 days as employed in 6  table 3 provides the estimated values and 95 confidence intervals for all five parameters with gaussian priors for the two average times with data obtained from 14 17  the most affected parameter in comparison with the previous estimates is the average time 1 which is also the one with widest confidence interval this behaviour is also evident from the markov chains for this parameter now simultaneously estimated figure 5 compares the theoretical predictions with the model incorporating the five estimated parameters as in table 3  against the full crt dataset for china confirming the improved agreement  the crt data for the accumulated reported infectious in brasil from february 25 th  when the first infected individual was reported up to march 29 th  is presented in the appendix first the exponential phase of the evolution was fitted taking the data from day 10 to 25 yielding the estimates of the three parameters 1 for instance in the analysis of the italy epidemic evolution reported in 6  with data from january 31 st to march 8 th  a comparable low attenuation factor of  0032 was identified it is also possible to observe the lower value of the parameter 0  in comparison to the value obtained for the china dataset which represents that only around 30 of the infected symptomatic individuals become in fact reported cases this result could reflect an initial protocol of not thoroughly testing the mildly symptomatic individuals or just a lack of enough testing kits this fact shall be discussed again further ahead when the impact of public health measures is analysed figure 6 illustrates the good agreement of brasils full dataset period from february 25th till march 29th with the mathematical model predictions after adopting the estimated values for the parameters in table 4  the theoretical crt curve is plotted together with the 95 confidence interval bounds for this simulated evolution it should be recalled that non-informative priors were adopted for the three parameters as in the china example and except for the transmission rate all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is next this parameter estimation is employed in the prediction of the covid-19 evolution in brasil five scenarios were here explored i the present public health interventions remain unchanged ii a stricter isolation is implemented from now on further reducing the transmission rate iii an attenuation on the social isolation policy leading to an increased transmission rate iv an increment on the fraction of reported cases through a massive blood testing campaign for instance forcing more unreported cases to become all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint reported ones thus isolating them earlier v a combination of public health measures acting on both reducing the transmission rate and on increasing the conversion factor of unreported to reported cases in the first scenario it is assumed that no further public health interventions are implemented other than those already reflected by the data which should be fully maintained throughout the control period and the epidemics should evolve from the present stage under the parameters above identified the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint finally in the fifth scenario the combination of public health measures acting on both the transmission rate and on the conversion factor or unreported to reported cases is analyzed for brasil therefore let us consider after day n240 50 improvement with respect to the value of here identified thus around 2 00831 and simultaneously increase the fraction of reported and unreported infectious cases to become  07185 also starting after day n240 with 05 the changes in the accumulated reported and unreported cases as shown in figure 11  are the most encouraging in the present analysis the predicted number of unreported infectious cases is now reaching after 150 days around 36770 individuals while the reported cases should reach 50006 individuals with a marked decrease to a total of around 86777 infectious cases about 30 reduction with respect to the base case the predicted evolution of the daily reported infectious cases would then show a peak at around t46 days of about 2196 reported cases again though this peak value is higher than for the base case before the public health improvements a number of these are of mild symptomatic cases that were moved from the unreported to the reported cases evolution thus moved to monitored isolation earlier and not necessarily requiring hospitalization the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint though the three parameters estimation provides a fairly good reproduction of the behaviour of the crt curve for brasil one may observe a change in the pattern of the evolution around day 30 that could not be entirely followed by the proposed model it is also a known fact that the initial amount of kits for blood testing that were purchased by the brazilian government were finished around this time and before being fully supplemented there could have been a reduction on the number of executed exams of the symptomatic individuals that might have affected the partition of reported to unreported cases by the end of this period covered by the present dataset therefore the more general model including the time variation of the partition ft eqs4cd is here implemented for a more refined inverse problem analysis it is then expected that a reduction on the f value can be identified   0  with an abrupt variation on the exponential behaviour here assumed as a sharp functional time dependence large  therefore an additional statistical inverse problem analysis is undertaken this time for estimating five parameters namely 0   0   and  aimed at improving the overall agreement with the crt data behaviour with a possible reduction on the partition of the reported and unreported the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint infectious cases with uniform distributions for all five parameters taking the previous estimates for the three first parameters an arbitrary guesses for  and  the five estimated parameters are shown in table 5  together with the 95 confidence interval for each parameter figure 12 shows the theoretical crt curve obtained with the five parameters estimation plotted together with the 95 confidence interval bounds for this simulated evolution one can see the marked reduction on the ft parameter from the estimates in table 5 which results in the increase of the unreported to reported infectious cases as is shown in figure 13 a for crt and cut predictions up to 150 days clearly the reduction on the testing and thus on the isolation of reported infectious individuals leads to an impressive increase on the total number of infected individuals after 150 days 723698 cases including unreported 609125 and reported cases 114572 figure 13 b presents the predicted evolution of the daily reported infectious cases which shows a peak at around t61 days of about 2672 reported cases hopefully this difficulty with the availability of enough testing kits that occurred around day 30 has been already solved and the desirable increase on the number of tests and reported cases will be apparent from the next few entries in the accumulated reported cases from the present results it is quite clear that the reduction on the testing has unfortunate consequences on the epidemic evolution at the end of this report the predicted results for crt provided the value of 5438 reported cases in comparison to the officially announced value of 5717 cases on march 31 st  2020 all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint figure 13 b -prediction of the daily reported data distribution drt with the five estimated parameter values from the available daily reported cases dataset for brasil from february 25 th up to march 29 th red dots the present work implements a mixed analytical-statistical inverse problem analysis to the prediction of epidemics evolution with focus on the covid-19 progression in brasil a siru-type model is implemented for the direct problem solution while a mixture of an analytical parametric estimation for the early phase epidemic exponential behavior with a bayesian inference approach for the entire period are considered for the inverse problem analysis the evolution of the covid-19 epidemy in china is considered for validation purposes by taking the first part of the dataset to estimate parameters and retaining the rest of the evolution data for direct comparison with the predicted results with excellent agreement then the same approach is applied to the brazilian case this time employing the available time series so far for the parametric estimates and then offering an evolution prediction also some public health intervention measures are critically examined in addition to those already implemented permitting the inspection of their impact on the overall dynamics of the disease proliferation clearly a combination of public health interventions can offer a considerable impact reduction on the disease progression within brasil as illustrated by the implemented modelling it was also analyzed the negative impact due to the scarcity of testing kits during a period which if not solved and even incremented would lead to an increase on the ratio of unreported to reported symptomatic cases and consequently on a dramatic epidemic evolution all rights reserved no reuse allowed without permission the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint which was not peer-reviewed is  httpsdoiorg1011012020033120049130 doi medrxiv preprint further improvement on the modelling is envisioned by enriching the model with latency effects age structure discrimination spatial demographic distribution dependence and recovery factor differentiation among isolated and non-isolated patients  a review of mathematical modeling artificial intelligence and datasets used in the study prediction and management of covid-19 youssoufa mohamadou aminou halidou pascalin kapen tiam   the world health organization declared that new coronavirus disease 2019 covid-19 was a public health emergency of international concern on january 30th 2020 1 2 by then there were a total number of 7818 confirmed cases of covid-19 globally with more than 1370 severe cases and 170 deaths the bulk of which was found in china 3 over the course of a few weeks the disease has propagated across the boundaries of china infecting nearly every country at the time of writing this paper may 01 2020 there is a total of 2397216 confirmed cases globally with 162956 deaths 4 symptoms of the disease include dry cough sore throat and fever although the majority of the cases are mild some cases could lead to acute respiratory distress syndrome ards severe pneumonia pulmonary oedema and organ failure 5 after the emergency declaration of who several works have been done in the terms of modeling and prediction to try and provide ways to either understand the disease propagation evaluate preventive measure put in place by authorities provide early and accurate detection of the disease just to name a few mathematical modeling has been used for several years in epidemiological studies 6 mathematical modeling of disease transmission and propagation helps in the prediction of the course of epidemics the design of mass vaccination programs and also it can provide guidance on what type of data are relevant in the study of the epidemics 7 some of the studies carried out in regards to the current covid-19 include modeling of the dynamic of covid-19 exploring the effect of prevention method like travel restriction of covid-19 and studying the effect of climate on the covid-19 propagation 8 on the other hand artificial intelligence ai is a tool used for prediction ai is the study and development of algorithms machines that mimic human intelligence ai has been successfully used in a several fields such as computer vision online advertising spam filtering robotics fraud detection and so on 9 10 in healthcare ai has also gained attention in terms of disease detection treatment selection patient monitoring drug discovery gene function annotation automated experiments automated data collection etc 11 12 as to what concerns the covid-19 ai has been used in medical image acquisition image segmentation and diagnosis 13 in this paper a review of the mathematical modeling and artificial intelligence used in the study estimation and prediction of covid-19 is presented the paper is divided into three parts the first presents the mathematical models used in the study of the pandemic the second presents the various ai applications in disease diagnosis and estimation and in the third part a list of available datasets for covid-19 is presented the review is divided into three parts each dealing with a specific aspect like mathematical modeling ai applications and available datasets for each of the three parts the items reviewed were grouped into topics and then a summary of each group is done in all a total number of 61 journal articles reports fact sheets and websites were reviewed the items reviewed were all published between december 2019 to april 2020 table 1 shows the structure of the review including the number of items reviewed and the main focus of the reviewed items
 choujun et al 23 used daily intercity migration data together with a seir model to generate a new model that describes the dynamics of covid-19 in china they collected the daily intercity migration data form 367 cities using a mobile application that tracks human migration they concluded that the number of infections in most cities in china would be highest between the middle of february to early march 2020 anca and kieran adapted a traditional seir model to study the specific dynamic compartments and epidemic parameters of covid-19 24 they analyzed the current management strategy of the pandemic including social distancing travel bans and service interruptions and closures for the generation of predictions and assessment of the efficiency of these control measures in 25 the combination of seir and regression models was used with john hopkins university dataset on covid-19 for the prediction of the change in the spreading of covid-19 the study presented in 26 used an age-structured susceptible-exposed-infected-removed seir model for physical distancing measurement and evaluation the authors showed that physical distancing measures were most effective if the gradual return to work started in april the study of the transmission of the covid-19 and its association with temperature and humidity using the seir model was initiated by xiao-jing et al 27 the outcomes of the study presented that raising the temperature and humidity values contributed to the control of transmission of the disease in 28 the seir model was adapted to investigate the potential community-wide impact of public use of face masks on the transmission dynamics and control of the covid-19 pandemic it was suggested that face masks should be used nation-wide and implemented immediately table 3
 a time-dependent susceptible-infected-recovered sir model to track the transmission rate and the recovering rate at a particular time was proposed in 29 they obtained a prediction error of 3 or less for confirmed cases and predicted that the day the recovering rate over took the transmission rate was on february 17 2020 in the hubei province of china wang et al 30 modified the sir model by adding different types of time-varying quarantine strategies such as government imposed mass isolation policies and micro-inspection measures at the community level to establish a method of calibrating cases of under-reported infections the sir model was also used to fit the cumulative data of covid-19 to an empirical form in china 31 it was reported that for given parameter values the sir model on the euclidean network obtained high accuracy on data form china and predict when the pandemic would be expected to be over in 32 a simple age-sensitive sir model which integrated known age-interaction contact patterns for the examination of potential effects of age-heterogeneous mitigations on an epidemic in a covid-19-like parameter regime was studied authors found that strict age-targeted mitigation strategies had the potential to reduce mortalities the age-structured sir model with social contact matrices and bayesian imputation was studied to evaluate the progress of the pandemic in india 33 the authors evaluated the influence of social distancing measures like workplace non-attendance and school closure on the transmission of the novel corona virus it was found that a three-week lockdown would be insufficient to prevent the spread of the disease a simple sir model modified to include certain variables of containment measures taken worldwide was used to study these measures 34 by comparing various scenarios it was shown that the infection progress strongly affected by the measures taken a susceptible-infectious-quarantined-recovered siqr model for the analysis of data in brazil was used 35 it was found that the number of quarantined individuals grew exponentially and stabilized the seiqr susceptible-exposed-infectious-quarantined-recovered model with time delays for latency and an asymptomatic phase was investigated 36 it was reported that time-varying social distancing using the seiqr model could reduce the number of infections by about 50recently a novel model known as bats-hosts-reservoir-people transmission network model was used to simulate the potential transmission from bats infection source to human 37 another method was developed where the age-specific susceptible-exposed-symptomatic-asymptomatic-recovered-seafood market seiarw model based on two suspected transmission routes was used to quantify age-specific transmission 38 the two routes were from market to person and from person to person the authors concluded that covid-19 transmissibility is higher in elderly persons as compared to young persons in 39 the influence of interventions and self-protection measures travel restriction quarantine of entry contact tracing isolation and wearing masks on covid-19 transmission dynamic in mainland china excluding hubei province was modeled using the markov chain monte carlo mcmc the results showed that the containment strategies were effective and magnificently suppressed the pandemic transmission it was also found that softening personal protection too early might lead to the spread of disease the spss modeler was also used to investigate the correlation between average daily temperatures and the growth rate of covid-19 in infected countries 40 it was shown that the pandemic rates were higher in case studies where the average temperature is lower finally in 41 a coupled ordinary differential equation metapopulation model for different courses on the disease in different age groups were developed it was shown that the economic lockdown could be safely reversed at any time without a substantial effect on the course of the disease in addition it was concluded that strict quarantines could not be necessary to keep the number of infected people low x-ray radiology consists of beaming x-ray photons onto a part of body to be imaged and collecting the photons that pass through that part of the body depending on the bodys tissue type it will attenuate block some of the incident photons this will create a shadowy image of the body on a detector located behind the body x-ray radiology is used to examine bone structure and detect infections in the lungs computed tomography ct takes the ides of x-ray radiography further by taking x-rays images of the body from multiple angles to produce cross-sectional images without dissecting the body these cross-sectional images also called slices are tomographic images and these contain more detail medical information than the conventional x-rays radiography ct images are used to detect abnormalities in the body like tumors and hemorrhage it can also be used to detect pulmonary embolisms excess fluid and pneumonia in the lungs 42 43 this makes it suitable for diagnosis of covid-19 which is a disease that attacks the lungs and the respiratory system in their study pan feng et al seek to verify the change obtained in the chest images of patients with covid-19 pneumonia the study was carried out on 4-day intervals from the first day of diagnosis to the day of total recovery excluded from this study are patients with complicated pneumonia with severe respiratory distress for non-severe cases the results of the chest scanner show a progress of lesions severity during the first 10 days then stabilizes thereafter according to this study almost all the patients presented a spike of the disease around the 10th day and the signs of improvement around the 14th day of the symptoms 44 in a series of experiments carried out in 3 days on 51 patients yicheng fang et al studied the performance of 2 methods of medical examinations on patients with covid-19 the results indicate that the sensitivity of chest ct to covid-19 is higher than the rt-pcr technique 98 for ct versus 71 for pcr when rt-pcr tests are negative chest ct can therefore be used on patients with clinical and epidemiological characteristics of covid-19 to confirm or refute the previous results 45 li yan et al also conducted the study to determine the rate of false diagnoses and the performance of ct scans on covid-19 their study was carried out on the first 51 patients confirmed by nucleic acid tests the study confirmed the high performance of the chest ct which produced a low rate of false diagnosis on covid-19 46 the principle of neural network nn is based on the collection of nodes called artificial neurons which freely model neurons in the brain based on examples without any prior knowledge without being programmed this system automatically generates identification characteristics when the algorithm uses multiple layers of neurons it is known as deep learning a convolutional neural network cnn is a deep learning algorithm which takes an image as input assign learnable weights to various features objects in the image so as to be able to differentiate one image from the other 9 10 wang et al in 49 used cnn with a dataset comprising of 13800 chest x-ray radiography images from 13725 patients so as to try and provide clinicians with a deeper insight into the critical factors affecting with covid-19 cases the reported an accuracy sensitivity and positive prediction value ppv of 926 871 and 964 respectively in 50 three models resnet50 inceptionv3 and inceptionresnetv2 based on cnn were proposed for detecting covid-19 in pneumonia infected patients from chest x-ray radiography images they used roc analyses and confusion matrices to evaluate the performances of the three models and found that the resnet50 model provided the best classification performance with an accuracy of 98 in a retrospective and multi-center study carried out by li et al 51 cnn was employed for the detection of covid-19 they extracted visual features from volumetric chest ct images of covid patients and classified them they reported that the method was not only able to detect covid-19 case but also to distinguish it from other community acquired pneumonia and non-pneumonic lung diseases in 52 a concept known as transfer learning where available data from one scenario is used to enhance accuracy of detection in a second scenario where there is lack of data was used on x-ray images from patients with ordinary bacterial pneumonia confirmed covid-19 cases and other normal infections the goal of the work was to evaluate the performance of some state-of-the-art cnn architectures for medical image classification they obtained an accuracy sensitivity and specificity of 9678 9866 and 9646 respectively and concluded that cnn with x-ray imaging might extract significant biomarkers related to covid-19 hemdan et al 53 on their part implemented seven different cnn architectures with the aim of assisting radiologists in the automatic diagnoses of covid-19 in x-ray images they validated the architectures on 50 chest x-ray images with half confirmed covid-19 cases they reported that the vgg19 and dense convolutional network densenet models had the best performance both with an accuracy of of 90 support vector machines svm are supervised learning methods used for regression classification and also outlier detection the aim of svm is to find a hyperplane in an n-dimensional space where n is the number of features that markedly classifies the input data in other words svm will work to find a plane that has the maximum distance between data points of separate classes support vectors are those data points that are closest to the hyperplane these data points affects the position and orientation of the hyperplane 67 barstugan et al 54 presented an early detection of covid-19 based on svm the algorithm was applied on abdominal computed tomography ct images four different image datasets of variable size 16x16 32x32 48x48 64x64 were created from 150 ct images features were extracted through grey level co-occurrence matrix glcm local directional pattern ldp grey level run length matrix glrlm grey-level size zone matrix glszm and discrete wavelet transform dwt algorithms svm was then used to classify the extracted features a maximum sensitivity and accuracy of 9756 and 9871 respectively were obtained with 10-fold cross-validation and glszm feature extraction method in 55 a combination of deep feature extractor and svm was used to detect covid-19 infection in x-ray images the proposed model combination of resnet50 and svm obtained an accuracy of 9538 in 57 svm was used on features extracted from chest x-ray radiography images for early detection of covid-19 cases the features were extracted through a multi-level thresholding of the images they obtained a classification accuracy of 9882 on a total of 40 contrast-enhanced chest x-ray images non-image data was also used with svm and data from emergency care admission exams to detect covid-19 cases de moraes et al 56 used svm and data from emergency care admission exams to detect covid-19 cases they collected data from 235 patients of which 43 were confirmed covid-19 cases they trained five machine learning algorithms namely logistic regression random forests gradient boosting trees neural networks and support vector machines on 70 of the patients and evaluated their performance on the remaining 30 they found out that the svm had the best performance with an accuracy of 85 and concluded that the method could be used to target which patient needs a laboratory covid-19 tests done on them in statistics logistic regression is used to model the probability each sample is assigned a probability between 0 and 1 it can be extended to model several classes of events in order to determine for example different objects in an image 68 although simpler than the cnn logistic regression also could be applied in the in depth study of the manifestation of covid-19 for instance in 58 logistic regression was applied to values provided by roc analysis in the aim of investigating clinical and ct features that indicates severity covid-19 through logistic regression analyses it was found that the clinical factors associated with severecritical covid-19 pneumonia were patient older than 50 years chest pain dyspnea comorbidities and cough among others in 59 deep features from covid-19 patient chest x-ray images were extracted using resnet152 and then smote was used to balance the data points of covid-19 and normal patients then finally machine learning algorithms like random forest and xgboost were used to classify according to the features they obtained an accuracy of 973 for random forest and 977 for xgboost naive bayes classifiers are among the simplest bayesian network models from the family of probabilistic classifiers coupled with the kernel density estimation they can reach high levels of precision in digital images classification 69 in the study of covid-19 it has also been used for classification in 60 the authors combined conventional statistical and machine learning in order to extract features from ct images the extracted features were then classified by hybrid classifier system based on naive bayes experimental evaluation of this method produced and accuracy of 9607 linear discriminant analysis lda is used to find a linear combination of features that characterizes or separates classes of objects or events in pattern recognition and machine learning this resulting combination can be used as a linear classifier for dimensionality reduction before the final classification 70 lda was used in 61 with the aim of investigating the characteristics and rules of hematology changes in patients suffering from covid-19 clinical and laboratory test results of the patients were analyzed and different hematological parameters were fitted using lda the nlrrdw  sd combined parameter was found to be the best indicator of the severity of covid-19 in patients with an accuracy of 938 decision trees is a technique that helps analyzing decisions by identifying the most likely strategy leading to the goal random forest on its part is essentially a collection of decision trees whose results are accumulated into a final result they have the ability to limit variance without increasing error due to bias in medical practice it is used to classify patient images 71 in 62 the chest ct images of 176 patients with covid-19 were used for severity assessment a random forest modeled and trained to evaluate the severity of covid-19 in patients based on quantitative features the rf model showed encouraging results with an accuracy of 875 shi et al proposed an infection size aware random forest method isarf their method had two steps the first one consisted of categorizing different groups while the second classified the images 63 they used an infection size feature defined as the ratio of the volume of infected regions to the total volume of whole segmented lung this infection size was then used in a 3 level random forest classifier that classified it into 4 groups they used a 5-fold cross-validation to evaluate the performance of the proposed algorithm and also compared it to other classifiers like logistic regression support vector machine and neural network nn they obtained a sensitivity specificity and accuracy of 907833879 respectively u-net was first proposed by ronneberger et al for segmentation of biomedical images 72 the u-net architecture has two paths namely a contraction path or the encoder and an expanding path or the decoder in the encoder successive convolutional and max pool down-sampling layers are used to extract the context of an image while in the decoder the discriminative features learnt in the encoder are projected onto the pixel space image so as to obtain a semantically segmented image the decoder is made up of a series of upsampling concatenation and then convolution operation u-net based algorithms were also used in the segmentation of medical images for the purpose of covid-19 detection chen et al proposed a new method called modified u-net structure to segment the regions of infected lungs with covid-19 they used aggregated residual network resnext for learning and complex features from the original images they also applied a soft attention mechanism that enhanced the model s ability to differentiate various symptoms of covid-19 64 in 65 attention u-net was used with an adversarial critic model to improve its performance they obtained an average dice score of 978 on 1047 chest x-ray images from three sources in 66 two methods are proposed namely the infnet and the semi-inf-net the inf-net uses implicit reverse attention and explicit edge attention to ameliorate the detection of infected regions in ct lung images the semi-infnet is a semi-supervised solution that helps to overcome the lack of high quality and labeled images they carried out extensive experiments on covid-19 datasets and showed that the proposed methods perform better than other segmentation methods unsupervised learning have also been used in the study of covid-19 unsupervised learning unlike supervised learning searches for previously undetected prototypes in a data stream without pre-existing labels and minimimum human intervention it makes it possible to model the densities of probability on the entries this algorithm makes it possible to detect abnormal parts of data which do not correspond to any group its application is in the field of density estimation in statistics 10 among the unsupervised learning used in covid-19 is k-means clustering which is a vector quantization algorithm it partitions n observations into k clusters in which each annotation belongs to the cluster with the neighboring mean serving as the princeps of the cluster 73 in both mathematical modeling and ai data is the raw material so the first step in the development of covid-19 applications is data collection over the course of few months there are multiple datasets that have been put online in regards to the covid-19 most if not all of these datasets are open source meaning that they are free for anyone to download and use also they are constantly being updated with new data from the field table 4 presents a collection of the open source datasets explored the following presents a comprehensive description of these datasets dong et al 74 currently provides one of the most complete database of the covid-19 situation the database known as the 2019 novel coronavirus visual dashboard operated is maintained by the johns hopkins university center for systems science and engineering jhu csse they obtained data from about 18 sources such as the who cdc and other governments agencies compiled and shared them in the form of an interactive map of the covid-19 situation map the database includes number of daily contamination active recovery and death it also contains the location stateprovince country longitude latitude number of people tested incident rate and hospitalization rate xu et al 75 are currently collecting and sharing health information on persons with covid-19 from local to national level together with other information from online reports the data are localized geographically and also indicate aspects like the symptoms and the dates of confirmation and admission and also travel record cohen et al 76 created a covid-19 image database by collecting x-ray images from various websites as well as publications the database is made up of 345 x-ray images zhao et al 77 created a computed tomography ct image database currently may 01 2020 containing 349 images of confirmed covid-19 cases along with 398 images of non-covid-19 cases the ct images are gathered from several covid19-related papers ma et al provided a dataset containing 20 labeled covid-19 ct images of the left lung the right lung and the infection type the labeling was done by two radiologists 78 the aim of their work was to establish a benchmark for ct image segmentation of lungs in regards to covid-19 two radiologists in based in oslo norway have shared two ct datasets the covid-19 ct segmentation dataset with 100 axial ct slices and the segmentation dataset nr 2 with 829 ct slices from more than 60 patients 79 the databases were manually segmented by radiology experts chen et al shared a covid-19 twitter dataset 80 this dataset contains an ongoing collection of tweets ids associated with covid-19 and which started from january 28 2020 such tweet ids include covid-19 coronavirus pandemic and so on they also tracked certain accounts like that of the who cdcgov and hhsgov as of may 01 2020 they collected more than 10 million tweets in many languages rabindra 81 is also collecting tweets using the lstm model deployed on a website the model continuously monitors real-time twitter feed for covid-19 related tweets it uses filters such as language en and tweeter keywords like covid19 coronavirus covid and so on as of may 01 2020 more than 30million tweets were collected havard dataverse also provides the global news dataset which contains covid-19 related global english news from gdelt 82 and the climate dataset which contains time series temperature humidity air quality and other monitored data in china from january 1 2020 83 the coronacases initiative which is a pro bono initiative of raioss desenvolvimento ltda and livon sade ltda also provides information on covid-19 cases on their website 84 in covid-19 and other pandemic studies other datasets such as population density mobility security incidents economic situation humanitarian condition data and healthcare workforce are important data that will ensure the accuracy of the studies several sources provide those datasets one of such sources is the worldpop which shares spatial demographic datasets from africa asia and central and south america 85 some of the datasets provided by worldpop are population data births internal migration age and sex data administrative areas and global flight data the humanitarian data exchange hdx coordinated by the un office for the coordination of humanitarian affairs ocha shares more than 17000 humanitarian datasets form 253 locations around the globe 86 the who on its part shares the global health workforce statistics 87 the dataset includes data on the number of health workers as well as hospital bed capacity in each country the tech-giants apple and google both released mobility reports on covid-19 apple called their dataset mobility trends reports 88 while google called it google covid-19 community mobility reports 89 both presents aggregated data that registers the daily use of various modes of transportation walking driving transit since the start of february 2020 as well as places visited or stayed in by users of their services the data was collected from customer requests for directions or location in apple and in google maps they also offer a useful visualization tool of the data our world in data on its part provides covid-19 testing dataset where they collect data that are based on tests carried out to establish if a person is currently infected 90 acaps 91 provides a dataset of government measures dataset also provides government measures implemented by governments all around the world in response to covid-19 while the armed conflict location  event data project acled 92 provides security incidents related to covid19 dataset the international monetary fund imf 93 and bfa global 94 both provide datasets on the key economic responses of governments and the effect of covid19 management measures on economy lastly the software providerc3ai compiled cleaned structured and standardized covid-19 data from most of the sources presented in this paper 95 the initiative known as c3ai covid-19 data lake contains analysis-ready covid-19 data in one place the service is free and the datasets are updated continuously it contains everything from time-series data to case reports also a github repository was created to collect covid-19 images regarding ai research papers and datasets it contain 19 datsets 11 review papers 18 clinical papers on covid19 images 54 ai-related papers 54 atrticles on cxr methods and 1 paper on line artefact quantification in lung ultrasound images 96 the use of mathematical modeling and ai with covid-19 data will increase our knowledge on the disease propagation evaluating prevention measures as well as early and accurate detection of the disease in patients however to arrive at this end a lot of data is needed to explore various models and ai algorithms the data available up till now are mostly of medical images for diagnosis and text based data for social impact analysis while the later may be generated by and readily available to a large number people the former on the other hand can only be generated in a specialized institution by a specialized professional this means that data in low resource setting are not available as these places do not have the sophisticated imaging equipment needed to generate such images 97 also it is well known in data science that datasets from different geographical locations may not hold the same information and this is especially true in terms of healthcare data more data types are therefore needed that can be easily generated easily anywhere on the globe so as to enhance and render the application of the mathematical models and ai algorithm possible for many these data types could be physiological measurements such as ecg spo2 body temperature that could be obtained using wearable devices 98 data concerning the type of preventive measures implemented by authorities are also not well documented in this work only a few of the dataset found provided that information however this information could help in the examination and optimization of the set measures thereby improving the situation in mathematical modeling most of the articles found in the writing of this paper are of covid-19 dynamics however modeling can be done with appropriate datasets to explore the effect of the variables like climate and preventive measure on the spread of covid-19 as explained earlier there is also not many studies on the correlation of environmental and climatic conditions to the covid-19 propagation in the work only two articles were found that addresses this issue and they both provide in different and interesting way of looking at the propagation of this diseases 27 28 simulation of second and third waves of covid-19 outbreaks will also help to enhance surveillance as countries start easing social restriction measures a study is needed to estimate possible hopspots for new outbreaks ai deep learning is powerful tool for early and accurate diagnosis of covid-19 and many articles have addressed it most of them apply convolutional neural networks cnn in their work for medical image classification few other studies apply the random forest and support vector machines there are also some that applied u-net and its variations for the segmentation of ct and x-ray images the authors of the ai algorithms reviewed here all claimed that their algorithm performs very well on test data however it is well known that good performance of an algorithm on test data does not mean that it will perform similarly when deployed on the field this is due to fact that in real life the data is more prone to noise and other artefact that are not usually present in the training and test data the lack of diverse annotated images is also not helping the situation in this review only 2 out of 18 studies were found to used annotated data from radiologists collaboration is needed between clinicians and ai experts in other to build a huge amount of annotated images of covid-19 also human in the loop or human augmentation can be another solution to overcome the problem caused by the disparity of an algorithms performance when applied to test data and when applied in the real world most of the studies reviewed used existing models while a few used well known models with some modifications those used with some modifications performed slightly better than the others stressing the need of developing hybrid models to build better and robust architectures much work is also needs to be done in terms of drug andor vaccine discovery treatment selection and contamination risk assessment for medical personnel 99 finally since most of the ai research objective on covid-19 is to find the optimal solution for diagnosis other algorithms like genetic programming and boosting adaboost should be explored so as to clear any doubt regarding their performances in conclusion covid-19 has spread rapidly all over the world creating an emergency situation mathematical modeling and ai have both shown to be reliable tools in the fight against this pandemic most of the modeling done were based on the susceptible-exposed-infected-removed seir model and the susceptible-infected-recovered sir model while most of the ai implementations were convolutional neural network cnn on x-ray and ct images several datasets concerning the covid-19 have been collected and shared open source however much work is needed to be done in terms of providing the public with a wide variety of data types and from many regions as possible also other ai and modeling applications in healthcare should be explored in regards to this covid-19  predicting the growth and trend of covid-19 pandemic using machine learning and cloud computing shreshth tuli shikhar tuli rakesh tuli sukhpal gill singh  the outbreak of covid-19 coronavirus namely sars-cov-2 has created a calamitous situation throughout the world the cumulative incidence of covid-19 is rapidly increasing day by day machine learning ml and cloud computing can be deployed very effectively to track the disease predict growth of the epidemic and design strategies and policy to manage its spread this study applies an improved mathematical model to analyse and predict the growth of the epidemic an ml-based improved model has been applied to predict the potential threat of covid-19 in countries worldwide we show that using iterative weighting for fitting generalized inverse weibull distribution a better fit can be obtained to develop a prediction framework this can be deployed on a cloud computing platform for more accurate and real-time prediction of the growth behavior of the epidemic a data driven approach with higher accuracy as here can be very useful for a proactive response from the government and citizens finally we propose a set of research opportunities and setup grounds for further practical applications predicted curves for some of the most affected countries can be seen at httpscollaborationcoraltelecomcovid  the novel coronavirus disease was first reported on 31 december 2019 in the wuhan hubei province china it started spreading rapidly across the world 1  the cumulative incidence of the causitive virus sars-cov-2 is rapidly increasing and has affected 196 countries and territories with usa spain italy uk and france being the most affected 2  world health organization who has declared the coronavirus outbreak a pandemic while the virus continues to spread 3  as on 4 may 2020 a total of 3581884 confirmed positive cases have been reported leading to 248558 deaths 2  the major difference between the pandemic caused by cov-2 and related viruses like sars and mers is the ability of cov-2 to spread rapidly through human contact and leave nearly 20 infected subjects as motivation and our contributions ml 11 can be utilized to handle large data and intelligently predict the spread of the disease cloud computing 12 can be used to rapidly enhance the prediction process using high-speed computations 7  novel energy-efficient edge systems can be used to procure data in order to bring down power consumption in this paper we present a prediction model deployed using fogbus framework 13 for accurate prediction of the number of covid-19 cases the rise and the fall of the number of cases in near future and the date when various countries may expect the pandemic to end we also provide a detailed comparison with a baseline model and show how catastrophic the effects can be if poorly fitting models are used we present a prediction scheme based on the ml model which can be used in remote cloud nodes for real-time prediction allowing governments and citizens to respond proactively finally we summarize this work and present various research directions article structure the rest of the paper is organized as follows section 2 presents the prediction model and performance comparison section 3 concludes the work and describes the future research opportunities section 4 provides details of open repositories for the dataset code and results machine learning ml and data science community are striving hard to improve the forecasts of epidemiological models and analyze the information flowing over twitter for the development of management strategies and the assessment of impact of policies to curb its spread various datasets in this regard have been openly released to the public yet there is a need to capture develop and analyse more data as the covid-19 grows worldwide 14 15  the novel coronavirus is leaving a deep socio-economic impact globally due to the ease of virus transmission primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes countries which are densely populated need to be on a higher alert 16  to gain more insight on how covid-19 is impacting the world population and to predict the number of covid-19 cases and dates when the pandemic may be expected to end in various countries we propose a machine learning model that can be run continuously on cloud data centers cdcs for accurate spread prediction and proactive development of strategic response by the government and citizens dataset the dataset used in this case study is the our world in data by hannah ritchie 1  the dataset is updated daily from the world health organization who situation reports 2  more details about the dataset are available at httpsourworldindataorgcoronavirus-source-data cloud framework the ml models are built to make a good advanced prediction of the number of new cases and the dates when the pandemic might end to provide fail-safe computation and quick data analysis we propose a framework to deploy these models on cloud datacenters as shown in figure 1 in a cloud based environment the government hospitals and private health-centers continuously send their positive patient count population density average and median age weather conditions health facilities etc are also to be integrated for enhancing the accuracy of the predictions for this case study we used three instances of single core azure b1s virtual machines with 1-gib ram ssd storage and 64-bit microsoft windows server 2016 1  we used the healthfog 11 framework leveraging the fogbus 13 for deploying multiple analysis tasks in an ensemble learning fashion to predict various metrics like the number of anticipated facilities to manage patients and the hospitals we analyzed that the cost of tracking patients on a daily basis amortized cpu consumption and cloud execution is 37 and only 12 usd per day as the dataset size increases computationally more powerful resources would be needed ml model many recent works have suggested that the covid-19 spread follows exponential distribution 17 18 19  as per empirical evaluations and previous datasets on sars-cov-1 virus pandemic many sources have shown that data corresponding to new cases with time has large number of outliers and may or may not follow a standard distribution like gaussian or exponential 20 21 22 23  in recent study by data-driven innovation laboratory singapore university of technology and design sutd 3  the regression curves were drawn using the susceptible-infected-recovered model 24 and gaussian distribution was deployed to estimate the number of cases with time however in the previously reported studies on the earlier version of the virus namely sara-cov-1 the data was reported to follow generalized inverse weibull giw distribution 25 better than gaussian as shown in figure 2 details of robust weibull fitting follow in the next section detailed comparison for sars-cov-2 has been described in the next section this fits the following function to the data here f x denotes the number of cases with x where x  0 is the time in number of days from the first case and     0  r are parameters of the model now we can find the appropriate values of the parameters   and  to minimize the error between the predicted cases y  f x and the actual cases  this can be done using the popular machine learning technique of levenberg-marquardt lm for curve fitting 26  however as various sources have suggested in initial stages of covid-19 the data has many outliers and noise this makes it hard to accurately predict the number of cases thus we propose an iterative weighting strategy and call our fitting technique robust fitting a diagrammatic representation of the iterative weighting process is shown in figure 3  the main idea is as follows we maintain weights for all data points i in every iteration n starting from 0 as w n i  first we fit a curve using the lm technique with weights of all data points as 1 thus w 0 i  1  i second we find the weight corresponding to every point for the next iteration w n1 i  as 1 azure cloud vms httpsazuremicrosoftcomen-aupricingcalculator 3 when will covid-19 end ddi lab sutd httpsddisutdedusgwhen-will-covid-19-end 3 all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 11 2020  httpsdoiorg1011012020050620091900 doi medrxiv preprint  simply in the above equation we first take tanhshrink function defined as tanhshrinkx  x  tanhx for the distances of all points along y axis from the curve d i  this is to have a higher value for points far from the curve and near 0 value for closer points this is then standardized by dividing with max value over all points and subtracted from 1 to get a weight corresponding to each point this weight is then standardized using sof tmax function so that sum of all weights is 1 the curve is fit again using lm method now with the new weights w n1 i  the algorithm converges when the sum total deviation of all weights becomes lower than a threshold value distribution model selection to find the best fitting distribution model for the data corresponding to covid-19 we studied the data on daily new confirmed covid cases five sets of global data on daily new covid-19 cases were used to fit parameters of different types of distributions finally we identified the best performing 5 distributions the results are shown in table 1  we observe that using the iteratively weighted approach the inverse weibull function fits the best to the covid-19 dataset as compared to the iterative versions of gaussian beta 4-parameter fisher-tippet extreme value distribution and log normal functions when applied to the same dataset iterative weibull showed an average mape of 12 lower than non-iteratively weighted weibull a step-by-step algorithm for iteratively weighted curve fitting using the giw distribution called robust weibull is given in algorithm 1 analysis and interpretation to compare the proposed robust weibull fitting model we use the baseline proposed by jianxi luo from sutd 3  the comparison metrics include mean squared error mse mean absolute percentage error mape and coefficient of determination r 2  table 2 shows the model predictions of the spread of the covid-19 for every major country for which sufficient data was available and model fits had r 2  05 using the proposed model as shown in the table the proposed model performs significantly better than the baseline 4 all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity  i   then break end for end procedure as shown in figure 4 4  the predictions of the baseline gaussian model deployed by sutd are overoptimistic following such models could lead to premature uplifting of the lockdown causing adverse effect on management of the epidemic having better fit models as proposed here could help plan a better strategy based on more accurate predictions and future scenarios figure 5 shows the total predicted number of cases for all countries across the globe here we have neglected those countries where the data is insufficient for making predictions or the number of days for data is less than 30 as shown in figure 4 explained in model section the fit curve can be used to predict the number of cases that will have to be dealt by the country assuming the same trend continues the figure illustrates that the maximum number of total cases will be in the north america region the number of cases will also be high in the european continent russia and eastern asia including china the original epicenter of the disease 4 curves and predictions of all countries have been given in appendix 5 all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity there is a need to explore social media and applying machine learning application to analyse social media data  in this study we have discussed how improved mathematical modelling machine learning and cloud computing can help to predict the growth of the epidemic proactively further a case study has been presented which shows the severity of the spread of cov-2 in countries worldwide using the proposed robust weibull model based on iterative weighting we show that our model is able to make statistically better predictions than the baseline the baseline gaussian model shows an over-optimistic picture of the covid-19 scenario a poorly fitting model could lead to a non optimal decision making leading to worsening of public health situation we propose the future directions as follows firstly other important parameters like population density distribution of age individual and community movements level of healthcare facilities available strain type and virulence of the virus etc need to be included in the regression model to further enhance the prediction accuracy secondly models like arima 27 can be integrated with weibull function for further time series analysis and predictions thirdly ml can be utilized to predict the structure and function of various proteins associated with cov-2 and their interaction with the host human proteins and cellular environment the contribution of various socio-economic variables that determine the vulnerability spread and progression of the epidemic can be predicted by developing suitable algorithms ai based proactive measures can be taken to prevent the spread of the virus to sensitive groups in the society real time sensors can be used for example in traffic camera or surveillance which track covid-19 symptoms based on visual imaging and tracking apps and inform respective hospitals and administrative authorities for punitive action 28  tracking needs to cover all stages from ports of entries to public places and hospitals 29  the research directions and challenges are summarized in figure 6  all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 11 2020  httpsdoiorg1011012020050620091900 doi medrxiv preprint all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 11 2020  httpsdoiorg1011012020050620091900 doi medrxiv preprint d e c all rights reserved no reuse allowed without permission which was not certified by peer review is the authorfunder who has granted medrxiv a license to display the preprint in perpetuity the copyright holder for this preprint this version posted may 11 2020  httpsdoiorg1011012020050620091900 doi medrxiv preprint  fakecovid-a multilingual cross-domain fact check news dataset for covid-19 gautam kishore shahi  durgesh nandini  in this paper we present a first multilingual cross-domain dataset of 5182 fact-checked news articles for covid-19 collected from 04012020 to 15052020 we have collected the fact-checked articles from 92 different fact-checking website after obtaining references from poynter and snopes we have manually annotated articles into 11 different categories of the fact-checked news according to their content the dataset is in 40 languages from 105 countries we have built a classifier to detect fake news and present results for the automatic fake news detection and its class our model achieves an f1 score of 076 to detect the false class and other fact check articles the fakecovid dataset is available at github 1  coronavirus disease 2019 covid-19 is a contagious disease caused by sars coronavirus 2 a virus closely related to the sars virus moriguchi et al 2020  the first case of the virus was discovered in the city of wuhan cascella et al 2020 in late december 2019 from the beginning of march it was recognised to be a global issue and was later declared a pandemic by who according to who covid-19 has spread to at least 215 countries around the globe organization and others 2020a people from all over the world are using keywords such as covid19 coronavirus etc for discussion so perhaps for the first time in history we see that humanity has been exposed to substantial interaction on a single subject in dozens of different languages and on many platforms in parallel the infodemic of rumours and misinformation related to the virus came to surface infodemic is a term coined by world health organization who to explain the misinformation of virus and it makes difficult for users to find reliable sources for any claim made on the pandemic either on the news or social media organization and others 2020b zarocostas 2020 hence browsing or gathering information from the news media or social media platforms without checking the correctness affects peoples psychology daily lives and behaviours on the other copyright c 2020 association for the advancement of artificial intelligence wwwaaaiorg all rights reserved 1 httpsgautamshahigithubiofakecovid hand misinformation is a piece of false information or inaccurate information it could be a false rumour wrong claim insult and prank around the globe scientists have been trying their best to discover remedies for the covid 19 and the infodemic that scares the planet by words have become manifolds threatening the fight against the pandemic not only involves finding remedies against the virus but it is also crucial to delicately deal with infodemic of misinformation so that the authenticity related to the disease and the psychological health of people around the globe is reassured many people are involved in the creation and consumption of misinformation related to covid-19 old messages and incidents are repurposed to spread fake news by somehow connecting them to the subject of the virus the government of various countries and mediators of social media platforms have been consistently trying to stop the publishing of fake news but the process has been ineffective due to numerous hurdles tidy 2020  one of the key issues with detecting misinformation related to covid-19 is that there is a lack of corpus to test methods for fake news detection in this paper we have presented a human-annotated multilingual cross-domain fact-checked set for the covid-19 according to the google trends from 1st january 2020 to 15th may 2020 the term fake news and coronavirus have been repeatedly searched in last three months the search rate was maximum during march 2020 as shown in figure 1 and 2 both trends follow the same curve during the march and april which indicates people are eager to know or check the truth of coronavirus  fake news detection is a cumbersome task for two reasons firstly it requires a specially trained human to make a clear distinction between fake news and real news and secondly a huge velocity veracity and diversity of fake news are available on various social media platforms newspapers and news channels in multiple domains due to the lack of efficient skills and human resources an automatic tool for fake news detection is required variety and velocity of fake news keep changing so the existing methods fail to detect misinformation consistently the main contribution of this work is to prepare an opensource data set for detection of misinformation at the time of the pandemic a machine learning-based classifier is built to in the following sections we discuss the related work data collection data annotation data cleaning and prepossessing section data exploration classification results discussion and finally we discuss the conclusion and ideas for future works to examine the fact check we need a reliable corpus to run our model from last few years a collection of small datasets related to fact-checking has been published these datasets are a mixture of different topics of fake news for instance us election 2016 wang 2017  most of them are only in the english language and have been collected from only a few sources which limit the diversity of fact-checking augenstein et al 2019 describes the different kinds of the fact-checking dataset available and their limitations the authors have come up with a multi-domain evidence-based fact-checking dataset they have also described the metadata of fact-check articles vlachos and riedel 2014 describes the task of fact-checking and construction of dataset using the process used by journalists several methods are published for automatic detection of fake news prez-rosas et al 2017  discuss the automatic detection of fake news and linguists difference in false and legitimate content zhou and zafarani 2018 analyse compare and summarise the several different methods available for fake news detection authors give an overview of methods which have been tested for fake news detection research has been done to look inside on the feature of a news story in the modern diaspora along with different kind of news story and its impact on people parikh and atrey 2018  both covid-19 pandemic and infodemic spread in parallel mesquita et al 2020 proposes a framework to fight against fake medical news because fake news can intensify the effect of covid-19 pandemic all health worker scientists the government are trying to fight against the fake medical news during march 2020 several cases have been discovered where people are consuming the partially true information about covid-19 on facebook and whatsapp without using the proper medical terms which creates a panic about medicine and prevention for covid-19 orso et al 2020  by analysing the search behaviour of people in italy from january to march 2020 and found that a large number of infodemic monikers were observed across italy rovetta and bhagavathula 2020 during the time of the pandemic fake news is spread all over the world in different languages the fact-news is covered in different domains like origin and spread conspiracy theory etc there is a lack of resources which are multilin-gual and cross-domain and have been collected from multiple sources contributions to solve the problems of a multi-purpose fact-checked corpus we provide an open-source dataset which is multilingual cross-domain and collected from 105 countries the corpus can be used for the several studies related to covid-19 and fake news this section explains the steps followed to collect the data from different fact-checking websites we have used the following data source for taking a reference to the fact checking website snopes-snopes snopes 2020 is an independent publication owned by snopes media group snopes verifies the correctness of misinformation spread across several topics as for the fact-checking process they manually verify the authenticity of the news article and performs a contextual analysis in response to the covid-19 infodemic snopes provides a collection of a fact-checked news article in different categories based on the topic of the article poynter-poynter poynter institute 2020 is a non-profit making institute of journalists in covid-19 crisis poynter came forward to inform and educate to avoid the circulation of the fake news poynter maintains an international fact-checking networkifcn the institute also started a hashtag coronavirusfacts and datoscoronavirus to gather the misinformation about covid-19 poynter maintains a database which collects fact-checked news from 91 fact-checking organization in 40 languages for data collection we used the poynter and snopes as a bridge to get the links to the original fact-checking website the steps involved in the process of data collection fact-check articles we weekly scraped the list of factchecked news mentioned at reference sources poynter  snopes and collected the list of the fact-checked news articles mentioned on their website we have assigned a unique identifier to each of them and its denoted by fcid source of articles from the list of fact-checked news articles we fetched the reference to the source of the fact-checked article which was published by the poynter and snopes this is represented by article source title from the collected link we fetched the tile of the news article in many cases the title provided by the reference providedpoynter  snopes is different from the original title given by the fact-checking website title provided by the reference provide is defined by reference title and denoted by ref title similarly source title is defined source title and represented as source title published date we collected the published date which refers to the date of publication of fact-checked articles by the fact-checking websites it is represented by published date content of articles once we have a source of a news article then we used beautiful shop richardson 2007  python-based library to crawl the html document object modeldom to gather other information like textual content country date of the article it is defined by source title class the fact checking website assign a class for each fact checked article each fact checking website have a set of classes designed by them for instance the classes may be true false partially false meaning that the article is true false or partially false respectively the articles are then elucidated by them into one of their designed classes a detailed description is mentioned in table 6 this attribute is denoted by class social media link fake news is primarily circulated over social media to be spread among a large number of people and the fact-checker provides a reference to social media posts so we have collected the presence of social media on the fact-checked articles once we extracted the content from the web page we looked for the social media presence in the news article to check the twitter presence we look for keyword twitter and status in the hyperlink for youtube we look for the keywords youtube and watch for reddit we look for the keyword reddit and the pattern redditcomr using regex similarly we check the presence of facebook and instagram using regex pattern this property is denoted by sm link fact checking website from the fact-checked references we also fetch the source of the article which is the factchecking company this property is denoted by verifiedby country we fetched the country of the articles which tells in which country the article was circulated sometimes the same articles were circulated in several countries in our dataset the maximum number of countries that an article appeared to be circulated around was four hence we divided the country attribute into 4 sub attributes namely country1 country2country3 and country4 to identify each country separately category fake news are circulated regarding several topics for instance origin or virus or international response the category attribute has been used to denote under which topic the article falls into the value of the attribute has been assigned by us after manually annotating the articles a detailed description of the annotations is mentioned in section data annotation language the fact check articles are circulated in different countries in several languages to identify the language of the article we created the attribute lang which denotes the language of the fact checked articles the process of language detection is discussed in section data cleaning and prepossessing we have collected 5182 articles circulated in 105 countries from 92 fact-checkers articles published were from 04012020 to 15052020 in 40 languages among which 408  articles are in the english language a summary of the dataset is presented in table 1  in the context of covid-19 there are variantscategory of fake news circulating so we annotated the articles for each category for annotation we defined the task to label the news article into a predefined category according to the content of the news article after doing the content analysis we decided the category of the news article annotators followed a standard procedure for annotation visit the source article using the link forward and look at the title and read the content of the article then decided the category of the article we have categorised the news articles into 11 distinct categories based on their contents and the categories were also reviewed by professional fact-checker the description of the categories are mentioned in table 7 due to our limitation with the knowledge of the language we have annotated 1951 fact check articles in three language english2116 hindi141 and german 47 annotation for annotation of news articles we selected three people based on background knowledge and linguistic knowledge data is being annotated by one annotator and the second annotator annotates the randomly chosen news article according to the language to calculate the intercoder reliability lombard snyder-duch and bracken 2002 agreement the first annotator is pursuing a masters degree in our chair and she has a good experience of working with data annotation the annotation guidelines were shared in the beginning and the student was asked to provide sample annotation after verifying the annotation quality the annotator was asked to label other data as a fluent english speaker and native german speaker she was asked to annotate english and german news article the second annotator is a phd student who is an experienced data scientist he is a fluent german speaker and native hindi speaker he annotated all the hindi news articles and some of the randomly chosen german news article the third annotator a phd student and working in the area of machine learning is fluent in english and is a native hindi speaker the annotator annotated randomly chosen english and hindi news article for each language we calculated reliability score to measure the agreement between two annotators the number of articles labelled by all three annotators and their intercoder reliability is shown in table 2  we have used the following cleaning process to remove unwanted information from collected data faulty urls some of the urls are entered wrong at the fact-checking website while some do not exist anymore so we manually analysed the faulty urls and either corrected them if they were wrongly entered or removed them if they ceased to exist missing title in some cases the title of the article is missing so we manually added source title after looking into the webpage of the article removing duplicates some of the fact-checked articles were duplicate so we filtered the unique article using the url of the article and removed the duplicates in this step we applied the basic natural language processingnlp preprocessing techniques to remove unwanted information from the data like lowercase removal of the short word tokenization etc for data cleaning we have used python library nltk loper and bird 2002  textblob loria 2018  and regular expression language detection the data collected were in multiple languages to identify the language of the content we performed language detection using langdetect a python based library shuyo 2014  we used the content of the articles to assign them their respective language abbreviations and contractions abbreviations and contractions of words both serve to shorten a word but while abbreviations omit the last few letters of the word contractions omit letters in the middle of the word abbreviations and contractions have become even more common with the internet texting and the need to keep messaging and posting text to a minimum spelling correction sometimes tweets may contain typo errors we used textblob a python library for text processing textblob returns the corrected words as output if any sentence has spelling mistakes in this section we performed the exploratory analysis of the dataset across country we have analysed the circulation of fact check data across different countries we found that india has a maximum number of cases of fake news followed by  we filtered the data using classes mentioned in table 5 we divided the data set into two class false and others false includes the articles which are labelled false by fact-checker there are 4132 articles in the false category apart from the false class the remaining 22 class are considered as other categories which counts 1050 facts checked articles we decided to go for two classes because 78  data is in the false category so if we can filter the false category from a stream of misinformation it will ease the job of early-stage screening for fact-checker we tested our model for the english language and the results are the total number of false and other class are 2116 and 500 respectively we have used a bert based classification model without fine-tuning as a single view to measure the performance of the algorithm we set hidden units as 300 and training epoch as 150 each training process continues until the restriction or validation loss is continued batch size is set to 1 and the learning rate is 0001 the classification result obtained is shown in table 5  gathering the fact check news article is a critical and challenging task due to reasons such as unavailability of references and presence of false references for some of the fact-checked article the reference links are not available or invalid so we could not collect details content text ref title this could be a mistake while aggregating the dataset poynter has multiple duplicates of news articles which filtered the fact-checked news article using ref url but there might be some other duplicates some topics of fake news are repurposed and covered by different fact-checking websites we have included all of them in our data because content-wise each fact-checking website uses different claim to verify the title for instance article title vladimir putin let lions loose in the streets of russia to keep them indoors during covid-19 and russia released 500 lions to ensure people would stay inside houses both talk about the same topic but fact-checked by factcrescendo and boom factcheck in this work we presented a corpus of fact-checked articles on covid-19 we collected news articles on covid-19 from two data sources poynter and snopes and crawled the textual contents from the sources an explanatory analysis is performed on the dataset to overview of data we also discovered fact check articles which include linkages to social media we manually annotated the fact-checked articles into 11 different categories finally we built a machine learningbased classifier to detect misinformation about covid-19 our classifier performed with an f1-score of 076 which helped in the initial screening of the propagation of the misinformation at the time of the pandemic the practical application of the classifier is to test the news article as false or true before doing the manual analysis the limitation of this work is that due to the lack of polyglots we provided the human annotated categories only for 3 languages one of the possible extension of this work could be analysis of the propagation of fake news across different social media platforms for instance a case study with twitter data is presented in shahi dirkson and majchrzak 2020  to optimise the search result of fake news on the search engine publisher can also embed the class or category of fact checked news using schema markup shahi nandini and kumari 2019  another possibility to broaden the work is to build a knowledge graph for fact check news so that the machine can easily process the data to answer user queries we have collected data from 92 distinct fact-checking website and they have published the data in 86 different classes we manually analyse and merge them into another class which has a similar meaning sometimes there is the only difference of a capital letter for instance false and false finally we concluded all 86 classes into 23 unique class the description of each class and its sourcefact checking website is mentioned in table 6 for manual annotation of categories of fact-check articles we have used a guideline which contains the definition of categories with an example a complete description of 11 categories is shown in table 7 wwwsnopescom the primary elements of the rated statements are demonstrably true however there are minor errors missing information or statements that need further clarification wwwsnopescom the rated statements are partially correct but leave out important details includes major errors or takes aspects out of context wwwsnopescom the rated statements contain both significant true and significant false elements such as exaggerations or false details the available evidence behind the rated statements may also be evenly weighted in support of and against the claim wwwsnopescom the rated statements are mostly false or not backed by evidence but there is more than one element of truth wwwanimalpolitico com mostly false the primary elements of the rated statements are demonstrably false however there are minor details that are accurate wwwpolitifactcom the rated statements are demonstrably false wwwhealthfeedback org four pinocchios or pants on fire the rated statements are demonstrably false and make a ridiculous claim major exaggeration or make fear-mongering statements with the intent to provoke a panic reaction wwwpolitifactcom the rated statements seem to be in line with available evidence but are used to reach erroneous conclusions are misattributed or there is no consensus on what is the correct interpretation of the evidence wwwcolombiacheck com the rated statements are backed by evidence however provided without necessary details or critical background knowledge and thus leaves the reader with a false understanding of reality wwwhealthfeedback org unsupported or unproven the rated statements are not backed by reliable evidence and can neither be proven right nor wrong falsification of the rated statement would require difficult or impossible methods and the claim is unverifiable wwwdubawaorg the rated statements contain claims that are beyond misleading or are based on methods that can be easily manipulated or framed in a manipulative way wwwfactcheckkz the rated statements appear to be accurate however they were taken out of context put into the wrong context framed with misleading information or used to create inaccurate connections wwwleadstoriescom the rated statements are based on evidence that is not scientifically acceptable nor reliable wwwellinikahoaxesgr the rated statements were published as a headline that does not match the articles content nor scientific evidence and was formulated with excessive markup in order to create a usually unjustified overly emotional response that creates the most trafficattention for the statements creator wwwraskrinkavanjeba the rated statements were made based on evidence that used to be accepted but has now been proven inaccurate or irrelevant wwwsnopesorg the rated statements were created as or based on content that by the creator and a wider audience was labelled as satire whether this label is inaccurate or not continued on next page the rated statements contain or are based on events that are so general or lack detail to an extent where those events may have happened to someone somewhere somewhen and are therefore essentially neither provable nor unprovable wwwsnopescom the rated statements were made based on some evidence that is usually blown out of proportion taken out of context and used to make wrong connections mixed with ridiculous and unsupported claims and can mostly be proven wrong the creator of the statement is not an expert but may give the impression to be an expert or have some form of authority in order to manipulate recipients to believe the claims wwwlemondefr the rated content is not a statement but an explanation of a fact or statement wwwagenciaocotecom this rating does not evaluate the truth of a statement instead indicated outlets or pages that are verified scams or describe verified scams wwwsnopescom the rated media content has been proven realnonmanipulatednon-edited but have been used misleadingly or is accompanied by false explanatory material describing the contents origin context andor meaning wwwsnopesorg the article discusses news covering theories statements research and facts regarding the origin and the spread of covid-19 among individuals local communities or the global population was covid-19 found in packages of toilet paper the article discusses news covering theories statements research and facts regarding possible prevention tactics or treatments for covid-19 including news regarding a vaccine the article discusses news covering the response of international governmental and administrative institutions to covid-19 does video show guns violence in the aftermath of the coronavirus outbreak in china conspiracy theories the article discusses news covering popular conspiracy theories regarding eg the existence origin spread and effects of covid-19 or individual or governmental involvement in its creation and spread does george soros own a lab that developed covid-19 the article discusses news covering individuals or researchers prophecies and predictions regarding aspects eg spread of covid-19 did nostradamus predict the covid-19 pandemic the article discusses news covering the impact of the covid-19 pandemic on businesses and industries or their response to the global pandemic coupons during the covid-19 pandemic humanitarian response the article discusses news covering the response of humanitarian organisations ngos or individuals with the aim of humanitarian aid to the covid-19 pandemic while everybody is busy with coronavirus authorities in grlitz in germany secretly brought asylum seekers into the city other diseases the article discusses news covering other diseases with reference to covid-19 in regard to similarities or development and spread at the same time as the covid-19 pandemic in china there is now also an outbreak of the hantavirus with a first recorded death the article discusses statements of viral social media posts eg viral memes or rapidly shared posts regarding covid-19 central park hospital tents housed thousands of abused children released from underground captivity the article discusses fear-mongering statements whose purpose appears to provoke of discriminatory reactions eg racism irreligion homophobia or panic viral publication says a video shows a gay party in italy few weeks before covid-19 media coverage the article which discuss how wellpoorly media is reporting on the coronavirus situation we did a qa on the facts about the coronavirus  